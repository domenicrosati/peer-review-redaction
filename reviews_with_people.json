[
  {
    "people": [
      "Doersch",
      "Dosovitsky"
    ],
    "review": "The paper proposes a formulation for unsupervised learning of ConvNets based on the distance between patches sampled from the same and different images. The novelty of the method is rather limited as it's similar to [Doersch et al. 2015] and [Dosovitsky et al. 2015]. The evaluation is only performed on the small datasets, which limits the potential impact of the contribution."
  },
  {
    "people": [
      "Dosovitskiy",
      "Dosovitskiy",
      "Dosovitskiy",
      "Dosovitskiy",
      "Zhao"
    ],
    "review": "This paper proposes an unsupervised training objective based on patch contrasting for visual representation learning using deep neural networks. In particular, the feature representations of the patches from the same image are encouraged to be closer than the those from different images. The distance ratios of positive training pairs are optimized. The proposed method are empirically shown to be effective as an initialization method for supervised training. \n\nStrengths:\n\n- The training objective is reasonable. In particular, high-level features show translation invariance. \n\n- The proposed methods are effective for initializing neural networks for supervised training on several datasets. \n\n\nWeaknesses:\n\n- The methods are technically similar to the \u201cexemplar network\u201d (Dosovitskiy 2015). Cropping patches from a single image can be taken as a type of data augmentation, which is comparable to the data augmentation of positive sample (the exemplar) in (Dosovitskiy 2015). \n\n- The paper is experimentally misleading.\nThe results reported in this paper are based on fine-tuning the whole network with supervision. However, in Table 2, the results of exemplar convnets (Dosovitskiy 2015) is from unsupervised feature learning (the network is not finetuned with labeled samples, and only a classifier is trained upon the features). Therefore, the comparison is not fair. I suspect that exemplar convnets (Dosovitskiy 2015) would achieve similar improvements from fine-tuning; so, without such comparisons (head-to-head comparison with and without fine-tuning based on the same architecture except for the loss), the experimental results are not fully convincing. \n\nRegarding the comparison to \u201cWhat-where\u201d autoencoder (Zhao et al, 2015), it will be interesting to compare against it in large-scale settings, as shown by Zhang et al, ICML 2016 (Augmenting Supervised Neural Networks with Unsupervised Objectives for Large-Scale Image Classification). Training an AlexNet is not very time-consuming with latest (e.g., TITAN-X level) GPUs. \n\nThe proposed method seems useful only for natural images where different patches from the same image can be similar to each other. \n"
  },
  {
    "people": [
      "Doersch",
      "Doersch"
    ],
    "review": "The proposed self supervised loss is formulated using a Siamese architecture and encourages patches from the same image to lie closer in feature space than a contrasting patch taken from a different, random image. The loss is very similar in spirit to that of Doersch et al. ICCV 2015 and Isola et al. ICLR 2016 workshop. It seems that the proposed loss is actually a simplified version of Doersch et al. ICCV 2015 in that it does not make use of the spatial offset, a freely available self supervised signal in natural images. Intuitively, it seems that the self-supervised problem posed by this method is strictly simpler, and therefore less powerful, than that of the aforementioned work. I would like to see more discussion on the comparison of these two approaches. Nevertheless the proposed method seems to be effective in achieving good empirical results using this simple loss. Though more implementation details should be provided, such as the effect of patch size, overlap between sampled patches, and any other important measures taken to avoid trivial solutions. "
  },
  {
    "people": [
      "Doersch",
      "Dosovitsky"
    ],
    "review": "The paper proposes a formulation for unsupervised learning of ConvNets based on the distance between patches sampled from the same and different images. The novelty of the method is rather limited as it's similar to [Doersch et al. 2015] and [Dosovitsky et al. 2015]. The evaluation is only performed on the small datasets, which limits the potential impact of the contribution."
  },
  {
    "people": [
      "Dosovitskiy",
      "Dosovitskiy",
      "Dosovitskiy",
      "Dosovitskiy",
      "Zhao"
    ],
    "review": "This paper proposes an unsupervised training objective based on patch contrasting for visual representation learning using deep neural networks. In particular, the feature representations of the patches from the same image are encouraged to be closer than the those from different images. The distance ratios of positive training pairs are optimized. The proposed method are empirically shown to be effective as an initialization method for supervised training. \n\nStrengths:\n\n- The training objective is reasonable. In particular, high-level features show translation invariance. \n\n- The proposed methods are effective for initializing neural networks for supervised training on several datasets. \n\n\nWeaknesses:\n\n- The methods are technically similar to the \u201cexemplar network\u201d (Dosovitskiy 2015). Cropping patches from a single image can be taken as a type of data augmentation, which is comparable to the data augmentation of positive sample (the exemplar) in (Dosovitskiy 2015). \n\n- The paper is experimentally misleading.\nThe results reported in this paper are based on fine-tuning the whole network with supervision. However, in Table 2, the results of exemplar convnets (Dosovitskiy 2015) is from unsupervised feature learning (the network is not finetuned with labeled samples, and only a classifier is trained upon the features). Therefore, the comparison is not fair. I suspect that exemplar convnets (Dosovitskiy 2015) would achieve similar improvements from fine-tuning; so, without such comparisons (head-to-head comparison with and without fine-tuning based on the same architecture except for the loss), the experimental results are not fully convincing. \n\nRegarding the comparison to \u201cWhat-where\u201d autoencoder (Zhao et al, 2015), it will be interesting to compare against it in large-scale settings, as shown by Zhang et al, ICML 2016 (Augmenting Supervised Neural Networks with Unsupervised Objectives for Large-Scale Image Classification). Training an AlexNet is not very time-consuming with latest (e.g., TITAN-X level) GPUs. \n\nThe proposed method seems useful only for natural images where different patches from the same image can be similar to each other. \n"
  },
  {
    "people": [
      "Doersch",
      "Doersch"
    ],
    "review": "The proposed self supervised loss is formulated using a Siamese architecture and encourages patches from the same image to lie closer in feature space than a contrasting patch taken from a different, random image. The loss is very similar in spirit to that of Doersch et al. ICCV 2015 and Isola et al. ICLR 2016 workshop. It seems that the proposed loss is actually a simplified version of Doersch et al. ICCV 2015 in that it does not make use of the spatial offset, a freely available self supervised signal in natural images. Intuitively, it seems that the self-supervised problem posed by this method is strictly simpler, and therefore less powerful, than that of the aforementioned work. I would like to see more discussion on the comparison of these two approaches. Nevertheless the proposed method seems to be effective in achieving good empirical results using this simple loss. Though more implementation details should be provided, such as the effect of patch size, overlap between sampled patches, and any other important measures taken to avoid trivial solutions. "
  },
  {
    "people": [
      "Kenton"
    ],
    "review": "The paper proposed an end-to-end machine learning model called dynamic reader for the machine reading comprehension task. Compared to earlier systems, the proposed model is able to extract and rank a set of answer candidates from a given document.\n\nThere are many recent models focusing on building good question answering systems by extracting phrases from a given article. It seems that there are two different aspects that are unique in this work:\n\n1.\tThe use of convolution model, and\n2.\tDynamic chunking\n\nConvolution network is often only used for modeling character-based word embeddings so I am curious about its effectiveness on representing phrases. Therefore, I wish there could be more analysis on how effective it is, as the authors do not compare the convolution framework to other alternative approaches such as LSTM. The comparisons are important, as the authors uses uni-gram, bi-gram and tri-gram information in the convolution network, and it is not clear to me that if tri-gram information is still needed for LSTM models.\n\nThe dynamic chunking is a good idea, and a very similar idea is proposed in some of the recent papers such as [Kenton et al, 16], which also targets at the same dataset. However, I would like to see more analysis on the dynamic chunking. Why this approach is a good approach for representing answer chunks? Given the representation of the chunk is constructed by the first and the end word representations generated by a convolution network, I am not sure about the ability of this representation to capture the long answer phrases.\n\nThe authors do not use character base embedding but use some of the previous trained NLP models. It would be interesting if the authors could show what are the advantages and disadvantages of using linguistic features compared to character embeddings.\n\nIn short, there are several good ideas proposed in the paper, but the lack of proper analysis make it difficult to judge how important the proposed techniques are.\n"
  },
  {
    "people": [
      "Kenton"
    ],
    "review": "The paper proposed an end-to-end machine learning model called dynamic reader for the machine reading comprehension task. Compared to earlier systems, the proposed model is able to extract and rank a set of answer candidates from a given document.\n\nThere are many recent models focusing on building good question answering systems by extracting phrases from a given article. It seems that there are two different aspects that are unique in this work:\n\n1.\tThe use of convolution model, and\n2.\tDynamic chunking\n\nConvolution network is often only used for modeling character-based word embeddings so I am curious about its effectiveness on representing phrases. Therefore, I wish there could be more analysis on how effective it is, as the authors do not compare the convolution framework to other alternative approaches such as LSTM. The comparisons are important, as the authors uses uni-gram, bi-gram and tri-gram information in the convolution network, and it is not clear to me that if tri-gram information is still needed for LSTM models.\n\nThe dynamic chunking is a good idea, and a very similar idea is proposed in some of the recent papers such as [Kenton et al, 16], which also targets at the same dataset. However, I would like to see more analysis on the dynamic chunking. Why this approach is a good approach for representing answer chunks? Given the representation of the chunk is constructed by the first and the end word representations generated by a convolution network, I am not sure about the ability of this representation to capture the long answer phrases.\n\nThe authors do not use character base embedding but use some of the previous trained NLP models. It would be interesting if the authors could show what are the advantages and disadvantages of using linguistic features compared to character embeddings.\n\nIn short, there are several good ideas proposed in the paper, but the lack of proper analysis make it difficult to judge how important the proposed techniques are.\n"
  },
  {
    "people": [
      "Mark Ring\u2019s"
    ],
    "review": "This is a solid paper that applies A3C to Doom, enhancing it with a collection of tricks so as to win one of the VizDoom competitions. I think it is fair to expect the competition aspect to overshadow the more scientific approach of justifying every design decision in isolation, but in fact the authors do a decent job at the latter.\n\nTwo of my concerns have remained unanswered (see AnonReviewer2, below). \n\nIn addition, the citation list is rather thin, for example reward shaping has a rich literature, as do incrementally more difficult task setups, dating back at least to Mark Ring\u2019s work in the 1990s. There has also been a lot of complementary work on other FPS games. I\u2019m not asking that the authors do any direct comparisons, but to give the reader a sense of context in which to place this."
  },
  {
    "people": [
      "Mark Ring\u2019s"
    ],
    "review": "This is a solid paper that applies A3C to Doom, enhancing it with a collection of tricks so as to win one of the VizDoom competitions. I think it is fair to expect the competition aspect to overshadow the more scientific approach of justifying every design decision in isolation, but in fact the authors do a decent job at the latter.\n\nTwo of my concerns have remained unanswered (see AnonReviewer2, below). \n\nIn addition, the citation list is rather thin, for example reward shaping has a rich literature, as do incrementally more difficult task setups, dating back at least to Mark Ring\u2019s work in the 1990s. There has also been a lot of complementary work on other FPS games. I\u2019m not asking that the authors do any direct comparisons, but to give the reader a sense of context in which to place this."
  },
  {
    "people": [
      "Wang",
      "Wieting"
    ],
    "review": "This is a good paper with an interesting probabilistic motivation for weighted bag of words models.\nThe (hopefully soon) added comparison to Wang and Manning will make it stronger. \nThough it is sad that for sufficiently large datasets, NB-SVM still works better.\n\nIn the second to last paragraph of the introduction you describe a problem of large cooccurrence counts which was already fixed by the Glove embeddings with their weighting function f.\n\nMinor comments:\n\n\"The capturing the similarities\" -- typo in line 2 of intro.\n\"Recently, (Wieting et al.,2016) learned\" -- use citet instead of parenthesized citation\n "
  },
  {
    "people": [
      "J. Mu",
      "P. Viswanath"
    ],
    "review": "We (J. Mu and P. Viswanath) thoroughly enjoyed the authors' previous work on linear algebraic structure of word senses (cf. "
  },
  {
    "people": [
      "Wang",
      "Wieting"
    ],
    "review": "This is a good paper with an interesting probabilistic motivation for weighted bag of words models.\nThe (hopefully soon) added comparison to Wang and Manning will make it stronger. \nThough it is sad that for sufficiently large datasets, NB-SVM still works better.\n\nIn the second to last paragraph of the introduction you describe a problem of large cooccurrence counts which was already fixed by the Glove embeddings with their weighting function f.\n\nMinor comments:\n\n\"The capturing the similarities\" -- typo in line 2 of intro.\n\"Recently, (Wieting et al.,2016) learned\" -- use citet instead of parenthesized citation\n "
  },
  {
    "people": [
      "J. Mu",
      "P. Viswanath"
    ],
    "review": "We (J. Mu and P. Viswanath) thoroughly enjoyed the authors' previous work on linear algebraic structure of word senses (cf. "
  },
  {
    "people": [
      "Yao",
      "Boulanger-Lewandowski"
    ],
    "review": "This paper proposed COCONET, which is a neural autoregressive model with convolution, to do music composition task. This paper also proposed to use blocked Gibbs sampling instead of the ancestral sampling of the original NADE model to generate better pieces of music. The experimental results showed that the NLL of COCONET is better than the other baselines and the human evaluation task by Amazon\u2019s Mechanical Turk illustrated that the model can generate compelling music.\n\nIn general, I think the paper is good. Using NADE based model with convolution operations on music generation tasks and using blocked Gibbs sampling contains some kind of novelty. However, the novelty of the paper is incremental, since the blocked Gibbs sampling for NADE model is already proposed by Yao et al., (2014) and the using NADE based model for music modeling has also been proposed by Boulanger-Lewandowski  et al., (2012)."
  },
  {
    "people": [
      "Yao"
    ],
    "review": "This paper applies an existing idea (Yao's block Gibbs sampling of NADE) to a music model. There is also prior art for applying NADE to music. The main novel and interesting result is that block Gibbs sampling (an approximation) actually improves performance, highlighting problems with NADE.\n \n This work is borderline for inclusion. The paper is mainly an application of existing ideas. The implications of the interesting results could perhaps have been explored further for a paper at a meeting on learning representations."
  },
  {
    "people": [
      "Yao"
    ],
    "review": "Thank you all for your reviews!  \n\nWe share your concern regarding quantitative comparison to other works, and regarding the generality of our method.  We have looked into the datasets used in prior works, but found that their preprocessing severely degraded the musical structure. For example, the temporal granularity used in Boulanger-Lewandowski et al. is too coarse: in several pieces, discarding all notes that do not have their onsets on the eighth-note grid results in sparse notes with long stretches of silence between them. The pieces become unrecognizable, and (worse) non-musical.  Downsampling or blurring does not work in symbolic music like it does in images: music is more similar to language, and using a coarse grid is analogous to removing words from a sentence and asking a model to learn this new distribution of broken sentences. This turns it into a different task, and makes qualitative judgement of samples not very meaningful.\n\nWe also considered applying our method to image data as suggested. Unlike (symbolic) music which is discrete and intricately structured, the domain of images is smooth and forgiving of small errors. It is plausible that the NADE sampling approach generates fine images (indeed, that's what Yao et al. found), for reasons that don't carry over to our domain of interest, which is music.\n\nThe development of a larger-scale, more diverse and higher-quality MIDI music dataset is a major component of our ongoing project.  We nonetheless believe that the advances we show are of sufficient interest to justify publication at this time."
  },
  {
    "people": [
      "Yao",
      "Uria",
      "Yao",
      "Yao",
      "Yao",
      "Bach"
    ],
    "review": "The paper tackles the task of music generation. They use an orderless NADE model for the task of \"fill in the notes\". Given a roll of T timesteps of pitches, they randomly mask out some pitches, and the model is trained to predict the missing notes. This follows how the orderless NADE model can be trained. During sampling, one normally follows an ancestral sampling procedure. For this, an ordering is defined over outputs, and one runs the model on the current input, samples one of the outputs according to the order, adds this output to the next input, and continues this procedure until all outputs have been sampled. The key point of the paper is that this is a bad sampling strategy. Instead, they suggest the strategy of Yao et al. 2014, which uses a blocked Gibbs sampling approach. The blocked Gibbs strategy instead masks N inputs randomly and independently, samples them, and repeats this procedure. The point of this strategy is the make sure the sampling chain mixes well, which will happen for large N. However, since the samples are independent, having a large N gives incoherent samples. Thus, the authors follow an annealed schedule for N, making it smaller over time, which will eventually reduce to ancestral sampling (giving global structure to the sample). They conduct a variety of experiments involving both normal metrics and human evaluations, and find that this blocked Gibbs sampling outperforms other sampling procedures.\n\nThis is a well written paper - great job.\n\nMy main problem with the paper is that having read Uria and Yao, I don't know how much I have learned from this work in the context of this being an ICLR submission. If this was submitted to some computational music / art conference, this paper would be a clear accept. However, for ICLR, I don't see enough novelty compared with previous works this builds upon. Orderless NADE is an established model. The blocked Gibbs sampling and annealing scheme are basically the exact same one used in Yao. Thus, the main novelty of this paper is its application to the music domain, and finding that Yao's method works better for sampling music. This is a good contribution, but more tailored to those working in the music domain. If the authors found that these results also hold for other domains like images (e.g. on CIFAR / tiny Imagenet) and text (e.g. document generation), then I would change my mind and accept this paper for ICLR. Even just trying musical domains other than Bach chorales would be useful. However, as it stands, the experiments are not convincing enough."
  },
  {
    "people": [
      "Bach"
    ],
    "review": "The paper presents a way to model the distribution of four-part Bach chorales using Convolutional Neural Networks. Furthermore it addresses the task of artificial music generation by sampling from the model using blocked Gibbs sampling and shows\n\nThe CNN model for the distribution seems very appropriate for the data at hand. Also the analysis of the proposed sampling schemes with the analogy between Gibbs sampling and human music composition are very interesting.\nI am not too sure about the evaluation though. Since the reported likelihoods are not directly comparable to previous work, I have difficulties judging the quality of the quantitative results.  For the human evaluation I would like to see the data for the direct comparisons between the models. E.g. How did NADE vs. Bach perform. Also I find the question: \u2018what piece of music do you prefer\u2019 a stronger test than the question \u2018what piece is more musical to you\u2019 because I don\u2019t really know what \u2018musical\u2019 means to the AMT workers.\n\nFinally, while I think the Bach Chorales are interesting musical pieces that deserve to be subject of the analysis but I find it hard to judge how well this modelling approach will transfer to other types of music which might have a very different data distribution.\n\nNevertheless, in conclusion, I believe this is an exciting model for an interesting task that produces non-trivial musical data.\n"
  },
  {
    "people": [
      "Yao",
      "Boulanger"
    ],
    "review": "This paper proposed COCONET, which is a neural autoregressive model with convolution, to do music composition task. This paper also proposed to use blocked Gibbs sampling instead of the ancestral sampling of the original NADE model to generate better pieces of music. The experimental results showed that the NLL of COCONET is better than the other baselines and the human evaluation task by Amazon\u2019s Mechanical Turk illustrated that the model can generate compelling music.\n\nIn general, I think the paper is good. Using NADE based model with convolution operations on music generation tasks and using blocked Gibbs sampling contains some kind of novelty. However, the novelty of the paper is incremental, since the blocked Gibbs sampling for NADE model is already proposed by Yao et al., (2014) and the using NADE based model for music modeling has also been proposed by Boulanger-Lewandowski  et al., (2012).\n\n"
  },
  {
    "people": [
      "Yao",
      "Boulanger-Lewandowski"
    ],
    "review": "This paper proposed COCONET, which is a neural autoregressive model with convolution, to do music composition task. This paper also proposed to use blocked Gibbs sampling instead of the ancestral sampling of the original NADE model to generate better pieces of music. The experimental results showed that the NLL of COCONET is better than the other baselines and the human evaluation task by Amazon\u2019s Mechanical Turk illustrated that the model can generate compelling music.\n\nIn general, I think the paper is good. Using NADE based model with convolution operations on music generation tasks and using blocked Gibbs sampling contains some kind of novelty. However, the novelty of the paper is incremental, since the blocked Gibbs sampling for NADE model is already proposed by Yao et al., (2014) and the using NADE based model for music modeling has also been proposed by Boulanger-Lewandowski  et al., (2012)."
  },
  {
    "people": [
      "Yao"
    ],
    "review": "This paper applies an existing idea (Yao's block Gibbs sampling of NADE) to a music model. There is also prior art for applying NADE to music. The main novel and interesting result is that block Gibbs sampling (an approximation) actually improves performance, highlighting problems with NADE.\n \n This work is borderline for inclusion. The paper is mainly an application of existing ideas. The implications of the interesting results could perhaps have been explored further for a paper at a meeting on learning representations."
  },
  {
    "people": [
      "Yao"
    ],
    "review": "Thank you all for your reviews!  \n\nWe share your concern regarding quantitative comparison to other works, and regarding the generality of our method.  We have looked into the datasets used in prior works, but found that their preprocessing severely degraded the musical structure. For example, the temporal granularity used in Boulanger-Lewandowski et al. is too coarse: in several pieces, discarding all notes that do not have their onsets on the eighth-note grid results in sparse notes with long stretches of silence between them. The pieces become unrecognizable, and (worse) non-musical.  Downsampling or blurring does not work in symbolic music like it does in images: music is more similar to language, and using a coarse grid is analogous to removing words from a sentence and asking a model to learn this new distribution of broken sentences. This turns it into a different task, and makes qualitative judgement of samples not very meaningful.\n\nWe also considered applying our method to image data as suggested. Unlike (symbolic) music which is discrete and intricately structured, the domain of images is smooth and forgiving of small errors. It is plausible that the NADE sampling approach generates fine images (indeed, that's what Yao et al. found), for reasons that don't carry over to our domain of interest, which is music.\n\nThe development of a larger-scale, more diverse and higher-quality MIDI music dataset is a major component of our ongoing project.  We nonetheless believe that the advances we show are of sufficient interest to justify publication at this time."
  },
  {
    "people": [
      "Yao",
      "Uria",
      "Yao",
      "Yao",
      "Yao",
      "Bach"
    ],
    "review": "The paper tackles the task of music generation. They use an orderless NADE model for the task of \"fill in the notes\". Given a roll of T timesteps of pitches, they randomly mask out some pitches, and the model is trained to predict the missing notes. This follows how the orderless NADE model can be trained. During sampling, one normally follows an ancestral sampling procedure. For this, an ordering is defined over outputs, and one runs the model on the current input, samples one of the outputs according to the order, adds this output to the next input, and continues this procedure until all outputs have been sampled. The key point of the paper is that this is a bad sampling strategy. Instead, they suggest the strategy of Yao et al. 2014, which uses a blocked Gibbs sampling approach. The blocked Gibbs strategy instead masks N inputs randomly and independently, samples them, and repeats this procedure. The point of this strategy is the make sure the sampling chain mixes well, which will happen for large N. However, since the samples are independent, having a large N gives incoherent samples. Thus, the authors follow an annealed schedule for N, making it smaller over time, which will eventually reduce to ancestral sampling (giving global structure to the sample). They conduct a variety of experiments involving both normal metrics and human evaluations, and find that this blocked Gibbs sampling outperforms other sampling procedures.\n\nThis is a well written paper - great job.\n\nMy main problem with the paper is that having read Uria and Yao, I don't know how much I have learned from this work in the context of this being an ICLR submission. If this was submitted to some computational music / art conference, this paper would be a clear accept. However, for ICLR, I don't see enough novelty compared with previous works this builds upon. Orderless NADE is an established model. The blocked Gibbs sampling and annealing scheme are basically the exact same one used in Yao. Thus, the main novelty of this paper is its application to the music domain, and finding that Yao's method works better for sampling music. This is a good contribution, but more tailored to those working in the music domain. If the authors found that these results also hold for other domains like images (e.g. on CIFAR / tiny Imagenet) and text (e.g. document generation), then I would change my mind and accept this paper for ICLR. Even just trying musical domains other than Bach chorales would be useful. However, as it stands, the experiments are not convincing enough."
  },
  {
    "people": [
      "Bach"
    ],
    "review": "The paper presents a way to model the distribution of four-part Bach chorales using Convolutional Neural Networks. Furthermore it addresses the task of artificial music generation by sampling from the model using blocked Gibbs sampling and shows\n\nThe CNN model for the distribution seems very appropriate for the data at hand. Also the analysis of the proposed sampling schemes with the analogy between Gibbs sampling and human music composition are very interesting.\nI am not too sure about the evaluation though. Since the reported likelihoods are not directly comparable to previous work, I have difficulties judging the quality of the quantitative results.  For the human evaluation I would like to see the data for the direct comparisons between the models. E.g. How did NADE vs. Bach perform. Also I find the question: \u2018what piece of music do you prefer\u2019 a stronger test than the question \u2018what piece is more musical to you\u2019 because I don\u2019t really know what \u2018musical\u2019 means to the AMT workers.\n\nFinally, while I think the Bach Chorales are interesting musical pieces that deserve to be subject of the analysis but I find it hard to judge how well this modelling approach will transfer to other types of music which might have a very different data distribution.\n\nNevertheless, in conclusion, I believe this is an exciting model for an interesting task that produces non-trivial musical data.\n"
  },
  {
    "people": [
      "Yao",
      "Boulanger"
    ],
    "review": "This paper proposed COCONET, which is a neural autoregressive model with convolution, to do music composition task. This paper also proposed to use blocked Gibbs sampling instead of the ancestral sampling of the original NADE model to generate better pieces of music. The experimental results showed that the NLL of COCONET is better than the other baselines and the human evaluation task by Amazon\u2019s Mechanical Turk illustrated that the model can generate compelling music.\n\nIn general, I think the paper is good. Using NADE based model with convolution operations on music generation tasks and using blocked Gibbs sampling contains some kind of novelty. However, the novelty of the paper is incremental, since the blocked Gibbs sampling for NADE model is already proposed by Yao et al., (2014) and the using NADE based model for music modeling has also been proposed by Boulanger-Lewandowski  et al., (2012).\n\n"
  },
  {
    "people": [
      "Debar",
      "Herve",
      "Monique Becker",
      "Didier Siboni",
      "Creech",
      "Gideon",
      "Jiankun Hu",
      "Staudemeyer",
      "Ralf C."
    ],
    "review": "The authors propose using an LSTM on a sequence of system calls to perform network intrusion detection (NIDS). The idea of using neural networks (in general) for NIDS is old [1]. The idea of using some sort of NN on top of a sequence of system calls for NIDS is published [2]. The idea of using LSTMs for NIDS is published [2]. The paper in [2] operates on counts of N-grams of system calls, rather than on the raw sequence, but that pre-processing does not seem \"heavy\" to me.\n\nOverall, the proposed system works as well as other proposed NIDS system, and the paper checks portability (which is good).\n\nBut, on the con side, I don't see this paper as adding a lot to the state-of-the-art in NIDS. Nor does is the paper well-matched to ICLR. I didn't learn a lot about representations from this paper: many people have thrown LSTM at sequence problems.\n\nTherefore, I think it's below threshold for ICLR. The authors may wish to submit to a security conference.\n\nReferences:\n1. Debar, Herve, Monique Becker, and Didier Siboni. \"A neural network component for an intrusion detection system.\" Research in Security and Privacy, 1992. Proceedings., 1992 IEEE Computer Society Symposium on. IEEE, 1992.\n2. Creech, Gideon, and Jiankun Hu. \"A semantic approach to host-based intrusion detection systems using contiguousand discontiguous system call patterns.\" IEEE Transactions on Computers 63.4 (2014): 807-819.\n3. Staudemeyer, Ralf C. \"Applying long short-term memory recurrent neural networks to intrusion detection.\" South African Computer Journal 56.1 (2015)."
  },
  {
    "people": [
      "Debar",
      "Herve",
      "Monique Becker",
      "Didier Siboni",
      "Creech",
      "Gideon",
      "Jiankun Hu",
      "Staudemeyer",
      "Ralf C."
    ],
    "review": "The authors propose using an LSTM on a sequence of system calls to perform network intrusion detection (NIDS). The idea of using neural networks (in general) for NIDS is old [1]. The idea of using some sort of NN on top of a sequence of system calls for NIDS is published [2]. The idea of using LSTMs for NIDS is published [2]. The paper in [2] operates on counts of N-grams of system calls, rather than on the raw sequence, but that pre-processing does not seem \"heavy\" to me.\n\nOverall, the proposed system works as well as other proposed NIDS system, and the paper checks portability (which is good).\n\nBut, on the con side, I don't see this paper as adding a lot to the state-of-the-art in NIDS. Nor does is the paper well-matched to ICLR. I didn't learn a lot about representations from this paper: many people have thrown LSTM at sequence problems.\n\nTherefore, I think it's below threshold for ICLR. The authors may wish to submit to a security conference.\n\nReferences:\n1. Debar, Herve, Monique Becker, and Didier Siboni. \"A neural network component for an intrusion detection system.\" Research in Security and Privacy, 1992. Proceedings., 1992 IEEE Computer Society Symposium on. IEEE, 1992.\n2. Creech, Gideon, and Jiankun Hu. \"A semantic approach to host-based intrusion detection systems using contiguousand discontiguous system call patterns.\" IEEE Transactions on Computers 63.4 (2014): 807-819.\n3. Staudemeyer, Ralf C. \"Applying long short-term memory recurrent neural networks to intrusion detection.\" South African Computer Journal 56.1 (2015)."
  },
  {
    "people": [
      "Kompella",
      "Srivastava"
    ],
    "review": "This paper explores the topic of intrinsic motivation in the context of deep RL. It proposes a couple of variants derived from an auxiliary model-learning process (prediction error, surprise and learning progress), and shows that those can help exploration on a number of continuous control tasks (and the Atari game \u201cventure\u201d, maybe).\n\nNovelty: none of the proposed types of intrinsic motivation are novel, and it\u2019s arguable whether the application to deep RL is novel (see e.g. Kompella et al 2012).\n\nPotential: the idea of seeking out states where a transition model is uncertain is sensible, but also limited -- I would encourage the authors to also discuss the limitations. For example in a game like Go the transition model is trivially learned, so this approach would revert to random exploration. So other forms of learning progress or surprise derived from the agent\u2019s competence instead might be more promising in the long run? See also Srivastava et al 2012 for further thoughts.\n\nComputation time: I find the paper\u2019s claimed superiority over VIME to be overblown: the gain seems to stem almost exclusively from a faster initialization, but have very similar per-step cost? So given that VIME is also performing very competitively, what arguments can you advance for your own method(s)?"
  },
  {
    "people": [
      "Schmidhuber",
      "Bellemare"
    ],
    "review": "The authors present a novel approach to surprise-based intrinsic motivation in deep reinforcement learning. The authors clearly explain the difference from other recent approaches to intrinsic motivation and back up their method with results from a broad class of discrete and continuous action domains. They present two tractable approximations to their framework - one which ignores the stochasticity of the true environmental dynamics, and one which approximates the rate of information gain (somewhat similar to Schmidhuber's formal theory of creativity, fun and intrinsic motivation). The results of this exploration bonus when added to TRPO are generally better than standard TRPO. However, I would have appreciated a more thorough comparison against other recent work on intrinsic motivation. For instance, Bellemare et al 2016 recently achieved significant performance gains on challenging Atari games like Montezuma's Revenge by combining DQN with an exploration bonus, however Montezuma's Revenge is not presented as an experiment here. Such comparisons would significantly improve the strength of the paper."
  },
  {
    "people": [
      "Kompella",
      "Srivastava"
    ],
    "review": "This paper explores the topic of intrinsic motivation in the context of deep RL. It proposes a couple of variants derived from an auxiliary model-learning process (prediction error, surprise and learning progress), and shows that those can help exploration on a number of continuous control tasks (and the Atari game \u201cventure\u201d, maybe).\n\nNovelty: none of the proposed types of intrinsic motivation are novel, and it\u2019s arguable whether the application to deep RL is novel (see e.g. Kompella et al 2012).\n\nPotential: the idea of seeking out states where a transition model is uncertain is sensible, but also limited -- I would encourage the authors to also discuss the limitations. For example in a game like Go the transition model is trivially learned, so this approach would revert to random exploration. So other forms of learning progress or surprise derived from the agent\u2019s competence instead might be more promising in the long run? See also Srivastava et al 2012 for further thoughts.\n\nComputation time: I find the paper\u2019s claimed superiority over VIME to be overblown: the gain seems to stem almost exclusively from a faster initialization, but have very similar per-step cost? So given that VIME is also performing very competitively, what arguments can you advance for your own method(s)?"
  },
  {
    "people": [
      "Kompella",
      "Srivastava"
    ],
    "review": "This paper explores the topic of intrinsic motivation in the context of deep RL. It proposes a couple of variants derived from an auxiliary model-learning process (prediction error, surprise and learning progress), and shows that those can help exploration on a number of continuous control tasks (and the Atari game \u201cventure\u201d, maybe).\n\nNovelty: none of the proposed types of intrinsic motivation are novel, and it\u2019s arguable whether the application to deep RL is novel (see e.g. Kompella et al 2012).\n\nPotential: the idea of seeking out states where a transition model is uncertain is sensible, but also limited -- I would encourage the authors to also discuss the limitations. For example in a game like Go the transition model is trivially learned, so this approach would revert to random exploration. So other forms of learning progress or surprise derived from the agent\u2019s competence instead might be more promising in the long run? See also Srivastava et al 2012 for further thoughts.\n\nComputation time: I find the paper\u2019s claimed superiority over VIME to be overblown: the gain seems to stem almost exclusively from a faster initialization, but have very similar per-step cost? So given that VIME is also performing very competitively, what arguments can you advance for your own method(s)?"
  },
  {
    "people": [
      "Schmidhuber",
      "Bellemare"
    ],
    "review": "The authors present a novel approach to surprise-based intrinsic motivation in deep reinforcement learning. The authors clearly explain the difference from other recent approaches to intrinsic motivation and back up their method with results from a broad class of discrete and continuous action domains. They present two tractable approximations to their framework - one which ignores the stochasticity of the true environmental dynamics, and one which approximates the rate of information gain (somewhat similar to Schmidhuber's formal theory of creativity, fun and intrinsic motivation). The results of this exploration bonus when added to TRPO are generally better than standard TRPO. However, I would have appreciated a more thorough comparison against other recent work on intrinsic motivation. For instance, Bellemare et al 2016 recently achieved significant performance gains on challenging Atari games like Montezuma's Revenge by combining DQN with an exploration bonus, however Montezuma's Revenge is not presented as an experiment here. Such comparisons would significantly improve the strength of the paper."
  },
  {
    "people": [
      "Kompella",
      "Srivastava"
    ],
    "review": "This paper explores the topic of intrinsic motivation in the context of deep RL. It proposes a couple of variants derived from an auxiliary model-learning process (prediction error, surprise and learning progress), and shows that those can help exploration on a number of continuous control tasks (and the Atari game \u201cventure\u201d, maybe).\n\nNovelty: none of the proposed types of intrinsic motivation are novel, and it\u2019s arguable whether the application to deep RL is novel (see e.g. Kompella et al 2012).\n\nPotential: the idea of seeking out states where a transition model is uncertain is sensible, but also limited -- I would encourage the authors to also discuss the limitations. For example in a game like Go the transition model is trivially learned, so this approach would revert to random exploration. So other forms of learning progress or surprise derived from the agent\u2019s competence instead might be more promising in the long run? See also Srivastava et al 2012 for further thoughts.\n\nComputation time: I find the paper\u2019s claimed superiority over VIME to be overblown: the gain seems to stem almost exclusively from a faster initialization, but have very similar per-step cost? So given that VIME is also performing very competitively, what arguments can you advance for your own method(s)?"
  },
  {
    "people": [
      "Li",
      "Bofang Li",
      "Tao Liu",
      "Xiaoyong Du",
      "Deyuan Zhang",
      "Zhe Zhao"
    ],
    "review": "This work reframes paragraph vectors from a generative point of view and in so doing, motivates the existing method of inferring paragraph vectors as well as applying a L2 regularizer on the paragraph embeddings. The work also motivates joint learning of a classifier on the paragraph vectors to perform text classification.\n\nThe paper has numerous citation issues both in formatting within the text and the formatting of the bibliography, e.g. on some occasions including first names, on others not. I suggest the authors use a software package like BibTex to have a more consistent bibliography. There seems to be little novelty in this work. \n\nThe authors claim that there is no proposed method for inferring unseen documents for paragraph vectors. This is untrue. In the original paragraph vector paper, the authors show that to get a new vector, the rest of the model parameters are held fixed and gradient descent is performed on the new paragraph vector. This means the original dataset is not needed when inferring a paragraph vector for new text. This work seems to be essentially doing the same thing when finding the MAP estimate for a new vector. Thus the only contribution from the generative paragraph vector framing is the regularization on the embedding matrix.\n\nThe supervised generative paragraph vector amounts to jointly training a linear classifier on the paragraph vectors, while inference for the paragraph vector is unchanged. For the n-gram based approach, the authors should cite Li et al., 2015.\n\nIn the experiments, table 1 and 2 are badly formatted with .0 being truncated. The authors also do not state the size of the paragraph vector. Finally the SGPV results are actually worse than that reported in the original paragraph vector paper where SST-1 got 48.7 and SST-2 got 86.3.\n\nBofang Li, Tao Liu, Xiaoyong Du, Deyuan Zhang, Zhe Zhao, Learning Document Embeddings by Predicting N-grams for Sentiment Classification of Long Movie Reviews, 2015."
  },
  {
    "people": [
      "Li",
      "Bofang Li",
      "Tao Liu",
      "Xiaoyong Du",
      "Deyuan Zhang",
      "Zhe Zhao"
    ],
    "review": "This work reframes paragraph vectors from a generative point of view and in so doing, motivates the existing method of inferring paragraph vectors as well as applying a L2 regularizer on the paragraph embeddings. The work also motivates joint learning of a classifier on the paragraph vectors to perform text classification.\n\nThe paper has numerous citation issues both in formatting within the text and the formatting of the bibliography, e.g. on some occasions including first names, on others not. I suggest the authors use a software package like BibTex to have a more consistent bibliography. There seems to be little novelty in this work. \n\nThe authors claim that there is no proposed method for inferring unseen documents for paragraph vectors. This is untrue. In the original paragraph vector paper, the authors show that to get a new vector, the rest of the model parameters are held fixed and gradient descent is performed on the new paragraph vector. This means the original dataset is not needed when inferring a paragraph vector for new text. This work seems to be essentially doing the same thing when finding the MAP estimate for a new vector. Thus the only contribution from the generative paragraph vector framing is the regularization on the embedding matrix.\n\nThe supervised generative paragraph vector amounts to jointly training a linear classifier on the paragraph vectors, while inference for the paragraph vector is unchanged. For the n-gram based approach, the authors should cite Li et al., 2015.\n\nIn the experiments, table 1 and 2 are badly formatted with .0 being truncated. The authors also do not state the size of the paragraph vector. Finally the SGPV results are actually worse than that reported in the original paragraph vector paper where SST-1 got 48.7 and SST-2 got 86.3.\n\nBofang Li, Tao Liu, Xiaoyong Du, Deyuan Zhang, Zhe Zhao, Learning Document Embeddings by Predicting N-grams for Sentiment Classification of Long Movie Reviews, 2015."
  },
  {
    "people": [
      "Li",
      "Bofang Li",
      "Tao Liu",
      "Xiaoyong Du",
      "Deyuan Zhang",
      "Zhe Zhao"
    ],
    "review": "This work reframes paragraph vectors from a generative point of view and in so doing, motivates the existing method of inferring paragraph vectors as well as applying a L2 regularizer on the paragraph embeddings. The work also motivates joint learning of a classifier on the paragraph vectors to perform text classification.\n\nThe paper has numerous citation issues both in formatting within the text and the formatting of the bibliography, e.g. on some occasions including first names, on others not. I suggest the authors use a software package like BibTex to have a more consistent bibliography. There seems to be little novelty in this work. \n\nThe authors claim that there is no proposed method for inferring unseen documents for paragraph vectors. This is untrue. In the original paragraph vector paper, the authors show that to get a new vector, the rest of the model parameters are held fixed and gradient descent is performed on the new paragraph vector. This means the original dataset is not needed when inferring a paragraph vector for new text. This work seems to be essentially doing the same thing when finding the MAP estimate for a new vector. Thus the only contribution from the generative paragraph vector framing is the regularization on the embedding matrix.\n\nThe supervised generative paragraph vector amounts to jointly training a linear classifier on the paragraph vectors, while inference for the paragraph vector is unchanged. For the n-gram based approach, the authors should cite Li et al., 2015.\n\nIn the experiments, table 1 and 2 are badly formatted with .0 being truncated. The authors also do not state the size of the paragraph vector. Finally the SGPV results are actually worse than that reported in the original paragraph vector paper where SST-1 got 48.7 and SST-2 got 86.3.\n\nBofang Li, Tao Liu, Xiaoyong Du, Deyuan Zhang, Zhe Zhao, Learning Document Embeddings by Predicting N-grams for Sentiment Classification of Long Movie Reviews, 2015."
  },
  {
    "people": [
      "Li",
      "Bofang Li",
      "Tao Liu",
      "Xiaoyong Du",
      "Deyuan Zhang",
      "Zhe Zhao"
    ],
    "review": "This work reframes paragraph vectors from a generative point of view and in so doing, motivates the existing method of inferring paragraph vectors as well as applying a L2 regularizer on the paragraph embeddings. The work also motivates joint learning of a classifier on the paragraph vectors to perform text classification.\n\nThe paper has numerous citation issues both in formatting within the text and the formatting of the bibliography, e.g. on some occasions including first names, on others not. I suggest the authors use a software package like BibTex to have a more consistent bibliography. There seems to be little novelty in this work. \n\nThe authors claim that there is no proposed method for inferring unseen documents for paragraph vectors. This is untrue. In the original paragraph vector paper, the authors show that to get a new vector, the rest of the model parameters are held fixed and gradient descent is performed on the new paragraph vector. This means the original dataset is not needed when inferring a paragraph vector for new text. This work seems to be essentially doing the same thing when finding the MAP estimate for a new vector. Thus the only contribution from the generative paragraph vector framing is the regularization on the embedding matrix.\n\nThe supervised generative paragraph vector amounts to jointly training a linear classifier on the paragraph vectors, while inference for the paragraph vector is unchanged. For the n-gram based approach, the authors should cite Li et al., 2015.\n\nIn the experiments, table 1 and 2 are badly formatted with .0 being truncated. The authors also do not state the size of the paragraph vector. Finally the SGPV results are actually worse than that reported in the original paragraph vector paper where SST-1 got 48.7 and SST-2 got 86.3.\n\nBofang Li, Tao Liu, Xiaoyong Du, Deyuan Zhang, Zhe Zhao, Learning Document Embeddings by Predicting N-grams for Sentiment Classification of Long Movie Reviews, 2015."
  },
  {
    "people": [
      "Gong"
    ],
    "review": "The paper proposes a series of tricks for compressing fast (linear) text classification models. The paper is clearly written, and the results are quite strong. The main compression is achieved via product quantization, a technique which has been explored in other applications within the neural network model compression literature. In addition to the Gong et al. work which was cited, it would be worth mentioning Quantized Convolutional Neural Networks for Mobile Devices (CVPR 2016, "
  },
  {
    "people": [
      "Gong"
    ],
    "review": "The paper proposes a series of tricks for compressing fast (linear) text classification models. The paper is clearly written, and the results are quite strong. The main compression is achieved via product quantization, a technique which has been explored in other applications within the neural network model compression literature. In addition to the Gong et al. work which was cited, it would be worth mentioning Quantized Convolutional Neural Networks for Mobile Devices (CVPR 2016, "
  },
  {
    "people": [
      "Bellman"
    ],
    "review": "The paper starts by pointing out the need for methods that perform both state and temporal representation learning for RL and which allow gaining insight into what is being learned (perhaps in order to allow a human operator to intervene if necessary). This is a very important goal from a practical point of view, and it is great to see research in this direction. For this reason, I would like to encourage the authors to pursue this further. However, I am not at all convinced that the current incarnation of this work is the right answer. Part of the issues are more related to presentation, part may require rethinking.\nIn order to get the \"interpretability\", the authors opt for some fairly specific ways of performing abstraction. For example, their skills always start In a single skill initiation state, and likewise end in one state. This seems unnecessarily restrictive, and it is not clear why this restriction is needed (other than convenience). Similarly, clustering is the basis for forming the higher level states, and there is a specific kind of clustering used here. Again, it is not clear why this has to be done via clustering as opposed to other methods. Ensuring temporal coherence in the particular form employed also seems restrictive. There is a reference to supplementary material where some of these choices are explained, but I could not find this in the posted version of the paper. The authors should either explain clearly why these specific choices are necessary, or (even better) try to think if they can be relaxed while still keeping interpretability. \nFrom a presentation point of view, the paper would benefit from formal definitions of AMDP and SAMDP, as well as from formal descriptions of the algorithms employed in constructing these representations (eg Bellman equations for the models, and update rules for the algorithms learning them). While intuitions are given, the math is not precisely stated. The overhead of constructing an SAMDP (computation time and space) should be clarified as well.\nThe experiments are well carried out and it is nice to have both gridworld experiments, where visualization are easy to perform and understand, as well as Atari games (gridworld still have their place despite what other reviewers might say). The results are positive, but because the proposed approach has many moving parts which rely on specific choices, significance and general ease of use are unclear at this point. Perhaps having the complete supplementary would have helped in this respect.\nSmall comment: The two lines after Eq 2 contain typos in the notation and a wrong sign in the equation."
  },
  {
    "people": [
      "Bellman"
    ],
    "review": "The paper starts by pointing out the need for methods that perform both state and temporal representation learning for RL and which allow gaining insight into what is being learned (perhaps in order to allow a human operator to intervene if necessary). This is a very important goal from a practical point of view, and it is great to see research in this direction. For this reason, I would like to encourage the authors to pursue this further. However, I am not at all convinced that the current incarnation of this work is the right answer. Part of the issues are more related to presentation, part may require rethinking.\nIn order to get the \"interpretability\", the authors opt for some fairly specific ways of performing abstraction. For example, their skills always start In a single skill initiation state, and likewise end in one state. This seems unnecessarily restrictive, and it is not clear why this restriction is needed (other than convenience). Similarly, clustering is the basis for forming the higher level states, and there is a specific kind of clustering used here. Again, it is not clear why this has to be done via clustering as opposed to other methods. Ensuring temporal coherence in the particular form employed also seems restrictive. There is a reference to supplementary material where some of these choices are explained, but I could not find this in the posted version of the paper. The authors should either explain clearly why these specific choices are necessary, or (even better) try to think if they can be relaxed while still keeping interpretability. \nFrom a presentation point of view, the paper would benefit from formal definitions of AMDP and SAMDP, as well as from formal descriptions of the algorithms employed in constructing these representations (eg Bellman equations for the models, and update rules for the algorithms learning them). While intuitions are given, the math is not precisely stated. The overhead of constructing an SAMDP (computation time and space) should be clarified as well.\nThe experiments are well carried out and it is nice to have both gridworld experiments, where visualization are easy to perform and understand, as well as Atari games (gridworld still have their place despite what other reviewers might say). The results are positive, but because the proposed approach has many moving parts which rely on specific choices, significance and general ease of use are unclear at this point. Perhaps having the complete supplementary would have helped in this respect.\nSmall comment: The two lines after Eq 2 contain typos in the notation and a wrong sign in the equation."
  },
  {
    "people": [
      "bernoulli"
    ],
    "review": "The authors propose a novel energy-function for RBMs, using the leaky relu max(cx, x) activation function for the hidden-units. Analogous to ReLU units in feed-forward networks, these leaky relu RBMs split the input space into a combinatorial number of regions, where each region defines p(v) as a truncated Gaussian. A further contribution of the paper is in proposing a novel sampling scheme for the leaky RBM: one can run a much shorter Markov chain by initializing it from a sample of the leaky RBM with c=1 (which yields a standard multi-variate normal over the visibles) and then slowly annealing c. In low-dimension a similar scheme is shown to outperform AIS for estimating the partition function. Experiments are performed on both CIFAR-10 and SVHN.\n\nThis is an interesting paper which I believe would be of interest to the ICLR community. The theoretical contributions are strong: the authors not only introduce a proper energy formulation of ReLU RBMs, but also a novel sampling mechanism and an improvement on AIS for estimating their partition function. \n\nUnfortunately, the experimental results are somewhat limited. The PCD baseline is notably absent. Including (bernoulli visible, leaky-relu hidden) would have allowed the authors to evaluate likelihoods on standard binary RBM datasets. As it stands, performance on CIFAR-10 and SVHN, while improved with leaky-relu, is a far cry from more recent generative models (VAE-based, or auto-regressive models). While this comparison may be unfair, it will certainly limit the wider appeal of the paper to the community. Furthermore, there is the issue of the costly projection method which is required to guarantee that the energy-function remain bounded (covariance matrix over each region be PSD). Again, while it may be fair to leave that for future work given the other contributions, this will further limit the appeal of the paper.\n\nPROS:\nIntroduces an energy function having the leaky-relu as an activation function\nIntroduces a novel sampling procedure based on annealing the leakiness parameter\nSimilar sampling scheme shown to outperform AIS\n\nCONS:\nResults are somewhat out of date\nMissing experiments on binary datasets (more comparable to prior RBM work)\nMissing PCD baseline\nCost of projection method\n"
  },
  {
    "people": [
      "bernoulli"
    ],
    "review": "The authors propose a novel energy-function for RBMs, using the leaky relu max(cx, x) activation function for the hidden-units. Analogous to ReLU units in feed-forward networks, these leaky relu RBMs split the input space into a combinatorial number of regions, where each region defines p(v) as a truncated Gaussian. A further contribution of the paper is in proposing a novel sampling scheme for the leaky RBM: one can run a much shorter Markov chain by initializing it from a sample of the leaky RBM with c=1 (which yields a standard multi-variate normal over the visibles) and then slowly annealing c. In low-dimension a similar scheme is shown to outperform AIS for estimating the partition function. Experiments are performed on both CIFAR-10 and SVHN.\n\nThis is an interesting paper which I believe would be of interest to the ICLR community. The theoretical contributions are strong: the authors not only introduce a proper energy formulation of ReLU RBMs, but also a novel sampling mechanism and an improvement on AIS for estimating their partition function. \n\nUnfortunately, the experimental results are somewhat limited. The PCD baseline is notably absent. Including (bernoulli visible, leaky-relu hidden) would have allowed the authors to evaluate likelihoods on standard binary RBM datasets. As it stands, performance on CIFAR-10 and SVHN, while improved with leaky-relu, is a far cry from more recent generative models (VAE-based, or auto-regressive models). While this comparison may be unfair, it will certainly limit the wider appeal of the paper to the community. Furthermore, there is the issue of the costly projection method which is required to guarantee that the energy-function remain bounded (covariance matrix over each region be PSD). Again, while it may be fair to leave that for future work given the other contributions, this will further limit the appeal of the paper.\n\nPROS:\nIntroduces an energy function having the leaky-relu as an activation function\nIntroduces a novel sampling procedure based on annealing the leakiness parameter\nSimilar sampling scheme shown to outperform AIS\n\nCONS:\nResults are somewhat out of date\nMissing experiments on binary datasets (more comparable to prior RBM work)\nMissing PCD baseline\nCost of projection method\n"
  },
  {
    "people": [
      "Boltzmann"
    ],
    "review": "This paper proposes a nice algorithm for improved exploration in policy search RL settings. The method essentially optimizes a weighted combination of expected reward (essentially the REINFORCE objective without an entropy regularizer) plus a term from the reward augmented maximum likelihood objective (from a recent NIPS paper), and show that the resulting update can be made with a fairly small modification to the REINFORCE algorithm. The authors show improved performance on several sequential \"program-like\" domains like copying a string, adding, etc.\n \n I'm recommending this paper for acceptance, as I think the contribution here is a good one, and the basic approach very nicely offers a better exploration policy than the typical Boltzmann policy, using a fairly trivial modification to REINFORCE. But after re-reading I'm less enthusiastic, simply because the delta over previous work (namely the RAML paper) doesn't seem incredibly substantial. None of the domains in the experiments seem substantially challenges, and the fact that it can improve over REINFORCE isn't necessarily amazing.\n \n Pros:\n + Well-motivated (and simple) modification to REINFORCE to get better exploration\n + Demonstrably better performance with seemingly less hyperparameter tunies\n \n Cons:\n - Delta over RAML work isn't that clear, essentially is just a weighted combination between REINFORCE and RAML (though in RL context)\n - Experiments are good, but not outstanding relative to simple baselines"
  },
  {
    "people": [
      "Schaul"
    ],
    "review": "\n\noverview:\nThis work proposes to link trajectory log-probabilities and rewards by defining under-appreciated rewards. This suggests that there is a linear relationship between trajectory rewards and their log-probability which can be exploited by measuring the resulting mismatch.\nThat is, when an action sequence under-appreciates its reward, its log-probability is increased.\nThis method is a simple modification to the well-known REINFORCE method, requiring only one extra hyperparameter \\tau, and intuitively provides us with a better exploration mechanism than \\epsilon-greedy or random exploration.\nThe method is tested on algorithmic environments, and compared to entropy-regularized REINFORCE and double Q-learning, and performs equally or better than those two baselines, especially in more complex environments.\n\n\nremarks:\n- the focus in the introduction on algorithmic tasks may be a double-edged sword. It is an interesting domain to test your hypothesis and benchmark your method. At the same time, it distracts the reader from the (IMO) generality of the proposed method.\n- in the introduction you say the reward is sparse, in section 6, on tasks 1-5, you then say there is a reward at each correct emission, i.e. each time step. This is only 'corrected' to end-of-episode-reward in section 7.4, after having discussed results. I'd move or mention this in section 6.\n- approach seems quite sensible to tau being in the same range as logpi(a|h), but you only try tau=0.1 for UREX. I'm not sure I understand nor agree with this experimentation choice.\n- an alternative to grid search is random search (Bergstra&Bengio, 2012). It may illustrate better hyperparameter robustness, and allow you to explore more in the same number of experiment.\n\nopinion:\n- An interesting approach to policy-gradient, to be sure. It tackles the very important question of \"how should agents explore?\"\n- I'm ambivalent to claiming that an algorithm is robust to hyperparmeters simply because it performs better on the selected hyperparameter range. All you really show is that it performs well some amount of time when the hyperparams lay in that range. Couldn't it be that MENT needs different hyperparameters? (Just being devil's advocate here)\n- I see why matching 1/tau with logpi is the obvious choice, but it implies a very strong prior: that the reward (to a factor of 1/tau) lies in the same space as the log policy. One point of failure I see (but correct me if I'm wrong) is that as the length of the trajectory grows the reward is expected to grow linearly, so short ways to get some reward will be less explored than long ways of getting the same reward, creating an imbalance unless the reward is shaped such that shorter trajectories get more reward (which is only the case in task 6).\n- It might have been good to also compare with methods explicitly trying to explore better with value-functions (e.g. prioritized experience replay, Schaul et al 2015)\n- At the risk of repeating myself, tau plays a major role in this method, but there is little analysis on its effect on experiments.\n\n\nThe methodology and reasoning is clearly explained and I think this paper communicates its message very well.\nThat message is novel, albeit a minor modification to a well-known algorithm, it is well motivated and, I think, a welcome addition to literature concerning exploration in RL.\nThe experiments are chosen accordingly, and results seem to reflect the hypothesis of the authors.\n\nI realize the tyranny of extensive experimentation and the scarcity of time, but I do think that this paper would benefit from more (or cleverer) experimentation, as well as demonstrating more explicitly the impact of the method on exploration.\nReading this paper convinced me that measuring mismatch between a trajectory's observed reward and its probability given the current policy is a clever (and well motivated) thing to do. Yet, I think that the paper could have a more convincing empirical argument, even if it is for toy tasks."
  },
  {
    "people": [
      "Boltzmann"
    ],
    "review": "This paper proposes a nice algorithm for improved exploration in policy search RL settings. The method essentially optimizes a weighted combination of expected reward (essentially the REINFORCE objective without an entropy regularizer) plus a term from the reward augmented maximum likelihood objective (from a recent NIPS paper), and show that the resulting update can be made with a fairly small modification to the REINFORCE algorithm. The authors show improved performance on several sequential \"program-like\" domains like copying a string, adding, etc.\n \n I'm recommending this paper for acceptance, as I think the contribution here is a good one, and the basic approach very nicely offers a better exploration policy than the typical Boltzmann policy, using a fairly trivial modification to REINFORCE. But after re-reading I'm less enthusiastic, simply because the delta over previous work (namely the RAML paper) doesn't seem incredibly substantial. None of the domains in the experiments seem substantially challenges, and the fact that it can improve over REINFORCE isn't necessarily amazing.\n \n Pros:\n + Well-motivated (and simple) modification to REINFORCE to get better exploration\n + Demonstrably better performance with seemingly less hyperparameter tunies\n \n Cons:\n - Delta over RAML work isn't that clear, essentially is just a weighted combination between REINFORCE and RAML (though in RL context)\n - Experiments are good, but not outstanding relative to simple baselines"
  },
  {
    "people": [
      "Schaul"
    ],
    "review": "\n\noverview:\nThis work proposes to link trajectory log-probabilities and rewards by defining under-appreciated rewards. This suggests that there is a linear relationship between trajectory rewards and their log-probability which can be exploited by measuring the resulting mismatch.\nThat is, when an action sequence under-appreciates its reward, its log-probability is increased.\nThis method is a simple modification to the well-known REINFORCE method, requiring only one extra hyperparameter \\tau, and intuitively provides us with a better exploration mechanism than \\epsilon-greedy or random exploration.\nThe method is tested on algorithmic environments, and compared to entropy-regularized REINFORCE and double Q-learning, and performs equally or better than those two baselines, especially in more complex environments.\n\n\nremarks:\n- the focus in the introduction on algorithmic tasks may be a double-edged sword. It is an interesting domain to test your hypothesis and benchmark your method. At the same time, it distracts the reader from the (IMO) generality of the proposed method.\n- in the introduction you say the reward is sparse, in section 6, on tasks 1-5, you then say there is a reward at each correct emission, i.e. each time step. This is only 'corrected' to end-of-episode-reward in section 7.4, after having discussed results. I'd move or mention this in section 6.\n- approach seems quite sensible to tau being in the same range as logpi(a|h), but you only try tau=0.1 for UREX. I'm not sure I understand nor agree with this experimentation choice.\n- an alternative to grid search is random search (Bergstra&Bengio, 2012). It may illustrate better hyperparameter robustness, and allow you to explore more in the same number of experiment.\n\nopinion:\n- An interesting approach to policy-gradient, to be sure. It tackles the very important question of \"how should agents explore?\"\n- I'm ambivalent to claiming that an algorithm is robust to hyperparmeters simply because it performs better on the selected hyperparameter range. All you really show is that it performs well some amount of time when the hyperparams lay in that range. Couldn't it be that MENT needs different hyperparameters? (Just being devil's advocate here)\n- I see why matching 1/tau with logpi is the obvious choice, but it implies a very strong prior: that the reward (to a factor of 1/tau) lies in the same space as the log policy. One point of failure I see (but correct me if I'm wrong) is that as the length of the trajectory grows the reward is expected to grow linearly, so short ways to get some reward will be less explored than long ways of getting the same reward, creating an imbalance unless the reward is shaped such that shorter trajectories get more reward (which is only the case in task 6).\n- It might have been good to also compare with methods explicitly trying to explore better with value-functions (e.g. prioritized experience replay, Schaul et al 2015)\n- At the risk of repeating myself, tau plays a major role in this method, but there is little analysis on its effect on experiments.\n\n\nThe methodology and reasoning is clearly explained and I think this paper communicates its message very well.\nThat message is novel, albeit a minor modification to a well-known algorithm, it is well motivated and, I think, a welcome addition to literature concerning exploration in RL.\nThe experiments are chosen accordingly, and results seem to reflect the hypothesis of the authors.\n\nI realize the tyranny of extensive experimentation and the scarcity of time, but I do think that this paper would benefit from more (or cleverer) experimentation, as well as demonstrating more explicitly the impact of the method on exploration.\nReading this paper convinced me that measuring mismatch between a trajectory's observed reward and its probability given the current policy is a clever (and well motivated) thing to do. Yet, I think that the paper could have a more convincing empirical argument, even if it is for toy tasks."
  },
  {
    "people": [
      "McIntosh"
    ],
    "review": "This is a clearly written paper with a nice, if straightforward, result: RNNs can be good predictive models of neuron firing rates in the retina.\n\nOn the one hand, the primary scientific contribution seems to just be to confirm that this approach works. On this particular stimulus locked task the gains from using the RNN seemed relatively modest, and it hasn't yet taught us anything new about the biology.\n\nOn the other hand, this (along with the concurrent work of McIntosh et al.) is introducing neural network modeling to a field that isn't currently using it, and where it should prove very effective.\n\nI think it would be very interesting to see the results of applying a framework like this one with LFP and other neurons as input and on a shorter discretization time scale.\n\nI suspect followup work building on this proof of concept will be increasingly exciting.\n\nMinor comments:\nSec 3.2:\nI didn't understand the role of the 0.833 ms bins.\nUse \"epoch\" throughout, rather than alternating between \"epoch\" and \"pass through data\".\n\nFig. 4 would be better with the x-axis on a log scale."
  },
  {
    "people": [
      "McIntosh"
    ],
    "review": "This is a clearly written paper with a nice, if straightforward, result: RNNs can be good predictive models of neuron firing rates in the retina.\n\nOn the one hand, the primary scientific contribution seems to just be to confirm that this approach works. On this particular stimulus locked task the gains from using the RNN seemed relatively modest, and it hasn't yet taught us anything new about the biology.\n\nOn the other hand, this (along with the concurrent work of McIntosh et al.) is introducing neural network modeling to a field that isn't currently using it, and where it should prove very effective.\n\nI think it would be very interesting to see the results of applying a framework like this one with LFP and other neurons as input and on a shorter discretization time scale.\n\nI suspect followup work building on this proof of concept will be increasingly exciting.\n\nMinor comments:\nSec 3.2:\nI didn't understand the role of the 0.833 ms bins.\nUse \"epoch\" throughout, rather than alternating between \"epoch\" and \"pass through data\".\n\nFig. 4 would be better with the x-axis on a log scale."
  },
  {
    "people": [
      "Kalman"
    ],
    "review": "Summary:  The authors present a simple RNN with linear dynamics for language modeling. The linear dynamics greatly enhance the interpretability of the model, as well as provide the potential to improve performance by caching the dynamics for common sub-sequences. Overall, the quantitative comparison on a benchmark task is underwhelming. It\u2019s unclear why the authors didn\u2019t consider a more common dataset, and they only considered a single dataset. On the other hand, they present a number of well-executed techniques for analyzing the behavior of the model, many of which would be impossible to do for a non-linear RNN. \n\nOverall, I recommend that the paper is accepted, despite the results. It provides an interesting read and an important contribution to the research dialogue. \n\nFeedback\n\nThe paper could be improved by shortening the number of analysis experiments and increasing the discussion of related sequence models. Some of the experiments were very compelling, whereas some of them (eg. 4.6) sort of feels like you\u2019re just showing the reader that the model fits the data well, not that the model has any particularly important property. We trust that the model fits the data well, since you get reasonable perplexity results. \n\nLSTMS/GRUs are great for for language modeling for data with rigid combinatorial structure, such as nested parenthesis. It would have been nice if you compared your model to non-linear methods on this sort of data. Don\u2019t be scared of negative results! It would be interesting if the non-linear methods were substantially better on these tasks. \n\nYou should definitely add a discussion of Belanger and Kakade 2015 to the related work. They have different motivations (fast, scalable learning algorithms) rather than you (interpretable latent state dynamics and simple credit assignment for future predictions given past). On the other hand, they also have linear dynamics, and look at the singular vectors of the transition matrix to analyze the model. \n\nMore broadly, it would be useful for readers if you discussed LDS more directly. A lot of this comparison came up in the openreview discussion, and I recommend folding this into the paper. For example, it would be useful to emphasize that the bias vectors correspond to columns of the Kalman gain matrix. \n\nOne last thing regarding LDS: your model corresponds to Kalman filtering but in an LDS you can also do Kalman smoothing, where state vectors are inferred using the future in addition to the past observations. Could you do something similar in your model?\n\nWhat if you said that each matrix is a sparse/convex combination of a set of dictionary matrices? This parameter sharing could provide even more interpretability, since the characters are then represented by the low-dimensional weights used to combine the dictionary elements. This could also provide more scalability to word-level problems. \n"
  },
  {
    "people": [
      "Belanger",
      "Kakade"
    ],
    "review": "Very cool work. Here's some possibly related work on treating text as one-hots coming from a latent linear dynamical system over unobserved embeddings, \"A Linear Dynamical System Model for Text\" by Belanger and Kakade: "
  },
  {
    "people": [
      "Kalman"
    ],
    "review": "Summary:  The authors present a simple RNN with linear dynamics for language modeling. The linear dynamics greatly enhance the interpretability of the model, as well as provide the potential to improve performance by caching the dynamics for common sub-sequences. Overall, the quantitative comparison on a benchmark task is underwhelming. It\u2019s unclear why the authors didn\u2019t consider a more common dataset, and they only considered a single dataset. On the other hand, they present a number of well-executed techniques for analyzing the behavior of the model, many of which would be impossible to do for a non-linear RNN. \n\nOverall, I recommend that the paper is accepted, despite the results. It provides an interesting read and an important contribution to the research dialogue. \n\nFeedback\n\nThe paper could be improved by shortening the number of analysis experiments and increasing the discussion of related sequence models. Some of the experiments were very compelling, whereas some of them (eg. 4.6) sort of feels like you\u2019re just showing the reader that the model fits the data well, not that the model has any particularly important property. We trust that the model fits the data well, since you get reasonable perplexity results. \n\nLSTMS/GRUs are great for for language modeling for data with rigid combinatorial structure, such as nested parenthesis. It would have been nice if you compared your model to non-linear methods on this sort of data. Don\u2019t be scared of negative results! It would be interesting if the non-linear methods were substantially better on these tasks. \n\nYou should definitely add a discussion of Belanger and Kakade 2015 to the related work. They have different motivations (fast, scalable learning algorithms) rather than you (interpretable latent state dynamics and simple credit assignment for future predictions given past). On the other hand, they also have linear dynamics, and look at the singular vectors of the transition matrix to analyze the model. \n\nMore broadly, it would be useful for readers if you discussed LDS more directly. A lot of this comparison came up in the openreview discussion, and I recommend folding this into the paper. For example, it would be useful to emphasize that the bias vectors correspond to columns of the Kalman gain matrix. \n\nOne last thing regarding LDS: your model corresponds to Kalman filtering but in an LDS you can also do Kalman smoothing, where state vectors are inferred using the future in addition to the past observations. Could you do something similar in your model?\n\nWhat if you said that each matrix is a sparse/convex combination of a set of dictionary matrices? This parameter sharing could provide even more interpretability, since the characters are then represented by the low-dimensional weights used to combine the dictionary elements. This could also provide more scalability to word-level problems. \n"
  },
  {
    "people": [
      "Belanger",
      "Kakade"
    ],
    "review": "Very cool work. Here's some possibly related work on treating text as one-hots coming from a latent linear dynamical system over unobserved embeddings, \"A Linear Dynamical System Model for Text\" by Belanger and Kakade: "
  },
  {
    "people": [
      "Hubara"
    ],
    "review": "CONTRIBUTIONS\nWhen training LSTMs, many of the intermediate gradients are close to zero due to the flat shape of the tanh and sigmoid nonlinearities far from the origin. This paper shows that rounding these small gradients to zero results in matrices with up to 80% sparsity during training, and that training character-level LSTM language models with this sparsification does not significantly change the final performance of the model. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training.\n\nNOVELTY\nThresholding gradients to induce sparsity and improve efficiency in RNN training is a novel result to my knowledge.\n\nMISSING CITATIONS\nPrior work has explored low-precision arithmetic for recurrent neural network language models:\n\nHubara et al, \u201cQuantized Neural Networks: Training Neural Networks with\nLow Precision Weights and Activations\u201d,"
  },
  {
    "people": [
      "Graves",
      "Sutskever"
    ],
    "review": "This paper presents the observation that it is possible to utilize sparse operations in the training of LSTM networks without loss of accuracy. This observation is novel (although not too surprising) to my knowledge, but I must state that I am not very familiar with research on fast RNN implmentations.\n\nMinor note:\nThe LSTM language model does not use a 'word2vec' layer. It is simply a linear embedding layer. Word2vec is the name of a specific model which is not directly to character level language models.\n\nThe paper presents the central observation clearly. However, it will be much more convincing if a well known dataset and experiment set up are used, such as Graves (2013) or Sutskever et al (2014), and actual training, validation and test performances are reported.\n\nWhile the main observation is certainly interesting, I think it is not sufficient to be the subject of a full conference paper without implementation (or simulation) and benchmarking of the promised speedups on multiple tasks. For example, how would the gains be affected by various architecture choices?\n\nAt present this is an interesting technical report and I would like to see more detailed results in the future."
  },
  {
    "people": [
      "Hubara"
    ],
    "review": "CONTRIBUTIONS\nWhen training LSTMs, many of the intermediate gradients are close to zero due to the flat shape of the tanh and sigmoid nonlinearities far from the origin. This paper shows that rounding these small gradients to zero results in matrices with up to 80% sparsity during training, and that training character-level LSTM language models with this sparsification does not significantly change the final performance of the model. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training.\n\nNOVELTY\nThresholding gradients to induce sparsity and improve efficiency in RNN training is a novel result to my knowledge.\n\nMISSING CITATIONS\nPrior work has explored low-precision arithmetic for recurrent neural network language models:\n\nHubara et al, \u201cQuantized Neural Networks: Training Neural Networks with\nLow Precision Weights and Activations\u201d, "
  },
  {
    "people": [
      "Hubara"
    ],
    "review": "CONTRIBUTIONS\nWhen training LSTMs, many of the intermediate gradients are close to zero due to the flat shape of the tanh and sigmoid nonlinearities far from the origin. This paper shows that rounding these small gradients to zero results in matrices with up to 80% sparsity during training, and that training character-level LSTM language models with this sparsification does not significantly change the final performance of the model. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training.\n\nNOVELTY\nThresholding gradients to induce sparsity and improve efficiency in RNN training is a novel result to my knowledge.\n\nMISSING CITATIONS\nPrior work has explored low-precision arithmetic for recurrent neural network language models:\n\nHubara et al, \u201cQuantized Neural Networks: Training Neural Networks with\nLow Precision Weights and Activations\u201d,"
  },
  {
    "people": [
      "Graves",
      "Sutskever"
    ],
    "review": "This paper presents the observation that it is possible to utilize sparse operations in the training of LSTM networks without loss of accuracy. This observation is novel (although not too surprising) to my knowledge, but I must state that I am not very familiar with research on fast RNN implmentations.\n\nMinor note:\nThe LSTM language model does not use a 'word2vec' layer. It is simply a linear embedding layer. Word2vec is the name of a specific model which is not directly to character level language models.\n\nThe paper presents the central observation clearly. However, it will be much more convincing if a well known dataset and experiment set up are used, such as Graves (2013) or Sutskever et al (2014), and actual training, validation and test performances are reported.\n\nWhile the main observation is certainly interesting, I think it is not sufficient to be the subject of a full conference paper without implementation (or simulation) and benchmarking of the promised speedups on multiple tasks. For example, how would the gains be affected by various architecture choices?\n\nAt present this is an interesting technical report and I would like to see more detailed results in the future."
  },
  {
    "people": [
      "Hubara"
    ],
    "review": "CONTRIBUTIONS\nWhen training LSTMs, many of the intermediate gradients are close to zero due to the flat shape of the tanh and sigmoid nonlinearities far from the origin. This paper shows that rounding these small gradients to zero results in matrices with up to 80% sparsity during training, and that training character-level LSTM language models with this sparsification does not significantly change the final performance of the model. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training.\n\nNOVELTY\nThresholding gradients to induce sparsity and improve efficiency in RNN training is a novel result to my knowledge.\n\nMISSING CITATIONS\nPrior work has explored low-precision arithmetic for recurrent neural network language models:\n\nHubara et al, \u201cQuantized Neural Networks: Training Neural Networks with\nLow Precision Weights and Activations\u201d, "
  },
  {
    "people": [
      "Lin",
      "Mitchell",
      "Ioffe",
      "Szegedy",
      "Morimoto"
    ],
    "review": "This paper proposes a new approach to model based reinforcement learning and\nevaluates it on 3 ATARI games. The approach involves training a model that\npredicts a sequence of rewards and probabilities of losing a life given a\ncontext of frames and a sequence of actions. The controller samples random\nsequences of actions and executes the one that balances the probabilities of\nearning a point and losing a life given some thresholds. The proposed system\nlearns to play 3 Atari games both individually and when trained on all 3 in a\nmulti-task setup at super-human level.\n\nThe results presented in the paper are very encouraging but there are many\nad-hoc design choices in the design of the system. The paper also provides\nlittle insight into the importance of the different components of the system.\n\nMain concerns:\n- The way predicted rewards and life loss probabilities are combined is very ad-hoc.\n  The natural way to do this would be by learning a Q-value, instead different\n  rules are devised for different games.\n- Is a model actually being learned and improved? It would be good to see\n  predictions for several actions sequences from some carefully chosen start\n  states. This would be good to see both on a game where the approach works and\n  on a game where it fails. The learning progress could also be measured by\n  plotting the training loss on a fixed holdout set of sequences.\n- How important is the proposed RRNN architecture? Would it still work without\n  the residual connections? Would a standard LSTM also work?\n\nMinor points:\n- Intro, paragraph 2 - There is a lot of much earlier work on using models in\n  RL. For example, see Dyna and \"Memory approaches to reinforcement learning in\n  non-Markovian domains\" by Lin and Mitchell to name just two.\n- Section 3.1 - Minor point, but using a_i to represent the observation is\n  unusual.  Why not use o_i for observations and a_i for actions?\n- Section 3.2.2 - Notation again, r_i was used earlier to represent the\n  reward at time i but it is being used again for something else.\n- Observation 1 seems somewhat out of place. Citing the layer normalization\n  paper for the motivation is enough.\n- Section 3.2.2, second last paragraph - How is memory decoupled from\n  computation here? Models like neural turning machines accomplish this by using\n  an external memory, but this looks like an RNN with skip connections.\n- Section 3.3, second paragraph - Whether the model overfits or not depends on\n  the data. The approach doesn't work with demonstrations precisely because it\n  would overfit.\n- Figure 4 - The reference for Batch Normalization should be Ioffe and Szegedy\n  instead of Morimoto et al.\n\nOverall I think the paper has some really promising ideas and encouraging\nresults but is missing a few exploratory/ablation experiments and some polish."
  },
  {
    "people": [
      "Mnih"
    ],
    "review": "This paper proposes a model-based reinforcement learning approach focusing on predicting future rewards given a current state and future actions. This is achieved with a \"residual recurrent neural network\", that outputs the expected reward increase at various time steps in the future. To demonstrate the usefulness of this approach, experiments are conducted on Atari games, with a simple playing strategy that consists in evaluating random sequences of moves and picking the one with highest expected reward (and low enough chance of dying). Interestingly, out of the 3 games tested, one of them exhibits better performance when the agent is trained in a multitask setting (i.e. learning all games simultaneously), hinting that transfer learning is occurring.\n\nThis submission is easy enough to read, and the reward prediction architecture looks like an original and sound idea. There are however several points that I believe prevent this work from reaching the ICLR bar, as detailed below.\n\nThe first issue is the discrepancy between the algorithm proposed in Section 3 vs its actual implementation in Section 4 (experiments): in Section 3 the output is supposed to be the expected accumulated reward in future time steps (as a single scalar), while in experiments it is instead two numbers, one which is the probability of dying and another one which is the probability of having a higher score without dying. This might work better, but it also means the idea as presented in the main body of the paper is not actually evaluated (and I guess it would not work well, as otherwise why implement it differently?)\n\nIn addition, the experimental results are quite limited: only on 3 games that were hand-picked to be easy enough, and no comparison to other RL techniques (DQN & friends). I realize that the main focus of the paper is not about exhibiting state-of-the-art results, since the policy being used is only a simple heuristic to show that the model predictions can ne used to drive decisions. That being said, I think experiments should have tried to demonstrate how to use this model to obtain better reinforcement learning algorithms: there is actually no reinforcement learning done here, since the model is a supervised algorithm, used in a manually-defined hardcoded policy. Another question that could have been addressed (but was not) in the experiments is how good these predictions are (e.g. classification error on dying probability, MSE on future rewards, ...), compared to simpler baselines.\n\nFinally, the paper's \"previous work\" section is too limited, focusing only on DQN and in particular saying very little on the topic of model-based RL. I think a paper like for instance \"Action-Conditional Video Prediction using Deep Networks in Atari Games\" should have been an obvious \"must cite\".\n\nMinor comments:\n- Notations are unusual, with \"a\" denoting a state rather than an action, this is potentially confusing and I see no reason to stray away from standard RL notations\n- Using a dot for tensor concatenation is not a great choice either, since the dot usually indicates a dot product\n- The r_i in 3.2.2 is a residual that has nothing to do with r_i the reward\n- c_i is defined as \"The control that was performed at time i\", but instead it seems to be the control performed at time i-1\n- There is a recurrent confusion between mean and median in 3.2.2\n- x should not be used in Observation 1 since the x from Fig. 3 does not go through layer normalization\n- The inequality in Observation 1 should be about |x_i|, not x_i\n- Observation 1 (with its proof) takes too much space for such a simple result\n- In 3.2.3 the first r_j should be r_i\n- The probability of dying comes out of nowhere in 3.3, since we do not know yet it will be an output of the model\n- \"Our approach is not able to learn from good strategies\" => did you mean \"*only* from good strategies\"?\n- Please say that in Fig. 4 \"fc\" means \"fully connected\"\n- It would be nice also to say how the architecture of Fig. 4 differs from the classical DQN architecture from Mnih et al (2015)\n- Please clarify r_j2 as per your answer in OpenReview comments\n- Table 3 says \"After one iteration\" but has \"PRL Iteration 2\" in it, which is confusing\n- \"Figure 5 shows that not only there is no degradation in Pong and Demon Attack\"=> to me it seems to be a bit worse, actually\n- \"A model that has learned only from random play is able to play at least 7 times better.\" => not clear where this 7 comes from\n- \"Demon Attack's plot in Figure 5c shows a potential problem we mentioned earlier\" => where was it mentioned?\n"
  },
  {
    "people": [
      "Lin",
      "Mitchell",
      "Ioffe",
      "Szegedy",
      "Morimoto"
    ],
    "review": "This paper proposes a new approach to model based reinforcement learning and\nevaluates it on 3 ATARI games. The approach involves training a model that\npredicts a sequence of rewards and probabilities of losing a life given a\ncontext of frames and a sequence of actions. The controller samples random\nsequences of actions and executes the one that balances the probabilities of\nearning a point and losing a life given some thresholds. The proposed system\nlearns to play 3 Atari games both individually and when trained on all 3 in a\nmulti-task setup at super-human level.\n\nThe results presented in the paper are very encouraging but there are many\nad-hoc design choices in the design of the system. The paper also provides\nlittle insight into the importance of the different components of the system.\n\nMain concerns:\n- The way predicted rewards and life loss probabilities are combined is very ad-hoc.\n  The natural way to do this would be by learning a Q-value, instead different\n  rules are devised for different games.\n- Is a model actually being learned and improved? It would be good to see\n  predictions for several actions sequences from some carefully chosen start\n  states. This would be good to see both on a game where the approach works and\n  on a game where it fails. The learning progress could also be measured by\n  plotting the training loss on a fixed holdout set of sequences.\n- How important is the proposed RRNN architecture? Would it still work without\n  the residual connections? Would a standard LSTM also work?\n\nMinor points:\n- Intro, paragraph 2 - There is a lot of much earlier work on using models in\n  RL. For example, see Dyna and \"Memory approaches to reinforcement learning in\n  non-Markovian domains\" by Lin and Mitchell to name just two.\n- Section 3.1 - Minor point, but using a_i to represent the observation is\n  unusual.  Why not use o_i for observations and a_i for actions?\n- Section 3.2.2 - Notation again, r_i was used earlier to represent the\n  reward at time i but it is being used again for something else.\n- Observation 1 seems somewhat out of place. Citing the layer normalization\n  paper for the motivation is enough.\n- Section 3.2.2, second last paragraph - How is memory decoupled from\n  computation here? Models like neural turning machines accomplish this by using\n  an external memory, but this looks like an RNN with skip connections.\n- Section 3.3, second paragraph - Whether the model overfits or not depends on\n  the data. The approach doesn't work with demonstrations precisely because it\n  would overfit.\n- Figure 4 - The reference for Batch Normalization should be Ioffe and Szegedy\n  instead of Morimoto et al.\n\nOverall I think the paper has some really promising ideas and encouraging\nresults but is missing a few exploratory/ablation experiments and some polish."
  },
  {
    "people": [
      "Lin",
      "Mitchell",
      "Ioffe",
      "Szegedy",
      "Morimoto"
    ],
    "review": "This paper proposes a new approach to model based reinforcement learning and\nevaluates it on 3 ATARI games. The approach involves training a model that\npredicts a sequence of rewards and probabilities of losing a life given a\ncontext of frames and a sequence of actions. The controller samples random\nsequences of actions and executes the one that balances the probabilities of\nearning a point and losing a life given some thresholds. The proposed system\nlearns to play 3 Atari games both individually and when trained on all 3 in a\nmulti-task setup at super-human level.\n\nThe results presented in the paper are very encouraging but there are many\nad-hoc design choices in the design of the system. The paper also provides\nlittle insight into the importance of the different components of the system.\n\nMain concerns:\n- The way predicted rewards and life loss probabilities are combined is very ad-hoc.\n  The natural way to do this would be by learning a Q-value, instead different\n  rules are devised for different games.\n- Is a model actually being learned and improved? It would be good to see\n  predictions for several actions sequences from some carefully chosen start\n  states. This would be good to see both on a game where the approach works and\n  on a game where it fails. The learning progress could also be measured by\n  plotting the training loss on a fixed holdout set of sequences.\n- How important is the proposed RRNN architecture? Would it still work without\n  the residual connections? Would a standard LSTM also work?\n\nMinor points:\n- Intro, paragraph 2 - There is a lot of much earlier work on using models in\n  RL. For example, see Dyna and \"Memory approaches to reinforcement learning in\n  non-Markovian domains\" by Lin and Mitchell to name just two.\n- Section 3.1 - Minor point, but using a_i to represent the observation is\n  unusual.  Why not use o_i for observations and a_i for actions?\n- Section 3.2.2 - Notation again, r_i was used earlier to represent the\n  reward at time i but it is being used again for something else.\n- Observation 1 seems somewhat out of place. Citing the layer normalization\n  paper for the motivation is enough.\n- Section 3.2.2, second last paragraph - How is memory decoupled from\n  computation here? Models like neural turning machines accomplish this by using\n  an external memory, but this looks like an RNN with skip connections.\n- Section 3.3, second paragraph - Whether the model overfits or not depends on\n  the data. The approach doesn't work with demonstrations precisely because it\n  would overfit.\n- Figure 4 - The reference for Batch Normalization should be Ioffe and Szegedy\n  instead of Morimoto et al.\n\nOverall I think the paper has some really promising ideas and encouraging\nresults but is missing a few exploratory/ablation experiments and some polish."
  },
  {
    "people": [
      "Mnih"
    ],
    "review": "This paper proposes a model-based reinforcement learning approach focusing on predicting future rewards given a current state and future actions. This is achieved with a \"residual recurrent neural network\", that outputs the expected reward increase at various time steps in the future. To demonstrate the usefulness of this approach, experiments are conducted on Atari games, with a simple playing strategy that consists in evaluating random sequences of moves and picking the one with highest expected reward (and low enough chance of dying). Interestingly, out of the 3 games tested, one of them exhibits better performance when the agent is trained in a multitask setting (i.e. learning all games simultaneously), hinting that transfer learning is occurring.\n\nThis submission is easy enough to read, and the reward prediction architecture looks like an original and sound idea. There are however several points that I believe prevent this work from reaching the ICLR bar, as detailed below.\n\nThe first issue is the discrepancy between the algorithm proposed in Section 3 vs its actual implementation in Section 4 (experiments): in Section 3 the output is supposed to be the expected accumulated reward in future time steps (as a single scalar), while in experiments it is instead two numbers, one which is the probability of dying and another one which is the probability of having a higher score without dying. This might work better, but it also means the idea as presented in the main body of the paper is not actually evaluated (and I guess it would not work well, as otherwise why implement it differently?)\n\nIn addition, the experimental results are quite limited: only on 3 games that were hand-picked to be easy enough, and no comparison to other RL techniques (DQN & friends). I realize that the main focus of the paper is not about exhibiting state-of-the-art results, since the policy being used is only a simple heuristic to show that the model predictions can ne used to drive decisions. That being said, I think experiments should have tried to demonstrate how to use this model to obtain better reinforcement learning algorithms: there is actually no reinforcement learning done here, since the model is a supervised algorithm, used in a manually-defined hardcoded policy. Another question that could have been addressed (but was not) in the experiments is how good these predictions are (e.g. classification error on dying probability, MSE on future rewards, ...), compared to simpler baselines.\n\nFinally, the paper's \"previous work\" section is too limited, focusing only on DQN and in particular saying very little on the topic of model-based RL. I think a paper like for instance \"Action-Conditional Video Prediction using Deep Networks in Atari Games\" should have been an obvious \"must cite\".\n\nMinor comments:\n- Notations are unusual, with \"a\" denoting a state rather than an action, this is potentially confusing and I see no reason to stray away from standard RL notations\n- Using a dot for tensor concatenation is not a great choice either, since the dot usually indicates a dot product\n- The r_i in 3.2.2 is a residual that has nothing to do with r_i the reward\n- c_i is defined as \"The control that was performed at time i\", but instead it seems to be the control performed at time i-1\n- There is a recurrent confusion between mean and median in 3.2.2\n- x should not be used in Observation 1 since the x from Fig. 3 does not go through layer normalization\n- The inequality in Observation 1 should be about |x_i|, not x_i\n- Observation 1 (with its proof) takes too much space for such a simple result\n- In 3.2.3 the first r_j should be r_i\n- The probability of dying comes out of nowhere in 3.3, since we do not know yet it will be an output of the model\n- \"Our approach is not able to learn from good strategies\" => did you mean \"*only* from good strategies\"?\n- Please say that in Fig. 4 \"fc\" means \"fully connected\"\n- It would be nice also to say how the architecture of Fig. 4 differs from the classical DQN architecture from Mnih et al (2015)\n- Please clarify r_j2 as per your answer in OpenReview comments\n- Table 3 says \"After one iteration\" but has \"PRL Iteration 2\" in it, which is confusing\n- \"Figure 5 shows that not only there is no degradation in Pong and Demon Attack\"=> to me it seems to be a bit worse, actually\n- \"A model that has learned only from random play is able to play at least 7 times better.\" => not clear where this 7 comes from\n- \"Demon Attack's plot in Figure 5c shows a potential problem we mentioned earlier\" => where was it mentioned?\n"
  },
  {
    "people": [
      "Lin",
      "Mitchell",
      "Ioffe",
      "Szegedy",
      "Morimoto"
    ],
    "review": "This paper proposes a new approach to model based reinforcement learning and\nevaluates it on 3 ATARI games. The approach involves training a model that\npredicts a sequence of rewards and probabilities of losing a life given a\ncontext of frames and a sequence of actions. The controller samples random\nsequences of actions and executes the one that balances the probabilities of\nearning a point and losing a life given some thresholds. The proposed system\nlearns to play 3 Atari games both individually and when trained on all 3 in a\nmulti-task setup at super-human level.\n\nThe results presented in the paper are very encouraging but there are many\nad-hoc design choices in the design of the system. The paper also provides\nlittle insight into the importance of the different components of the system.\n\nMain concerns:\n- The way predicted rewards and life loss probabilities are combined is very ad-hoc.\n  The natural way to do this would be by learning a Q-value, instead different\n  rules are devised for different games.\n- Is a model actually being learned and improved? It would be good to see\n  predictions for several actions sequences from some carefully chosen start\n  states. This would be good to see both on a game where the approach works and\n  on a game where it fails. The learning progress could also be measured by\n  plotting the training loss on a fixed holdout set of sequences.\n- How important is the proposed RRNN architecture? Would it still work without\n  the residual connections? Would a standard LSTM also work?\n\nMinor points:\n- Intro, paragraph 2 - There is a lot of much earlier work on using models in\n  RL. For example, see Dyna and \"Memory approaches to reinforcement learning in\n  non-Markovian domains\" by Lin and Mitchell to name just two.\n- Section 3.1 - Minor point, but using a_i to represent the observation is\n  unusual.  Why not use o_i for observations and a_i for actions?\n- Section 3.2.2 - Notation again, r_i was used earlier to represent the\n  reward at time i but it is being used again for something else.\n- Observation 1 seems somewhat out of place. Citing the layer normalization\n  paper for the motivation is enough.\n- Section 3.2.2, second last paragraph - How is memory decoupled from\n  computation here? Models like neural turning machines accomplish this by using\n  an external memory, but this looks like an RNN with skip connections.\n- Section 3.3, second paragraph - Whether the model overfits or not depends on\n  the data. The approach doesn't work with demonstrations precisely because it\n  would overfit.\n- Figure 4 - The reference for Batch Normalization should be Ioffe and Szegedy\n  instead of Morimoto et al.\n\nOverall I think the paper has some really promising ideas and encouraging\nresults but is missing a few exploratory/ablation experiments and some polish."
  },
  {
    "people": [
      "F. R. Bach",
      "M. I. Jordan",
      "Ek",
      "Shon",
      "Damianou",
      "Damianou",
      "Lawrence"
    ],
    "review": "This paper considers the case where multiple views of data are learned through a probabilistic deep neural network formulation. This makes the model non-linear (unlike e.g. CCA) but makes inference difficult. Therefore, the VAE framework is invoked for inference.\n\nIn [Ref 1] the authors show that maximum likelihood estimation based on their linear latent model leads to the canonical correlation directions. But in the non-linear case with DNNs it's not clear (at least with the present analysis) what the solution is wrt to the canonical directions. There's no such analysis in the paper, hence I find it a stretch to refer to this model as a CCA type of model. In contrast, e.g. DCCA / DCCAE are taking the canonical correlation between features into account inside the objective and provide interpretations.\n\n[Ref 1] F. R. Bach and M. I. Jordan. A probabilistic interpretation of canonical correlation analysis. Technical Report 688, 2005.\n\nThere is also a significant body of very related work on non-linear multi-view models which is not discussed in this paper. For example, there's been probabilistic non-linear multi-view models [Ref 2, 3], also extended to the Bayesian case with common/private spaces [Ref 4] and the variational / deep learning case [Ref 5].\n\n[Ref 2] Ek et al. Gaussian process latent variable models for human pose estimation. MLMI, 2007.\n[Ref 3] Shon et al. Learning shared latent structure for image synthesis and robotic imitation. NIPS, 2006.\n[Ref 4] Damianou et al. Manifold relevance determination. ICML, 2012.\n[Ref 5] Damianou and Lawrence. Deep Gaussian processes. AISTATS, 2013.\n\nI can see the utility of this model as bringing together two elements: multi-view modeling and VAEs. This seems like an obvious idea but to the best of my knowledge it hasn't been done before and is actually a potentially very useful model.\n\nHowever, the question is, what is the proper way of extending VAE to multiple views? The paper didn't convince me that VAE can work well with multiple views using the shown straightforward construction. Specifically, VCCA doesn't seem to promote the state of the art in terms of results (it actually is overall below the SOA), while the VCCA-private seems a quite ill-posed model: the dimensionalities d have to be manually tuned with exhaustive search; further, the actual model does not provide a consinstent way of encouraging the private and common variables to avoid learning redundant information. Relying only on dropout for this seems a quite ad-hoc solution (in fact, from Fig. 4 (ver2) it seems that the dropout rate is quite crucial). Perhaps good performance might be achieved with a lot of tuning (which might be why the FLICKR results got better in ver2 without changing the model), but it seems quite difficult to optimize for the above reasons. From a purely experimental point of view, VCCA-private doesn't seem to promote the SOA either. Of course one wouldn't expect any new published paper to beat all previous baselines, but it seems that extension of VAE to multiple views is a very interesting idea which deserves some more investigation of how to do it efficiently.\n\nAnother issue is the approximate posterior being parameterized only from one of the views. This makes the model less useful as a generic multi-view model, since it will misbehave in tasks other than classification. But if classification is the main objective, then one should compare to a proper classification model, e.g. a feedforward neural network.\n\nThe plots of Fig. 8 are very nice. Overall, the paper convinced me that there is merit in attaching multiple views to VAE. However, it didn't convince me a) that the proposed way to achieve this is practical b) that there is a connection to CCA (other than being a method for multiple views). The bottom line is that, although the paper is interesting, it needs a little more work.\n"
  },
  {
    "people": [
      "Rezende",
      "Mohamed",
      "Tran",
      "Rezende",
      "D. J.",
      "Mohamed",
      "S.",
      "Tran",
      "Ranganath",
      "Blei"
    ],
    "review": "UPDATE: I have read the replies on this thread. My opinion has not changed.\n\nThe authors propose deep VCCA, a deep version of the probabilistic CCA model by using likelihoods parameterized by nonlinear functions (neural nets). Variational inference is applied with an inference network and reparameterization gradients. An additional variant, termed VCCA-private, is also introduced, which includes local latent variables for each data point (view). A connection to the multi view auto encoder is also shown.\n\nSince the development of black box variational inference and variational auto-encoders, the methodology in model-specific papers like this one are arguably not very interesting. The model is a straightforward extension of probabilistic CCA with neural net parameterized likelihoods. Inference is mechanically the same as any black box approach using the reparameterization gradient and inference networks. The approach also uses a mean-field approximation, which is quite old given the many recent developments in more expressive approximations (see, e.g., Rezende and Mohamed (2015); Tran et al. (2016)).\n\nThe connection to multi-view auto encoders is at first insightful, but no more than the difference between MAP and variational inference. This is a well-known insight: in the abstract, the authors argue that the key distinction is the additional sampling, but ultimately what matters is the KL regularizer. Even with noisy samples, the variances of a normal variational approximation would collapse to zero and thus become a point mass approximation, equivalent to optimizing a point estimate from the MVAE objective. (I suspect the authors know this to some degree due to their remarks in the paper, but it is unclear.)\n\nThat said, I think the paper has strong merits in application. The experiments are strong, comparing to alternative multi-view approaches under a number of interesting data sets. While the use of \"private variables\" is simple, they demonstrate how it can successfully disentangle the per-view latent representation from the shared view. It would have been preferable to compare to methods using probabilistic inference, such as full Bayes for the linear CCA. \n\nThere are also a number of approximations taken to almost be standard in the paper which may not be necessary, such as the use of a mean-field family or the use of an inference network. To separate out how much the approximate inference is influencing the fit of the model, I strongly recommend using MCMC and non-amortized variational inference on at least one experiment. \n\n+ Rezende, D. J., & Mohamed, S. (2015). Variational Inference with Normalizing Flows. Presented at the International Conference on Machine Learning.\n+ Tran, D., Ranganath, R., & Blei, D. M. (2016). The Variational Gaussian Process. Presented at the International Conference on Learning Representations."
  },
  {
    "people": [
      "F. R. Bach",
      "M. I. Jordan",
      "Ek",
      "Shon",
      "Damianou",
      "Damianou",
      "Lawrence"
    ],
    "review": "This paper considers the case where multiple views of data are learned through a probabilistic deep neural network formulation. This makes the model non-linear (unlike e.g. CCA) but makes inference difficult. Therefore, the VAE framework is invoked for inference.\n\nIn [Ref 1] the authors show that maximum likelihood estimation based on their linear latent model leads to the canonical correlation directions. But in the non-linear case with DNNs it's not clear (at least with the present analysis) what the solution is wrt to the canonical directions. There's no such analysis in the paper, hence I find it a stretch to refer to this model as a CCA type of model. In contrast, e.g. DCCA / DCCAE are taking the canonical correlation between features into account inside the objective and provide interpretations.\n\n[Ref 1] F. R. Bach and M. I. Jordan. A probabilistic interpretation of canonical correlation analysis. Technical Report 688, 2005.\n\nThere is also a significant body of very related work on non-linear multi-view models which is not discussed in this paper. For example, there's been probabilistic non-linear multi-view models [Ref 2, 3], also extended to the Bayesian case with common/private spaces [Ref 4] and the variational / deep learning case [Ref 5].\n\n[Ref 2] Ek et al. Gaussian process latent variable models for human pose estimation. MLMI, 2007.\n[Ref 3] Shon et al. Learning shared latent structure for image synthesis and robotic imitation. NIPS, 2006.\n[Ref 4] Damianou et al. Manifold relevance determination. ICML, 2012.\n[Ref 5] Damianou and Lawrence. Deep Gaussian processes. AISTATS, 2013.\n\nI can see the utility of this model as bringing together two elements: multi-view modeling and VAEs. This seems like an obvious idea but to the best of my knowledge it hasn't been done before and is actually a potentially very useful model.\n\nHowever, the question is, what is the proper way of extending VAE to multiple views? The paper didn't convince me that VAE can work well with multiple views using the shown straightforward construction. Specifically, VCCA doesn't seem to promote the state of the art in terms of results (it actually is overall below the SOA), while the VCCA-private seems a quite ill-posed model: the dimensionalities d have to be manually tuned with exhaustive search; further, the actual model does not provide a consinstent way of encouraging the private and common variables to avoid learning redundant information. Relying only on dropout for this seems a quite ad-hoc solution (in fact, from Fig. 4 (ver2) it seems that the dropout rate is quite crucial). Perhaps good performance might be achieved with a lot of tuning (which might be why the FLICKR results got better in ver2 without changing the model), but it seems quite difficult to optimize for the above reasons. From a purely experimental point of view, VCCA-private doesn't seem to promote the SOA either. Of course one wouldn't expect any new published paper to beat all previous baselines, but it seems that extension of VAE to multiple views is a very interesting idea which deserves some more investigation of how to do it efficiently.\n\nAnother issue is the approximate posterior being parameterized only from one of the views. This makes the model less useful as a generic multi-view model, since it will misbehave in tasks other than classification. But if classification is the main objective, then one should compare to a proper classification model, e.g. a feedforward neural network.\n\nThe plots of Fig. 8 are very nice. Overall, the paper convinced me that there is merit in attaching multiple views to VAE. However, it didn't convince me a) that the proposed way to achieve this is practical b) that there is a connection to CCA (other than being a method for multiple views). The bottom line is that, although the paper is interesting, it needs a little more work.\n"
  },
  {
    "people": [
      "Rezende",
      "Mohamed",
      "Tran",
      "Rezende",
      "D. J.",
      "Mohamed",
      "S.",
      "Tran",
      "Ranganath",
      "Blei"
    ],
    "review": "UPDATE: I have read the replies on this thread. My opinion has not changed.\n\nThe authors propose deep VCCA, a deep version of the probabilistic CCA model by using likelihoods parameterized by nonlinear functions (neural nets). Variational inference is applied with an inference network and reparameterization gradients. An additional variant, termed VCCA-private, is also introduced, which includes local latent variables for each data point (view). A connection to the multi view auto encoder is also shown.\n\nSince the development of black box variational inference and variational auto-encoders, the methodology in model-specific papers like this one are arguably not very interesting. The model is a straightforward extension of probabilistic CCA with neural net parameterized likelihoods. Inference is mechanically the same as any black box approach using the reparameterization gradient and inference networks. The approach also uses a mean-field approximation, which is quite old given the many recent developments in more expressive approximations (see, e.g., Rezende and Mohamed (2015); Tran et al. (2016)).\n\nThe connection to multi-view auto encoders is at first insightful, but no more than the difference between MAP and variational inference. This is a well-known insight: in the abstract, the authors argue that the key distinction is the additional sampling, but ultimately what matters is the KL regularizer. Even with noisy samples, the variances of a normal variational approximation would collapse to zero and thus become a point mass approximation, equivalent to optimizing a point estimate from the MVAE objective. (I suspect the authors know this to some degree due to their remarks in the paper, but it is unclear.)\n\nThat said, I think the paper has strong merits in application. The experiments are strong, comparing to alternative multi-view approaches under a number of interesting data sets. While the use of \"private variables\" is simple, they demonstrate how it can successfully disentangle the per-view latent representation from the shared view. It would have been preferable to compare to methods using probabilistic inference, such as full Bayes for the linear CCA. \n\nThere are also a number of approximations taken to almost be standard in the paper which may not be necessary, such as the use of a mean-field family or the use of an inference network. To separate out how much the approximate inference is influencing the fit of the model, I strongly recommend using MCMC and non-amortized variational inference on at least one experiment. \n\n+ Rezende, D. J., & Mohamed, S. (2015). Variational Inference with Normalizing Flows. Presented at the International Conference on Machine Learning.\n+ Tran, D., Ranganath, R., & Blei, D. M. (2016). The Variational Gaussian Process. Presented at the International Conference on Learning Representations."
  },
  {
    "people": [
      "Blickhan"
    ],
    "review": "Thank you for the feedback and comments in the reviews + questions.\nPlease find below a summary of our further reflections, which we will be incorporating into the paper.\n\nAdditional experiments\n\nWe are currently performing several additional experiments, which we will add to the next version of the paper:\n- Analysis of variance of performance for multiple runs of training (Anon Rev 2)\n- Sensitivity to exploration noise, i.e., covariance matrix, (Anon Rev 2)\n- Impact of network architecture (Anon Rev 1)\n\nRe:   Relevance of paper to ICLR audience\n\n1) While much is known about learning input (state) representations, much less is known about the extent to which output (action) representations matter for continuous control policies, and whether useful \u201coutput representations\u201d can be learned. This paper doesn\u2019t provide all the answers, but it shows that output representations do matter (for locomotion): they significantly impact the learning rate, robustness, overall performance, and policy query rate.\n\n2) Many methods for continuous action spaces use physics-based simulations of locomotion as one of their key examples. However, the use of torques as the defacto unit of action is rather arbitrary; it ignores the notion that the passive properties of  actuation mechanisms, e.g., muscles, can make a significant contribution to motion control, i.e., \u201cintelligence by mechanics\u201d (Blickhan et al., 2007).This can be thought of as a kind of partitioning of the computations between the control and the physical system. Our paper provides a first look at the impact of such \u201cpartitioning\u201d.\n\n3) A question that we would like to draw more attention to is: should musculo-tendon systems (as seen in nature) and other actuation details be considered as part of the control policy or part of the environment?  I.e., can they be safely ignored because the control policy would \u201clearn the useful aspects\u201d anyhow? \n\nRe: generalization of results\n\nHere are our thoughts on this;  we will update the paper to reflect these.\n\n1) Admittedly, we do not provide a concrete answer for the generalization to arbitrary objective functions, and we will clearly state this in the paper. However physics-based locomotion is commonly used as an example in continuous-action RL work. While we do optimize for only one reward function (imitation), we do explore results for different characters, different motions, different query rates, and overall policy robustness. We believe that these are all critical dimensions for testing capabilities and generalization for locomotion control. The imitation objective can also be used as a motion prior and so it is likely to be useful more generally.  It further allows us to produce more natural motions (subjectively speaking) than previously seen for similar work. We hope that the capability of learning more natural locomotion policies, even if this arrives in part via the objective function rather than a theoretical advancement, will be of broad interest to the ICLR community.\n\n2) We believe that no single action parameterization will be the best for all problems. However, since objectives for motion control problems are often naturally expressed in terms of kinematic properties, higher-level actions such as target joint angles and velocities may be effective for a wide variety of motion control problems. We hope that our work will help open discussions around the choice of action parameterizations.\n\nRe: specificity of results to the use of a reference pose cost\n\nWhile the reward terms are mainly calculated according to joint positions and velocities, the real challenge for the control policy lies with: (a) learning to compensate for various state-dependent forces, such as gravity and ground-reaction forces, and (b) learning strategies such as foot-placement that are needed to maintain balance for all the locomotion gaits. The reference pose terms provides no information on how to achieve these \u201chidden\u201d aspects of motion control that will ultimately determine the success of the locomotion policy. There is also only a weak correlation between the action space and the actual reference poses for any of the action spaces; the action space trajectories illustrated in Figure 11 are all quite different from the actual right hip angle trajectory (not shown, but it varies smoothly over time).\n\nRe: considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work. (Anon Rev 1)\n\nWe enforce torque limits and joint limits, which are shared across all actuation models. We do not currently include activation time delays for any of the models.\n"
  },
  {
    "people": [
      "Blickhan"
    ],
    "review": "Thank you for the feedback and comments in the reviews + questions.\nPlease find below a summary of our further reflections, which we will be incorporating into the paper.\n\nAdditional experiments\n\nWe are currently performing several additional experiments, which we will add to the next version of the paper:\n- Analysis of variance of performance for multiple runs of training (Anon Rev 2)\n- Sensitivity to exploration noise, i.e., covariance matrix, (Anon Rev 2)\n- Impact of network architecture (Anon Rev 1)\n\nRe:   Relevance of paper to ICLR audience\n\n1) While much is known about learning input (state) representations, much less is known about the extent to which output (action) representations matter for continuous control policies, and whether useful \u201coutput representations\u201d can be learned. This paper doesn\u2019t provide all the answers, but it shows that output representations do matter (for locomotion): they significantly impact the learning rate, robustness, overall performance, and policy query rate.\n\n2) Many methods for continuous action spaces use physics-based simulations of locomotion as one of their key examples. However, the use of torques as the defacto unit of action is rather arbitrary; it ignores the notion that the passive properties of  actuation mechanisms, e.g., muscles, can make a significant contribution to motion control, i.e., \u201cintelligence by mechanics\u201d (Blickhan et al., 2007).This can be thought of as a kind of partitioning of the computations between the control and the physical system. Our paper provides a first look at the impact of such \u201cpartitioning\u201d.\n\n3) A question that we would like to draw more attention to is: should musculo-tendon systems (as seen in nature) and other actuation details be considered as part of the control policy or part of the environment?  I.e., can they be safely ignored because the control policy would \u201clearn the useful aspects\u201d anyhow? \n\nRe: generalization of results\n\nHere are our thoughts on this;  we will update the paper to reflect these.\n\n1) Admittedly, we do not provide a concrete answer for the generalization to arbitrary objective functions, and we will clearly state this in the paper. However physics-based locomotion is commonly used as an example in continuous-action RL work. While we do optimize for only one reward function (imitation), we do explore results for different characters, different motions, different query rates, and overall policy robustness. We believe that these are all critical dimensions for testing capabilities and generalization for locomotion control. The imitation objective can also be used as a motion prior and so it is likely to be useful more generally.  It further allows us to produce more natural motions (subjectively speaking) than previously seen for similar work. We hope that the capability of learning more natural locomotion policies, even if this arrives in part via the objective function rather than a theoretical advancement, will be of broad interest to the ICLR community.\n\n2) We believe that no single action parameterization will be the best for all problems. However, since objectives for motion control problems are often naturally expressed in terms of kinematic properties, higher-level actions such as target joint angles and velocities may be effective for a wide variety of motion control problems. We hope that our work will help open discussions around the choice of action parameterizations.\n\nRe: specificity of results to the use of a reference pose cost\n\nWhile the reward terms are mainly calculated according to joint positions and velocities, the real challenge for the control policy lies with: (a) learning to compensate for various state-dependent forces, such as gravity and ground-reaction forces, and (b) learning strategies such as foot-placement that are needed to maintain balance for all the locomotion gaits. The reference pose terms provides no information on how to achieve these \u201chidden\u201d aspects of motion control that will ultimately determine the success of the locomotion policy. There is also only a weak correlation between the action space and the actual reference poses for any of the action spaces; the action space trajectories illustrated in Figure 11 are all quite different from the actual right hip angle trajectory (not shown, but it varies smoothly over time).\n\nRe: considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work. (Anon Rev 1)\n\nWe enforce torque limits and joint limits, which are shared across all actuation models. We do not currently include activation time delays for any of the models.\n"
  },
  {
    "people": [
      "Rezende",
      "Mohamed",
      "Wu",
      "Sohl-Dickstein et al."
    ],
    "review": "Summary:\nThis paper introduces a heuristic approach for training a deep directed generative model, where similar to the transition operator of a Markov chain each layer samples from the same conditional distribution. Similar to optimizing a variational lower bound, the approach is to approximate the gradient by replacing the posterior over latents with an alternative distribution. However, the approximating distribution is not updated to improve the lower bound but heuristically constructed in each step. A further difference to variational optimization is that the conditional distributions are optimized greedily rather than following the gradient of the joint log-likelihood.\n\nReview:\nThe proposed approach is interesting and to me seems worth exploring more. Given that there are approaches for training the same class of models which are 1) theoretically more sound, 2) of similar computational complexity, and 3) work well in practice (e.g. Rezende & Mohamed, 2015), I am nevertheless not sure of its potential to generate impact. My bigger concern, however, is that the empirical evaluation is still quite limited.\n\nI appreciate the authors included proper estimates of the log-likelihood. This will enable and encourage future comparisons with this method on continuous MNIST. However, the authors should point out that the numbers taken from Wu et al. (2016) are not representative of the performance of a VAE. (From the paper: \u201cTherefore, the log-likelihood values we report should not be compared directly against networks which have a more flexible observation model.\u201d \u201cSuch observation models can easily achieve much higher log-likelihood scores, [\u2026].\u201d)\n\nComparisons with inpainting results using other methods would have been nice. How practical is the proposed approach compared to other approaches? Similar to the diffusion approach by Sohl-Dickstein et al. (2015), the proposed approach seems to be both efficient and effective for inpainting. Not making this a bigger point and performing the proper evaluations seems like a missed opportunity.\n\nMinor:\n\u2013\u00a0I am missing citations for \u201cordered visible dimension sampling\u201d\n\u2013\u00a0Typos and frequent incorrect use of \\citet and \\citep"
  },
  {
    "people": [
      "Rezende",
      "Mohamed",
      "Salimans",
      "Dinh",
      "Wu",
      "Parzen",
      "Sohl-Dickstein et al",
      "Wu"
    ],
    "review": "We updated the paper, the main changes are:\n\nAdded better log-likelihood estimates (one stochastic lower bound and one based on importance sampling)\n\nAdded curves showing that log-likelihood bound improves as infusion training progresses \n\nAdded references to related and relevant works: Rezende & Mohamed, 2015;  Salimans et al. 2015; Dinh et al. 2014; Wu et al. 2016.\n\nAdded results tables for likelihood comparison with models from the literature (Parzen estimates by Sohl-Dickstein et al. 2015 and AIS estimates by Wu et al. 2016)\n\nAdded further experimental details.\n\nAdded an Appendix containing details regarding the infusion rate schedule as well as examples of infused and sampled chains on cifar10 and celebA\n\nCorrected the typos mentioned by the reviewers\n"
  },
  {
    "people": [
      "Sohl-Dickstein",
      "Tim Salimans",
      "Diedrik P. Kingma",
      "Max Welling"
    ],
    "review": "The paper presents a method for training a generative model via an iterative denoising procedure. The denoising process is initialized with a random sample from a crude approximation to the data distribution and produces a high quality sample via multiple denoising steps. Training is performed by setting-up a Markov chain that slowly blends propositions from the current denoising model with a real example from the data distribution; using this chain the current denoising model is updated towards reproducing the changed, \"better\", samples from the blending process.\n\nThis is a clearly written paper that considers an interesting approach for training generative models. I was intrigued by the simplicity of the presented approach and really enjoyed reading the paper.\nThe proposed method is novel although it has clear ties to other recent work aiming to use denoising models for sampling from distributions such as the work by Sohl-Dickstein and the recent work on using DAEs as generative models.\nI think this general direction of research is important. The proposed procedure takes inspiration from the perspective of generating samples by minimizing an energy function via transitions along a Markov chain and, if successful, it can potentially sidestep many problems of current procedures for training directed generative models such as:\n- convergence and mode coverage problems as in generative adversarial networks\n- problems with modeling multi-modal distributions which can arise when a too restrictive approximate inference model is paired with a powerful generative model\n\nThat being said, another method that seems promising for addressing these issues that also has some superficially similarity to the presented work is the idea of combining Hamiltonian Monte Carlo inference with variational inference as in [1]. As such I am not entirely convinced that the method presented here will be able to perform better than the mentioned paper; although it might be simpler to train. Similarly, although I agree that using a MCMC chain to generate samples via a MC-EM like procedure is likely very costly I am not convinced such a procedure won't at least also work reasonably well for the simple MNIST example. In general a more direct comparison between different inference methods using an MCMC chain like procedure would be nice to have but I understand that this is perhaps out of the scope of this paper. One thing that I would have expected, however, is a direct comparison to the procedure from Sohl-Dickstein in terms of sampling steps and generation quality as it is so directly related.\n\nOther major points (good and bad):\n- Although in general the method is explained well some training details are missing. Most importantly it is never mentioned how alpha or omega are set (I am assuming omega is 0.01 as that is the increase mentioned in the experimental setup). It is also unclear how alpha affects the capabilities of the generator. While it intuitively seems reasonable to use a small alpha over many steps to ensure slow blending of the two distributions it is not clear how necessary this is or at what point the procedure would break (I assume alpha = 1 won't work as the generator then would have to magically denoise a sample from the relatively uninformative draw from p0 ?). The authors do mention in one of the figure captions that the denoising model does not produce good samples in only 1-2 steps but that might also be an artifact of training the model with small alpha (at least I see no a priori reason for this). More experiments should be carried out here.\n- No infusion chains or generating chains are shown for any of the more complicated data distributions, this is unfortunate as I feel these would be interesting to look at.\n- The paper does a good job at evaluating the model with respect to several different metrics. The bound on the log-likelihood is nice to have as well!\n- Unfortunately the current approach does not come with any theoretical guarantees. It is unclear for what choices of alpha the procedure will work and whether there is some deeper connection to MCMC sampling or energy based models. In my eyes this does not subtract from the value of the paper but would perhaps be worth a short sentence in the conclusion.\n\nMinor points:\n- The second reference seems broken\n- Figure 3 starts at 100 epochs and, as a result, contains little information. Perhaps it would be more useful to show the complete training procedure and put the x-axis on a log-scale ?\n- The explanation regarding the convolutional networks you use makes no sense to me. You write that you use the same structure as in the \"Improved GANs\" paper which, unlike your model, generates samples from a fixed length random input. I thus suppose you don't really use a generator with 1 fully connected network followed by up-convolutions but rather have several stages of convolutions followed by a fully connected layer and then up-convolutions ?\n- The choice of parametrizing the variance via a sigmoid output unit is somewhat unusual, was there a specific reason for this choice ?\n- footnote 1 contains errors: \"This allow to\" -> \"allows to\",  \"few informations\" -> \"little information\". \"This force the network\" -> \"forces\"\n- Page 1 error: etc...\n- Page 4 error: \"operator should to learn\"\n\n[1] Markov Chain Monte Carlo and Variational Inference: Bridging the Gap, Tim Salimans and Diedrik P. Kingma and Max Welling, ICML 2015\n\n\n>>> Update <<<<\nCopied here from my response below: \n\nI believe the response of the authors clarifies all open issues. I strongly believe the paper should be accepted to the conference. The only remaining issue I have with the paper is that, as the authors acknowledge the architecture of the generator is likely highly sub-optimal and might hamper the performance of the method in the evaluation. This however does not at all subtract from any of the main points of the paper.\n\nI am thus keeping my score as a clear accept. I want to emphasize that I believe the paper should be published (just in case the review process results in some form of cut-off threshold that is high due to overall \"inflated\" review scores).\n"
  },
  {
    "people": [
      "Rezende",
      "Mohamed",
      "Wu",
      "Sohl-Dickstein et al."
    ],
    "review": "Summary:\nThis paper introduces a heuristic approach for training a deep directed generative model, where similar to the transition operator of a Markov chain each layer samples from the same conditional distribution. Similar to optimizing a variational lower bound, the approach is to approximate the gradient by replacing the posterior over latents with an alternative distribution. However, the approximating distribution is not updated to improve the lower bound but heuristically constructed in each step. A further difference to variational optimization is that the conditional distributions are optimized greedily rather than following the gradient of the joint log-likelihood.\n\nReview:\nThe proposed approach is interesting and to me seems worth exploring more. Given that there are approaches for training the same class of models which are 1) theoretically more sound, 2) of similar computational complexity, and 3) work well in practice (e.g. Rezende & Mohamed, 2015), I am nevertheless not sure of its potential to generate impact. My bigger concern, however, is that the empirical evaluation is still quite limited.\n\nI appreciate the authors included proper estimates of the log-likelihood. This will enable and encourage future comparisons with this method on continuous MNIST. However, the authors should point out that the numbers taken from Wu et al. (2016) are not representative of the performance of a VAE. (From the paper: \u201cTherefore, the log-likelihood values we report should not be compared directly against networks which have a more flexible observation model.\u201d \u201cSuch observation models can easily achieve much higher log-likelihood scores, [\u2026].\u201d)\n\nComparisons with inpainting results using other methods would have been nice. How practical is the proposed approach compared to other approaches? Similar to the diffusion approach by Sohl-Dickstein et al. (2015), the proposed approach seems to be both efficient and effective for inpainting. Not making this a bigger point and performing the proper evaluations seems like a missed opportunity.\n\nMinor:\n\u2013\u00a0I am missing citations for \u201cordered visible dimension sampling\u201d\n\u2013\u00a0Typos and frequent incorrect use of \\citet and \\citep"
  },
  {
    "people": [
      "Rezende",
      "Mohamed",
      "Wu",
      "Sohl-Dickstein et al."
    ],
    "review": "Summary:\nThis paper introduces a heuristic approach for training a deep directed generative model, where similar to the transition operator of a Markov chain each layer samples from the same conditional distribution. Similar to optimizing a variational lower bound, the approach is to approximate the gradient by replacing the posterior over latents with an alternative distribution. However, the approximating distribution is not updated to improve the lower bound but heuristically constructed in each step. A further difference to variational optimization is that the conditional distributions are optimized greedily rather than following the gradient of the joint log-likelihood.\n\nReview:\nThe proposed approach is interesting and to me seems worth exploring more. Given that there are approaches for training the same class of models which are 1) theoretically more sound, 2) of similar computational complexity, and 3) work well in practice (e.g. Rezende & Mohamed, 2015), I am nevertheless not sure of its potential to generate impact. My bigger concern, however, is that the empirical evaluation is still quite limited.\n\nI appreciate the authors included proper estimates of the log-likelihood. This will enable and encourage future comparisons with this method on continuous MNIST. However, the authors should point out that the numbers taken from Wu et al. (2016) are not representative of the performance of a VAE. (From the paper: \u201cTherefore, the log-likelihood values we report should not be compared directly against networks which have a more flexible observation model.\u201d \u201cSuch observation models can easily achieve much higher log-likelihood scores, [\u2026].\u201d)\n\nComparisons with inpainting results using other methods would have been nice. How practical is the proposed approach compared to other approaches? Similar to the diffusion approach by Sohl-Dickstein et al. (2015), the proposed approach seems to be both efficient and effective for inpainting. Not making this a bigger point and performing the proper evaluations seems like a missed opportunity.\n\nMinor:\n\u2013\u00a0I am missing citations for \u201cordered visible dimension sampling\u201d\n\u2013\u00a0Typos and frequent incorrect use of \\citet and \\citep"
  },
  {
    "people": [
      "Rezende",
      "Mohamed",
      "Salimans",
      "Dinh",
      "Wu",
      "Parzen",
      "Sohl-Dickstein et al",
      "Wu"
    ],
    "review": "We updated the paper, the main changes are:\n\nAdded better log-likelihood estimates (one stochastic lower bound and one based on importance sampling)\n\nAdded curves showing that log-likelihood bound improves as infusion training progresses \n\nAdded references to related and relevant works: Rezende & Mohamed, 2015;  Salimans et al. 2015; Dinh et al. 2014; Wu et al. 2016.\n\nAdded results tables for likelihood comparison with models from the literature (Parzen estimates by Sohl-Dickstein et al. 2015 and AIS estimates by Wu et al. 2016)\n\nAdded further experimental details.\n\nAdded an Appendix containing details regarding the infusion rate schedule as well as examples of infused and sampled chains on cifar10 and celebA\n\nCorrected the typos mentioned by the reviewers\n"
  },
  {
    "people": [
      "Sohl-Dickstein",
      "Tim Salimans",
      "Diedrik P. Kingma",
      "Max Welling"
    ],
    "review": "The paper presents a method for training a generative model via an iterative denoising procedure. The denoising process is initialized with a random sample from a crude approximation to the data distribution and produces a high quality sample via multiple denoising steps. Training is performed by setting-up a Markov chain that slowly blends propositions from the current denoising model with a real example from the data distribution; using this chain the current denoising model is updated towards reproducing the changed, \"better\", samples from the blending process.\n\nThis is a clearly written paper that considers an interesting approach for training generative models. I was intrigued by the simplicity of the presented approach and really enjoyed reading the paper.\nThe proposed method is novel although it has clear ties to other recent work aiming to use denoising models for sampling from distributions such as the work by Sohl-Dickstein and the recent work on using DAEs as generative models.\nI think this general direction of research is important. The proposed procedure takes inspiration from the perspective of generating samples by minimizing an energy function via transitions along a Markov chain and, if successful, it can potentially sidestep many problems of current procedures for training directed generative models such as:\n- convergence and mode coverage problems as in generative adversarial networks\n- problems with modeling multi-modal distributions which can arise when a too restrictive approximate inference model is paired with a powerful generative model\n\nThat being said, another method that seems promising for addressing these issues that also has some superficially similarity to the presented work is the idea of combining Hamiltonian Monte Carlo inference with variational inference as in [1]. As such I am not entirely convinced that the method presented here will be able to perform better than the mentioned paper; although it might be simpler to train. Similarly, although I agree that using a MCMC chain to generate samples via a MC-EM like procedure is likely very costly I am not convinced such a procedure won't at least also work reasonably well for the simple MNIST example. In general a more direct comparison between different inference methods using an MCMC chain like procedure would be nice to have but I understand that this is perhaps out of the scope of this paper. One thing that I would have expected, however, is a direct comparison to the procedure from Sohl-Dickstein in terms of sampling steps and generation quality as it is so directly related.\n\nOther major points (good and bad):\n- Although in general the method is explained well some training details are missing. Most importantly it is never mentioned how alpha or omega are set (I am assuming omega is 0.01 as that is the increase mentioned in the experimental setup). It is also unclear how alpha affects the capabilities of the generator. While it intuitively seems reasonable to use a small alpha over many steps to ensure slow blending of the two distributions it is not clear how necessary this is or at what point the procedure would break (I assume alpha = 1 won't work as the generator then would have to magically denoise a sample from the relatively uninformative draw from p0 ?). The authors do mention in one of the figure captions that the denoising model does not produce good samples in only 1-2 steps but that might also be an artifact of training the model with small alpha (at least I see no a priori reason for this). More experiments should be carried out here.\n- No infusion chains or generating chains are shown for any of the more complicated data distributions, this is unfortunate as I feel these would be interesting to look at.\n- The paper does a good job at evaluating the model with respect to several different metrics. The bound on the log-likelihood is nice to have as well!\n- Unfortunately the current approach does not come with any theoretical guarantees. It is unclear for what choices of alpha the procedure will work and whether there is some deeper connection to MCMC sampling or energy based models. In my eyes this does not subtract from the value of the paper but would perhaps be worth a short sentence in the conclusion.\n\nMinor points:\n- The second reference seems broken\n- Figure 3 starts at 100 epochs and, as a result, contains little information. Perhaps it would be more useful to show the complete training procedure and put the x-axis on a log-scale ?\n- The explanation regarding the convolutional networks you use makes no sense to me. You write that you use the same structure as in the \"Improved GANs\" paper which, unlike your model, generates samples from a fixed length random input. I thus suppose you don't really use a generator with 1 fully connected network followed by up-convolutions but rather have several stages of convolutions followed by a fully connected layer and then up-convolutions ?\n- The choice of parametrizing the variance via a sigmoid output unit is somewhat unusual, was there a specific reason for this choice ?\n- footnote 1 contains errors: \"This allow to\" -> \"allows to\",  \"few informations\" -> \"little information\". \"This force the network\" -> \"forces\"\n- Page 1 error: etc...\n- Page 4 error: \"operator should to learn\"\n\n[1] Markov Chain Monte Carlo and Variational Inference: Bridging the Gap, Tim Salimans and Diedrik P. Kingma and Max Welling, ICML 2015\n\n\n>>> Update <<<<\nCopied here from my response below: \n\nI believe the response of the authors clarifies all open issues. I strongly believe the paper should be accepted to the conference. The only remaining issue I have with the paper is that, as the authors acknowledge the architecture of the generator is likely highly sub-optimal and might hamper the performance of the method in the evaluation. This however does not at all subtract from any of the main points of the paper.\n\nI am thus keeping my score as a clear accept. I want to emphasize that I believe the paper should be published (just in case the review process results in some form of cut-off threshold that is high due to overall \"inflated\" review scores).\n"
  },
  {
    "people": [
      "Rezende",
      "Mohamed",
      "Wu",
      "Sohl-Dickstein et al."
    ],
    "review": "Summary:\nThis paper introduces a heuristic approach for training a deep directed generative model, where similar to the transition operator of a Markov chain each layer samples from the same conditional distribution. Similar to optimizing a variational lower bound, the approach is to approximate the gradient by replacing the posterior over latents with an alternative distribution. However, the approximating distribution is not updated to improve the lower bound but heuristically constructed in each step. A further difference to variational optimization is that the conditional distributions are optimized greedily rather than following the gradient of the joint log-likelihood.\n\nReview:\nThe proposed approach is interesting and to me seems worth exploring more. Given that there are approaches for training the same class of models which are 1) theoretically more sound, 2) of similar computational complexity, and 3) work well in practice (e.g. Rezende & Mohamed, 2015), I am nevertheless not sure of its potential to generate impact. My bigger concern, however, is that the empirical evaluation is still quite limited.\n\nI appreciate the authors included proper estimates of the log-likelihood. This will enable and encourage future comparisons with this method on continuous MNIST. However, the authors should point out that the numbers taken from Wu et al. (2016) are not representative of the performance of a VAE. (From the paper: \u201cTherefore, the log-likelihood values we report should not be compared directly against networks which have a more flexible observation model.\u201d \u201cSuch observation models can easily achieve much higher log-likelihood scores, [\u2026].\u201d)\n\nComparisons with inpainting results using other methods would have been nice. How practical is the proposed approach compared to other approaches? Similar to the diffusion approach by Sohl-Dickstein et al. (2015), the proposed approach seems to be both efficient and effective for inpainting. Not making this a bigger point and performing the proper evaluations seems like a missed opportunity.\n\nMinor:\n\u2013\u00a0I am missing citations for \u201cordered visible dimension sampling\u201d\n\u2013\u00a0Typos and frequent incorrect use of \\citet and \\citep"
  },
  {
    "people": [
      "Panayotov",
      "Panayotov",
      "Panyotov",
      "Bayse"
    ],
    "review": "This submission proposes a letter-level decoder with a variation of the CTC approach they call ASG, where the blank symbol is dropped and replaced by letter repetition symbols, and where explicit normalization is dropped. Both the description of a letter-level model (though not novel), as well as the CTC-variant are interesting. \n\nThe approach is evaluated on the LibriSpeech task. The authors claim that their approach is competitive. They compare their modelling variant ASG to CTC, but a comparison of the letter-level approach to available word-level results are missing. Compared to the results obtained in Panayotov et al. 2015, the performance obtained here seems only comparable to word-level GMM/HMM models, but worse than word-level hybrid DNN/HMM models, though Panayotov et al. also appled speaker adaptation, which was not done, as far as I can see. I suggest to add a comparison to Panyotov's results (in addition to mentioning Baidu's results on Librispeech, which are not comparable due to much larger amounts of training data), to allow readers to get a quantitative idea. As pointed out by the authors in the text, Baidu's GPU implementation for CTC is more aimed at larger vocabularies, therefore the comparison to GPU in Tables 1a-c do not seem to be helpful for this work, without further discussing the implementations.\n\nYou are using quite a huge analysis window (nearly 2s). Even though other authors also use windows up to 0.5s to 1s (e.g. MRASTA features), some comments on how you arrive at such a large window, and what advantages you observe for it, would be interesting.\n\nThe submission is well written, though more details on the experiences with using non-normalized (transition) scores and beam pruning would be desirable. Table 1 would be better readable if the units of the numbers shown in a/b/c would be shown within the tables, and not only in the caption.\n\nPrior (partial) publications of this work (your NIPS end-to-end workshop paper) should clearly be mentioned/referenced.\n\nWhat do you mean by transition \"scalars\"?\n\nI do not repeat further comments here, which were already given in the pre-review period.\n\nMinor comments:\n - Sec. 2.3, end of 2nd sentence: train properly the model -> train the model properly\n   End of same paragraph: boostrap -> bootstrap (such errors should be avoided by performing an automatic spell check)\n - Sec. 2.3: Bayse -> Bayes\n - definition of logadd is wrong (see comment) - (applies also for your NIPS end-to-end workshop paper).\n - line before Eq. (3): all possible sequence of letters -> all possible sequences of letters (plural)\n - Sec. 2.4, first line: threholding -> thresholding (spell check..)\n - Figure 4: mention the corpus used here - dev?"
  },
  {
    "people": [
      "Zhang"
    ],
    "review": "This paper describes an end-to-end system for speech recognition that uses a linear conditional random field framework.  A convnet estimates node potentials, while transition scores are provided by trained scalar values.  The convnet acoustic model computes scores for letters, not phones, which reduces the need for expert knowledge in training the system.  At test time, scores from a word-level language model, the convnet node potentials, learned letter-to-letter transition scores, and a word insertion penalty are combined to find the best-scoring word hypothesis.  The model may be trained from the raw audio waveform, power spectra, or MFCC features using conditional maximum likelihood estimation.  Experiments on the Librispeech corpus show that the model achieves a 7.2% WER on the test-clean set from Librispeech using MFCC features, a 9.4% WER using power spectral features, and a 10.1% WER using the raw waveform.\n\nPros\n+ It is interesting to see that a convnet trained from scratch using conditional maximum likelihood can perform reasonably well in a speech recognition system for English that uses graphemic (letter-based) acoustic models instead of phonetic models.  This is a promising research direction.\n\nCons\n- The paper is missing a lot of context / prior work that deserves to be cited.  In addition to the papers I already mentioned in various comments, the authors should also be aware of another 2016 Interspeech paper: Zhang et al., \"Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks\", "
  },
  {
    "people": [
      "H. Sak",
      "D. Povey's"
    ],
    "review": "\u200bThere have been numerous works \u200bon learning from raw waveforms and training letter-based CTC networks for speech recognition, however, there are very few works on combining both of them with purely ConvNet as it is done in this paper. It is interesting to see results on a large scale corpus such as Librispeech that is used in this paper, though some baseline results from hybrid NN/HMM systems should be provided. To readers, it is unclear how this system is close to state-of-the-art only from Table 2.\n\nThe key contribution of this paper may be the end-to-end sequence training criterion for their CTC variant (where the blank symbol is dropped), which may be viewed as sequence training of CTC as H. Sak, et al. \"Learning acoustic frame labeling for speech recognition with recurrent neural networks\", 2015. However, instead of generating the denominator lattices using a frame-level trained CTC model first, this paper directly compute the sequence-level loss by considering all the competing hypothesis in the normalizer. Therefore, the model is trained end-to-end. From this perspective, it is closely related to D. Povey's LF-MMI for sequence-training of HMMs. As another reviewer has pointed out, references and discussions on that should be provided. \n\nThis approach should be more expensive than frame-level training of CTCs, however, from Table 1, the authors' implementation is much faster. Did the systems there use the same sampling rate? You said at the end of 2.2 that the step size for your model is 20ms. Is it also the same for Baidu's CTC system. Also, have you tried increasing the step size, e.g. to 30ms or 40ms, as people have found that it may work (equally) better, while significantly cut down the computational cost."
  },
  {
    "people": [
      "Graves",
      "Bahdanau",
      "Graves",
      "Bahdanau",
      "Chan",
      "Zweig",
      "Zweig",
      "Alex Graves"
    ],
    "review": "Dear authors,\n\nHere are some missing relevant citations.\n\nYou should definitely cite the original paper that used CTC with characters.\nGraves et al., \"Towards End-to-End Speech Recognition with Recurrent Neural Networks\", in ICML 2014.\n\nYou should probably also cite and have a related work section with attention-based models such as:\nChan et al., \"Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition\", in ICASSP 2016.\nBahdanau et al., \"End-to-End Attention-based Large Vocabulary Speech Recognition\", in ICASSP 2016.\n\nboth of which are highly relevant to end-to-end ASR.\n\nQuestion:\nWhy did you use Librispeech as opposed to WSJ and/or SWBD. Most end-to-end ASR papers publish on WSJ, especially since there is an established benchmark for comparison (i.e., Graves et al., 2014, Bahdanau et al., 2016, Chan et al., 2016). SWBD also has much stronger benchmarks from the general speech community, and even for end-to-end ASR (i.e., see MSR's CTC paper by Zweig et., \"Advances in All-Neural Speech Recognition\", 2016). You should also definitely comment and compare to Zweig's paper, since they used a similar encoding mechanism.\n\nQuestion:\nIs \"Letter Error Rate\" (LER) the common terminology? From Alex Graves papers and others I see \"Character Error Rate\" (CER). What is the difference?\n\nQuestion:\nVery cool that you can combine wav+cnns+ctc->ASR, but still a little bit disappointed that handcrafted features perform better. Do you expect this to change with more data?"
  },
  {
    "people": [
      "Panayotov",
      "Panayotov",
      "Panyotov",
      "Bayse"
    ],
    "review": "This submission proposes a letter-level decoder with a variation of the CTC approach they call ASG, where the blank symbol is dropped and replaced by letter repetition symbols, and where explicit normalization is dropped. Both the description of a letter-level model (though not novel), as well as the CTC-variant are interesting. \n\nThe approach is evaluated on the LibriSpeech task. The authors claim that their approach is competitive. They compare their modelling variant ASG to CTC, but a comparison of the letter-level approach to available word-level results are missing. Compared to the results obtained in Panayotov et al. 2015, the performance obtained here seems only comparable to word-level GMM/HMM models, but worse than word-level hybrid DNN/HMM models, though Panayotov et al. also appled speaker adaptation, which was not done, as far as I can see. I suggest to add a comparison to Panyotov's results (in addition to mentioning Baidu's results on Librispeech, which are not comparable due to much larger amounts of training data), to allow readers to get a quantitative idea. As pointed out by the authors in the text, Baidu's GPU implementation for CTC is more aimed at larger vocabularies, therefore the comparison to GPU in Tables 1a-c do not seem to be helpful for this work, without further discussing the implementations.\n\nYou are using quite a huge analysis window (nearly 2s). Even though other authors also use windows up to 0.5s to 1s (e.g. MRASTA features), some comments on how you arrive at such a large window, and what advantages you observe for it, would be interesting.\n\nThe submission is well written, though more details on the experiences with using non-normalized (transition) scores and beam pruning would be desirable. Table 1 would be better readable if the units of the numbers shown in a/b/c would be shown within the tables, and not only in the caption.\n\nPrior (partial) publications of this work (your NIPS end-to-end workshop paper) should clearly be mentioned/referenced.\n\nWhat do you mean by transition \"scalars\"?\n\nI do not repeat further comments here, which were already given in the pre-review period.\n\nMinor comments:\n - Sec. 2.3, end of 2nd sentence: train properly the model -> train the model properly\n   End of same paragraph: boostrap -> bootstrap (such errors should be avoided by performing an automatic spell check)\n - Sec. 2.3: Bayse -> Bayes\n - definition of logadd is wrong (see comment) - (applies also for your NIPS end-to-end workshop paper).\n - line before Eq. (3): all possible sequence of letters -> all possible sequences of letters (plural)\n - Sec. 2.4, first line: threholding -> thresholding (spell check..)\n - Figure 4: mention the corpus used here - dev?\n"
  },
  {
    "people": [
      "Panayotov",
      "Panayotov",
      "Panyotov",
      "Bayse"
    ],
    "review": "This submission proposes a letter-level decoder with a variation of the CTC approach they call ASG, where the blank symbol is dropped and replaced by letter repetition symbols, and where explicit normalization is dropped. Both the description of a letter-level model (though not novel), as well as the CTC-variant are interesting. \n\nThe approach is evaluated on the LibriSpeech task. The authors claim that their approach is competitive. They compare their modelling variant ASG to CTC, but a comparison of the letter-level approach to available word-level results are missing. Compared to the results obtained in Panayotov et al. 2015, the performance obtained here seems only comparable to word-level GMM/HMM models, but worse than word-level hybrid DNN/HMM models, though Panayotov et al. also appled speaker adaptation, which was not done, as far as I can see. I suggest to add a comparison to Panyotov's results (in addition to mentioning Baidu's results on Librispeech, which are not comparable due to much larger amounts of training data), to allow readers to get a quantitative idea. As pointed out by the authors in the text, Baidu's GPU implementation for CTC is more aimed at larger vocabularies, therefore the comparison to GPU in Tables 1a-c do not seem to be helpful for this work, without further discussing the implementations.\n\nYou are using quite a huge analysis window (nearly 2s). Even though other authors also use windows up to 0.5s to 1s (e.g. MRASTA features), some comments on how you arrive at such a large window, and what advantages you observe for it, would be interesting.\n\nThe submission is well written, though more details on the experiences with using non-normalized (transition) scores and beam pruning would be desirable. Table 1 would be better readable if the units of the numbers shown in a/b/c would be shown within the tables, and not only in the caption.\n\nPrior (partial) publications of this work (your NIPS end-to-end workshop paper) should clearly be mentioned/referenced.\n\nWhat do you mean by transition \"scalars\"?\n\nI do not repeat further comments here, which were already given in the pre-review period.\n\nMinor comments:\n - Sec. 2.3, end of 2nd sentence: train properly the model -> train the model properly\n   End of same paragraph: boostrap -> bootstrap (such errors should be avoided by performing an automatic spell check)\n - Sec. 2.3: Bayse -> Bayes\n - definition of logadd is wrong (see comment) - (applies also for your NIPS end-to-end workshop paper).\n - line before Eq. (3): all possible sequence of letters -> all possible sequences of letters (plural)\n - Sec. 2.4, first line: threholding -> thresholding (spell check..)\n - Figure 4: mention the corpus used here - dev?"
  },
  {
    "people": [
      "Zhang"
    ],
    "review": "This paper describes an end-to-end system for speech recognition that uses a linear conditional random field framework.  A convnet estimates node potentials, while transition scores are provided by trained scalar values.  The convnet acoustic model computes scores for letters, not phones, which reduces the need for expert knowledge in training the system.  At test time, scores from a word-level language model, the convnet node potentials, learned letter-to-letter transition scores, and a word insertion penalty are combined to find the best-scoring word hypothesis.  The model may be trained from the raw audio waveform, power spectra, or MFCC features using conditional maximum likelihood estimation.  Experiments on the Librispeech corpus show that the model achieves a 7.2% WER on the test-clean set from Librispeech using MFCC features, a 9.4% WER using power spectral features, and a 10.1% WER using the raw waveform.\n\nPros\n+ It is interesting to see that a convnet trained from scratch using conditional maximum likelihood can perform reasonably well in a speech recognition system for English that uses graphemic (letter-based) acoustic models instead of phonetic models.  This is a promising research direction.\n\nCons\n- The paper is missing a lot of context / prior work that deserves to be cited.  In addition to the papers I already mentioned in various comments, the authors should also be aware of another 2016 Interspeech paper: Zhang et al., \"Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks\", "
  },
  {
    "people": [
      "H. Sak",
      "D. Povey's"
    ],
    "review": "\u200bThere have been numerous works \u200bon learning from raw waveforms and training letter-based CTC networks for speech recognition, however, there are very few works on combining both of them with purely ConvNet as it is done in this paper. It is interesting to see results on a large scale corpus such as Librispeech that is used in this paper, though some baseline results from hybrid NN/HMM systems should be provided. To readers, it is unclear how this system is close to state-of-the-art only from Table 2.\n\nThe key contribution of this paper may be the end-to-end sequence training criterion for their CTC variant (where the blank symbol is dropped), which may be viewed as sequence training of CTC as H. Sak, et al. \"Learning acoustic frame labeling for speech recognition with recurrent neural networks\", 2015. However, instead of generating the denominator lattices using a frame-level trained CTC model first, this paper directly compute the sequence-level loss by considering all the competing hypothesis in the normalizer. Therefore, the model is trained end-to-end. From this perspective, it is closely related to D. Povey's LF-MMI for sequence-training of HMMs. As another reviewer has pointed out, references and discussions on that should be provided. \n\nThis approach should be more expensive than frame-level training of CTCs, however, from Table 1, the authors' implementation is much faster. Did the systems there use the same sampling rate? You said at the end of 2.2 that the step size for your model is 20ms. Is it also the same for Baidu's CTC system. Also, have you tried increasing the step size, e.g. to 30ms or 40ms, as people have found that it may work (equally) better, while significantly cut down the computational cost."
  },
  {
    "people": [
      "Graves",
      "Bahdanau",
      "Graves",
      "Bahdanau",
      "Chan",
      "Zweig",
      "Zweig",
      "Alex Graves"
    ],
    "review": "Dear authors,\n\nHere are some missing relevant citations.\n\nYou should definitely cite the original paper that used CTC with characters.\nGraves et al., \"Towards End-to-End Speech Recognition with Recurrent Neural Networks\", in ICML 2014.\n\nYou should probably also cite and have a related work section with attention-based models such as:\nChan et al., \"Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition\", in ICASSP 2016.\nBahdanau et al., \"End-to-End Attention-based Large Vocabulary Speech Recognition\", in ICASSP 2016.\n\nboth of which are highly relevant to end-to-end ASR.\n\nQuestion:\nWhy did you use Librispeech as opposed to WSJ and/or SWBD. Most end-to-end ASR papers publish on WSJ, especially since there is an established benchmark for comparison (i.e., Graves et al., 2014, Bahdanau et al., 2016, Chan et al., 2016). SWBD also has much stronger benchmarks from the general speech community, and even for end-to-end ASR (i.e., see MSR's CTC paper by Zweig et., \"Advances in All-Neural Speech Recognition\", 2016). You should also definitely comment and compare to Zweig's paper, since they used a similar encoding mechanism.\n\nQuestion:\nIs \"Letter Error Rate\" (LER) the common terminology? From Alex Graves papers and others I see \"Character Error Rate\" (CER). What is the difference?\n\nQuestion:\nVery cool that you can combine wav+cnns+ctc->ASR, but still a little bit disappointed that handcrafted features perform better. Do you expect this to change with more data?"
  },
  {
    "people": [
      "Panayotov",
      "Panayotov",
      "Panyotov",
      "Bayse"
    ],
    "review": "This submission proposes a letter-level decoder with a variation of the CTC approach they call ASG, where the blank symbol is dropped and replaced by letter repetition symbols, and where explicit normalization is dropped. Both the description of a letter-level model (though not novel), as well as the CTC-variant are interesting. \n\nThe approach is evaluated on the LibriSpeech task. The authors claim that their approach is competitive. They compare their modelling variant ASG to CTC, but a comparison of the letter-level approach to available word-level results are missing. Compared to the results obtained in Panayotov et al. 2015, the performance obtained here seems only comparable to word-level GMM/HMM models, but worse than word-level hybrid DNN/HMM models, though Panayotov et al. also appled speaker adaptation, which was not done, as far as I can see. I suggest to add a comparison to Panyotov's results (in addition to mentioning Baidu's results on Librispeech, which are not comparable due to much larger amounts of training data), to allow readers to get a quantitative idea. As pointed out by the authors in the text, Baidu's GPU implementation for CTC is more aimed at larger vocabularies, therefore the comparison to GPU in Tables 1a-c do not seem to be helpful for this work, without further discussing the implementations.\n\nYou are using quite a huge analysis window (nearly 2s). Even though other authors also use windows up to 0.5s to 1s (e.g. MRASTA features), some comments on how you arrive at such a large window, and what advantages you observe for it, would be interesting.\n\nThe submission is well written, though more details on the experiences with using non-normalized (transition) scores and beam pruning would be desirable. Table 1 would be better readable if the units of the numbers shown in a/b/c would be shown within the tables, and not only in the caption.\n\nPrior (partial) publications of this work (your NIPS end-to-end workshop paper) should clearly be mentioned/referenced.\n\nWhat do you mean by transition \"scalars\"?\n\nI do not repeat further comments here, which were already given in the pre-review period.\n\nMinor comments:\n - Sec. 2.3, end of 2nd sentence: train properly the model -> train the model properly\n   End of same paragraph: boostrap -> bootstrap (such errors should be avoided by performing an automatic spell check)\n - Sec. 2.3: Bayse -> Bayes\n - definition of logadd is wrong (see comment) - (applies also for your NIPS end-to-end workshop paper).\n - line before Eq. (3): all possible sequence of letters -> all possible sequences of letters (plural)\n - Sec. 2.4, first line: threholding -> thresholding (spell check..)\n - Figure 4: mention the corpus used here - dev?\n"
  },
  {
    "people": [
      "Finn",
      "Mathieu",
      "Finn"
    ],
    "review": "An interesting architecture that accumulates and continuously corrects mistakes as you see more and more of a video sequence.\n\nClarity: The video you generated seems very helpful towards understanding the information flow in your network, it would be nice to link to it from the paper.\n\n \"Our model with hyperparameters optimized for KITTI underperforms the model of Finn et al. (2016), but outperforms the previous state-of-the-art model by Mathieu et al. (2016).\"\n\n It is not clear how different are the train and test sequences at the moment, since standard benchmarks do not really exist for video prediction and each author picks his/her favorite. Underperforming Finn et al 206 at the H3.6m Walking videos is a bit disappointing.\n\n"
  },
  {
    "people": [
      "Brabandere",
      "Ionescu",
      "Finn",
      "Mathieu"
    ],
    "review": "In response to the helpful comments and questions, we have made several changes to the manuscript:\n\n1.  In our original manuscript, we primarily compared the PredNet to a CNN-LSTM Encoder-Decoder, which we chose because it serves as a tight control for the more novel elements of our architecture. However, we agree that it is useful to compare against other published architectures.  One reason that this isn\u2019t a trivial task is because a standard benchmark for next frame prediction arguably has yet to be established.  Another issue is that published models are often optimized for performance on particular datasets, so evaluating competing models on KITTI/CalTech isn\u2019t necessarily fair to those models.  Searching the very recent literature, we found that the most relevant comparison to make is probably against the DFN model by Brabandere et al. (2016), which was recently presented at NIPS and was developed concurrently with our work.  One of their experiments was on a 64x64 pixel, grayscale car-cam dataset.  Training our KITTI model on this dataset, we outperform their results by 29%.  To compare against another concurrently developed model, also published at NIPS 2016, we have additionally evaluated on the Human3.6M dataset (Ionescu et al., 2014).  Our model with hyperparameters optimized for KITTI underperforms the model of Finn et al. (2016), but outperforms the previous state-of-the-art model by Mathieu et al. (2016).  We have added all of these comparisons to the appendix.\n\n2.  To make the main text more clear and concise, and to properly explain all of the necessary details, we have moved portions of the steering angle analysis to the appendix.  Our main point has been to demonstrate that our model learns a representation of important underlying factors, using other models as points of reference, so we have emphasized this.\n\nAt the reviewer\u2019s suggestion, we have added a video clip to help illustrate the flow of information in the network: "
  },
  {
    "people": [
      "Finn",
      "Mathieu",
      "Finn"
    ],
    "review": "An interesting architecture that accumulates and continuously corrects mistakes as you see more and more of a video sequence.\n\nClarity: The video you generated seems very helpful towards understanding the information flow in your network, it would be nice to link to it from the paper.\n\n \"Our model with hyperparameters optimized for KITTI underperforms the model of Finn et al. (2016), but outperforms the previous state-of-the-art model by Mathieu et al. (2016).\"\n\n It is not clear how different are the train and test sequences at the moment, since standard benchmarks do not really exist for video prediction and each author picks his/her favorite. Underperforming Finn et al 206 at the H3.6m Walking videos is a bit disappointing.\n\n"
  },
  {
    "people": [
      "Brabandere",
      "Ionescu",
      "Finn",
      "Mathieu"
    ],
    "review": "In response to the helpful comments and questions, we have made several changes to the manuscript:\n\n1.  In our original manuscript, we primarily compared the PredNet to a CNN-LSTM Encoder-Decoder, which we chose because it serves as a tight control for the more novel elements of our architecture. However, we agree that it is useful to compare against other published architectures.  One reason that this isn\u2019t a trivial task is because a standard benchmark for next frame prediction arguably has yet to be established.  Another issue is that published models are often optimized for performance on particular datasets, so evaluating competing models on KITTI/CalTech isn\u2019t necessarily fair to those models.  Searching the very recent literature, we found that the most relevant comparison to make is probably against the DFN model by Brabandere et al. (2016), which was recently presented at NIPS and was developed concurrently with our work.  One of their experiments was on a 64x64 pixel, grayscale car-cam dataset.  Training our KITTI model on this dataset, we outperform their results by 29%.  To compare against another concurrently developed model, also published at NIPS 2016, we have additionally evaluated on the Human3.6M dataset (Ionescu et al., 2014).  Our model with hyperparameters optimized for KITTI underperforms the model of Finn et al. (2016), but outperforms the previous state-of-the-art model by Mathieu et al. (2016).  We have added all of these comparisons to the appendix.\n\n2.  To make the main text more clear and concise, and to properly explain all of the necessary details, we have moved portions of the steering angle analysis to the appendix.  Our main point has been to demonstrate that our model learns a representation of important underlying factors, using other models as points of reference, so we have emphasized this.\n\nAt the reviewer\u2019s suggestion, we have added a video clip to help illustrate the flow of information in the network: "
  },
  {
    "people": [
      "Rae"
    ],
    "review": "I find this paper not very compelling.  The basic idea seems to be that we can put a fast neighbor searcher into a memory augmented net to make the memory lookups scalable.  However, this was precisely the point of Rae et al.    There are a  number of standardized neighbor searchers; I don't understand why the authors choose to use their own (which they do not benchmark against the standards).   Moreover, they test on a problem where there is no clear need for (vector based) fast-nn, because one can use hashing on the text.     I also find the repeated distinction between \"mips\" and \"nns\" distracting; most libraries that can do one can do the other, or inputs can be modified  to switch between the problems; indeed the authors do this when they convert to the  \"mcss\" problem."
  },
  {
    "people": [
      "Rae"
    ],
    "review": "I find this paper not very compelling.  The basic idea seems to be that we can put a fast neighbor searcher into a memory augmented net to make the memory lookups scalable.  However, this was precisely the point of Rae et al.    There are a  number of standardized neighbor searchers; I don't understand why the authors choose to use their own (which they do not benchmark against the standards).   Moreover, they test on a problem where there is no clear need for (vector based) fast-nn, because one can use hashing on the text.     I also find the repeated distinction between \"mips\" and \"nns\" distracting; most libraries that can do one can do the other, or inputs can be modified  to switch between the problems; indeed the authors do this when they convert to the  \"mcss\" problem.\n"
  },
  {
    "people": [
      "Rae"
    ],
    "review": "I find this paper not very compelling.  The basic idea seems to be that we can put a fast neighbor searcher into a memory augmented net to make the memory lookups scalable.  However, this was precisely the point of Rae et al.    There are a  number of standardized neighbor searchers; I don't understand why the authors choose to use their own (which they do not benchmark against the standards).   Moreover, they test on a problem where there is no clear need for (vector based) fast-nn, because one can use hashing on the text.     I also find the repeated distinction between \"mips\" and \"nns\" distracting; most libraries that can do one can do the other, or inputs can be modified  to switch between the problems; indeed the authors do this when they convert to the  \"mcss\" problem."
  },
  {
    "people": [
      "Rae"
    ],
    "review": "I find this paper not very compelling.  The basic idea seems to be that we can put a fast neighbor searcher into a memory augmented net to make the memory lookups scalable.  However, this was precisely the point of Rae et al.    There are a  number of standardized neighbor searchers; I don't understand why the authors choose to use their own (which they do not benchmark against the standards).   Moreover, they test on a problem where there is no clear need for (vector based) fast-nn, because one can use hashing on the text.     I also find the repeated distinction between \"mips\" and \"nns\" distracting; most libraries that can do one can do the other, or inputs can be modified  to switch between the problems; indeed the authors do this when they convert to the  \"mcss\" problem.\n"
  },
  {
    "people": [
      "Xingchao Peng",
      "Baochen Sun",
      "Karim Ali",
      "Kate Saenko",
      "Francisco Massa",
      "Bryan C. Russell",
      "Mathieu Aubry"
    ],
    "review": "This paper addresses the problem of decoding barcode-like markers depicted in an image.  The main insight is to train a CNN from generated data produced from a GAN.  The GAN is trained using unlabeled images, and leverages a \"3D model\" that undergoes learnt image transformations (e.g., blur, lighting, background).  The parameters for the image transformations are trained such that it confuses a GAN discriminator.  A CNN is trained using images generated from the GAN and compared with hand-crafted features and from training with real images.  The proposed method out-performs both baselines on decoding the barcode markers.\n\nThe proposed GAN architecture could potentially be interesting.  However, I won\u2019t champion the paper as the evaluation could be improved.\n\nA critical missing baseline is a comparison against a generic GAN.  Without this it\u2019s hard to judge the benefit of the more structured GAN.  Also, it would be worth seeing the result when one combines generated and real images for the final task. \n\nA couple of references that are relevant to this work (for object detection using rendered views of 3D shapes):\n\n[A] Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko, Learning Deep Object Detectors from 3D Models; ICCV, 2015.\n\n[B] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. CVPR 2016.\n\nThe problem domain (decoding barcode markers on bees) is limited.  It would be great to see this applied to another problem domain, e.g., object detection from 3D models as shown in paper reference [A], where direct comparison against prior work could be performed.  \n\nI found the writing to be somewhat vague throughout.  For instance, on first reading of the introduction it is not clear what exactly is the contribution of the paper.  \n\nMinor comments:\n\nFig 3 - Are these really renders from a 3D model?  The images look like 2D images, perhaps spatially warped via a homography.  \n\nPage 3: \"chapter\" => \"section\".\n\nIn Table 2, what is the loss used for the DCNN?\n\nFig 9 (a) - The last four images look like they have strange artifacts. Can you explain these?"
  },
  {
    "people": [
      "Xingchao Peng",
      "Baochen Sun",
      "Karim Ali",
      "Kate Saenko",
      "Francisco Massa",
      "Bryan C. Russell",
      "Mathieu Aubry"
    ],
    "review": "This paper addresses the problem of decoding barcode-like markers depicted in an image.  The main insight is to train a CNN from generated data produced from a GAN.  The GAN is trained using unlabeled images, and leverages a \"3D model\" that undergoes learnt image transformations (e.g., blur, lighting, background).  The parameters for the image transformations are trained such that it confuses a GAN discriminator.  A CNN is trained using images generated from the GAN and compared with hand-crafted features and from training with real images.  The proposed method out-performs both baselines on decoding the barcode markers.\n\nThe proposed GAN architecture could potentially be interesting.  However, I won\u2019t champion the paper as the evaluation could be improved.\n\nA critical missing baseline is a comparison against a generic GAN.  Without this it\u2019s hard to judge the benefit of the more structured GAN.  Also, it would be worth seeing the result when one combines generated and real images for the final task. \n\nA couple of references that are relevant to this work (for object detection using rendered views of 3D shapes):\n\n[A] Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko, Learning Deep Object Detectors from 3D Models; ICCV, 2015.\n\n[B] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. CVPR 2016.\n\nThe problem domain (decoding barcode markers on bees) is limited.  It would be great to see this applied to another problem domain, e.g., object detection from 3D models as shown in paper reference [A], where direct comparison against prior work could be performed.  \n\nI found the writing to be somewhat vague throughout.  For instance, on first reading of the introduction it is not clear what exactly is the contribution of the paper.  \n\nMinor comments:\n\nFig 3 - Are these really renders from a 3D model?  The images look like 2D images, perhaps spatially warped via a homography.  \n\nPage 3: \"chapter\" => \"section\".\n\nIn Table 2, what is the loss used for the DCNN?\n\nFig 9 (a) - The last four images look like they have strange artifacts. Can you explain these?\n"
  },
  {
    "people": [
      "Xingchao Peng",
      "Baochen Sun",
      "Karim Ali",
      "Kate Saenko",
      "Francisco Massa",
      "Bryan C. Russell",
      "Mathieu Aubry"
    ],
    "review": "This paper addresses the problem of decoding barcode-like markers depicted in an image.  The main insight is to train a CNN from generated data produced from a GAN.  The GAN is trained using unlabeled images, and leverages a \"3D model\" that undergoes learnt image transformations (e.g., blur, lighting, background).  The parameters for the image transformations are trained such that it confuses a GAN discriminator.  A CNN is trained using images generated from the GAN and compared with hand-crafted features and from training with real images.  The proposed method out-performs both baselines on decoding the barcode markers.\n\nThe proposed GAN architecture could potentially be interesting.  However, I won\u2019t champion the paper as the evaluation could be improved.\n\nA critical missing baseline is a comparison against a generic GAN.  Without this it\u2019s hard to judge the benefit of the more structured GAN.  Also, it would be worth seeing the result when one combines generated and real images for the final task. \n\nA couple of references that are relevant to this work (for object detection using rendered views of 3D shapes):\n\n[A] Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko, Learning Deep Object Detectors from 3D Models; ICCV, 2015.\n\n[B] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. CVPR 2016.\n\nThe problem domain (decoding barcode markers on bees) is limited.  It would be great to see this applied to another problem domain, e.g., object detection from 3D models as shown in paper reference [A], where direct comparison against prior work could be performed.  \n\nI found the writing to be somewhat vague throughout.  For instance, on first reading of the introduction it is not clear what exactly is the contribution of the paper.  \n\nMinor comments:\n\nFig 3 - Are these really renders from a 3D model?  The images look like 2D images, perhaps spatially warped via a homography.  \n\nPage 3: \"chapter\" => \"section\".\n\nIn Table 2, what is the loss used for the DCNN?\n\nFig 9 (a) - The last four images look like they have strange artifacts. Can you explain these?"
  },
  {
    "people": [
      "Xingchao Peng",
      "Baochen Sun",
      "Karim Ali",
      "Kate Saenko",
      "Francisco Massa",
      "Bryan C. Russell",
      "Mathieu Aubry"
    ],
    "review": "This paper addresses the problem of decoding barcode-like markers depicted in an image.  The main insight is to train a CNN from generated data produced from a GAN.  The GAN is trained using unlabeled images, and leverages a \"3D model\" that undergoes learnt image transformations (e.g., blur, lighting, background).  The parameters for the image transformations are trained such that it confuses a GAN discriminator.  A CNN is trained using images generated from the GAN and compared with hand-crafted features and from training with real images.  The proposed method out-performs both baselines on decoding the barcode markers.\n\nThe proposed GAN architecture could potentially be interesting.  However, I won\u2019t champion the paper as the evaluation could be improved.\n\nA critical missing baseline is a comparison against a generic GAN.  Without this it\u2019s hard to judge the benefit of the more structured GAN.  Also, it would be worth seeing the result when one combines generated and real images for the final task. \n\nA couple of references that are relevant to this work (for object detection using rendered views of 3D shapes):\n\n[A] Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko, Learning Deep Object Detectors from 3D Models; ICCV, 2015.\n\n[B] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. CVPR 2016.\n\nThe problem domain (decoding barcode markers on bees) is limited.  It would be great to see this applied to another problem domain, e.g., object detection from 3D models as shown in paper reference [A], where direct comparison against prior work could be performed.  \n\nI found the writing to be somewhat vague throughout.  For instance, on first reading of the introduction it is not clear what exactly is the contribution of the paper.  \n\nMinor comments:\n\nFig 3 - Are these really renders from a 3D model?  The images look like 2D images, perhaps spatially warped via a homography.  \n\nPage 3: \"chapter\" => \"section\".\n\nIn Table 2, what is the loss used for the DCNN?\n\nFig 9 (a) - The last four images look like they have strange artifacts. Can you explain these?\n"
  },
  {
    "people": [
      "Hinton",
      "Mikolov",
      "Lin",
      "Koutn\u00edk",
      "Lin",
      "Socher",
      "Pascanu",
      "Mikolov",
      "Mikolov",
      "Lin",
      "Fernandez",
      "Ring"
    ],
    "review": "This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016).\n\nTheir model is organized as a set of layers that aim at capturing the information form different \u201clevel of abstraction\u201d. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. \n\nThe experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries.\n\nOverall this paper presents a strong and novel model with promising experimental results.\n\n\n\nOn a minor note, I have few remarks/complaints about the writing and the related work:\n\n- In the introduction:\n\u201cOne of the key principles of learning in deep neural networks as well as in the human brain\u201d : please provide evidence for the \u201chuman brain\u201d part of this claim.\n\u201cFor modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances\u201d I believe you re missing Mikolov et al. 2010 in the references.\n\u201cin spite of the fact that hierarchical multiscale structures naturally exist in many temporal data\u201d: missing reference to Lin et al., 1996\n\n- in the related work:\n\u201cA more recent model, the clockwork RNN (CW-RNN) (Koutn\u00edk et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)\u201d : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995.\nWhile the above models focus on online prediction problems, where a prediction needs to be made\u2026\u201d: I believe there is a lot of missing references, in particular to Socher\u2019s work or older recursive networks.\n\u201cThe norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)\u201d: this is not the first work using gradient clipping. I believe it was introduced in Mikolov et al., 2010.\n\nMissing references:\n\u201cRecurrent neural network based language model.\u201d, Mikolov et al. 2010\n\u201cLearning long-term dependencies in NARX recurrent neural networks\u201d, Lin et al. 1996\n\u201cSequence labelling in structured domains with hierarchical recurrent neural networks\u201c, Fernandez et al. 2007\n\u201cLearning sequential tasks by incrementally adding  higher  orders\u201d, Ring, 1993"
  },
  {
    "people": [
      "Hinton",
      "Mikolov",
      "Lin",
      "Koutn\u00edk",
      "Lin",
      "Socher",
      "Pascanu",
      "Mikolov",
      "Mikolov",
      "Lin",
      "Fernandez",
      "Ring"
    ],
    "review": "This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016).\n\nTheir model is organized as a set of layers that aim at capturing the information form different \u201clevel of abstraction\u201d. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. \n\nThe experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries.\n\nOverall this paper presents a strong and novel model with promising experimental results.\n\n\n\nOn a minor note, I have few remarks/complaints about the writing and the related work:\n\n- In the introduction:\n\u201cOne of the key principles of learning in deep neural networks as well as in the human brain\u201d : please provide evidence for the \u201chuman brain\u201d part of this claim.\n\u201cFor modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances\u201d I believe you re missing Mikolov et al. 2010 in the references.\n\u201cin spite of the fact that hierarchical multiscale structures naturally exist in many temporal data\u201d: missing reference to Lin et al., 1996\n\n- in the related work:\n\u201cA more recent model, the clockwork RNN (CW-RNN) (Koutn\u00edk et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)\u201d : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995.\nWhile the above models focus on online prediction problems, where a prediction needs to be made\u2026\u201d: I believe there is a lot of missing references, in particular to Socher\u2019s work or older recursive networks.\n\u201cThe norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)\u201d: this is not the first work using gradient clipping. I believe it was introduced in Mikolov et al., 2010.\n\nMissing references:\n\u201cRecurrent neural network based language model.\u201d, Mikolov et al. 2010\n\u201cLearning long-term dependencies in NARX recurrent neural networks\u201d, Lin et al. 1996\n\u201cSequence labelling in structured domains with hierarchical recurrent neural networks\u201c, Fernandez et al. 2007\n\u201cLearning sequential tasks by incrementally adding  higher  orders\u201d, Ring, 1993\n"
  },
  {
    "people": [
      "Hinton",
      "Mikolov",
      "Lin",
      "Koutn\u00edk",
      "Lin",
      "Socher",
      "Pascanu",
      "Mikolov",
      "Mikolov",
      "Lin",
      "Fernandez",
      "Ring"
    ],
    "review": "This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016).\n\nTheir model is organized as a set of layers that aim at capturing the information form different \u201clevel of abstraction\u201d. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. \n\nThe experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries.\n\nOverall this paper presents a strong and novel model with promising experimental results.\n\n\n\nOn a minor note, I have few remarks/complaints about the writing and the related work:\n\n- In the introduction:\n\u201cOne of the key principles of learning in deep neural networks as well as in the human brain\u201d : please provide evidence for the \u201chuman brain\u201d part of this claim.\n\u201cFor modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances\u201d I believe you re missing Mikolov et al. 2010 in the references.\n\u201cin spite of the fact that hierarchical multiscale structures naturally exist in many temporal data\u201d: missing reference to Lin et al., 1996\n\n- in the related work:\n\u201cA more recent model, the clockwork RNN (CW-RNN) (Koutn\u00edk et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)\u201d : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995.\nWhile the above models focus on online prediction problems, where a prediction needs to be made\u2026\u201d: I believe there is a lot of missing references, in particular to Socher\u2019s work or older recursive networks.\n\u201cThe norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)\u201d: this is not the first work using gradient clipping. I believe it was introduced in Mikolov et al., 2010.\n\nMissing references:\n\u201cRecurrent neural network based language model.\u201d, Mikolov et al. 2010\n\u201cLearning long-term dependencies in NARX recurrent neural networks\u201d, Lin et al. 1996\n\u201cSequence labelling in structured domains with hierarchical recurrent neural networks\u201c, Fernandez et al. 2007\n\u201cLearning sequential tasks by incrementally adding  higher  orders\u201d, Ring, 1993"
  },
  {
    "people": [
      "Hinton",
      "Mikolov",
      "Lin",
      "Koutn\u00edk",
      "Lin",
      "Socher",
      "Pascanu",
      "Mikolov",
      "Mikolov",
      "Lin",
      "Fernandez",
      "Ring"
    ],
    "review": "This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016).\n\nTheir model is organized as a set of layers that aim at capturing the information form different \u201clevel of abstraction\u201d. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. \n\nThe experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries.\n\nOverall this paper presents a strong and novel model with promising experimental results.\n\n\n\nOn a minor note, I have few remarks/complaints about the writing and the related work:\n\n- In the introduction:\n\u201cOne of the key principles of learning in deep neural networks as well as in the human brain\u201d : please provide evidence for the \u201chuman brain\u201d part of this claim.\n\u201cFor modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances\u201d I believe you re missing Mikolov et al. 2010 in the references.\n\u201cin spite of the fact that hierarchical multiscale structures naturally exist in many temporal data\u201d: missing reference to Lin et al., 1996\n\n- in the related work:\n\u201cA more recent model, the clockwork RNN (CW-RNN) (Koutn\u00edk et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)\u201d : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995.\nWhile the above models focus on online prediction problems, where a prediction needs to be made\u2026\u201d: I believe there is a lot of missing references, in particular to Socher\u2019s work or older recursive networks.\n\u201cThe norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)\u201d: this is not the first work using gradient clipping. I believe it was introduced in Mikolov et al., 2010.\n\nMissing references:\n\u201cRecurrent neural network based language model.\u201d, Mikolov et al. 2010\n\u201cLearning long-term dependencies in NARX recurrent neural networks\u201d, Lin et al. 1996\n\u201cSequence labelling in structured domains with hierarchical recurrent neural networks\u201c, Fernandez et al. 2007\n\u201cLearning sequential tasks by incrementally adding  higher  orders\u201d, Ring, 1993\n"
  },
  {
    "people": [
      "G. Jagannathan",
      "C. Monteleoni",
      "K. Pillaipakkamnatt"
    ],
    "review": "Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough.\n\nOne caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and/or clarify what can be generalized about the observed learning performance.\n\nAnother caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in order to have a meaningful comparison to the epsilon-delta values reported in related work.\n\nFinally, as has been acknowledged in the paper, the present approach may not work on other natural data types. Experiments on other data sets is strongly encouraged. Also, please cite the data sets used.\n\nOther comments:\n\nDiscussion of certain parts of the related work are thorough. However, please add some survey/discussion of the related work on differentially-private semi-supervised learning. For example, in the context of random forests, the following paper also proposed differentially-private semi-supervised learning via a teacher-learner approach (although not denoted as \u201cteacher-learner\u201d). The only time the private labeled data is used is when learning the \u201cprimary ensemble.\u201d  A \"secondary ensemble\" is then learned only from the unlabeled (non-private) data, with pseudo-labels generated by the primary ensemble.\n\nG. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy. Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013.\n\nSection C. does a nice comparison of approaches. Please make sure the quantitative results here constitute an apples-to-apples comparison with the GAN results. \n\nThe paper is extremely well-written, for the most part. Some places needing clarification include:\n- Last paragraph of 3.1. \u201call teachers\u2026.get the same training data\u2026.\u201d This should be rephrased to make it clear that it is not the same w.r.t. all the teachers, but w.r.t. the same teacher on the neighboring database.\n- 4.1: The authors state: \u201cThe number n of teachers is limited by a trade-off between the classification task\u2019s complexity and the available data.\u201d However, since this tradeoff is not formalized, the statement is imprecise. In particular, if the analysis is done in the i.i.d. setting, the tradeoff would also likely depend on the relation of the target hypothesis to the data distribution.\n- Discussion of figure 3 was rather unclear in the text and caption and should be revised for clarity. In the text section, at first the explanation seems to imply that a larger gap is better (as is also indicated in the caption). However later it is stated that the gap stays under 20%. These sentences seem contradictory, which is likely not what was intended."
  },
  {
    "people": [
      "Salimans"
    ],
    "review": "Thank you for providing an interesting paper.\n\nIn the paper, the student model is trained by semi-supervised fashion as suggested in (Salimans et al., 2016).\n\nAs far as I understand, teacher's ensembles are using for supervised learning and nonsensitive data is for unsupervised learning.\n\nSo, my question is \"Where is the generator ?\".\n\nThe aggregation of teacher network is treated as the generator in GAN framework? "
  },
  {
    "people": [
      "G. Jagannathan",
      "C. Monteleoni",
      "K. Pillaipakkamnatt"
    ],
    "review": "Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough.\n\nOne caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and/or clarify what can be generalized about the observed learning performance.\n\nAnother caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in order to have a meaningful comparison to the epsilon-delta values reported in related work.\n\nFinally, as has been acknowledged in the paper, the present approach may not work on other natural data types. Experiments on other data sets is strongly encouraged. Also, please cite the data sets used.\n\nOther comments:\n\nDiscussion of certain parts of the related work are thorough. However, please add some survey/discussion of the related work on differentially-private semi-supervised learning. For example, in the context of random forests, the following paper also proposed differentially-private semi-supervised learning via a teacher-learner approach (although not denoted as \u201cteacher-learner\u201d). The only time the private labeled data is used is when learning the \u201cprimary ensemble.\u201d  A \"secondary ensemble\" is then learned only from the unlabeled (non-private) data, with pseudo-labels generated by the primary ensemble.\n\nG. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy. Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013.\n\nSection C. does a nice comparison of approaches. Please make sure the quantitative results here constitute an apples-to-apples comparison with the GAN results. \n\nThe paper is extremely well-written, for the most part. Some places needing clarification include:\n- Last paragraph of 3.1. \u201call teachers\u2026.get the same training data\u2026.\u201d This should be rephrased to make it clear that it is not the same w.r.t. all the teachers, but w.r.t. the same teacher on the neighboring database.\n- 4.1: The authors state: \u201cThe number n of teachers is limited by a trade-off between the classification task\u2019s complexity and the available data.\u201d However, since this tradeoff is not formalized, the statement is imprecise. In particular, if the analysis is done in the i.i.d. setting, the tradeoff would also likely depend on the relation of the target hypothesis to the data distribution.\n- Discussion of figure 3 was rather unclear in the text and caption and should be revised for clarity. In the text section, at first the explanation seems to imply that a larger gap is better (as is also indicated in the caption). However later it is stated that the gap stays under 20%. These sentences seem contradictory, which is likely not what was intended."
  },
  {
    "people": [
      "G. Jagannathan",
      "C. Monteleoni",
      "K. Pillaipakkamnatt"
    ],
    "review": "Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough.\n\nOne caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and/or clarify what can be generalized about the observed learning performance.\n\nAnother caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in order to have a meaningful comparison to the epsilon-delta values reported in related work.\n\nFinally, as has been acknowledged in the paper, the present approach may not work on other natural data types. Experiments on other data sets is strongly encouraged. Also, please cite the data sets used.\n\nOther comments:\n\nDiscussion of certain parts of the related work are thorough. However, please add some survey/discussion of the related work on differentially-private semi-supervised learning. For example, in the context of random forests, the following paper also proposed differentially-private semi-supervised learning via a teacher-learner approach (although not denoted as \u201cteacher-learner\u201d). The only time the private labeled data is used is when learning the \u201cprimary ensemble.\u201d  A \"secondary ensemble\" is then learned only from the unlabeled (non-private) data, with pseudo-labels generated by the primary ensemble.\n\nG. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy. Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013.\n\nSection C. does a nice comparison of approaches. Please make sure the quantitative results here constitute an apples-to-apples comparison with the GAN results. \n\nThe paper is extremely well-written, for the most part. Some places needing clarification include:\n- Last paragraph of 3.1. \u201call teachers\u2026.get the same training data\u2026.\u201d This should be rephrased to make it clear that it is not the same w.r.t. all the teachers, but w.r.t. the same teacher on the neighboring database.\n- 4.1: The authors state: \u201cThe number n of teachers is limited by a trade-off between the classification task\u2019s complexity and the available data.\u201d However, since this tradeoff is not formalized, the statement is imprecise. In particular, if the analysis is done in the i.i.d. setting, the tradeoff would also likely depend on the relation of the target hypothesis to the data distribution.\n- Discussion of figure 3 was rather unclear in the text and caption and should be revised for clarity. In the text section, at first the explanation seems to imply that a larger gap is better (as is also indicated in the caption). However later it is stated that the gap stays under 20%. These sentences seem contradictory, which is likely not what was intended."
  },
  {
    "people": [
      "Salimans"
    ],
    "review": "Thank you for providing an interesting paper.\n\nIn the paper, the student model is trained by semi-supervised fashion as suggested in (Salimans et al., 2016).\n\nAs far as I understand, teacher's ensembles are using for supervised learning and nonsensitive data is for unsupervised learning.\n\nSo, my question is \"Where is the generator ?\".\n\nThe aggregation of teacher network is treated as the generator in GAN framework? "
  },
  {
    "people": [
      "G. Jagannathan",
      "C. Monteleoni",
      "K. Pillaipakkamnatt"
    ],
    "review": "Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough.\n\nOne caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and/or clarify what can be generalized about the observed learning performance.\n\nAnother caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in order to have a meaningful comparison to the epsilon-delta values reported in related work.\n\nFinally, as has been acknowledged in the paper, the present approach may not work on other natural data types. Experiments on other data sets is strongly encouraged. Also, please cite the data sets used.\n\nOther comments:\n\nDiscussion of certain parts of the related work are thorough. However, please add some survey/discussion of the related work on differentially-private semi-supervised learning. For example, in the context of random forests, the following paper also proposed differentially-private semi-supervised learning via a teacher-learner approach (although not denoted as \u201cteacher-learner\u201d). The only time the private labeled data is used is when learning the \u201cprimary ensemble.\u201d  A \"secondary ensemble\" is then learned only from the unlabeled (non-private) data, with pseudo-labels generated by the primary ensemble.\n\nG. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy. Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013.\n\nSection C. does a nice comparison of approaches. Please make sure the quantitative results here constitute an apples-to-apples comparison with the GAN results. \n\nThe paper is extremely well-written, for the most part. Some places needing clarification include:\n- Last paragraph of 3.1. \u201call teachers\u2026.get the same training data\u2026.\u201d This should be rephrased to make it clear that it is not the same w.r.t. all the teachers, but w.r.t. the same teacher on the neighboring database.\n- 4.1: The authors state: \u201cThe number n of teachers is limited by a trade-off between the classification task\u2019s complexity and the available data.\u201d However, since this tradeoff is not formalized, the statement is imprecise. In particular, if the analysis is done in the i.i.d. setting, the tradeoff would also likely depend on the relation of the target hypothesis to the data distribution.\n- Discussion of figure 3 was rather unclear in the text and caption and should be revised for clarity. In the text section, at first the explanation seems to imply that a larger gap is better (as is also indicated in the caption). However later it is stated that the gap stays under 20%. These sentences seem contradictory, which is likely not what was intended."
  },
  {
    "people": [
      "Gerd Gigerenzer"
    ],
    "review": "The machine learning field has profited from hundreds of yearly publications that propose a new algorithm, which outperforms baseline algorithms and has a theoretical proof for some idealized form of the algorithm. This paper, however, is different. It does not propose a new algorithm. It incidentally uses an RL algorithm, but many other RL algorithms could have been used just as well. This paper is not about new models or algorithms.\n\nSo what is this paper about? It is an initial research step toward understanding objects and intuitive reasoning in physical worlds. Why is this important? Our best AI agents currently fail on simple control tasks and simple games, such as Montezuma\u2019s Revenge, because when they look at a screen that has a ladder, a key and a skull they don\u2019t immediately know that keys open doors, that skulls are probably hazardous and best avoided, that ladders allow us to defy gravity, etc. The understanding of physics, relations and objects enables children to solve seemingly simple problems that our best existing AI agents do not come close to begin to solve.   \n\nEndowing our agents with knowledge of objects would help enormously with planning, reasoning and exploration. Yet, this is far from trivial. First, what is an object? It turns out this is not an easy question to answer and has baffled psychologists and philosophers. To simplify matters, in this paper we restrict our attention to rigid bodies. Staring at a thing in one\u2019s hands is not enough to understand what it is. Vision or open loop perception in general is not enough. Trying to pull the thing apart to see whether it is a single entity might help. (Children do indeed love to tear things like paper apart to understand them.) Touching the thing to see what happens also helps --- perhaps it lights up and starts beeping. Further interaction might reveal that it enables us to remotely talk to someone else, to tweet, etc. Much of the knowledge gained is the result of interaction, that is perception by action. \n\nThis paper is also about designing tasks to understand how agents acquire intuitive reasoning and experimentation strategies in physical worlds. An abundant body of evidence in psychology --- see eg the works of Gerd Gigerenzer "
  },
  {
    "people": [
      "Agrawal"
    ],
    "review": "This paper purports to investigate the ability of RL agents to perform \u2018physics experiments\u2019 in an environment, to infer physical properties about the objects in that environment. The problem is very well motivated; indeed, inferring the physical properties of objects is a crucial skill for intelligent agents, and there has been relatively little work in this direction, particularly in deep RL. The paper is also well-written.\n\nAs there are no architectural or theoretical contributions of the paper (and none are claimed), the main novelty comes in the task application \u2013 using a recurrent A3C model for two tasks that simulate an agent interacting with an environment to infer physical properties of objects. More specifically, two tasks are considered \u2013 moving blocks to determine their mass, and poking towers such that they fall to determine the number of rigid bodies they are composed of. These of course represent a very limited cross-section of the prerequisite abilities for an agent to understand physics. This in itself is not a bad thing, but since there is no comparison of different (simpler) RL agents on the tasks, it is difficult to determine if the tasks selected are challenging. As mentioned in the pre-review question, the \u2018Which is Heavier\u2019 task seems quite easy due to the actuator set-up, and the fact that the model simply must learn to take the difference between successive block positions (which are directly encoded as features in most experiments).  Thus, it is not particularly surprising that the RL agent can solve the proposed tasks. \n\nThe main claim beyond solving two proposed tasks related to physics simulation is that \u201cthe agents learn different strategies for these tasks that balance the cost of gathering information against the cost of making mistakes\u201d. The \u2018cost of gathering information\u2019 is implemented by multiplying the reward with a value of gamma < 1. This is somewhat interesting behaviour, but is hardly surprising given the problem setup.\n\nOne item the authors highlight is that their approach of learning about physical object properties through interaction is different from many previous approaches, which use visual cues. However, the authors also note that this in itself is not novel, and has been explored in other work (e.g. Agrawal et al. (2016)). I think it\u2019s crucial for the authors to discuss these approaches in more detail (potentially along with removing some other, less relevant information from the related work section), and specifically highlight why the proposed tasks in this paper are interesting compared to, for example, learning to move objects towards certain end positions by poking them.\n\nTo discern the level of contribution of the paper, one must ask the following questions: \n\n1)\thow much do these two tasks contribute (above previous work) to the goal of having agents learn the properties of objects by interaction; and\n2)\thow much do the results of the RL agent on these tasks contribute to our understanding of agents that interact with their environment to learn physical properties of objects? \n\nIt is difficult to know exactly, but due to the concerns outlined above, I am not convinced that the answers to (1) or (2) are \u201cto a significant extent\u201d. In particular, for (1), since the proposed agent is able to essentially solve both tasks, it is not clear that the tasks can be used to benchmark more advanced agents (e.g. it can\u2019t be used as a set of bAbI-like tasks). \n\nAnother possible concern, as pointed out by Reviewer 3, is that the description of the model is extremely concise. It would be nice to have, for example, a diagram illustrating the inputs and outputs to the model at each time step, to ease replication.\n\nOverall, it is important to make progress towards agents that can learn to discover physical properties of their environment, and the paper contributes in this direction. However, the technical contributions of this paper are rather limited \u2013 thus, it is not clear to what extent the paper pushes forward research in this direction beyond previous work that is mentioned. It would be nice, for example, to have some discussion about the future of agents that learn physics from interaction (speculation on more difficult versions of the tasks in this paper), and how the proposed approach fits into that picture.  \n\n---------------\nEDIT: score updated, see comments below\n"
  },
  {
    "people": [
      "Gerd Gigerenzer"
    ],
    "review": "The machine learning field has profited from hundreds of yearly publications that propose a new algorithm, which outperforms baseline algorithms and has a theoretical proof for some idealized form of the algorithm. This paper, however, is different. It does not propose a new algorithm. It incidentally uses an RL algorithm, but many other RL algorithms could have been used just as well. This paper is not about new models or algorithms.\n\nSo what is this paper about? It is an initial research step toward understanding objects and intuitive reasoning in physical worlds. Why is this important? Our best AI agents currently fail on simple control tasks and simple games, such as Montezuma\u2019s Revenge, because when they look at a screen that has a ladder, a key and a skull they don\u2019t immediately know that keys open doors, that skulls are probably hazardous and best avoided, that ladders allow us to defy gravity, etc. The understanding of physics, relations and objects enables children to solve seemingly simple problems that our best existing AI agents do not come close to begin to solve.   \n\nEndowing our agents with knowledge of objects would help enormously with planning, reasoning and exploration. Yet, this is far from trivial. First, what is an object? It turns out this is not an easy question to answer and has baffled psychologists and philosophers. To simplify matters, in this paper we restrict our attention to rigid bodies. Staring at a thing in one\u2019s hands is not enough to understand what it is. Vision or open loop perception in general is not enough. Trying to pull the thing apart to see whether it is a single entity might help. (Children do indeed love to tear things like paper apart to understand them.) Touching the thing to see what happens also helps --- perhaps it lights up and starts beeping. Further interaction might reveal that it enables us to remotely talk to someone else, to tweet, etc. Much of the knowledge gained is the result of interaction, that is perception by action. \n\nThis paper is also about designing tasks to understand how agents acquire intuitive reasoning and experimentation strategies in physical worlds. An abundant body of evidence in psychology --- see eg the works of Gerd Gigerenzer "
  },
  {
    "people": [
      "Agrawal"
    ],
    "review": "This paper purports to investigate the ability of RL agents to perform \u2018physics experiments\u2019 in an environment, to infer physical properties about the objects in that environment. The problem is very well motivated; indeed, inferring the physical properties of objects is a crucial skill for intelligent agents, and there has been relatively little work in this direction, particularly in deep RL. The paper is also well-written.\n\nAs there are no architectural or theoretical contributions of the paper (and none are claimed), the main novelty comes in the task application \u2013 using a recurrent A3C model for two tasks that simulate an agent interacting with an environment to infer physical properties of objects. More specifically, two tasks are considered \u2013 moving blocks to determine their mass, and poking towers such that they fall to determine the number of rigid bodies they are composed of. These of course represent a very limited cross-section of the prerequisite abilities for an agent to understand physics. This in itself is not a bad thing, but since there is no comparison of different (simpler) RL agents on the tasks, it is difficult to determine if the tasks selected are challenging. As mentioned in the pre-review question, the \u2018Which is Heavier\u2019 task seems quite easy due to the actuator set-up, and the fact that the model simply must learn to take the difference between successive block positions (which are directly encoded as features in most experiments).  Thus, it is not particularly surprising that the RL agent can solve the proposed tasks. \n\nThe main claim beyond solving two proposed tasks related to physics simulation is that \u201cthe agents learn different strategies for these tasks that balance the cost of gathering information against the cost of making mistakes\u201d. The \u2018cost of gathering information\u2019 is implemented by multiplying the reward with a value of gamma < 1. This is somewhat interesting behaviour, but is hardly surprising given the problem setup.\n\nOne item the authors highlight is that their approach of learning about physical object properties through interaction is different from many previous approaches, which use visual cues. However, the authors also note that this in itself is not novel, and has been explored in other work (e.g. Agrawal et al. (2016)). I think it\u2019s crucial for the authors to discuss these approaches in more detail (potentially along with removing some other, less relevant information from the related work section), and specifically highlight why the proposed tasks in this paper are interesting compared to, for example, learning to move objects towards certain end positions by poking them.\n\nTo discern the level of contribution of the paper, one must ask the following questions: \n\n1)\thow much do these two tasks contribute (above previous work) to the goal of having agents learn the properties of objects by interaction; and\n2)\thow much do the results of the RL agent on these tasks contribute to our understanding of agents that interact with their environment to learn physical properties of objects? \n\nIt is difficult to know exactly, but due to the concerns outlined above, I am not convinced that the answers to (1) or (2) are \u201cto a significant extent\u201d. In particular, for (1), since the proposed agent is able to essentially solve both tasks, it is not clear that the tasks can be used to benchmark more advanced agents (e.g. it can\u2019t be used as a set of bAbI-like tasks). \n\nAnother possible concern, as pointed out by Reviewer 3, is that the description of the model is extremely concise. It would be nice to have, for example, a diagram illustrating the inputs and outputs to the model at each time step, to ease replication.\n\nOverall, it is important to make progress towards agents that can learn to discover physical properties of their environment, and the paper contributes in this direction. However, the technical contributions of this paper are rather limited \u2013 thus, it is not clear to what extent the paper pushes forward research in this direction beyond previous work that is mentioned. It would be nice, for example, to have some discussion about the future of agents that learn physics from interaction (speculation on more difficult versions of the tasks in this paper), and how the proposed approach fits into that picture.  \n\n---------------\nEDIT: score updated, see comments below\n"
  },
  {
    "people": [
      "Kim"
    ],
    "review": "This paper introduces a large-scale multi-model product classification system. The model consists of three modules, Image CNN (VGG 16 architecture), text CNN (Kim 2014) and decision-level fusion policies. The authors have tried several fusion methods: including policies taking inputs from text and image CNN probabilities; choose either CNN; average the predictions; end-to-end training. Experimental results show that text CNN alone works better than image CNN and multi-model fusion can improve the accuracy by a small margin. It is a little bit surprising that end-to-end feature level fusion works worse than text CNN alone. The writing is clear and there are a lot of useful practical experiences of learning large-scale model. However, I lean toward rejecting the paper because the following:\n\n1) No other dataset reported. The authors haven't mentioned releasing the walmart dataset and it is going to be really hard to reproduce the results without the dataset. \n2) Technical novelty is limited. All the decision-level fusion policies have been investigated by some previous methods before. \n3) Performance gain is also limited. "
  },
  {
    "people": [
      "Jung-Woo Ha",
      "Hyuna Pyo",
      "Jung-Woo Ha",
      "Jeonghee Kim",
      "Jung-Woo Ha",
      "Hyuna Pyo",
      "Jeonghee Kim"
    ],
    "review": "Hi, I am Jung-Woo Ha and the authour of a paper you cited in your work. \n\nIn the references of your manuscript, I think that \"Hyuna Pyo, Jung-Woo Ha, and Jeonghee Kim. Large-scale item categorization in e-commerce using\nmultiple recurrent neural networks. 2010.\" should be changed into \"Jung-Woo Ha, Hyuna Pyo, and Jeonghee Kim. Large-scale item categorization in e-commerce using\nmultiple recurrent neural networks. In Proceedings of KDD 2016.\"\nThe url is "
  },
  {
    "people": [
      "Kim"
    ],
    "review": "This paper introduces a large-scale multi-model product classification system. The model consists of three modules, Image CNN (VGG 16 architecture), text CNN (Kim 2014) and decision-level fusion policies. The authors have tried several fusion methods: including policies taking inputs from text and image CNN probabilities; choose either CNN; average the predictions; end-to-end training. Experimental results show that text CNN alone works better than image CNN and multi-model fusion can improve the accuracy by a small margin. It is a little bit surprising that end-to-end feature level fusion works worse than text CNN alone. The writing is clear and there are a lot of useful practical experiences of learning large-scale model. However, I lean toward rejecting the paper because the following:\n\n1) No other dataset reported. The authors haven't mentioned releasing the walmart dataset and it is going to be really hard to reproduce the results without the dataset. \n2) Technical novelty is limited. All the decision-level fusion policies have been investigated by some previous methods before. \n3) Performance gain is also limited. "
  },
  {
    "people": [
      "Jung-Woo Ha",
      "Hyuna Pyo",
      "Jung-Woo Ha",
      "Jeonghee Kim",
      "Jung-Woo Ha",
      "Hyuna Pyo",
      "Jeonghee Kim"
    ],
    "review": "Hi, I am Jung-Woo Ha and the authour of a paper you cited in your work. \n\nIn the references of your manuscript, I think that \"Hyuna Pyo, Jung-Woo Ha, and Jeonghee Kim. Large-scale item categorization in e-commerce using\nmultiple recurrent neural networks. 2010.\" should be changed into \"Jung-Woo Ha, Hyuna Pyo, and Jeonghee Kim. Large-scale item categorization in e-commerce using\nmultiple recurrent neural networks. In Proceedings of KDD 2016.\"\nThe url is "
  },
  {
    "people": [
      "Miyato"
    ],
    "review": "This work combines a LDA-type topic model with a RNN and models this by having an additive effect on the predictive distribution via the topic parameters. A variational auto-encoder is used to infer the topic distribution for a given piece of text and the RNN is trained as a RNNLM. The last hidden state of the RNNLM and the topic parameters are then concatenated to use as a feature representation.\n\nThe paper is well written and easy to understand. Using the topic as an additive effect on the vocabulary allows for easy inference but intuitively I would expect the topic to affect the dynamics too, e.g. the state of the RNN. The results on using this model as a feature extractor for IMDB are quite strong. Is the RNN fine-tuned on the labelled IMDB data? However, the results for PTB are weaker. From the original paper, an ensemble of 2 LSTMs is able to match the topicRNN score. This method of jointly modelling topics and a language model seems effective and relatively easy to implement.\n\nFinally, the IMDB result is no longer state of the art since this result appeared in May (Miyato et al., Adversarial Training Methods for Semi-Supervised Text Classification).\n\nSome questions:\nHow important is the stop word modelling? What do the results look like if l_t = 0.5 for all t?\n\nIt seems surprising that the RNN was more effective than the LSTM. Was gradient clipping tried in the topicLSTM case? Do GRUs also fail to work?\n\nIt is also unfortunate that the model requires a stop-word list. Is the link in footnote 4 the one that is used in the experiments?\n\nDoes factoring out the topics in this way allow the RNN to scale better with more neurons? How reasonable does the topic distribution look for individual documents? How peaked do they tend to be? Can you show some examples of the inferred distribution? The topics look odd for IMDB with the top word of two of the topics being the same: 'campbell'. It would be interesting to compare these topics with those inferred by LDA on the same datasets.\n\nMinor comments:\nBelow figure 2: GHz -> GB\n\\Gamma is not defined."
  },
  {
    "people": [
      "Hanna Wallach",
      "Amit Gruber"
    ],
    "review": "Though the have been attempts to incorporate both \"topic-like\" and \"sequence-like\" methods in the past (e.g, the work of Hanna Wallach, Amit Gruber and other), they were quite computationally expensive, especially when high-order ngrams are incorporated. This is a modern take on this challenge: using RNNs and the VAE / inference network framework. The results are quite convincing, and the paper is well written.\n \n Pros:\n -- clean and simple model\n -- sufficiently convincing experimentation\n \n Cons:\n -- other ways to model interaction between RNN and topic representation could be considered (see comments of R2 and R1)"
  },
  {
    "people": [
      "Miyato"
    ],
    "review": "This work combines a LDA-type topic model with a RNN and models this by having an additive effect on the predictive distribution via the topic parameters. A variational auto-encoder is used to infer the topic distribution for a given piece of text and the RNN is trained as a RNNLM. The last hidden state of the RNNLM and the topic parameters are then concatenated to use as a feature representation.\n\nThe paper is well written and easy to understand. Using the topic as an additive effect on the vocabulary allows for easy inference but intuitively I would expect the topic to affect the dynamics too, e.g. the state of the RNN. The results on using this model as a feature extractor for IMDB are quite strong. Is the RNN fine-tuned on the labelled IMDB data? However, the results for PTB are weaker. From the original paper, an ensemble of 2 LSTMs is able to match the topicRNN score. This method of jointly modelling topics and a language model seems effective and relatively easy to implement.\n\nFinally, the IMDB result is no longer state of the art since this result appeared in May (Miyato et al., Adversarial Training Methods for Semi-Supervised Text Classification).\n\nSome questions:\nHow important is the stop word modelling? What do the results look like if l_t = 0.5 for all t?\n\nIt seems surprising that the RNN was more effective than the LSTM. Was gradient clipping tried in the topicLSTM case? Do GRUs also fail to work?\n\nIt is also unfortunate that the model requires a stop-word list. Is the link in footnote 4 the one that is used in the experiments?\n\nDoes factoring out the topics in this way allow the RNN to scale better with more neurons? How reasonable does the topic distribution look for individual documents? How peaked do they tend to be? Can you show some examples of the inferred distribution? The topics look odd for IMDB with the top word of two of the topics being the same: 'campbell'. It would be interesting to compare these topics with those inferred by LDA on the same datasets.\n\nMinor comments:\nBelow figure 2: GHz -> GB\n\\Gamma is not defined."
  },
  {
    "people": [
      "Miyato"
    ],
    "review": "This work combines a LDA-type topic model with a RNN and models this by having an additive effect on the predictive distribution via the topic parameters. A variational auto-encoder is used to infer the topic distribution for a given piece of text and the RNN is trained as a RNNLM. The last hidden state of the RNNLM and the topic parameters are then concatenated to use as a feature representation.\n\nThe paper is well written and easy to understand. Using the topic as an additive effect on the vocabulary allows for easy inference but intuitively I would expect the topic to affect the dynamics too, e.g. the state of the RNN. The results on using this model as a feature extractor for IMDB are quite strong. Is the RNN fine-tuned on the labelled IMDB data? However, the results for PTB are weaker. From the original paper, an ensemble of 2 LSTMs is able to match the topicRNN score. This method of jointly modelling topics and a language model seems effective and relatively easy to implement.\n\nFinally, the IMDB result is no longer state of the art since this result appeared in May (Miyato et al., Adversarial Training Methods for Semi-Supervised Text Classification).\n\nSome questions:\nHow important is the stop word modelling? What do the results look like if l_t = 0.5 for all t?\n\nIt seems surprising that the RNN was more effective than the LSTM. Was gradient clipping tried in the topicLSTM case? Do GRUs also fail to work?\n\nIt is also unfortunate that the model requires a stop-word list. Is the link in footnote 4 the one that is used in the experiments?\n\nDoes factoring out the topics in this way allow the RNN to scale better with more neurons? How reasonable does the topic distribution look for individual documents? How peaked do they tend to be? Can you show some examples of the inferred distribution? The topics look odd for IMDB with the top word of two of the topics being the same: 'campbell'. It would be interesting to compare these topics with those inferred by LDA on the same datasets.\n\nMinor comments:\nBelow figure 2: GHz -> GB\n\\Gamma is not defined."
  },
  {
    "people": [
      "Hanna Wallach",
      "Amit Gruber"
    ],
    "review": "Though the have been attempts to incorporate both \"topic-like\" and \"sequence-like\" methods in the past (e.g, the work of Hanna Wallach, Amit Gruber and other), they were quite computationally expensive, especially when high-order ngrams are incorporated. This is a modern take on this challenge: using RNNs and the VAE / inference network framework. The results are quite convincing, and the paper is well written.\n \n Pros:\n -- clean and simple model\n -- sufficiently convincing experimentation\n \n Cons:\n -- other ways to model interaction between RNN and topic representation could be considered (see comments of R2 and R1)"
  },
  {
    "people": [
      "Miyato"
    ],
    "review": "This work combines a LDA-type topic model with a RNN and models this by having an additive effect on the predictive distribution via the topic parameters. A variational auto-encoder is used to infer the topic distribution for a given piece of text and the RNN is trained as a RNNLM. The last hidden state of the RNNLM and the topic parameters are then concatenated to use as a feature representation.\n\nThe paper is well written and easy to understand. Using the topic as an additive effect on the vocabulary allows for easy inference but intuitively I would expect the topic to affect the dynamics too, e.g. the state of the RNN. The results on using this model as a feature extractor for IMDB are quite strong. Is the RNN fine-tuned on the labelled IMDB data? However, the results for PTB are weaker. From the original paper, an ensemble of 2 LSTMs is able to match the topicRNN score. This method of jointly modelling topics and a language model seems effective and relatively easy to implement.\n\nFinally, the IMDB result is no longer state of the art since this result appeared in May (Miyato et al., Adversarial Training Methods for Semi-Supervised Text Classification).\n\nSome questions:\nHow important is the stop word modelling? What do the results look like if l_t = 0.5 for all t?\n\nIt seems surprising that the RNN was more effective than the LSTM. Was gradient clipping tried in the topicLSTM case? Do GRUs also fail to work?\n\nIt is also unfortunate that the model requires a stop-word list. Is the link in footnote 4 the one that is used in the experiments?\n\nDoes factoring out the topics in this way allow the RNN to scale better with more neurons? How reasonable does the topic distribution look for individual documents? How peaked do they tend to be? Can you show some examples of the inferred distribution? The topics look odd for IMDB with the top word of two of the topics being the same: 'campbell'. It would be interesting to compare these topics with those inferred by LDA on the same datasets.\n\nMinor comments:\nBelow figure 2: GHz -> GB\n\\Gamma is not defined."
  },
  {
    "people": [
      "Kurakin"
    ],
    "review": "I reviewed the manuscript on December 5th.\n\nSummary:\nThe authors investigate the phenomenon of adversarial perturbations and ask whether one may build a system to independently detect an adversarial data point -- if one could detect an adversarial example, then might prevent a machine from automatically processing it. Importantly, the authors investigate whether it is possible to build an adversarial detector which is resilient to adversarial examples built against *both* the classifier and the detector. Their results suggest that training a detector in this more difficult setting still yields gains but does not entirely resolve the problem of detecting adversarial examples.\n\nMajor comments:\n\nThe authors describe a novel approach for dealing with adversarial examples from a security standpoint -- namely, build an independent system to detect the adversary so a human might intervene in those cases. \n\nA potential confound of this approach is that an adversary might respond by constructing adversarial examples to fool *both* the original classifier and the new detector. If that were possible, then this approach is moot since an attacker could always outwit the original system. To their credit, the authors show that building a 'dynamic' detector to detect adversarial examples but also be resilient to an adversary mitigates this potential escalation (worse case from 55% to 70% detection rate). Even though the 'dynamic' detector  demonstrates positive gains, I am concerned about overall scores. Detecting adversarial examples at this rate would not be a reliable security procedure.\n\nMy second comment is about 'model transferability'. My definition of 'model transferability' is different then the one used in the paper. My definition means that one constructs an adversarial example on one network and measures how well the adversarial examples attack a second trained model -- where the second model has been trained with different initial conditions. (The author's definition of 'transferability' is based on seeing how well the detector generalizes across training methods). 'Model transferability' (per my definition) is quite important because it measures how general an adversarial example is across all models -- and not specific to a given trained model. Different methods have different levels of 'model transferability' (Kurakin et al, 2016) and I am concerned how well the detector they built would be able to detect adversarial examples across *all models* and not just the trained model in question. In other words, a good detector would be able to detect adversarial examples from any network and not just one particularly trained network. This question seems largely unaddressed in this paper but perhaps I missed some subtle point in their descriptions.\n\nMinor comments:\n\nIf there were any points in the bottom-left of the Figure 2 left, then this would be very important to see -- perhaps move the legend to highlight if the area contains no points.\n\n- X-axis label is wrong in Figure 2 right.\n\nMeasure the transferability of the detector?\n\n- How is \\sigma labeled on Figure 5?\n\n- Whenever an image is constructed to be an 'adversary', has the image actually been tested to see if it is adversarial? In other words, does the adversarial image actually result in a misclassification by the original network?"
  },
  {
    "people": [
      "Kurakin"
    ],
    "review": "I reviewed the manuscript on December 5th.\n\nSummary:\nThe authors investigate the phenomenon of adversarial perturbations and ask whether one may build a system to independently detect an adversarial data point -- if one could detect an adversarial example, then might prevent a machine from automatically processing it. Importantly, the authors investigate whether it is possible to build an adversarial detector which is resilient to adversarial examples built against *both* the classifier and the detector. Their results suggest that training a detector in this more difficult setting still yields gains but does not entirely resolve the problem of detecting adversarial examples.\n\nMajor comments:\n\nThe authors describe a novel approach for dealing with adversarial examples from a security standpoint -- namely, build an independent system to detect the adversary so a human might intervene in those cases. \n\nA potential confound of this approach is that an adversary might respond by constructing adversarial examples to fool *both* the original classifier and the new detector. If that were possible, then this approach is moot since an attacker could always outwit the original system. To their credit, the authors show that building a 'dynamic' detector to detect adversarial examples but also be resilient to an adversary mitigates this potential escalation (worse case from 55% to 70% detection rate). Even though the 'dynamic' detector  demonstrates positive gains, I am concerned about overall scores. Detecting adversarial examples at this rate would not be a reliable security procedure.\n\nMy second comment is about 'model transferability'. My definition of 'model transferability' is different then the one used in the paper. My definition means that one constructs an adversarial example on one network and measures how well the adversarial examples attack a second trained model -- where the second model has been trained with different initial conditions. (The author's definition of 'transferability' is based on seeing how well the detector generalizes across training methods). 'Model transferability' (per my definition) is quite important because it measures how general an adversarial example is across all models -- and not specific to a given trained model. Different methods have different levels of 'model transferability' (Kurakin et al, 2016) and I am concerned how well the detector they built would be able to detect adversarial examples across *all models* and not just the trained model in question. In other words, a good detector would be able to detect adversarial examples from any network and not just one particularly trained network. This question seems largely unaddressed in this paper but perhaps I missed some subtle point in their descriptions.\n\nMinor comments:\n\nIf there were any points in the bottom-left of the Figure 2 left, then this would be very important to see -- perhaps move the legend to highlight if the area contains no points.\n\n- X-axis label is wrong in Figure 2 right.\n\nMeasure the transferability of the detector?\n\n- How is \\sigma labeled on Figure 5?\n\n- Whenever an image is constructed to be an 'adversary', has the image actually been tested to see if it is adversarial? In other words, does the adversarial image actually result in a misclassification by the original network?"
  },
  {
    "people": [
      "Kurakin"
    ],
    "review": "I reviewed the manuscript on December 5th.\n\nSummary:\nThe authors investigate the phenomenon of adversarial perturbations and ask whether one may build a system to independently detect an adversarial data point -- if one could detect an adversarial example, then might prevent a machine from automatically processing it. Importantly, the authors investigate whether it is possible to build an adversarial detector which is resilient to adversarial examples built against *both* the classifier and the detector. Their results suggest that training a detector in this more difficult setting still yields gains but does not entirely resolve the problem of detecting adversarial examples.\n\nMajor comments:\n\nThe authors describe a novel approach for dealing with adversarial examples from a security standpoint -- namely, build an independent system to detect the adversary so a human might intervene in those cases. \n\nA potential confound of this approach is that an adversary might respond by constructing adversarial examples to fool *both* the original classifier and the new detector. If that were possible, then this approach is moot since an attacker could always outwit the original system. To their credit, the authors show that building a 'dynamic' detector to detect adversarial examples but also be resilient to an adversary mitigates this potential escalation (worse case from 55% to 70% detection rate). Even though the 'dynamic' detector  demonstrates positive gains, I am concerned about overall scores. Detecting adversarial examples at this rate would not be a reliable security procedure.\n\nMy second comment is about 'model transferability'. My definition of 'model transferability' is different then the one used in the paper. My definition means that one constructs an adversarial example on one network and measures how well the adversarial examples attack a second trained model -- where the second model has been trained with different initial conditions. (The author's definition of 'transferability' is based on seeing how well the detector generalizes across training methods). 'Model transferability' (per my definition) is quite important because it measures how general an adversarial example is across all models -- and not specific to a given trained model. Different methods have different levels of 'model transferability' (Kurakin et al, 2016) and I am concerned how well the detector they built would be able to detect adversarial examples across *all models* and not just the trained model in question. In other words, a good detector would be able to detect adversarial examples from any network and not just one particularly trained network. This question seems largely unaddressed in this paper but perhaps I missed some subtle point in their descriptions.\n\nMinor comments:\n\nIf there were any points in the bottom-left of the Figure 2 left, then this would be very important to see -- perhaps move the legend to highlight if the area contains no points.\n\n- X-axis label is wrong in Figure 2 right.\n\nMeasure the transferability of the detector?\n\n- How is \\sigma labeled on Figure 5?\n\n- Whenever an image is constructed to be an 'adversary', has the image actually been tested to see if it is adversarial? In other words, does the adversarial image actually result in a misclassification by the original network?"
  },
  {
    "people": [
      "Kurakin"
    ],
    "review": "I reviewed the manuscript on December 5th.\n\nSummary:\nThe authors investigate the phenomenon of adversarial perturbations and ask whether one may build a system to independently detect an adversarial data point -- if one could detect an adversarial example, then might prevent a machine from automatically processing it. Importantly, the authors investigate whether it is possible to build an adversarial detector which is resilient to adversarial examples built against *both* the classifier and the detector. Their results suggest that training a detector in this more difficult setting still yields gains but does not entirely resolve the problem of detecting adversarial examples.\n\nMajor comments:\n\nThe authors describe a novel approach for dealing with adversarial examples from a security standpoint -- namely, build an independent system to detect the adversary so a human might intervene in those cases. \n\nA potential confound of this approach is that an adversary might respond by constructing adversarial examples to fool *both* the original classifier and the new detector. If that were possible, then this approach is moot since an attacker could always outwit the original system. To their credit, the authors show that building a 'dynamic' detector to detect adversarial examples but also be resilient to an adversary mitigates this potential escalation (worse case from 55% to 70% detection rate). Even though the 'dynamic' detector  demonstrates positive gains, I am concerned about overall scores. Detecting adversarial examples at this rate would not be a reliable security procedure.\n\nMy second comment is about 'model transferability'. My definition of 'model transferability' is different then the one used in the paper. My definition means that one constructs an adversarial example on one network and measures how well the adversarial examples attack a second trained model -- where the second model has been trained with different initial conditions. (The author's definition of 'transferability' is based on seeing how well the detector generalizes across training methods). 'Model transferability' (per my definition) is quite important because it measures how general an adversarial example is across all models -- and not specific to a given trained model. Different methods have different levels of 'model transferability' (Kurakin et al, 2016) and I am concerned how well the detector they built would be able to detect adversarial examples across *all models* and not just the trained model in question. In other words, a good detector would be able to detect adversarial examples from any network and not just one particularly trained network. This question seems largely unaddressed in this paper but perhaps I missed some subtle point in their descriptions.\n\nMinor comments:\n\nIf there were any points in the bottom-left of the Figure 2 left, then this would be very important to see -- perhaps move the legend to highlight if the area contains no points.\n\n- X-axis label is wrong in Figure 2 right.\n\nMeasure the transferability of the detector?\n\n- How is \\sigma labeled on Figure 5?\n\n- Whenever an image is constructed to be an 'adversary', has the image actually been tested to see if it is adversarial? In other words, does the adversarial image actually result in a misclassification by the original network?"
  },
  {
    "people": [
      "Alexey Dosoviskiy",
      "Thomas Brox"
    ],
    "review": "This work is really interesting, but I find the network architecture and loss functions in this work are extremely similar to that in the work \"Generating Images with Perceptual Similarity Metrics based on Deep Networks\" by Alexey Dosoviskiy and Thomas Brox.  \n\nThe loss function in this work vs that of the latter: \nL_const is equal to L_feat;\nL_tid is equall to L_img;\nL_GANG is equal to L_adv.\n\nHowever, I do not see the later's name in your reference paper list.  Besides, have you forgotten to print a 'log' before D_3(x) in your L_D loss  on page 4?"
  },
  {
    "people": [
      "Alexey Dosoviskiy",
      "Thomas Brox"
    ],
    "review": "This work is really interesting, but I find the network architecture and loss functions in this work are extremely similar to that in the work \"Generating Images with Perceptual Similarity Metrics based on Deep Networks\" by Alexey Dosoviskiy and Thomas Brox.  \n\nThe loss function in this work vs that of the latter: \nL_const is equal to L_feat;\nL_tid is equall to L_img;\nL_GANG is equal to L_adv.\n\nHowever, I do not see the later's name in your reference paper list.  Besides, have you forgotten to print a 'log' before D_3(x) in your L_D loss  on page 4?"
  },
  {
    "people": [
      "Piech"
    ],
    "review": "This paper proposes a model that is able to infer a program from input/output example pairs, focusing on a restricted domain-specific language that captures a fairly wide variety of string transformations, similar to that used by Flash Fill in Excel.  The approach is to model successive \u201cextensions\u201d of a program tree conditioned on some embedding of the input/output pairs.  Extension probabilities are computed as a function of leaf and production rule embeddings \u2014 one of the main contributions is the so-called \u201cRecursive-Reverse-Recursive Neural Net\u201d which computes a globally aware embedding of a leaf by doing something that looks like belief propagation on a tree (but training this operation in an end-to-end differentiable way).\n\nThere are many strong points about this paper.  In contrast with some of the related work in the deep learning community, I can imagine this being used in an actual application in the near future.  The R3NN idea is a good one and the authors motivate it quite well.  Moreover, the authors have explored many variants of this model to understand what works well and what does not.  Finally, the exposition is clear (even if it is a long paper), which made this paper a pleasure to read.  Some weaknesses of this paper: the results are still not super accurate, perhaps because the model has only been trained on small programs but is being asked to infer programs that should be much longer.  And it\u2019s unclear why the authors did not simply train on longer programs\u2026  It also seems that the number of I/O pairs is fixed?  So if I had more I/O pairs, the model might not be able to use those additional pairs (and based on the experiments, more pairs can hurt\u2026).  Overall however, I would certainly like to see this paper accepted at ICLR.\n\nOther miscellaneous comments:\n* Too many e\u2019s in the expansion probability expression \u2014 might be better just to write \u201cSoftmax\u201d.\n* There is a comment about adding a bidirectional LSTM to process the global leaf representations before calculating scores, but no details are given on how this is done (as far as I can see).\n* The authors claim that using hyperbolic tangent activation functions is important \u2014 I\u2019d be interested in some more discussion on this and why something like ReLU would not be good.\n* It\u2019s unclear to me how batching was done in this setting since each program has a different tree topology.  More discussion on this would be appreciated.  Related to this, it would be good to add details on optimization algorithm (SGD?  Adagrad?  Adam?), learning rate schedules and how weights were initialized.  At the moment, the results are not particularly reproducible.\n* In Figure 6 (unsolved benchmarks), it would be great to add the program sizes for these harder examples (i.e., did the approach fail because these benchmarks require long programs?  Or was it some other reason?)\n* There is a missing related work by Piech et al (Learning Program Embeddings\u2026) where the authors trained a recursive neural network (that matched abstract syntax trees for programs submitted to an online course) to predict program output (but did not synthesize programs)."
  },
  {
    "people": [
      "Piech"
    ],
    "review": "This paper proposes a model that is able to infer a program from input/output example pairs, focusing on a restricted domain-specific language that captures a fairly wide variety of string transformations, similar to that used by Flash Fill in Excel.  The approach is to model successive \u201cextensions\u201d of a program tree conditioned on some embedding of the input/output pairs.  Extension probabilities are computed as a function of leaf and production rule embeddings \u2014 one of the main contributions is the so-called \u201cRecursive-Reverse-Recursive Neural Net\u201d which computes a globally aware embedding of a leaf by doing something that looks like belief propagation on a tree (but training this operation in an end-to-end differentiable way).\n\nThere are many strong points about this paper.  In contrast with some of the related work in the deep learning community, I can imagine this being used in an actual application in the near future.  The R3NN idea is a good one and the authors motivate it quite well.  Moreover, the authors have explored many variants of this model to understand what works well and what does not.  Finally, the exposition is clear (even if it is a long paper), which made this paper a pleasure to read.  Some weaknesses of this paper: the results are still not super accurate, perhaps because the model has only been trained on small programs but is being asked to infer programs that should be much longer.  And it\u2019s unclear why the authors did not simply train on longer programs\u2026  It also seems that the number of I/O pairs is fixed?  So if I had more I/O pairs, the model might not be able to use those additional pairs (and based on the experiments, more pairs can hurt\u2026).  Overall however, I would certainly like to see this paper accepted at ICLR.\n\nOther miscellaneous comments:\n* Too many e\u2019s in the expansion probability expression \u2014 might be better just to write \u201cSoftmax\u201d.\n* There is a comment about adding a bidirectional LSTM to process the global leaf representations before calculating scores, but no details are given on how this is done (as far as I can see).\n* The authors claim that using hyperbolic tangent activation functions is important \u2014 I\u2019d be interested in some more discussion on this and why something like ReLU would not be good.\n* It\u2019s unclear to me how batching was done in this setting since each program has a different tree topology.  More discussion on this would be appreciated.  Related to this, it would be good to add details on optimization algorithm (SGD?  Adagrad?  Adam?), learning rate schedules and how weights were initialized.  At the moment, the results are not particularly reproducible.\n* In Figure 6 (unsolved benchmarks), it would be great to add the program sizes for these harder examples (i.e., did the approach fail because these benchmarks require long programs?  Or was it some other reason?)\n* There is a missing related work by Piech et al (Learning Program Embeddings\u2026) where the authors trained a recursive neural network (that matched abstract syntax trees for programs submitted to an online course) to predict program output (but did not synthesize programs).\n"
  },
  {
    "people": [
      "Piech"
    ],
    "review": "This paper proposes a model that is able to infer a program from input/output example pairs, focusing on a restricted domain-specific language that captures a fairly wide variety of string transformations, similar to that used by Flash Fill in Excel.  The approach is to model successive \u201cextensions\u201d of a program tree conditioned on some embedding of the input/output pairs.  Extension probabilities are computed as a function of leaf and production rule embeddings \u2014 one of the main contributions is the so-called \u201cRecursive-Reverse-Recursive Neural Net\u201d which computes a globally aware embedding of a leaf by doing something that looks like belief propagation on a tree (but training this operation in an end-to-end differentiable way).\n\nThere are many strong points about this paper.  In contrast with some of the related work in the deep learning community, I can imagine this being used in an actual application in the near future.  The R3NN idea is a good one and the authors motivate it quite well.  Moreover, the authors have explored many variants of this model to understand what works well and what does not.  Finally, the exposition is clear (even if it is a long paper), which made this paper a pleasure to read.  Some weaknesses of this paper: the results are still not super accurate, perhaps because the model has only been trained on small programs but is being asked to infer programs that should be much longer.  And it\u2019s unclear why the authors did not simply train on longer programs\u2026  It also seems that the number of I/O pairs is fixed?  So if I had more I/O pairs, the model might not be able to use those additional pairs (and based on the experiments, more pairs can hurt\u2026).  Overall however, I would certainly like to see this paper accepted at ICLR.\n\nOther miscellaneous comments:\n* Too many e\u2019s in the expansion probability expression \u2014 might be better just to write \u201cSoftmax\u201d.\n* There is a comment about adding a bidirectional LSTM to process the global leaf representations before calculating scores, but no details are given on how this is done (as far as I can see).\n* The authors claim that using hyperbolic tangent activation functions is important \u2014 I\u2019d be interested in some more discussion on this and why something like ReLU would not be good.\n* It\u2019s unclear to me how batching was done in this setting since each program has a different tree topology.  More discussion on this would be appreciated.  Related to this, it would be good to add details on optimization algorithm (SGD?  Adagrad?  Adam?), learning rate schedules and how weights were initialized.  At the moment, the results are not particularly reproducible.\n* In Figure 6 (unsolved benchmarks), it would be great to add the program sizes for these harder examples (i.e., did the approach fail because these benchmarks require long programs?  Or was it some other reason?)\n* There is a missing related work by Piech et al (Learning Program Embeddings\u2026) where the authors trained a recursive neural network (that matched abstract syntax trees for programs submitted to an online course) to predict program output (but did not synthesize programs)."
  },
  {
    "people": [
      "Piech"
    ],
    "review": "This paper proposes a model that is able to infer a program from input/output example pairs, focusing on a restricted domain-specific language that captures a fairly wide variety of string transformations, similar to that used by Flash Fill in Excel.  The approach is to model successive \u201cextensions\u201d of a program tree conditioned on some embedding of the input/output pairs.  Extension probabilities are computed as a function of leaf and production rule embeddings \u2014 one of the main contributions is the so-called \u201cRecursive-Reverse-Recursive Neural Net\u201d which computes a globally aware embedding of a leaf by doing something that looks like belief propagation on a tree (but training this operation in an end-to-end differentiable way).\n\nThere are many strong points about this paper.  In contrast with some of the related work in the deep learning community, I can imagine this being used in an actual application in the near future.  The R3NN idea is a good one and the authors motivate it quite well.  Moreover, the authors have explored many variants of this model to understand what works well and what does not.  Finally, the exposition is clear (even if it is a long paper), which made this paper a pleasure to read.  Some weaknesses of this paper: the results are still not super accurate, perhaps because the model has only been trained on small programs but is being asked to infer programs that should be much longer.  And it\u2019s unclear why the authors did not simply train on longer programs\u2026  It also seems that the number of I/O pairs is fixed?  So if I had more I/O pairs, the model might not be able to use those additional pairs (and based on the experiments, more pairs can hurt\u2026).  Overall however, I would certainly like to see this paper accepted at ICLR.\n\nOther miscellaneous comments:\n* Too many e\u2019s in the expansion probability expression \u2014 might be better just to write \u201cSoftmax\u201d.\n* There is a comment about adding a bidirectional LSTM to process the global leaf representations before calculating scores, but no details are given on how this is done (as far as I can see).\n* The authors claim that using hyperbolic tangent activation functions is important \u2014 I\u2019d be interested in some more discussion on this and why something like ReLU would not be good.\n* It\u2019s unclear to me how batching was done in this setting since each program has a different tree topology.  More discussion on this would be appreciated.  Related to this, it would be good to add details on optimization algorithm (SGD?  Adagrad?  Adam?), learning rate schedules and how weights were initialized.  At the moment, the results are not particularly reproducible.\n* In Figure 6 (unsolved benchmarks), it would be great to add the program sizes for these harder examples (i.e., did the approach fail because these benchmarks require long programs?  Or was it some other reason?)\n* There is a missing related work by Piech et al (Learning Program Embeddings\u2026) where the authors trained a recursive neural network (that matched abstract syntax trees for programs submitted to an online course) to predict program output (but did not synthesize programs).\n"
  },
  {
    "people": [
      "Hawkes"
    ],
    "review": "A nice paper, with sufficient experimental validation, and the idea of incorporating a form of change point detection is good. However, the technical contribution relative to the NIPS paper by the same authors is not significant, in that it primarily involves using an RNN instead of a Hawkes process to model the temporal dynamics. The results are significantly better than this earlier paper -- the authors should explore if this is due only to the RNN, or to the optimization method."
  },
  {
    "people": [
      "Chen",
      "Wang"
    ],
    "review": "Dear reviewers, we have revised our paper according to your insightful suggestions and comments.\n\n1) We highlight the importance and difficulty of modeling the nonlinearity in the point process models in the introduction part. \n\n2) We added the discussion with Chen et.al ICML 2013 and a detailed comparison with Wang et.al NIPS 2016.\n \n3) We added the experiment on the large Yelp dataset, which contains 1,005 users and 47,924 items. We run our algorithm and all the baselines on this one. The result of our algorithm is consistently better than alternatives. \n\n4) We added the section 6.4.1 explaining the quantitative results we get in the experiment. Specifically, we studied the performance of different history length of users, and how the diversity of tastes of users affect the results by visualizing the user-item interaction graph. \n\n5) We also added the section 6.4.2 that quantitatively compare the effect of different history length on Yelp dataset. \n"
  },
  {
    "people": [
      "Wang"
    ],
    "review": "This paper proposes a method to model time changing dynamics in collaborative filtering.\nComments:\n1) The main idea of the paper is build upon similar to a previous work by the same group of author (Wang et.al KDD), the major difference appears to be change some of the latent factors to be RNN\n2) The author describes a BPTT technique to train the model  \n3) The author introduced time prediction as a new metric to evaluate the effectiveness of time dependent model. However, this need to be condition on a given user-item pair.\n4) It would be interesting to consider other metrics, for example\n- The switching time where a user changes his/her to another item \n- Jointly predict the next item and switching time. \nIn summary, this is a paper that improves over an existing work on time dynamics model in recommender system. The time prediction metric is interesting and opens up interesting discussion on how we should evaluate recommender systems when time is involved (see also comments).\n\n \n"
  },
  {
    "people": [
      "Hawkes"
    ],
    "review": "A nice paper, with sufficient experimental validation, and the idea of incorporating a form of change point detection is good. However, the technical contribution relative to the NIPS paper by the same authors is not significant, in that it primarily involves using an RNN instead of a Hawkes process to model the temporal dynamics. The results are significantly better than this earlier paper -- the authors should explore if this is due only to the RNN, or to the optimization method."
  },
  {
    "people": [
      "Chen",
      "Wang"
    ],
    "review": "Dear reviewers, we have revised our paper according to your insightful suggestions and comments.\n\n1) We highlight the importance and difficulty of modeling the nonlinearity in the point process models in the introduction part. \n\n2) We added the discussion with Chen et.al ICML 2013 and a detailed comparison with Wang et.al NIPS 2016.\n \n3) We added the experiment on the large Yelp dataset, which contains 1,005 users and 47,924 items. We run our algorithm and all the baselines on this one. The result of our algorithm is consistently better than alternatives. \n\n4) We added the section 6.4.1 explaining the quantitative results we get in the experiment. Specifically, we studied the performance of different history length of users, and how the diversity of tastes of users affect the results by visualizing the user-item interaction graph. \n\n5) We also added the section 6.4.2 that quantitatively compare the effect of different history length on Yelp dataset. \n"
  },
  {
    "people": [
      "Wang"
    ],
    "review": "This paper proposes a method to model time changing dynamics in collaborative filtering.\nComments:\n1) The main idea of the paper is build upon similar to a previous work by the same group of author (Wang et.al KDD), the major difference appears to be change some of the latent factors to be RNN\n2) The author describes a BPTT technique to train the model  \n3) The author introduced time prediction as a new metric to evaluate the effectiveness of time dependent model. However, this need to be condition on a given user-item pair.\n4) It would be interesting to consider other metrics, for example\n- The switching time where a user changes his/her to another item \n- Jointly predict the next item and switching time. \nIn summary, this is a paper that improves over an existing work on time dynamics model in recommender system. The time prediction metric is interesting and opens up interesting discussion on how we should evaluate recommender systems when time is involved (see also comments).\n\n \n"
  },
  {
    "people": [
      "Magoulas"
    ],
    "review": "This paper proposes a boosting based ensemble procedure for residual networks by adopting the Deep Incremental Boosting method that was used for CNN's(Mosca & Magoulas, 2016a). At each step t, a new block of layers are added to the network at a position p_t and the weights of all layers are copied to the current network to speed up training.\n\nThe method is not sufficiently novel since the steps of Deep Incremental Boosting are slightly adopted. Instead of adding a layer to the end of the network, this version adds a block of layers to a position p_t (starts at a selected position p_0) and merges layer accordingly hence slightly adopts DIB. \n\nThe empirical analysis does not use any data-augmentation. It is not clear whether the improvements (if there is) of the ensemble disappear after data-augmentation.  Also, one of the main baselines, DIB has no-skip connections therefore this can negatively affect the fair comparison. The authors argue that they did not involve state of art Res Nets since their analysis focuses on the ensemble approach, however any potential improvement of the ensemble can be compensated with an inherent feature of Res Net variant. The boosting procedure can be computationally restrictive in case of ImageNet training and Res Net variants may perform much better in that case too. Therefore the baselines should include the state of art Res Nets and Dense Convolutional networks hence current results are preliminary.\n\nIn addition, it is not clear how sensitive the boosting to the selection of injection point.\n\nThis paper adopts DIB to Res Nets and provides some empirical analysis however the contribution is not sufficiently novel and the empirical results are not satisfactory for demonstrating that the method is significant.\n\nPros\n-provides some preliminary results for boosting of Res Nets\nCons\n-not sufficiently novel: an incremental approach \n-empirical analysis is not satisfactory"
  },
  {
    "people": [
      "Springenberg"
    ],
    "review": "The paper under consideration proposes a set of procedures for incrementally expanding a residual network by adding layers via a boosting criterion.\n\nThe main barrier to publication is the weak empirical validation. The tasks considered are quite small scale in 2016 (and MNIST with a convolutional net is basically an uninteresting test by this point). The paper doesn't compare to the literature, and CIFAR-10 results fail to improve upon rather simple, single-network published baselines (Springenberg et al, 2015 for example, obtains 92% without data augmentation) and I'm pretty sure there's a simple ResNet result somewhere that outshines these too. The CIFAR100 results are a little bit interesting as they are better than I'm used to seeing (I haven't done a recent literature crawl), and this is unsurprising -- you'd expect ensembles to do well when there's a dearth of labeled training data, and here there are only a few hundred per label. But then it's typical on both CIFAR10 and CIFAR100 to use simple data augmentation schemes which aren't employed here, and these and other forms of regularization are a simpler alternative to a complicated iterative augmentation scheme like this.\n\nIt'd be easier to sell this method either as an option for scarce labeled datasets where data augmentation is non-trivial (but then for most image-related applications, random crops and reflections are easy and valid), but that would necessitate different benchmarks, and comparison against simpler methods like said data augmentation, dropout (especially, due to the ensemble interpretation), and so on."
  },
  {
    "people": [
      "Magoulas"
    ],
    "review": "This paper proposes a boosting based ensemble procedure for residual networks by adopting the Deep Incremental Boosting method that was used for CNN's(Mosca & Magoulas, 2016a). At each step t, a new block of layers are added to the network at a position p_t and the weights of all layers are copied to the current network to speed up training.\n\nThe method is not sufficiently novel since the steps of Deep Incremental Boosting are slightly adopted. Instead of adding a layer to the end of the network, this version adds a block of layers to a position p_t (starts at a selected position p_0) and merges layer accordingly hence slightly adopts DIB. \n\nThe empirical analysis does not use any data-augmentation. It is not clear whether the improvements (if there is) of the ensemble disappear after data-augmentation.  Also, one of the main baselines, DIB has no-skip connections therefore this can negatively affect the fair comparison. The authors argue that they did not involve state of art Res Nets since their analysis focuses on the ensemble approach, however any potential improvement of the ensemble can be compensated with an inherent feature of Res Net variant. The boosting procedure can be computationally restrictive in case of ImageNet training and Res Net variants may perform much better in that case too. Therefore the baselines should include the state of art Res Nets and Dense Convolutional networks hence current results are preliminary.\n\nIn addition, it is not clear how sensitive the boosting to the selection of injection point.\n\nThis paper adopts DIB to Res Nets and provides some empirical analysis however the contribution is not sufficiently novel and the empirical results are not satisfactory for demonstrating that the method is significant.\n\nPros\n-provides some preliminary results for boosting of Res Nets\nCons\n-not sufficiently novel: an incremental approach \n-empirical analysis is not satisfactory"
  },
  {
    "people": [
      "Springenberg"
    ],
    "review": "The paper under consideration proposes a set of procedures for incrementally expanding a residual network by adding layers via a boosting criterion.\n\nThe main barrier to publication is the weak empirical validation. The tasks considered are quite small scale in 2016 (and MNIST with a convolutional net is basically an uninteresting test by this point). The paper doesn't compare to the literature, and CIFAR-10 results fail to improve upon rather simple, single-network published baselines (Springenberg et al, 2015 for example, obtains 92% without data augmentation) and I'm pretty sure there's a simple ResNet result somewhere that outshines these too. The CIFAR100 results are a little bit interesting as they are better than I'm used to seeing (I haven't done a recent literature crawl), and this is unsurprising -- you'd expect ensembles to do well when there's a dearth of labeled training data, and here there are only a few hundred per label. But then it's typical on both CIFAR10 and CIFAR100 to use simple data augmentation schemes which aren't employed here, and these and other forms of regularization are a simpler alternative to a complicated iterative augmentation scheme like this.\n\nIt'd be easier to sell this method either as an option for scarce labeled datasets where data augmentation is non-trivial (but then for most image-related applications, random crops and reflections are easy and valid), but that would necessitate different benchmarks, and comparison against simpler methods like said data augmentation, dropout (especially, due to the ensemble interpretation), and so on."
  },
  {
    "people": [
      "Anselmi",
      "F."
    ],
    "review": "This paper proposes a new learning model \"Compositional Kernel Machines (CKMs)\" that extends the classic kernel machines by constructing compositional kernel functions using sum-product networks. This paper considers the convnets as nicely learned nonlinear decision functions and resort their success in classification to their compositional nature. This perspective motivates the design of compositional kernel functions and the sum-product implementation is indeed interesting. I agree the composition is important for convnets, but it is not the whole story of convnets' success. One essential difference between convnets and CKMs is that all the kernels in convnets are learned directly from data while CKMs still build on top of feature descriptors. This, I believe, limits the representation power of CKMs. A recent paper \"Deep Convolutional Networks are Hierarchical Kernel Machines\" by Anselmi, F. et al. seems to be interesting to the authors.\nExperiments seem to be preliminary in this paper. It's good to see promising results of CKMs on small NORB, but it is quite important to show competitive results on recent classification standard benchmarks, such as MNIST, CIFAR10/100 and even Imagenet, in order to establish a novel learning model. In NORB compositions, CKMs seem to be better than convnets at classifying images by their dominant objects. I suspect it is because the use of sparse ORB features. It will be great if this paper could show the accuracy of ORB features with matching kernel SVMs. Some details about this experiment need further clarification, such as what are the high and low probabilities of sampling from each collections and how many images are generated. In NORB Symmetries, CKMs show better performance than convnets with small data, but the convnets seem not converged yet. Could it be possible to show results with larger dataset?"
  },
  {
    "people": [
      "Anselmi",
      "F."
    ],
    "review": "This paper proposes a new learning model \"Compositional Kernel Machines (CKMs)\" that extends the classic kernel machines by constructing compositional kernel functions using sum-product networks. This paper considers the convnets as nicely learned nonlinear decision functions and resort their success in classification to their compositional nature. This perspective motivates the design of compositional kernel functions and the sum-product implementation is indeed interesting. I agree the composition is important for convnets, but it is not the whole story of convnets' success. One essential difference between convnets and CKMs is that all the kernels in convnets are learned directly from data while CKMs still build on top of feature descriptors. This, I believe, limits the representation power of CKMs. A recent paper \"Deep Convolutional Networks are Hierarchical Kernel Machines\" by Anselmi, F. et al. seems to be interesting to the authors.\nExperiments seem to be preliminary in this paper. It's good to see promising results of CKMs on small NORB, but it is quite important to show competitive results on recent classification standard benchmarks, such as MNIST, CIFAR10/100 and even Imagenet, in order to establish a novel learning model. In NORB compositions, CKMs seem to be better than convnets at classifying images by their dominant objects. I suspect it is because the use of sparse ORB features. It will be great if this paper could show the accuracy of ORB features with matching kernel SVMs. Some details about this experiment need further clarification, such as what are the high and low probabilities of sampling from each collections and how many images are generated. In NORB Symmetries, CKMs show better performance than convnets with small data, but the convnets seem not converged yet. Could it be possible to show results with larger dataset?"
  },
  {
    "people": [
      "Zeiler",
      "Fergus",
      "Brian Chu"
    ],
    "review": "We've uploaded an updated version of the paper which addresses reviewers' concerns and makes several improvements. Apologies for the late revision, mainly due to the time-consuming ImageNet experiments. \n\n- To further confirm the iterative estimation, and contrast our view with the findings of Zeiler and Fergus (2014), we did a literature survey and planned on adding some visualizations of ResNet features.  We found a technical report from Brian Chu at Berkeley who applied known visualization techniques to Resnets independently and report exactly the behaviour we were expecting: The features within a stage get refined and sharpened. So now we refer to their paper and (with their kind permission) reproduce one of their visualizations. We have added a discussion of these findings which support our proposed view. \n\n- We've added mean+-std results for the experiments in Section 5.1.\nThese experiments take a long time, so we performed only three runs each, but the results are stable enough for comparison.\n\n- We've elaborated a bit further on the role of batch normalization in Section 5.1. \n\n- improved language and organization\n\nApart from these changes we've also started investigating the unexpected behaviour of stage 4, by creating modified architectures. We've added three blocks to that stage and we've tried adding a fifth stage. We found that both variants improve performance to ca. 6.8% top5 error.\nBut the average estimation error stays high for the first few blocks and only later starts to decrease: [-0.32, -0.31, -0.30, -0.27, -0.17] (compare Figure 3)\nWe are further investigating this and we'll add our findings as soon as they paint a coherent picture.  But as of now, we didn't consider them to be interesting enough to be included in the paper. \n"
  },
  {
    "people": [
      "Zeiler",
      "Fergus",
      "Brian Chu"
    ],
    "review": "We've uploaded an updated version of the paper which addresses reviewers' concerns and makes several improvements. Apologies for the late revision, mainly due to the time-consuming ImageNet experiments. \n\n- To further confirm the iterative estimation, and contrast our view with the findings of Zeiler and Fergus (2014), we did a literature survey and planned on adding some visualizations of ResNet features.  We found a technical report from Brian Chu at Berkeley who applied known visualization techniques to Resnets independently and report exactly the behaviour we were expecting: The features within a stage get refined and sharpened. So now we refer to their paper and (with their kind permission) reproduce one of their visualizations. We have added a discussion of these findings which support our proposed view. \n\n- We've added mean+-std results for the experiments in Section 5.1.\nThese experiments take a long time, so we performed only three runs each, but the results are stable enough for comparison.\n\n- We've elaborated a bit further on the role of batch normalization in Section 5.1. \n\n- improved language and organization\n\nApart from these changes we've also started investigating the unexpected behaviour of stage 4, by creating modified architectures. We've added three blocks to that stage and we've tried adding a fifth stage. We found that both variants improve performance to ca. 6.8% top5 error.\nBut the average estimation error stays high for the first few blocks and only later starts to decrease: [-0.32, -0.31, -0.30, -0.27, -0.17] (compare Figure 3)\nWe are further investigating this and we'll add our findings as soon as they paint a coherent picture.  But as of now, we didn't consider them to be interesting enough to be included in the paper. \n"
  },
  {
    "people": [
      "Jean"
    ],
    "review": "The paper presents the semantic embedding model for multi-label prediction.\nIn my questions, I pointed that the proposed approach assumes the number of labels to predict is known, and the authors said this was an orthogonal question, although I don't think it is!\nI was trying to understand how different is SEM from a basic MLP with softmax output which would be trained with a two step approach instead of stochastic gradient descent. It seems reasonable given their similarity to compare to this very basic baseline.\nRegarding the sampling strategy to estimate the posterior distribution, and the difference with Jean et al, I agree it is slightly different but I think you should definitely refer to it and point to the differences.\nOne last question: why is it called \"semantic\" embeddings? usually this term is used to show some semantic meaning between trained embeddings, but this doesn't seem to appear in this paper.\n"
  },
  {
    "people": [
      "Jean"
    ],
    "review": "The paper presents the semantic embedding model for multi-label prediction.\nIn my questions, I pointed that the proposed approach assumes the number of labels to predict is known, and the authors said this was an orthogonal question, although I don't think it is!\nI was trying to understand how different is SEM from a basic MLP with softmax output which would be trained with a two step approach instead of stochastic gradient descent. It seems reasonable given their similarity to compare to this very basic baseline.\nRegarding the sampling strategy to estimate the posterior distribution, and the difference with Jean et al, I agree it is slightly different but I think you should definitely refer to it and point to the differences.\nOne last question: why is it called \"semantic\" embeddings? usually this term is used to show some semantic meaning between trained embeddings, but this doesn't seem to appear in this paper.\n"
  },
  {
    "people": [
      "Yuan",
      "Tang"
    ],
    "review": "The paper tries to present a first step towards solving the difficult problem of \"learning from limited number of demonstrations\". The paper tries to present 3 contributions towards this effort:\n1. unsupervised segmentation of videos to identify intermediate steps in a process\n2. define reward function based on feature selection for each sub-task\n\nPros:\n+ The paper is a first attempt to solve a very challenging problem, where a robot is taught real-world tasks with very few visual demonstrations and without further retraining.\n+ The method is well motivated and tries to transfer the priors learned from object classification task (through deep network features) to address the problem of limited training examples.\n+ As demonstrated in Fig. 3, the reward functions could be more interpretable and correlate with transitions between subtasks.\n+ Breaking a video into subtasks helps a video demonstration-based method achieve comparable performance with a method which requires full instrumentation for complex real-world tasks like door opening.\n\nCons:\n1. Unsupervised video segmentation can serve as a good starting point to identify subtasks. However, there are multiple prior works in this domain which need to be referenced and compared with. Particularly, video shot detection and shot segmentation works try to identify abrupt change in video to break it into visually diverse shots. These methods could be easily augmented with CNN-features.\n(Note that there are multiple papers in this domain, eg. refer to survey in Yuan et al. Trans. on Circuits and Systems for video tech. 2007)\n\n2. The authors claim that they did not find it necessary to identify commonalities across demonstrations. This limits the scope of the problem drastically and requires the demonstrations to follow very specific set of constraints. Again, it is to be noted that there is past literature (video co-segmentation, eg. Tang et al. ECCV'14) which uses these commonalities to perform unsupervised video segmentation.\n\n3. The unsupervised temporal video segmentation approach in the paper is only compared to a very simple random baseline for a few sample videos. However, given the large amount of literature in this domain, it is difficult to judge the novelty and significance of the proposed approach from these experiments.\n\n4. The authors hypothesize that \"sparse independent features exists which can discriminate a wide range of unseen inputs\" and encode this intuition through the feature selection strategy. Again, the validity of the hypothesis is not experimentally well demonstrated. For instance, comparison to a simple linear classifier for subtasks would have been useful.\n\nOverall, the paper presents a simple approach based on the idea that recognizing sub-goals in an unsupervised fashion would help in learning from few visual demonstrations. This is well motivated as a first-step towards a difficult task. However, the methods and claims presented in the paper need to be analyzed and compared with better baselines."
  },
  {
    "people": [
      "Yuan",
      "Tang"
    ],
    "review": "The paper tries to present a first step towards solving the difficult problem of \"learning from limited number of demonstrations\". The paper tries to present 3 contributions towards this effort:\n1. unsupervised segmentation of videos to identify intermediate steps in a process\n2. define reward function based on feature selection for each sub-task\n\nPros:\n+ The paper is a first attempt to solve a very challenging problem, where a robot is taught real-world tasks with very few visual demonstrations and without further retraining.\n+ The method is well motivated and tries to transfer the priors learned from object classification task (through deep network features) to address the problem of limited training examples.\n+ As demonstrated in Fig. 3, the reward functions could be more interpretable and correlate with transitions between subtasks.\n+ Breaking a video into subtasks helps a video demonstration-based method achieve comparable performance with a method which requires full instrumentation for complex real-world tasks like door opening.\n\nCons:\n1. Unsupervised video segmentation can serve as a good starting point to identify subtasks. However, there are multiple prior works in this domain which need to be referenced and compared with. Particularly, video shot detection and shot segmentation works try to identify abrupt change in video to break it into visually diverse shots. These methods could be easily augmented with CNN-features.\n(Note that there are multiple papers in this domain, eg. refer to survey in Yuan et al. Trans. on Circuits and Systems for video tech. 2007)\n\n2. The authors claim that they did not find it necessary to identify commonalities across demonstrations. This limits the scope of the problem drastically and requires the demonstrations to follow very specific set of constraints. Again, it is to be noted that there is past literature (video co-segmentation, eg. Tang et al. ECCV'14) which uses these commonalities to perform unsupervised video segmentation.\n\n3. The unsupervised temporal video segmentation approach in the paper is only compared to a very simple random baseline for a few sample videos. However, given the large amount of literature in this domain, it is difficult to judge the novelty and significance of the proposed approach from these experiments.\n\n4. The authors hypothesize that \"sparse independent features exists which can discriminate a wide range of unseen inputs\" and encode this intuition through the feature selection strategy. Again, the validity of the hypothesis is not experimentally well demonstrated. For instance, comparison to a simple linear classifier for subtasks would have been useful.\n\nOverall, the paper presents a simple approach based on the idea that recognizing sub-goals in an unsupervised fashion would help in learning from few visual demonstrations. This is well motivated as a first-step towards a difficult task. However, the methods and claims presented in the paper need to be analyzed and compared with better baselines."
  },
  {
    "people": [
      "Graves",
      "Wu",
      "Chung",
      "Graves",
      "Wu"
    ],
    "review": "TLDR: The authors present Variable Computation in Recurrent Neural Networks (VCRNN). VCRNN is similar in nature to Adaptive Computation Time (Graves et al., 2016). Imagine a vanilla RNN, at each timestep only a subset (i.e., \"variable computation\") of the state is updated. Experimental results are not convincing, there is limited comparison to other cited work and basic LSTM baseline.\n\n=== Gating Mechanism ===\nAt each timestep, VCRNN generates a m_t vector which can be seen as a gating mechanism.  Based off this m_t vector, a D-first (D-first as in literally the first D RNN states) subset of the vanilla RNN state is gated to be updated or not. Extra hyperparams epsilon and \\bar{m} are needed -- authors did not give us a value or explain how this was selected or how sensitive and critical these hyperparms are.\n\nThis mechanism while novel, feels a bit clunky and awkward. It does not feel well principled that only the D-first states get updated, rather than a generalized solution where any subset of the state can be updated.\n\nA short section in the text comparing to the soft-gating mechanisms of GRUs/LSTMs/Multiplicative RNNs (Wu et al., 2016) would be nice as well.\n\n=== Variable Computation ===\nOne of the arguments made is that their VCRNN model can save computation versus vanilla RNNs. While this may be technically true, in practice this is probably not the case. The size of the RNNs they compare to do not saturate any modern GPU cores. In theory computation might be saved, but in practice there will probably be no difference in wallclock time. The authors also did not report any wallclock numbers, which makes this argument hard to sell.\n\n=== Evaluation ===\nThis reviewer wished there was more citations to other work for comparison and a stronger baseline (than just a vanilla RNN). First, LSTMs are very simple and quite standard nowadays -- there is a lack of comparison to any basic stacked LSTM architecture in all the experiments.\n\nThe PTB BPC numbers are quite discouraging as well (compared to state-of-the-art). The VCRNN does not beat the basic vanilla RNN baseline. The authors also only cite/compare to a basic RNN architecture, however there has been many contributions since a basic RNN architecture that performs vastly better. Please see Chung et al., 2016 Table 1. Chung et al., 2016 also experimented w/ PTB BPC and they cite and compare to a large number of other (important) contributions.\n\nOne cool experiment the authors did is graph the per-character computation of VCRNN (i.e., see Figure 2). It shows after a space/word boundary, we use more computation! Cool! However, this makes me wonder what a GRU/LSTM does as well? What is the magnitude of the of the change in the state vector after a space in GRU/LSTM -- I suspect them to do something similar.\n\n=== Minor ===\n* Please add Equations numbers to the paper, hard to refer to in a review and discussion!\n\nReferences\nChung et al., \"Hierarchical Multiscale Recurrent Neural Networks,\" in 2016.\nGraves et al., \"Adaptive Computation Time for Recurrent Neural Networks,\" in 2016.\nWu et al., \"On Multiplicative Integration with Recurrent Neural Networks,\" in 2016."
  },
  {
    "people": [
      "Graves",
      "Wu",
      "Chung",
      "Graves",
      "Wu"
    ],
    "review": "TLDR: The authors present Variable Computation in Recurrent Neural Networks (VCRNN). VCRNN is similar in nature to Adaptive Computation Time (Graves et al., 2016). Imagine a vanilla RNN, at each timestep only a subset (i.e., \"variable computation\") of the state is updated. Experimental results are not convincing, there is limited comparison to other cited work and basic LSTM baseline.\n\n=== Gating Mechanism ===\nAt each timestep, VCRNN generates a m_t vector which can be seen as a gating mechanism.  Based off this m_t vector, a D-first (D-first as in literally the first D RNN states) subset of the vanilla RNN state is gated to be updated or not. Extra hyperparams epsilon and \\bar{m} are needed -- authors did not give us a value or explain how this was selected or how sensitive and critical these hyperparms are.\n\nThis mechanism while novel, feels a bit clunky and awkward. It does not feel well principled that only the D-first states get updated, rather than a generalized solution where any subset of the state can be updated.\n\nA short section in the text comparing to the soft-gating mechanisms of GRUs/LSTMs/Multiplicative RNNs (Wu et al., 2016) would be nice as well.\n\n=== Variable Computation ===\nOne of the arguments made is that their VCRNN model can save computation versus vanilla RNNs. While this may be technically true, in practice this is probably not the case. The size of the RNNs they compare to do not saturate any modern GPU cores. In theory computation might be saved, but in practice there will probably be no difference in wallclock time. The authors also did not report any wallclock numbers, which makes this argument hard to sell.\n\n=== Evaluation ===\nThis reviewer wished there was more citations to other work for comparison and a stronger baseline (than just a vanilla RNN). First, LSTMs are very simple and quite standard nowadays -- there is a lack of comparison to any basic stacked LSTM architecture in all the experiments.\n\nThe PTB BPC numbers are quite discouraging as well (compared to state-of-the-art). The VCRNN does not beat the basic vanilla RNN baseline. The authors also only cite/compare to a basic RNN architecture, however there has been many contributions since a basic RNN architecture that performs vastly better. Please see Chung et al., 2016 Table 1. Chung et al., 2016 also experimented w/ PTB BPC and they cite and compare to a large number of other (important) contributions.\n\nOne cool experiment the authors did is graph the per-character computation of VCRNN (i.e., see Figure 2). It shows after a space/word boundary, we use more computation! Cool! However, this makes me wonder what a GRU/LSTM does as well? What is the magnitude of the of the change in the state vector after a space in GRU/LSTM -- I suspect them to do something similar.\n\n=== Minor ===\n* Please add Equations numbers to the paper, hard to refer to in a review and discussion!\n\nReferences\nChung et al., \"Hierarchical Multiscale Recurrent Neural Networks,\" in 2016.\nGraves et al., \"Adaptive Computation Time for Recurrent Neural Networks,\" in 2016.\nWu et al., \"On Multiplicative Integration with Recurrent Neural Networks,\" in 2016."
  },
  {
    "people": [
      "Wen",
      "Wei",
      "Lebedev",
      "Vadim",
      "Victor Lempitsky",
      "Alvarez",
      "Jose M.",
      "Mathieu Salzmann"
    ],
    "review": "1) Wen, Wei, et al. \"Learning structured sparsity in deep neural networks.\" Advances in Neural Information Processing Systems. 2016.\n2) Lebedev, Vadim, and Victor Lempitsky. \"Fast convnets using group-wise brain damage.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.\n3) Alvarez, Jose M., and Mathieu Salzmann. \"Learning the Number of Neurons in Deep Networks.\" Advances in Neural Information Processing Systems. 2016."
  },
  {
    "people": [
      "Wen",
      "Wei",
      "Lebedev",
      "Vadim",
      "Victor Lempitsky",
      "Alvarez",
      "Jose M.",
      "Mathieu Salzmann"
    ],
    "review": "1) Wen, Wei, et al. \"Learning structured sparsity in deep neural networks.\" Advances in Neural Information Processing Systems. 2016.\n2) Lebedev, Vadim, and Victor Lempitsky. \"Fast convnets using group-wise brain damage.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.\n3) Alvarez, Jose M., and Mathieu Salzmann. \"Learning the Number of Neurons in Deep Networks.\" Advances in Neural Information Processing Systems. 2016."
  },
  {
    "people": [
      "Zhao"
    ],
    "review": "The submission explores several alternatives to provide the generator function in generative adversarial training with additional gradient information. The exposition starts by describing a general formulation about how this additional gradient information (termed K(p_gen) could be added to the generative adversarial training objective function (Equation 1). Next, the authors prove that the shape of the optimal discriminator does indeed depend on the added gradient information (Proposition 3.1), which is unsurprising. Finally, the authors propose three particular alternatives to construct K(p_gen): the negative entropy of the generator distribution, the L2 norm of the generator distribution, and a constant function (which resembles the EBGAN objective of Zhao et al, 2016).\n\nThe exposition moves then to an experimental evaluation of the method, which sets K(p_gen) to be the approximate entropy of the generator distribution. At this point, my intuition is that the objective function under study is the vanilla GAN objective, plus a regularization term that encourages diversity (high entropy) in the generator distribution. The hope of the authors is that this regularization will transform the discriminator into an estimate of the energy landscape of the data distribution.\n\nThe experimental evaluation proceeds by 1) showing the contour plots of the obtained generator distribution for a 2D problem, 2) studying the generation diversity in MNIST digits, and 3) showing some samples for CIFAR-10 and CelebA. The 2D problem results are convincing, since one can clearly observe that the discriminator scores translate into unnormalized values of the density function. The MNIST results offer good intuition also: the more prototypical digits are assigned larger scores (unnormalized densities) by the discriminator, and the less prototypical digits are assigned smaller scores. The sample experiments from Section 5.3 are less convincing, since no samples from baseline models are provided for comparison.\n\nTo this end, I would recommend the authors to clarify three aspects. First, we have seen that entropy regularization leads to a discriminator that estimates the energy landscape of the data distribution. But, how does this regularization reshape the generator function? It would be nice to see the mean MNIST digit according to the generator, and some other statistics if possible. Second, how do the samples produced by the proposed methods compare (visually speaking) to the state-of-the art? Third, what are the *shortcomings* of this method versus vanilla GAN? Too much computational overhead? What are the qualitative and quantitative differences between the two entropy estimators proposed in the manuscript?\n\nOverall, a clearly written paper. I vote for acceptance.\n\nAs an open question to the authors: What breakthroughs should we pursue to derive a GAN objective where the discriminator is an estimate of the data density function, after training?"
  },
  {
    "people": [
      "Zhao"
    ],
    "review": "The submission explores several alternatives to provide the generator function in generative adversarial training with additional gradient information. The exposition starts by describing a general formulation about how this additional gradient information (termed K(p_gen) could be added to the generative adversarial training objective function (Equation 1). Next, the authors prove that the shape of the optimal discriminator does indeed depend on the added gradient information (Proposition 3.1), which is unsurprising. Finally, the authors propose three particular alternatives to construct K(p_gen): the negative entropy of the generator distribution, the L2 norm of the generator distribution, and a constant function (which resembles the EBGAN objective of Zhao et al, 2016).\n\nThe exposition moves then to an experimental evaluation of the method, which sets K(p_gen) to be the approximate entropy of the generator distribution. At this point, my intuition is that the objective function under study is the vanilla GAN objective, plus a regularization term that encourages diversity (high entropy) in the generator distribution. The hope of the authors is that this regularization will transform the discriminator into an estimate of the energy landscape of the data distribution.\n\nThe experimental evaluation proceeds by 1) showing the contour plots of the obtained generator distribution for a 2D problem, 2) studying the generation diversity in MNIST digits, and 3) showing some samples for CIFAR-10 and CelebA. The 2D problem results are convincing, since one can clearly observe that the discriminator scores translate into unnormalized values of the density function. The MNIST results offer good intuition also: the more prototypical digits are assigned larger scores (unnormalized densities) by the discriminator, and the less prototypical digits are assigned smaller scores. The sample experiments from Section 5.3 are less convincing, since no samples from baseline models are provided for comparison.\n\nTo this end, I would recommend the authors to clarify three aspects. First, we have seen that entropy regularization leads to a discriminator that estimates the energy landscape of the data distribution. But, how does this regularization reshape the generator function? It would be nice to see the mean MNIST digit according to the generator, and some other statistics if possible. Second, how do the samples produced by the proposed methods compare (visually speaking) to the state-of-the art? Third, what are the *shortcomings* of this method versus vanilla GAN? Too much computational overhead? What are the qualitative and quantitative differences between the two entropy estimators proposed in the manuscript?\n\nOverall, a clearly written paper. I vote for acceptance.\n\nAs an open question to the authors: What breakthroughs should we pursue to derive a GAN objective where the discriminator is an estimate of the data density function, after training?\n\n"
  },
  {
    "people": [
      "Zhao"
    ],
    "review": "The submission explores several alternatives to provide the generator function in generative adversarial training with additional gradient information. The exposition starts by describing a general formulation about how this additional gradient information (termed K(p_gen) could be added to the generative adversarial training objective function (Equation 1). Next, the authors prove that the shape of the optimal discriminator does indeed depend on the added gradient information (Proposition 3.1), which is unsurprising. Finally, the authors propose three particular alternatives to construct K(p_gen): the negative entropy of the generator distribution, the L2 norm of the generator distribution, and a constant function (which resembles the EBGAN objective of Zhao et al, 2016).\n\nThe exposition moves then to an experimental evaluation of the method, which sets K(p_gen) to be the approximate entropy of the generator distribution. At this point, my intuition is that the objective function under study is the vanilla GAN objective, plus a regularization term that encourages diversity (high entropy) in the generator distribution. The hope of the authors is that this regularization will transform the discriminator into an estimate of the energy landscape of the data distribution.\n\nThe experimental evaluation proceeds by 1) showing the contour plots of the obtained generator distribution for a 2D problem, 2) studying the generation diversity in MNIST digits, and 3) showing some samples for CIFAR-10 and CelebA. The 2D problem results are convincing, since one can clearly observe that the discriminator scores translate into unnormalized values of the density function. The MNIST results offer good intuition also: the more prototypical digits are assigned larger scores (unnormalized densities) by the discriminator, and the less prototypical digits are assigned smaller scores. The sample experiments from Section 5.3 are less convincing, since no samples from baseline models are provided for comparison.\n\nTo this end, I would recommend the authors to clarify three aspects. First, we have seen that entropy regularization leads to a discriminator that estimates the energy landscape of the data distribution. But, how does this regularization reshape the generator function? It would be nice to see the mean MNIST digit according to the generator, and some other statistics if possible. Second, how do the samples produced by the proposed methods compare (visually speaking) to the state-of-the art? Third, what are the *shortcomings* of this method versus vanilla GAN? Too much computational overhead? What are the qualitative and quantitative differences between the two entropy estimators proposed in the manuscript?\n\nOverall, a clearly written paper. I vote for acceptance.\n\nAs an open question to the authors: What breakthroughs should we pursue to derive a GAN objective where the discriminator is an estimate of the data density function, after training?"
  },
  {
    "people": [
      "Zhao"
    ],
    "review": "The submission explores several alternatives to provide the generator function in generative adversarial training with additional gradient information. The exposition starts by describing a general formulation about how this additional gradient information (termed K(p_gen) could be added to the generative adversarial training objective function (Equation 1). Next, the authors prove that the shape of the optimal discriminator does indeed depend on the added gradient information (Proposition 3.1), which is unsurprising. Finally, the authors propose three particular alternatives to construct K(p_gen): the negative entropy of the generator distribution, the L2 norm of the generator distribution, and a constant function (which resembles the EBGAN objective of Zhao et al, 2016).\n\nThe exposition moves then to an experimental evaluation of the method, which sets K(p_gen) to be the approximate entropy of the generator distribution. At this point, my intuition is that the objective function under study is the vanilla GAN objective, plus a regularization term that encourages diversity (high entropy) in the generator distribution. The hope of the authors is that this regularization will transform the discriminator into an estimate of the energy landscape of the data distribution.\n\nThe experimental evaluation proceeds by 1) showing the contour plots of the obtained generator distribution for a 2D problem, 2) studying the generation diversity in MNIST digits, and 3) showing some samples for CIFAR-10 and CelebA. The 2D problem results are convincing, since one can clearly observe that the discriminator scores translate into unnormalized values of the density function. The MNIST results offer good intuition also: the more prototypical digits are assigned larger scores (unnormalized densities) by the discriminator, and the less prototypical digits are assigned smaller scores. The sample experiments from Section 5.3 are less convincing, since no samples from baseline models are provided for comparison.\n\nTo this end, I would recommend the authors to clarify three aspects. First, we have seen that entropy regularization leads to a discriminator that estimates the energy landscape of the data distribution. But, how does this regularization reshape the generator function? It would be nice to see the mean MNIST digit according to the generator, and some other statistics if possible. Second, how do the samples produced by the proposed methods compare (visually speaking) to the state-of-the art? Third, what are the *shortcomings* of this method versus vanilla GAN? Too much computational overhead? What are the qualitative and quantitative differences between the two entropy estimators proposed in the manuscript?\n\nOverall, a clearly written paper. I vote for acceptance.\n\nAs an open question to the authors: What breakthroughs should we pursue to derive a GAN objective where the discriminator is an estimate of the data density function, after training?\n\n"
  },
  {
    "people": [
      "Bertsekas"
    ],
    "review": "We thank AnonReviewer4 for the input.  We respond to your points individually:\n\nFirst, you make an interesting connection with the KLD, but that is not quite applicable here: our formulation has c_k(E T_k)^2 instead of c_k E(T_k ^2), which can't be cast straightforwardly in a VI framework. Having this term is useful because:\n\n+ The true optimum has log density sum eta_i T_i (eta is as in equation 2), with no c_i T_i^2 term. If we didn't have the last term, we would be minimizing the KLD to a log density sum lambda_i T_i, which would only recover the maximum entropy distribution if lambda actually corresponded to eta. Like we mentioned on the paper, computing eta is computationally intractable.\n\n+ Having this term enables the use of the augmented Lagrangian method, giving us the theoretical guarantees discussed on our previous answers, which ensures that we recover the maximum entropy distribution (up to regularity and expressivity of the model class, as discussed).\n\nSecond, you commented about novelty in the paper.  We respectfully disagree.  For example, the above KLD example is in fact different than suggested, so the MEFN is not the typical variational inference setup.  Further, we think the introduction of the ME problem is novel to the deep learning community, and conversely, we have introduced deep learning to the ME problem, a literature in which there is substantial need for estimation and sampling techniques.  Mechanically, constrained optimization is rarely used in the deep learning community, and we introduce new steps there.  Finally, per your and others\u2019 requests, we think our new expanded results section adds novelty and state of the art performance to those problem domains (see below).\n\nThird, you asked for more complex data.  Thank you.  We have significantly addressed this concern in the new version of the paper (see updated pdf).  Section 4.3 now details a texture synthesis problem, and demonstrates that the MEFN performs consistently with state of the art in terms of matching texture-net moment constraints, all while outperforming substantially in terms of sample variability.  As the purpose of this class of implicit generative models is to generate a diverse sample set of plausible images, having the ME component in this problem domain is critical and adds to the novelty and impact of our work. \n\nTo your point about step 8 of our algorithm, it simply corresponds to the stochastic version of the usual augmented Lagrangian method, namely lambda_{k+1} = lambda_k + c_k * T(phi_k). This basically corresponds to a gradient step in lambda with step size c_k. The step size is justified by the theoretical derivation of the augmented Lagrangian method, which can be read in detail in Bertsekas (2014)."
  },
  {
    "people": [
      "Ulyanov",
      "Dinh"
    ],
    "review": "We have significantly revised the paper in response to reviewer and other comments.  Major changes include:\n\n+ Most significantly, we added new experiments applying MEFN to texture synthesis (Section 4.3).  Current state-of-the-art (Ulyanov et al 2016) defines a complicated texture loss and then learns a network that generates images with a small texture loss, with excellent results.  One potential downside in this construction, common to many state of the art deep learning approaches, is that there is no objective encouraging sample diversity; this implies the extreme pathological case where the distribution is simply a point mass on the training image.   We apply the real NVP (Dinh et al 2016), a recently proposed flow tailored to image applications, as our network structure in an MEFN framework.  The MEFN provides images with qualitative textures at least as good as existing state of the art (and this is quantitatively supported), and critically the MEFN produces samples with significantly more diversity (as quantified in a variety of ways).  This experiment highlights the difficulty of obtaining the max entropy distribution in high dimensions and with complicated constraints, and our MEFN handles this setting well and improves the current state-of-the-art in texture synthesis.  As such we feel that this enhanced results section speaks to any concerns about experiments and further increases the novelty and utility of the MEFN framework.\n\n+ In line with the point about sample diversity, we also augmented our Dirichlet experiments of Section 4.1 to show the danger of not considering the entropy term: there moment matching is achieved per the objective, but sample diversity is significantly lost.  This result foreshadows the more important implication of this fact seen in Section 4.3.\n\n+ We have thoroughly revised the text to respond to other reviewer comments and to clarify the contributions and novelties introduced in this work, which we and the reviewers felt were not adequately emphasized before."
  },
  {
    "people": [
      "Ivan Ustyuzhaninov",
      "Wieland Brendel",
      "Leon Gatys",
      "Matthias Bethge"
    ],
    "review": "We thank our reviewers for their input. We did some minor updates to the manuscript. Here are our answers to the points raised in the reviews:\n\n1. Augmented Lagrangian with stochastic objective and constraints, and how our use of a hypothesis test helps (AnonReviewer2 and AnonReviewer1):\n\nThe augmented Lagrangian method transforms a constrained optimization problem into a sequence of unconstrained optimization problems (similar to a log barrier or other interior point methods).  Thus, as long as we are confident in convergence for each unconstrained problem, the overall convergence of the augmented Lagrangian is inherited. This fact remains the case regardless of the underlying optimizer, be it a standard noiseless gradient method, or (as is in this case) an SGD method.  This explanation certainly is somewhat informal, but our experience empirically is that there are no incremental problems with a series of SGD unconstrained optimizations save the minor issue addressed below.\n\nA potential minor issue with the augmented Lagrangian method is that if c is too large, although theoretically not an issue, in practice this will make the unconstrained problems ill-conditioned thus making it hard to solve them numerically. The update rules for c are designed to address this issue. In our experiments we found that sometimes the random nature of our constraint estimates caused c to be updated when it shouldn't (this was the case for the Dirichlet experiments, not the financial ones). Our hypothesis test solution aims to have a more conservative updating rule in order to avoid this issue, and it performed well in practice. It should also be noted that when the number of samples used for the hypothesis test goes to infinity, the hypothesis test becomes the regular update rule (i.e. noiseless).\n\nTo the best of our knowledge, there are few sources in the literature on constrained stochastic optimization. There is research addressing constrained optimization when we only have access to a single sample ([1]), but this is not our case as we have access to as many samples as desired. There is a paper ([2]) in which it is proved that alternating a gradient step with a lambda update will work (similar to what we do: alternating optimizing with many gradient steps and performing a lambda update), but the issue of updating c is not touched there, it is just assumed that c is large enough. This does however, at least partially, justify our augmented Lagrangian approach.\n\n\n2. Higher-dimensional Experiments (AnonReviewer2 and AnonReviewer1):\n\nFirst, we note that evaluating the baseline in higher-dimensional settings is not trivial: recovering the Gibbs distribution is computationally intractable.  That said, we are currently working on applying our method to generate texture images. We believe this might be a particularly interesting higher dimensional application, as the networks trained to accomplish this task are usually trained to match some statistics, having no guarantee that the generated images are \"fair\" samples of the objective texture instead of simply very similar images to the input one.  To that end, another paper currently under review at ICLR, \"What does it take to generate natural textures?\" by Ivan Ustyuzhaninov, Wieland Brendel, Leon Gatys and Matthias Bethge ("
  },
  {
    "people": [
      "Bertsekas"
    ],
    "review": "We thank AnonReviewer4 for the input.  We respond to your points individually:\n\nFirst, you make an interesting connection with the KLD, but that is not quite applicable here: our formulation has c_k(E T_k)^2 instead of c_k E(T_k ^2), which can't be cast straightforwardly in a VI framework. Having this term is useful because:\n\n+ The true optimum has log density sum eta_i T_i (eta is as in equation 2), with no c_i T_i^2 term. If we didn't have the last term, we would be minimizing the KLD to a log density sum lambda_i T_i, which would only recover the maximum entropy distribution if lambda actually corresponded to eta. Like we mentioned on the paper, computing eta is computationally intractable.\n\n+ Having this term enables the use of the augmented Lagrangian method, giving us the theoretical guarantees discussed on our previous answers, which ensures that we recover the maximum entropy distribution (up to regularity and expressivity of the model class, as discussed).\n\nSecond, you commented about novelty in the paper.  We respectfully disagree.  For example, the above KLD example is in fact different than suggested, so the MEFN is not the typical variational inference setup.  Further, we think the introduction of the ME problem is novel to the deep learning community, and conversely, we have introduced deep learning to the ME problem, a literature in which there is substantial need for estimation and sampling techniques.  Mechanically, constrained optimization is rarely used in the deep learning community, and we introduce new steps there.  Finally, per your and others\u2019 requests, we think our new expanded results section adds novelty and state of the art performance to those problem domains (see below).\n\nThird, you asked for more complex data.  Thank you.  We have significantly addressed this concern in the new version of the paper (see updated pdf).  Section 4.3 now details a texture synthesis problem, and demonstrates that the MEFN performs consistently with state of the art in terms of matching texture-net moment constraints, all while outperforming substantially in terms of sample variability.  As the purpose of this class of implicit generative models is to generate a diverse sample set of plausible images, having the ME component in this problem domain is critical and adds to the novelty and impact of our work. \n\nTo your point about step 8 of our algorithm, it simply corresponds to the stochastic version of the usual augmented Lagrangian method, namely lambda_{k+1} = lambda_k + c_k * T(phi_k). This basically corresponds to a gradient step in lambda with step size c_k. The step size is justified by the theoretical derivation of the augmented Lagrangian method, which can be read in detail in Bertsekas (2014)."
  },
  {
    "people": [
      "Ulyanov",
      "Dinh"
    ],
    "review": "We have significantly revised the paper in response to reviewer and other comments.  Major changes include:\n\n+ Most significantly, we added new experiments applying MEFN to texture synthesis (Section 4.3).  Current state-of-the-art (Ulyanov et al 2016) defines a complicated texture loss and then learns a network that generates images with a small texture loss, with excellent results.  One potential downside in this construction, common to many state of the art deep learning approaches, is that there is no objective encouraging sample diversity; this implies the extreme pathological case where the distribution is simply a point mass on the training image.   We apply the real NVP (Dinh et al 2016), a recently proposed flow tailored to image applications, as our network structure in an MEFN framework.  The MEFN provides images with qualitative textures at least as good as existing state of the art (and this is quantitatively supported), and critically the MEFN produces samples with significantly more diversity (as quantified in a variety of ways).  This experiment highlights the difficulty of obtaining the max entropy distribution in high dimensions and with complicated constraints, and our MEFN handles this setting well and improves the current state-of-the-art in texture synthesis.  As such we feel that this enhanced results section speaks to any concerns about experiments and further increases the novelty and utility of the MEFN framework.\n\n+ In line with the point about sample diversity, we also augmented our Dirichlet experiments of Section 4.1 to show the danger of not considering the entropy term: there moment matching is achieved per the objective, but sample diversity is significantly lost.  This result foreshadows the more important implication of this fact seen in Section 4.3.\n\n+ We have thoroughly revised the text to respond to other reviewer comments and to clarify the contributions and novelties introduced in this work, which we and the reviewers felt were not adequately emphasized before."
  },
  {
    "people": [
      "Ivan Ustyuzhaninov",
      "Wieland Brendel",
      "Leon Gatys",
      "Matthias Bethge"
    ],
    "review": "We thank our reviewers for their input. We did some minor updates to the manuscript. Here are our answers to the points raised in the reviews:\n\n1. Augmented Lagrangian with stochastic objective and constraints, and how our use of a hypothesis test helps (AnonReviewer2 and AnonReviewer1):\n\nThe augmented Lagrangian method transforms a constrained optimization problem into a sequence of unconstrained optimization problems (similar to a log barrier or other interior point methods).  Thus, as long as we are confident in convergence for each unconstrained problem, the overall convergence of the augmented Lagrangian is inherited. This fact remains the case regardless of the underlying optimizer, be it a standard noiseless gradient method, or (as is in this case) an SGD method.  This explanation certainly is somewhat informal, but our experience empirically is that there are no incremental problems with a series of SGD unconstrained optimizations save the minor issue addressed below.\n\nA potential minor issue with the augmented Lagrangian method is that if c is too large, although theoretically not an issue, in practice this will make the unconstrained problems ill-conditioned thus making it hard to solve them numerically. The update rules for c are designed to address this issue. In our experiments we found that sometimes the random nature of our constraint estimates caused c to be updated when it shouldn't (this was the case for the Dirichlet experiments, not the financial ones). Our hypothesis test solution aims to have a more conservative updating rule in order to avoid this issue, and it performed well in practice. It should also be noted that when the number of samples used for the hypothesis test goes to infinity, the hypothesis test becomes the regular update rule (i.e. noiseless).\n\nTo the best of our knowledge, there are few sources in the literature on constrained stochastic optimization. There is research addressing constrained optimization when we only have access to a single sample ([1]), but this is not our case as we have access to as many samples as desired. There is a paper ([2]) in which it is proved that alternating a gradient step with a lambda update will work (similar to what we do: alternating optimizing with many gradient steps and performing a lambda update), but the issue of updating c is not touched there, it is just assumed that c is large enough. This does however, at least partially, justify our augmented Lagrangian approach.\n\n\n2. Higher-dimensional Experiments (AnonReviewer2 and AnonReviewer1):\n\nFirst, we note that evaluating the baseline in higher-dimensional settings is not trivial: recovering the Gibbs distribution is computationally intractable.  That said, we are currently working on applying our method to generate texture images. We believe this might be a particularly interesting higher dimensional application, as the networks trained to accomplish this task are usually trained to match some statistics, having no guarantee that the generated images are \"fair\" samples of the objective texture instead of simply very similar images to the input one.  To that end, another paper currently under review at ICLR, \"What does it take to generate natural textures?\" by Ivan Ustyuzhaninov, Wieland Brendel, Leon Gatys and Matthias Bethge ("
  },
  {
    "people": [
      "Antol"
    ],
    "review": "Paper Summary: \nThis paper presents a new comprehension dataset called NewsQA dataset, containing 100,000 question-answer pairs from over 10,000 news articles from CNN. The dataset is collected through a four-stage process -- article filtering, question collection, answer collection and answer validation. Examples from the dataset are divided into different types based on answer types and reasoning required to answer questions. Human and machine performances on NewsQA are reported and compared with SQuAD.\n\nPaper Strengths: \n-- I agree that models can benefit from diverse set of datasets. This dataset is collected from news articles, hence might pose different sets of problems from current popular datasets such as SQuAD.\n-- The proposed dataset is sufficiently large for data hungry deep learning models to train. \n-- The inclusion of questions with null answers is a nice property to have.\n-- A good amount of thought has gone into formulating the four-stage data collection process.\n-- The proposed BARB model is performing as good as a published state-of-the-art model, while being much faster.    \n\nPaper Weaknesses: \n-- Human evaluation is weak. Two near-native English speakers' performance on 100 examples each can hardly be a representative of the complete dataset. Also, what is the model performance on these 200 examples?\n-- Not that it is necessary for this paper, but to clearly demonstrate that this dataset is harder than SQuAD, the authors should either calculate the human performance the same way as SQuAD or calculate human performances on both NewsQA and SQuAD in some other consistent manner on large enough subsets which are good representatives of the complete datasets. Dataset from other communities such as VQA dataset (Antol et al., ICCV 2015) also use the same method as SQuAD to compute human performance. \n-- Section 3.5 says that 86% of questions have answers agreed upon by atleast 2 workers. Why is this number inconsistent with the 4.5% of questions which have answers without agreement after validation (last line in Section 4.1)?\n-- Is the same article shown to multiple Questioners? If yes, is it ensured that the Questioners asking questions about the same article are not asking the same/similar questions?\n-- Authors mention that they keep the same hyperparameters as SQuAD. What are the accuracies if the hyperparameters are tuned using a validation set from NewsQA?\n-- 500 examples which are labeled for reasoning types do not seem enough to represent the complete dataset. Also, what is the model performance on these 500 examples?\n-- Which model's performance has been shown in Figure 1?\n-- Are the two \"students\" graduate/undergraduate students or researchers?\n-- Test set seems to be very small.\n-- Suggestion: Answer validation step is nice, but maybe the dataset can be released in 2 versions -- one with all the answers collected in 3rd stage (without the validation step), and one in the current format with the validation step. \n\nPreliminary Evaluation: \nThe proposed dataset is a large scale machine comprehension dataset collected from news articles, which in my suggestion, is diverse enough from existing datasets that state-of-the-art models can definitely benefit from it. With a better human evaluation, I think this paper will make a good poster."
  },
  {
    "people": [
      "Antol"
    ],
    "review": "Paper Summary: \nThis paper presents a new comprehension dataset called NewsQA dataset, containing 100,000 question-answer pairs from over 10,000 news articles from CNN. The dataset is collected through a four-stage process -- article filtering, question collection, answer collection and answer validation. Examples from the dataset are divided into different types based on answer types and reasoning required to answer questions. Human and machine performances on NewsQA are reported and compared with SQuAD.\n\nPaper Strengths: \n-- I agree that models can benefit from diverse set of datasets. This dataset is collected from news articles, hence might pose different sets of problems from current popular datasets such as SQuAD.\n-- The proposed dataset is sufficiently large for data hungry deep learning models to train. \n-- The inclusion of questions with null answers is a nice property to have.\n-- A good amount of thought has gone into formulating the four-stage data collection process.\n-- The proposed BARB model is performing as good as a published state-of-the-art model, while being much faster.    \n\nPaper Weaknesses: \n-- Human evaluation is weak. Two near-native English speakers' performance on 100 examples each can hardly be a representative of the complete dataset. Also, what is the model performance on these 200 examples?\n-- Not that it is necessary for this paper, but to clearly demonstrate that this dataset is harder than SQuAD, the authors should either calculate the human performance the same way as SQuAD or calculate human performances on both NewsQA and SQuAD in some other consistent manner on large enough subsets which are good representatives of the complete datasets. Dataset from other communities such as VQA dataset (Antol et al., ICCV 2015) also use the same method as SQuAD to compute human performance. \n-- Section 3.5 says that 86% of questions have answers agreed upon by atleast 2 workers. Why is this number inconsistent with the 4.5% of questions which have answers without agreement after validation (last line in Section 4.1)?\n-- Is the same article shown to multiple Questioners? If yes, is it ensured that the Questioners asking questions about the same article are not asking the same/similar questions?\n-- Authors mention that they keep the same hyperparameters as SQuAD. What are the accuracies if the hyperparameters are tuned using a validation set from NewsQA?\n-- 500 examples which are labeled for reasoning types do not seem enough to represent the complete dataset. Also, what is the model performance on these 500 examples?\n-- Which model's performance has been shown in Figure 1?\n-- Are the two \"students\" graduate/undergraduate students or researchers?\n-- Test set seems to be very small.\n-- Suggestion: Answer validation step is nice, but maybe the dataset can be released in 2 versions -- one with all the answers collected in 3rd stage (without the validation step), and one in the current format with the validation step. \n\nPreliminary Evaluation: \nThe proposed dataset is a large scale machine comprehension dataset collected from news articles, which in my suggestion, is diverse enough from existing datasets that state-of-the-art models can definitely benefit from it. With a better human evaluation, I think this paper will make a good poster. "
  },
  {
    "people": [
      "Antol"
    ],
    "review": "Paper Summary: \nThis paper presents a new comprehension dataset called NewsQA dataset, containing 100,000 question-answer pairs from over 10,000 news articles from CNN. The dataset is collected through a four-stage process -- article filtering, question collection, answer collection and answer validation. Examples from the dataset are divided into different types based on answer types and reasoning required to answer questions. Human and machine performances on NewsQA are reported and compared with SQuAD.\n\nPaper Strengths: \n-- I agree that models can benefit from diverse set of datasets. This dataset is collected from news articles, hence might pose different sets of problems from current popular datasets such as SQuAD.\n-- The proposed dataset is sufficiently large for data hungry deep learning models to train. \n-- The inclusion of questions with null answers is a nice property to have.\n-- A good amount of thought has gone into formulating the four-stage data collection process.\n-- The proposed BARB model is performing as good as a published state-of-the-art model, while being much faster.    \n\nPaper Weaknesses: \n-- Human evaluation is weak. Two near-native English speakers' performance on 100 examples each can hardly be a representative of the complete dataset. Also, what is the model performance on these 200 examples?\n-- Not that it is necessary for this paper, but to clearly demonstrate that this dataset is harder than SQuAD, the authors should either calculate the human performance the same way as SQuAD or calculate human performances on both NewsQA and SQuAD in some other consistent manner on large enough subsets which are good representatives of the complete datasets. Dataset from other communities such as VQA dataset (Antol et al., ICCV 2015) also use the same method as SQuAD to compute human performance. \n-- Section 3.5 says that 86% of questions have answers agreed upon by atleast 2 workers. Why is this number inconsistent with the 4.5% of questions which have answers without agreement after validation (last line in Section 4.1)?\n-- Is the same article shown to multiple Questioners? If yes, is it ensured that the Questioners asking questions about the same article are not asking the same/similar questions?\n-- Authors mention that they keep the same hyperparameters as SQuAD. What are the accuracies if the hyperparameters are tuned using a validation set from NewsQA?\n-- 500 examples which are labeled for reasoning types do not seem enough to represent the complete dataset. Also, what is the model performance on these 500 examples?\n-- Which model's performance has been shown in Figure 1?\n-- Are the two \"students\" graduate/undergraduate students or researchers?\n-- Test set seems to be very small.\n-- Suggestion: Answer validation step is nice, but maybe the dataset can be released in 2 versions -- one with all the answers collected in 3rd stage (without the validation step), and one in the current format with the validation step. \n\nPreliminary Evaluation: \nThe proposed dataset is a large scale machine comprehension dataset collected from news articles, which in my suggestion, is diverse enough from existing datasets that state-of-the-art models can definitely benefit from it. With a better human evaluation, I think this paper will make a good poster."
  },
  {
    "people": [
      "Antol"
    ],
    "review": "Paper Summary: \nThis paper presents a new comprehension dataset called NewsQA dataset, containing 100,000 question-answer pairs from over 10,000 news articles from CNN. The dataset is collected through a four-stage process -- article filtering, question collection, answer collection and answer validation. Examples from the dataset are divided into different types based on answer types and reasoning required to answer questions. Human and machine performances on NewsQA are reported and compared with SQuAD.\n\nPaper Strengths: \n-- I agree that models can benefit from diverse set of datasets. This dataset is collected from news articles, hence might pose different sets of problems from current popular datasets such as SQuAD.\n-- The proposed dataset is sufficiently large for data hungry deep learning models to train. \n-- The inclusion of questions with null answers is a nice property to have.\n-- A good amount of thought has gone into formulating the four-stage data collection process.\n-- The proposed BARB model is performing as good as a published state-of-the-art model, while being much faster.    \n\nPaper Weaknesses: \n-- Human evaluation is weak. Two near-native English speakers' performance on 100 examples each can hardly be a representative of the complete dataset. Also, what is the model performance on these 200 examples?\n-- Not that it is necessary for this paper, but to clearly demonstrate that this dataset is harder than SQuAD, the authors should either calculate the human performance the same way as SQuAD or calculate human performances on both NewsQA and SQuAD in some other consistent manner on large enough subsets which are good representatives of the complete datasets. Dataset from other communities such as VQA dataset (Antol et al., ICCV 2015) also use the same method as SQuAD to compute human performance. \n-- Section 3.5 says that 86% of questions have answers agreed upon by atleast 2 workers. Why is this number inconsistent with the 4.5% of questions which have answers without agreement after validation (last line in Section 4.1)?\n-- Is the same article shown to multiple Questioners? If yes, is it ensured that the Questioners asking questions about the same article are not asking the same/similar questions?\n-- Authors mention that they keep the same hyperparameters as SQuAD. What are the accuracies if the hyperparameters are tuned using a validation set from NewsQA?\n-- 500 examples which are labeled for reasoning types do not seem enough to represent the complete dataset. Also, what is the model performance on these 500 examples?\n-- Which model's performance has been shown in Figure 1?\n-- Are the two \"students\" graduate/undergraduate students or researchers?\n-- Test set seems to be very small.\n-- Suggestion: Answer validation step is nice, but maybe the dataset can be released in 2 versions -- one with all the answers collected in 3rd stage (without the validation step), and one in the current format with the validation step. \n\nPreliminary Evaluation: \nThe proposed dataset is a large scale machine comprehension dataset collected from news articles, which in my suggestion, is diverse enough from existing datasets that state-of-the-art models can definitely benefit from it. With a better human evaluation, I think this paper will make a good poster. "
  },
  {
    "people": [
      "Kulkarni",
      "Singh"
    ],
    "review": "The paper presents an on-policy method to predict future intrinsic measurements. All the experiments are performed in the game of Doom (vizDoom to be exact), and instead of just predicting win/loss or the number of frags (score), the authors trained their model to predict (a sequence of) triplets of (health, ammunition, frags), weighted by (a sequence of) \"goal\" triplets that they provided as input. Changing the weights of the goal triplet is a way to perform/guide exploration. At test time, one can act by maximizing the long term goal only.\n\nThe results are impressive, as this model won the 2016 vizDoom competition. The experimental section of the paper seems sound:\n - There are comparisons of DFP with A3C, DQN, and an attempt to compare with DSR (a recent similar approach from Kulkarni et al., 2016). DFP outperforms other approaches (or equal them when they reach a ceiling / optimum, as for A3C in scenario D1).\n - There is an ablation study that supports the thesis that all the \"added complexity\" of the paper's model is useful.\n\nPredicting intrinsic motivation (Singh et al. 2004), auxiliary variables, and forward modelling, are well-studied domains of reinforcement learning. The version that I read (December 4th revision) adequately references prior work, even if it is not completely exhaustive. \n\nA few comments (nitpicks) on the form:\n - Doom is described as a 3D environment, whereas it is actually a 2D environment (the height is not a discriminative/actionable dimension) presented in (fake) 3D.\n - The use of \"P\" in (2) (and subsequently) may be misleading as it stands for prediction but not probability (as is normally the case for P).\n - The double use of \"j\" (admittedly, with different fonts) in (6) may be misleading.\n - Results tables could repeat the units of the measurements (in particular as they are heterogenous in Table 1).\n\nI think that this paper is a clear accept. One could argue that experiments could be conducted on different environments or that the novelty is limited, but I feel that \"correct\" (no-nonsense, experimentally sound on Doom, appendix providing details for reproducibility) and \"milestone\" (vizDoom winner) papers should get published."
  },
  {
    "people": [
      "Kulkarni",
      "Singh"
    ],
    "review": "The paper presents an on-policy method to predict future intrinsic measurements. All the experiments are performed in the game of Doom (vizDoom to be exact), and instead of just predicting win/loss or the number of frags (score), the authors trained their model to predict (a sequence of) triplets of (health, ammunition, frags), weighted by (a sequence of) \"goal\" triplets that they provided as input. Changing the weights of the goal triplet is a way to perform/guide exploration. At test time, one can act by maximizing the long term goal only.\n\nThe results are impressive, as this model won the 2016 vizDoom competition. The experimental section of the paper seems sound:\n - There are comparisons of DFP with A3C, DQN, and an attempt to compare with DSR (a recent similar approach from Kulkarni et al., 2016). DFP outperforms other approaches (or equal them when they reach a ceiling / optimum, as for A3C in scenario D1).\n - There is an ablation study that supports the thesis that all the \"added complexity\" of the paper's model is useful.\n\nPredicting intrinsic motivation (Singh et al. 2004), auxiliary variables, and forward modelling, are well-studied domains of reinforcement learning. The version that I read (December 4th revision) adequately references prior work, even if it is not completely exhaustive. \n\nA few comments (nitpicks) on the form:\n - Doom is described as a 3D environment, whereas it is actually a 2D environment (the height is not a discriminative/actionable dimension) presented in (fake) 3D.\n - The use of \"P\" in (2) (and subsequently) may be misleading as it stands for prediction but not probability (as is normally the case for P).\n - The double use of \"j\" (admittedly, with different fonts) in (6) may be misleading.\n - Results tables could repeat the units of the measurements (in particular as they are heterogenous in Table 1).\n\nI think that this paper is a clear accept. One could argue that experiments could be conducted on different environments or that the novelty is limited, but I feel that \"correct\" (no-nonsense, experimentally sound on Doom, appendix providing details for reproducibility) and \"milestone\" (vizDoom winner) papers should get published."
  },
  {
    "people": [
      "Kulkarni",
      "Singh"
    ],
    "review": "The paper presents an on-policy method to predict future intrinsic measurements. All the experiments are performed in the game of Doom (vizDoom to be exact), and instead of just predicting win/loss or the number of frags (score), the authors trained their model to predict (a sequence of) triplets of (health, ammunition, frags), weighted by (a sequence of) \"goal\" triplets that they provided as input. Changing the weights of the goal triplet is a way to perform/guide exploration. At test time, one can act by maximizing the long term goal only.\n\nThe results are impressive, as this model won the 2016 vizDoom competition. The experimental section of the paper seems sound:\n - There are comparisons of DFP with A3C, DQN, and an attempt to compare with DSR (a recent similar approach from Kulkarni et al., 2016). DFP outperforms other approaches (or equal them when they reach a ceiling / optimum, as for A3C in scenario D1).\n - There is an ablation study that supports the thesis that all the \"added complexity\" of the paper's model is useful.\n\nPredicting intrinsic motivation (Singh et al. 2004), auxiliary variables, and forward modelling, are well-studied domains of reinforcement learning. The version that I read (December 4th revision) adequately references prior work, even if it is not completely exhaustive. \n\nA few comments (nitpicks) on the form:\n - Doom is described as a 3D environment, whereas it is actually a 2D environment (the height is not a discriminative/actionable dimension) presented in (fake) 3D.\n - The use of \"P\" in (2) (and subsequently) may be misleading as it stands for prediction but not probability (as is normally the case for P).\n - The double use of \"j\" (admittedly, with different fonts) in (6) may be misleading.\n - Results tables could repeat the units of the measurements (in particular as they are heterogenous in Table 1).\n\nI think that this paper is a clear accept. One could argue that experiments could be conducted on different environments or that the novelty is limited, but I feel that \"correct\" (no-nonsense, experimentally sound on Doom, appendix providing details for reproducibility) and \"milestone\" (vizDoom winner) papers should get published."
  },
  {
    "people": [
      "Kulkarni",
      "Singh"
    ],
    "review": "The paper presents an on-policy method to predict future intrinsic measurements. All the experiments are performed in the game of Doom (vizDoom to be exact), and instead of just predicting win/loss or the number of frags (score), the authors trained their model to predict (a sequence of) triplets of (health, ammunition, frags), weighted by (a sequence of) \"goal\" triplets that they provided as input. Changing the weights of the goal triplet is a way to perform/guide exploration. At test time, one can act by maximizing the long term goal only.\n\nThe results are impressive, as this model won the 2016 vizDoom competition. The experimental section of the paper seems sound:\n - There are comparisons of DFP with A3C, DQN, and an attempt to compare with DSR (a recent similar approach from Kulkarni et al., 2016). DFP outperforms other approaches (or equal them when they reach a ceiling / optimum, as for A3C in scenario D1).\n - There is an ablation study that supports the thesis that all the \"added complexity\" of the paper's model is useful.\n\nPredicting intrinsic motivation (Singh et al. 2004), auxiliary variables, and forward modelling, are well-studied domains of reinforcement learning. The version that I read (December 4th revision) adequately references prior work, even if it is not completely exhaustive. \n\nA few comments (nitpicks) on the form:\n - Doom is described as a 3D environment, whereas it is actually a 2D environment (the height is not a discriminative/actionable dimension) presented in (fake) 3D.\n - The use of \"P\" in (2) (and subsequently) may be misleading as it stands for prediction but not probability (as is normally the case for P).\n - The double use of \"j\" (admittedly, with different fonts) in (6) may be misleading.\n - Results tables could repeat the units of the measurements (in particular as they are heterogenous in Table 1).\n\nI think that this paper is a clear accept. One could argue that experiments could be conducted on different environments or that the novelty is limited, but I feel that \"correct\" (no-nonsense, experimentally sound on Doom, appendix providing details for reproducibility) and \"milestone\" (vizDoom winner) papers should get published."
  },
  {
    "people": [
      "Greff"
    ],
    "review": "We have also added a section (3.3) that connects our technique to the Unrolled Iterative Estimation interpretation of Highway and ResNets. We further elaborate on how the learned k values are indicators of the abstraction jumps in representations defined in Greff et al, and analyze the k values for both experiments (MNIST and CIFAR) under this perspective."
  },
  {
    "people": [
      "Srivastava"
    ],
    "review": "This paper proposes to learn a single scalar gating parameter instead of a full gating tensor in highway networks. The claim is that such gating is easier to learn and allows a network to flexibly utilize computation.\n\nThe basic idea of the paper is simple and is clearly presented. It is a natural simplification of highway networks to allow easily \"shutting off\" layers while keeping number of additional parameters low. However, in this regard the paper leaves out a few key points. Firstly, it does not mention that the gates in highway networks are data-dependent which is potentially more powerful than learning a fixed gate for all units and independent of data. Secondly, it does not do a fair comparison with highway networks to show that this simpler formulation is indeed easier to learn.\n\nDid the authors try their original design of u = g(k)f(x) + (1 - g(k))x where f(x) is a plain layer instead of a residual layer? Based on the arguments made in the paper, this should work fine. Why wasn't it tested? If it doesn't work, are the arguments incorrect or incomplete?\n\nFor the MNIST experiments, since the hyperparameters are fixed, the plots are misleading if any dependence on hyperparameters exists for the different models. This experiment appears to be based on Srivastava et al (2015). If it is indeed designed to test optimization at aggressive depths, then apart from doing a hyperparameter search, the authors should not use regularization such as dropout or batch norm, which do not appear in the theoretical arguments for the architecture.\n\nFor CIFAR experiments, the obtained improvements compared to the baseline (wide resnets) are very small and therefore it is important to report the standard deviations (or all results) in both cases. It's not clear that the differences are significant.\n\nSome questions regarding g(): Was g() always ReLU? Doesn't this have potential problems with g(k) becoming 0 and never recovering? Does this also mean that for the wide resnet in Fig 7, most residual blocks are zeroed out since k < 0?"
  },
  {
    "people": [
      "Greff"
    ],
    "review": "We have also added a section (3.3) that connects our technique to the Unrolled Iterative Estimation interpretation of Highway and ResNets. We further elaborate on how the learned k values are indicators of the abstraction jumps in representations defined in Greff et al, and analyze the k values for both experiments (MNIST and CIFAR) under this perspective."
  },
  {
    "people": [
      "Srivastava"
    ],
    "review": "This paper proposes to learn a single scalar gating parameter instead of a full gating tensor in highway networks. The claim is that such gating is easier to learn and allows a network to flexibly utilize computation.\n\nThe basic idea of the paper is simple and is clearly presented. It is a natural simplification of highway networks to allow easily \"shutting off\" layers while keeping number of additional parameters low. However, in this regard the paper leaves out a few key points. Firstly, it does not mention that the gates in highway networks are data-dependent which is potentially more powerful than learning a fixed gate for all units and independent of data. Secondly, it does not do a fair comparison with highway networks to show that this simpler formulation is indeed easier to learn.\n\nDid the authors try their original design of u = g(k)f(x) + (1 - g(k))x where f(x) is a plain layer instead of a residual layer? Based on the arguments made in the paper, this should work fine. Why wasn't it tested? If it doesn't work, are the arguments incorrect or incomplete?\n\nFor the MNIST experiments, since the hyperparameters are fixed, the plots are misleading if any dependence on hyperparameters exists for the different models. This experiment appears to be based on Srivastava et al (2015). If it is indeed designed to test optimization at aggressive depths, then apart from doing a hyperparameter search, the authors should not use regularization such as dropout or batch norm, which do not appear in the theoretical arguments for the architecture.\n\nFor CIFAR experiments, the obtained improvements compared to the baseline (wide resnets) are very small and therefore it is important to report the standard deviations (or all results) in both cases. It's not clear that the differences are significant.\n\nSome questions regarding g(): Was g() always ReLU? Doesn't this have potential problems with g(k) becoming 0 and never recovering? Does this also mean that for the wide resnet in Fig 7, most residual blocks are zeroed out since k < 0?"
  },
  {
    "people": [
      "Scott Reed",
      "Nando de Freitas",
      "Marcin Andrychowicz",
      "Karol Kurach"
    ],
    "review": "Thank you to all the reviewers for their comments. We believe that these\nare the primary points raised by the reviewers:\n\n1. A comparison with a neural programming technique which does not\n   generate code would be a valuable addition to our experiments.\n\n   We did not include a comparison to a network which does not\n   generate source code because because they usually require\n   substantially more training data than the 5 input-output examples\n   we provide to have any success at generalization.\n   While it would be interesting to show this effect in experiments,\n   the wide variety of different models and training strategies,\n   the custom structure of list data and list-aware objective function in our work,\n   together with the lack of released standard implementations,\n   makes it unclear what neural programming baseline (and with what\n   training regime) would be appropriate.\n\n2. The tasks our network can learn are simple. In particular, we do\n   not consider sorting or merging problems.\n\n   Although the tasks that we considered are simple, they are more complex\n   than the tasks which many other neural programming approaches can handle,\n   particularly as we test for perfect generalization.\n\n   The approaches to learning to sort do so using program traces,\n   which provide much stronger supervision than the input-output examples\n   that we use [1], or use specialized memory representations that simplify\n   sorting [2]. \n\n3. This paper does not conclusively show that gradient-based\n   evaluators are appropriate for program induction.\n\n   This paper certainly does not contradict the findings of the\n   original TerpreT paper that discrete solvers are good backends for\n   program induction. Our motivation in this paper is to improve gradient-based\n   program search, and to understand the effect of different design choices\n   that arise when building differentiable interpreters. Even if gradient descent\n   isn't currently the best method for program induction, we believe it merits\n   further study. It is a very new idea, and it's feasible to us that seemingly\n   subtle design decisions could make a big difference in its performance (indeed,\n   this is one take-away from our experiments). Further, since gradient descent\n   is very different from alternatives, improving its performance may enable \n   new uses of program synthesis such as jointly inducing programs and training\n   neural network subcomponents as in [3], using SGD to scale up to large data\n   sets, or giving new ways of thinking about noisy data in program synthesis.\n\n   Finally, the recommendations for the design of such evaluators\n   discussed in this paper are not necessarily restricted to TerpreT-based models,\n   and we believe that our design recommendations apply to related\n   neural architectures that try to learn algorithmic patterns.\n\n4. How will this model generalize to programs that can't be solved\n   using the prefix-loop-suffix structure?\n\n   Defining new program structures is simple, and our current\n   implementation allows the optimizer to choose between several\n   program structures. More loops could be added if desired, and more\n   looping schemes could be added to extend the class of programs\n   which can be learned.\n\n5. TerpreT is not yet publicly available.\n\n   TerpreT is nearly ready for public release. Approval for open-sourcing\n   under the MIT license has been obtained, and we are currently in the\n   process of documenting the source code to publish it (with all models\n   used in this paper).\n\n\nReferences:\n[1] Scott Reed and Nando de Freitas. Neural Programmer-Interpreters.\n    In ICLR 2016.\n[2] Marcin Andrychowicz, Karol Kurach. Learning Efficient Algorithms\n    with Hierarchical Attentive Memory. "
  },
  {
    "people": [
      "Scott Reed",
      "Nando de Freitas",
      "Marcin Andrychowicz",
      "Karol Kurach"
    ],
    "review": "Thank you to all the reviewers for their comments. We believe that these\nare the primary points raised by the reviewers:\n\n1. A comparison with a neural programming technique which does not\n   generate code would be a valuable addition to our experiments.\n\n   We did not include a comparison to a network which does not\n   generate source code because because they usually require\n   substantially more training data than the 5 input-output examples\n   we provide to have any success at generalization.\n   While it would be interesting to show this effect in experiments,\n   the wide variety of different models and training strategies,\n   the custom structure of list data and list-aware objective function in our work,\n   together with the lack of released standard implementations,\n   makes it unclear what neural programming baseline (and with what\n   training regime) would be appropriate.\n\n2. The tasks our network can learn are simple. In particular, we do\n   not consider sorting or merging problems.\n\n   Although the tasks that we considered are simple, they are more complex\n   than the tasks which many other neural programming approaches can handle,\n   particularly as we test for perfect generalization.\n\n   The approaches to learning to sort do so using program traces,\n   which provide much stronger supervision than the input-output examples\n   that we use [1], or use specialized memory representations that simplify\n   sorting [2]. \n\n3. This paper does not conclusively show that gradient-based\n   evaluators are appropriate for program induction.\n\n   This paper certainly does not contradict the findings of the\n   original TerpreT paper that discrete solvers are good backends for\n   program induction. Our motivation in this paper is to improve gradient-based\n   program search, and to understand the effect of different design choices\n   that arise when building differentiable interpreters. Even if gradient descent\n   isn't currently the best method for program induction, we believe it merits\n   further study. It is a very new idea, and it's feasible to us that seemingly\n   subtle design decisions could make a big difference in its performance (indeed,\n   this is one take-away from our experiments). Further, since gradient descent\n   is very different from alternatives, improving its performance may enable \n   new uses of program synthesis such as jointly inducing programs and training\n   neural network subcomponents as in [3], using SGD to scale up to large data\n   sets, or giving new ways of thinking about noisy data in program synthesis.\n\n   Finally, the recommendations for the design of such evaluators\n   discussed in this paper are not necessarily restricted to TerpreT-based models,\n   and we believe that our design recommendations apply to related\n   neural architectures that try to learn algorithmic patterns.\n\n4. How will this model generalize to programs that can't be solved\n   using the prefix-loop-suffix structure?\n\n   Defining new program structures is simple, and our current\n   implementation allows the optimizer to choose between several\n   program structures. More loops could be added if desired, and more\n   looping schemes could be added to extend the class of programs\n   which can be learned.\n\n5. TerpreT is not yet publicly available.\n\n   TerpreT is nearly ready for public release. Approval for open-sourcing\n   under the MIT license has been obtained, and we are currently in the\n   process of documenting the source code to publish it (with all models\n   used in this paper).\n\n\nReferences:\n[1] Scott Reed and Nando de Freitas. Neural Programmer-Interpreters.\n    In ICLR 2016.\n[2] Marcin Andrychowicz, Karol Kurach. Learning Efficient Algorithms\n    with Hierarchical Attentive Memory. "
  },
  {
    "people": [
      "Ngiam"
    ],
    "review": "CONTRIBUTIONS \nThis paper introduces a method for learning semantic \"word-like\" units jointly from audio and visual data. The authors use a multimodal neural network architecture which accepts both image and audio (as spectrograms) inputs. Joint training allows one to embed both image and spoken language captions into a shared representation space. Audio-visual groundings are generated by measuring affinity between image patches and audio clips. This allows the model to relate specific visual regions to specific audio segments. Experiments cover image search (audio to image) and annotation (image to audio) tasks and acoustic word discovery.\n\n\nNOVELTY+SIGNIFICANCE\nAs correctly mentioned in Section 1.2, the computer vision and natural language communities have studied multimodal learning for use in image captioning and retrieval. With regards to multimodal learning, this paper offers incremental advancements since it primarily uses a novel combination of input modalities (audio and images).\n\nHowever, bidirectional image/audio retrieval has already been explored by the authors in prior work (Harwath et al, NIPS 2016). Apart from minor differences in data and CNN architecture, the training procedure in this submission is identical to this prior work. The novelty in this submission is therefore the procedure for using the trained model for associating image regions with audio subsequences.\n\nThe methods employed for this association are relatively straightforward combination of standard techniques with limited novelty. The trained model is used to compute alignment scores between densely sampled image regions and audio subsequences; from these alignment scores a number of heuristics are applied to associate clusters of image regions with clusters of audio subsequences.\n\n\nMISSING CITATION\nThere is a lot of work in this area spanning computer vision, natural language, and speech recognition. One key missing reference:\n\nNgiam, et al. \"Multimodal deep learning.\" ICML 2011\n\n\nPOSITIVE POINTS\n- Using more data and an improved CNN architecture, this paper improves on prior work for bidirectional image/audio retrieval\n- The presented method performs efficient acoustic pattern discovery\n- The audio-visual grounding combined with the image and acoustic cluster analysis is successful at discovering audio-visual cluster pairs\n\nNEGATIVE POINTS\n- Limited novelty, especially compared with Harwath et al, NIPS 2016\n- Although it gives good results, the clustering method has limited novelty and feels heuristic\n- The proposed method includes many hyperparameters (patch size, acoustic duration, VAD threshold, IoU threshold, number of k-means clusters, etc) and there is no discussion of how these were set or the sensitivity of the method to these choices"
  },
  {
    "people": [
      "Ngiam"
    ],
    "review": "CONTRIBUTIONS \nThis paper introduces a method for learning semantic \"word-like\" units jointly from audio and visual data. The authors use a multimodal neural network architecture which accepts both image and audio (as spectrograms) inputs. Joint training allows one to embed both image and spoken language captions into a shared representation space. Audio-visual groundings are generated by measuring affinity between image patches and audio clips. This allows the model to relate specific visual regions to specific audio segments. Experiments cover image search (audio to image) and annotation (image to audio) tasks and acoustic word discovery.\n\n\nNOVELTY+SIGNIFICANCE\nAs correctly mentioned in Section 1.2, the computer vision and natural language communities have studied multimodal learning for use in image captioning and retrieval. With regards to multimodal learning, this paper offers incremental advancements since it primarily uses a novel combination of input modalities (audio and images).\n\nHowever, bidirectional image/audio retrieval has already been explored by the authors in prior work (Harwath et al, NIPS 2016). Apart from minor differences in data and CNN architecture, the training procedure in this submission is identical to this prior work. The novelty in this submission is therefore the procedure for using the trained model for associating image regions with audio subsequences.\n\nThe methods employed for this association are relatively straightforward combination of standard techniques with limited novelty. The trained model is used to compute alignment scores between densely sampled image regions and audio subsequences; from these alignment scores a number of heuristics are applied to associate clusters of image regions with clusters of audio subsequences.\n\n\nMISSING CITATION\nThere is a lot of work in this area spanning computer vision, natural language, and speech recognition. One key missing reference:\n\nNgiam, et al. \"Multimodal deep learning.\" ICML 2011\n\n\nPOSITIVE POINTS\n- Using more data and an improved CNN architecture, this paper improves on prior work for bidirectional image/audio retrieval\n- The presented method performs efficient acoustic pattern discovery\n- The audio-visual grounding combined with the image and acoustic cluster analysis is successful at discovering audio-visual cluster pairs\n\nNEGATIVE POINTS\n- Limited novelty, especially compared with Harwath et al, NIPS 2016\n- Although it gives good results, the clustering method has limited novelty and feels heuristic\n- The proposed method includes many hyperparameters (patch size, acoustic duration, VAD threshold, IoU threshold, number of k-means clusters, etc) and there is no discussion of how these were set or the sensitivity of the method to these choices\n"
  },
  {
    "people": [
      "Ngiam"
    ],
    "review": "CONTRIBUTIONS \nThis paper introduces a method for learning semantic \"word-like\" units jointly from audio and visual data. The authors use a multimodal neural network architecture which accepts both image and audio (as spectrograms) inputs. Joint training allows one to embed both image and spoken language captions into a shared representation space. Audio-visual groundings are generated by measuring affinity between image patches and audio clips. This allows the model to relate specific visual regions to specific audio segments. Experiments cover image search (audio to image) and annotation (image to audio) tasks and acoustic word discovery.\n\n\nNOVELTY+SIGNIFICANCE\nAs correctly mentioned in Section 1.2, the computer vision and natural language communities have studied multimodal learning for use in image captioning and retrieval. With regards to multimodal learning, this paper offers incremental advancements since it primarily uses a novel combination of input modalities (audio and images).\n\nHowever, bidirectional image/audio retrieval has already been explored by the authors in prior work (Harwath et al, NIPS 2016). Apart from minor differences in data and CNN architecture, the training procedure in this submission is identical to this prior work. The novelty in this submission is therefore the procedure for using the trained model for associating image regions with audio subsequences.\n\nThe methods employed for this association are relatively straightforward combination of standard techniques with limited novelty. The trained model is used to compute alignment scores between densely sampled image regions and audio subsequences; from these alignment scores a number of heuristics are applied to associate clusters of image regions with clusters of audio subsequences.\n\n\nMISSING CITATION\nThere is a lot of work in this area spanning computer vision, natural language, and speech recognition. One key missing reference:\n\nNgiam, et al. \"Multimodal deep learning.\" ICML 2011\n\n\nPOSITIVE POINTS\n- Using more data and an improved CNN architecture, this paper improves on prior work for bidirectional image/audio retrieval\n- The presented method performs efficient acoustic pattern discovery\n- The audio-visual grounding combined with the image and acoustic cluster analysis is successful at discovering audio-visual cluster pairs\n\nNEGATIVE POINTS\n- Limited novelty, especially compared with Harwath et al, NIPS 2016\n- Although it gives good results, the clustering method has limited novelty and feels heuristic\n- The proposed method includes many hyperparameters (patch size, acoustic duration, VAD threshold, IoU threshold, number of k-means clusters, etc) and there is no discussion of how these were set or the sensitivity of the method to these choices"
  },
  {
    "people": [
      "Ngiam"
    ],
    "review": "CONTRIBUTIONS \nThis paper introduces a method for learning semantic \"word-like\" units jointly from audio and visual data. The authors use a multimodal neural network architecture which accepts both image and audio (as spectrograms) inputs. Joint training allows one to embed both image and spoken language captions into a shared representation space. Audio-visual groundings are generated by measuring affinity between image patches and audio clips. This allows the model to relate specific visual regions to specific audio segments. Experiments cover image search (audio to image) and annotation (image to audio) tasks and acoustic word discovery.\n\n\nNOVELTY+SIGNIFICANCE\nAs correctly mentioned in Section 1.2, the computer vision and natural language communities have studied multimodal learning for use in image captioning and retrieval. With regards to multimodal learning, this paper offers incremental advancements since it primarily uses a novel combination of input modalities (audio and images).\n\nHowever, bidirectional image/audio retrieval has already been explored by the authors in prior work (Harwath et al, NIPS 2016). Apart from minor differences in data and CNN architecture, the training procedure in this submission is identical to this prior work. The novelty in this submission is therefore the procedure for using the trained model for associating image regions with audio subsequences.\n\nThe methods employed for this association are relatively straightforward combination of standard techniques with limited novelty. The trained model is used to compute alignment scores between densely sampled image regions and audio subsequences; from these alignment scores a number of heuristics are applied to associate clusters of image regions with clusters of audio subsequences.\n\n\nMISSING CITATION\nThere is a lot of work in this area spanning computer vision, natural language, and speech recognition. One key missing reference:\n\nNgiam, et al. \"Multimodal deep learning.\" ICML 2011\n\n\nPOSITIVE POINTS\n- Using more data and an improved CNN architecture, this paper improves on prior work for bidirectional image/audio retrieval\n- The presented method performs efficient acoustic pattern discovery\n- The audio-visual grounding combined with the image and acoustic cluster analysis is successful at discovering audio-visual cluster pairs\n\nNEGATIVE POINTS\n- Limited novelty, especially compared with Harwath et al, NIPS 2016\n- Although it gives good results, the clustering method has limited novelty and feels heuristic\n- The proposed method includes many hyperparameters (patch size, acoustic duration, VAD threshold, IoU threshold, number of k-means clusters, etc) and there is no discussion of how these were set or the sensitivity of the method to these choices\n"
  },
  {
    "people": [
      "Kaiser",
      "Sutskever"
    ],
    "review": "The authors investigate the neural GPU model introduced by Kaiser and Sutskever. In section 3 they claim its performance is due to the O(n^2) number of steps it can perform for each example. In the subsequent section they highlight the importance of curriculum training and empirically show that larger models generalize better. In section 5 they construct examples that reveal failure modes. In the last section they compare the performance given different input formats.\n\nThe paper is well written. It contains an exhaustive set of experiments which provide insight into the details of training the neural GPU model. It pushes the boundary of algorithms that can be learned further. On the other hand, the paper seems to lack a coherent message. It also fails to provide any insight into the how and why of the observations made (i.e. why curriculum training is essential and why certain failure modes exist).\n\nThe introduction contains several statements which should be qualified or explained further. As far as I am aware statistical learning theory does not guarantee that empirical risk minimization is consistent when the number of parameters is larger than the number of examples; the generalization performance depends on the VC dimension of the function space instead. Furthermore, the suggested link between adversarial examples and learning algorithms is tenuous, and references or a further explanation should be provided for the contentious statement that deep neural networks are able to match the performance of any parallel machine learning algorithm.\n\nThe authors argue that the neural GPU performs O(n^2) \u201csteps\u201d on each example, which allows it to learn algorithms with super-linear complexity such as multiplication. This analysis seems to overlook the parallel nature of the neural GPU architecture: Both addition and multiplication have O(log n) time complexity when parallelism is used (cf. a carry-lookahead adder and a Wallace tree respectively).\n\nIn section 4 the authors show that their larger models generalize better, which they argue is not self-evident. However, since both training and test error decrease it is likely that the smaller models are underfitting, in which case it is not counter-intuitive at all that a larger model would have better generalization error.\n\nIt is interesting to see that progressively decreasing the number of terms and increasing the radix of the number system works well as a learning curriculum, although it would be nice to have a stronger intuitive or theoretical justification for the latter.\n\nThe final section claims that neural GPUs are cellular automata. Further justification for this statement would be useful, since cellular automata are discrete models, and the equivalence between both models is in no way obvious. The relationship between global operations and changing the input format is circuitous.\n\nIn conclusion, the paper provides some useful insights into the neural GPU model, but does not introduce original extensions to the model and does not explain any fundamental limitations. Several statements require stronger substantiation.\n\nPro:\n\n* Well written\n* Exhaustive set of experiments\n* Learning algorithms with decimal representation\n* Available source code\n\nCons:\n\n* No coherent hypothesis/premise advanced\n* Two or three bold statements without explanation or references\n* Some unclarity in experimental details\n* Limited novelty and originality factor\n\nTypos: add minus in \u201cchance of carrying k digits is 10^k\u201d (section 5); remove \u201care\u201d from \u201cthe larger models with 512 filters are achieve\u201d (section 4); add \u201ca\u201d in \u201csuch model doesn\u2019t generalize\u201d (section 4)."
  },
  {
    "people": [
      "Kaiser",
      "Sutskever"
    ],
    "review": "The authors investigate the neural GPU model introduced by Kaiser and Sutskever. In section 3 they claim its performance is due to the O(n^2) number of steps it can perform for each example. In the subsequent section they highlight the importance of curriculum training and empirically show that larger models generalize better. In section 5 they construct examples that reveal failure modes. In the last section they compare the performance given different input formats.\n\nThe paper is well written. It contains an exhaustive set of experiments which provide insight into the details of training the neural GPU model. It pushes the boundary of algorithms that can be learned further. On the other hand, the paper seems to lack a coherent message. It also fails to provide any insight into the how and why of the observations made (i.e. why curriculum training is essential and why certain failure modes exist).\n\nThe introduction contains several statements which should be qualified or explained further. As far as I am aware statistical learning theory does not guarantee that empirical risk minimization is consistent when the number of parameters is larger than the number of examples; the generalization performance depends on the VC dimension of the function space instead. Furthermore, the suggested link between adversarial examples and learning algorithms is tenuous, and references or a further explanation should be provided for the contentious statement that deep neural networks are able to match the performance of any parallel machine learning algorithm.\n\nThe authors argue that the neural GPU performs O(n^2) \u201csteps\u201d on each example, which allows it to learn algorithms with super-linear complexity such as multiplication. This analysis seems to overlook the parallel nature of the neural GPU architecture: Both addition and multiplication have O(log n) time complexity when parallelism is used (cf. a carry-lookahead adder and a Wallace tree respectively).\n\nIn section 4 the authors show that their larger models generalize better, which they argue is not self-evident. However, since both training and test error decrease it is likely that the smaller models are underfitting, in which case it is not counter-intuitive at all that a larger model would have better generalization error.\n\nIt is interesting to see that progressively decreasing the number of terms and increasing the radix of the number system works well as a learning curriculum, although it would be nice to have a stronger intuitive or theoretical justification for the latter.\n\nThe final section claims that neural GPUs are cellular automata. Further justification for this statement would be useful, since cellular automata are discrete models, and the equivalence between both models is in no way obvious. The relationship between global operations and changing the input format is circuitous.\n\nIn conclusion, the paper provides some useful insights into the neural GPU model, but does not introduce original extensions to the model and does not explain any fundamental limitations. Several statements require stronger substantiation.\n\nPro:\n\n* Well written\n* Exhaustive set of experiments\n* Learning algorithms with decimal representation\n* Available source code\n\nCons:\n\n* No coherent hypothesis/premise advanced\n* Two or three bold statements without explanation or references\n* Some unclarity in experimental details\n* Limited novelty and originality factor\n\nTypos: add minus in \u201cchance of carrying k digits is 10^k\u201d (section 5); remove \u201care\u201d from \u201cthe larger models with 512 filters are achieve\u201d (section 4); add \u201ca\u201d in \u201csuch model doesn\u2019t generalize\u201d (section 4)."
  },
  {
    "people": [
      "Soumith Chintala"
    ],
    "review": "This paper presents an attention based recurrent approach to one-shot learning. It reports quite strong experimental results (surpassing human performance/HBPL) on the Omniglot dataset, which is somewhat surprising because it seems to make use of very standard neural network machinery. The authors also note that other have helped verify the results (did Soumith Chintala reproduce the results?) and do provide source code.\n\nAfter reading this paper, I'm left a little perplexed as to where the big performance improvements are coming from as it seems to share a lot of the same components of previous work. If the author's could report result from a broader suite of experiments like in previous work (e.g matching networks), it would much more convincing. An ablation study would also help with understanding why this model does so well."
  },
  {
    "people": [
      "Soumith Chintala"
    ],
    "review": "This paper presents an attention based recurrent approach to one-shot learning. It reports quite strong experimental results (surpassing human performance/HBPL) on the Omniglot dataset, which is somewhat surprising because it seems to make use of very standard neural network machinery. The authors also note that other have helped verify the results (did Soumith Chintala reproduce the results?) and do provide source code.\n\nAfter reading this paper, I'm left a little perplexed as to where the big performance improvements are coming from as it seems to share a lot of the same components of previous work. If the author's could report result from a broader suite of experiments like in previous work (e.g matching networks), it would much more convincing. An ablation study would also help with understanding why this model does so well."
  },
  {
    "people": [
      "Chen"
    ],
    "review": "This paper describe an implementation of delayed synchronize SGD method for multi-GPU deep ne training.\nComments\n1) The described manual implementation of delayed synchronization and state protection is helpful. However, such dependency been implemented by a dependency scheduler, without doing threading manually.\n2) The overlap of computation and communication is a known technique implemented in existing solutions such as TensorFlow(as described in Chen et.al) and MXNet. The claimed contribution of this point is somewhat limited.\n3) The convergence accuracy is only reported for the beginning iterations and only on AlexNet. It would be more helpful to include convergence curve till the end for all compared networks.\n\nIn summary, this is paper implements a variant of delayed SyncSGD approach. I find the novelty of the system somewhat limited (due to comment (2)). The experiments should have been improved to demonstrate the advantage of proposed approach.\n\n\n\n\n"
  },
  {
    "people": [
      "Chen",
      "Chen",
      "Chen"
    ],
    "review": "Full disclosure: I am an author on the follow-up paper to Chen et al. (2016).\n\nAlthough the paper cites Chen et al. (2016) as a starting point of the proposed approach, it does not mention the key message of Chen et al. (2016) of using backup workers to hide straggler effects in synchronous gradient descent. Were backup workers used as part of the synchronous approaches?\n\nAlso, the paper does not evaluate the wallclock time to reach convergence or a given accuracy. Even though the AGD approach has more iterations per second, it also leads to poorer convergence (as acknowledged in Table 1). As a specific example, to reach 0.01688 accuracy requires <2000 iterations of SGD, <3000 iterations of AGD (1 comm), and 5000 iterations of AGD (2 comm). Hence, for AGD to be faster than SGD in wallclock time, AGD must have at 1.5x iterations per second relative to SGD, which does not appear to the case from Figure 1. Of course, this example is only for a low accuracy of 0.01688, and the proposed AGD could very well reach the final convergence of 54% in lesser time. It would be very helpful if the authors could provide that information."
  },
  {
    "people": [
      "Chen"
    ],
    "review": "This paper describe an implementation of delayed synchronize SGD method for multi-GPU deep ne training.\nComments\n1) The described manual implementation of delayed synchronization and state protection is helpful. However, such dependency been implemented by a dependency scheduler, without doing threading manually.\n2) The overlap of computation and communication is a known technique implemented in existing solutions such as TensorFlow(as described in Chen et.al) and MXNet. The claimed contribution of this point is somewhat limited.\n3) The convergence accuracy is only reported for the beginning iterations and only on AlexNet. It would be more helpful to include convergence curve till the end for all compared networks.\n\nIn summary, this is paper implements a variant of delayed SyncSGD approach. I find the novelty of the system somewhat limited (due to comment (2)). The experiments should have been improved to demonstrate the advantage of proposed approach.\n\n\n\n\n"
  },
  {
    "people": [
      "Chen",
      "Chen",
      "Chen"
    ],
    "review": "Full disclosure: I am an author on the follow-up paper to Chen et al. (2016).\n\nAlthough the paper cites Chen et al. (2016) as a starting point of the proposed approach, it does not mention the key message of Chen et al. (2016) of using backup workers to hide straggler effects in synchronous gradient descent. Were backup workers used as part of the synchronous approaches?\n\nAlso, the paper does not evaluate the wallclock time to reach convergence or a given accuracy. Even though the AGD approach has more iterations per second, it also leads to poorer convergence (as acknowledged in Table 1). As a specific example, to reach 0.01688 accuracy requires <2000 iterations of SGD, <3000 iterations of AGD (1 comm), and 5000 iterations of AGD (2 comm). Hence, for AGD to be faster than SGD in wallclock time, AGD must have at 1.5x iterations per second relative to SGD, which does not appear to the case from Figure 1. Of course, this example is only for a low accuracy of 0.01688, and the proposed AGD could very well reach the final convergence of 54% in lesser time. It would be very helpful if the authors could provide that information."
  },
  {
    "people": [
      "Pythagoras"
    ],
    "review": "The paper presents a new dataset and initial machine-learning results for an interesting problem, namely, higher-order logic theorem proving. This dataset is of great potential value in the development of deep-learning approaches for (mathematical) reasoning.\n \n As a personal side note: It would be great if the camera-ready version of the paper would provide somewhat more context on how the state-of-the-art approaches in automatic theorem proving perform on the conjectures in HolStep. Also, it would be good to clarify how the dataset makes sure there is no \"overlap\" between the training and test set: for instance, a typical proof of the Cauchy-Schwarz inequality employs the Pythagorean theorem: how can we be sure that we don't have Cauchy-Schwarz in the training set and Pythagoras in the test set?"
  },
  {
    "people": [
      "Pythagoras"
    ],
    "review": "The paper presents a new dataset and initial machine-learning results for an interesting problem, namely, higher-order logic theorem proving. This dataset is of great potential value in the development of deep-learning approaches for (mathematical) reasoning.\n \n As a personal side note: It would be great if the camera-ready version of the paper would provide somewhat more context on how the state-of-the-art approaches in automatic theorem proving perform on the conjectures in HolStep. Also, it would be good to clarify how the dataset makes sure there is no \"overlap\" between the training and test set: for instance, a typical proof of the Cauchy-Schwarz inequality employs the Pythagorean theorem: how can we be sure that we don't have Cauchy-Schwarz in the training set and Pythagoras in the test set?"
  },
  {
    "people": [
      "Cao"
    ],
    "review": "First, I'd like to thank the authors for their answers and clarifications.\nI find, the presentation of the multi-stage version of the model much clearer now.\n\nPros:\n\n+ The paper states a sparse coding problem using cosine loss, which allows to solve the problem in a single pass.\n\n+ The energy-based formulation allows bi-directional coding that incorporates top-down and bottom-up information in the feature extraction process. \n\nCons:\n\n+ The cost of running the evaluation could be large in the  multi-class setting, rendering the approach less attractive and the computational cost comparable to recurrent architectures.\n\n+ While the model is competitive and improves over the baseline, the paper would be more convincing with other comparisons (see text). The experimental evaluation is limited (a single database and a single baseline)\n\n------\n\nThe motivation of the sparse coding scheme is to perform inference in a feed forward manner. This property does not hold in the multi stage setting, thus optimization would be required (as clarified by the authors).\n\nHaving an efficient way of performing a bi-directional coding scheme is very interesting. As the authors clarified, this could not necessarily be the case, as the model needs to be evaluated many times for performing a classification.\n\nMaybe an interesting combination would be to run the model without any class-specific bias, and evaluation only the top K predictions with the energy-based setting.\n\nHaving said this, it would be good to include a discussion (if not direct comparisons) of the trade-offs of using a model as the one proposed by Cao et al. Eg. computational costs, performance.\n\nUsing the bidirectional coding only on the top layers seems reasonable: one can get a good low level representation in a class agnostic way. This, however could be studied in more detail, for instance showing empirically the trade offs. If I understand correctly, now only one setting is being reported.\n\nFinally, the authors mention that one benefit of using the architecture derived from the proposed coding method is the spherical normalization scheme, which can lead to smoother optimization dynamics. Does the baseline (or model) use batch-normalization? If not, seems relevant to test.\n\n\nMinor comments:\n\nI find figure 2 (d) confusing. I would not plot this setting as it does not lead to a function (as the authors state in the text)."
  },
  {
    "people": [
      "Cao"
    ],
    "review": "\nFirst, I'd like to thank the authors for their answers and clarifications.\nI find, the presentation of the multi-stage version of the model much clearer now.\n\nPros:\n\n+ The paper states a sparse coding problem using cosine loss, which allows to solve the problem in a single pass.\n\n+ The energy-based formulation allows bi-directional coding that incorporates top-down and bottom-up information in the feature extraction process. \n\nCons:\n\n+ The cost of running the evaluation could be large in the  multi-class setting, rendering the approach less attractive and the computational cost comparable to recurrent architectures.\n\n+ While the model is competitive and improves over the baseline, the paper would be more convincing with other comparisons (see text). The experimental evaluation is limited (a single database and a single baseline)\n\n------\n\nThe motivation of the sparse coding scheme is to perform inference in a feed forward manner. This property does not hold in the multi stage setting, thus optimization would be required (as clarified by the authors).\n\nHaving an efficient way of performing a bi-directional coding scheme is very interesting. As the authors clarified, this could not necessarily be the case, as the model needs to be evaluated many times for performing a classification.\n\nMaybe an interesting combination would be to run the model without any class-specific bias, and evaluation only the top K predictions with the energy-based setting.\n\nHaving said this, it would be good to include a discussion (if not direct comparisons) of the trade-offs of using a model as the one proposed by Cao et al. Eg. computational costs, performance.\n\nUsing the bidirectional coding only on the top layers seems reasonable: one can get a good low level representation in a class agnostic way. This, however could be studied in more detail, for instance showing empirically the trade offs. If I understand correctly, now only one setting is being reported.\n\nFinally, the authors mention that one benefit of using the architecture derived from the proposed coding method is the spherical normalization scheme, which can lead to smoother optimization dynamics. Does the baseline (or model) use batch-normalization? If not, seems relevant to test.\n\n\nMinor comments:\n\nI find figure 2 (d) confusing. I would not plot this setting as it does not lead to a function (as the authors state in the text).\n\n\n"
  },
  {
    "people": [
      "Cao"
    ],
    "review": "First, I'd like to thank the authors for their answers and clarifications.\nI find, the presentation of the multi-stage version of the model much clearer now.\n\nPros:\n\n+ The paper states a sparse coding problem using cosine loss, which allows to solve the problem in a single pass.\n\n+ The energy-based formulation allows bi-directional coding that incorporates top-down and bottom-up information in the feature extraction process. \n\nCons:\n\n+ The cost of running the evaluation could be large in the  multi-class setting, rendering the approach less attractive and the computational cost comparable to recurrent architectures.\n\n+ While the model is competitive and improves over the baseline, the paper would be more convincing with other comparisons (see text). The experimental evaluation is limited (a single database and a single baseline)\n\n------\n\nThe motivation of the sparse coding scheme is to perform inference in a feed forward manner. This property does not hold in the multi stage setting, thus optimization would be required (as clarified by the authors).\n\nHaving an efficient way of performing a bi-directional coding scheme is very interesting. As the authors clarified, this could not necessarily be the case, as the model needs to be evaluated many times for performing a classification.\n\nMaybe an interesting combination would be to run the model without any class-specific bias, and evaluation only the top K predictions with the energy-based setting.\n\nHaving said this, it would be good to include a discussion (if not direct comparisons) of the trade-offs of using a model as the one proposed by Cao et al. Eg. computational costs, performance.\n\nUsing the bidirectional coding only on the top layers seems reasonable: one can get a good low level representation in a class agnostic way. This, however could be studied in more detail, for instance showing empirically the trade offs. If I understand correctly, now only one setting is being reported.\n\nFinally, the authors mention that one benefit of using the architecture derived from the proposed coding method is the spherical normalization scheme, which can lead to smoother optimization dynamics. Does the baseline (or model) use batch-normalization? If not, seems relevant to test.\n\n\nMinor comments:\n\nI find figure 2 (d) confusing. I would not plot this setting as it does not lead to a function (as the authors state in the text)."
  },
  {
    "people": [
      "Cao"
    ],
    "review": "\nFirst, I'd like to thank the authors for their answers and clarifications.\nI find, the presentation of the multi-stage version of the model much clearer now.\n\nPros:\n\n+ The paper states a sparse coding problem using cosine loss, which allows to solve the problem in a single pass.\n\n+ The energy-based formulation allows bi-directional coding that incorporates top-down and bottom-up information in the feature extraction process. \n\nCons:\n\n+ The cost of running the evaluation could be large in the  multi-class setting, rendering the approach less attractive and the computational cost comparable to recurrent architectures.\n\n+ While the model is competitive and improves over the baseline, the paper would be more convincing with other comparisons (see text). The experimental evaluation is limited (a single database and a single baseline)\n\n------\n\nThe motivation of the sparse coding scheme is to perform inference in a feed forward manner. This property does not hold in the multi stage setting, thus optimization would be required (as clarified by the authors).\n\nHaving an efficient way of performing a bi-directional coding scheme is very interesting. As the authors clarified, this could not necessarily be the case, as the model needs to be evaluated many times for performing a classification.\n\nMaybe an interesting combination would be to run the model without any class-specific bias, and evaluation only the top K predictions with the energy-based setting.\n\nHaving said this, it would be good to include a discussion (if not direct comparisons) of the trade-offs of using a model as the one proposed by Cao et al. Eg. computational costs, performance.\n\nUsing the bidirectional coding only on the top layers seems reasonable: one can get a good low level representation in a class agnostic way. This, however could be studied in more detail, for instance showing empirically the trade offs. If I understand correctly, now only one setting is being reported.\n\nFinally, the authors mention that one benefit of using the architecture derived from the proposed coding method is the spherical normalization scheme, which can lead to smoother optimization dynamics. Does the baseline (or model) use batch-normalization? If not, seems relevant to test.\n\n\nMinor comments:\n\nI find figure 2 (d) confusing. I would not plot this setting as it does not lead to a function (as the authors state in the text).\n\n\n"
  },
  {
    "people": [
      "Bayes"
    ],
    "review": "Strengths\n\n- interesting to explore the connection between ReLU DNN and simplified SFNN\n- small task (MNIST)  is used to demonstrate the usefulness of the proposed training methods experimentally\n- the proposed, multi-stage training methods are simple to implement (despite lacking theoretical rigor)\n\n\nWeaknesses\n\n-no results are reported on real tasks with large training set\n\n-not clear exploration on the scalability of the learning methods when training data becomes larger\n\n-when the hidden layers become stochastic, the model shares uncertainty representation with deep Bayes networks or deep generative models (Deep Discriminative and Generative Models for Pattern Recognition , book chapter in \u201cPattern Recognition and Computer Vision\u201d, November 2015, Download PDF). Such connections should be discussed, especially wrt the use of uncertainty representation to benefit pattern recognition (i.e. supervised learning via Bayes rule) and to benefit the use of domain knowledge such as \u201cexplaining away\u201d.\n\n-would like to see connections with variational autoencoder models and training, which is also stochastic with hidden layers"
  },
  {
    "people": [
      "Bayes"
    ],
    "review": "Strengths\n\n- interesting to explore the connection between ReLU DNN and simplified SFNN\n- small task (MNIST)  is used to demonstrate the usefulness of the proposed training methods experimentally\n- the proposed, multi-stage training methods are simple to implement (despite lacking theoretical rigor)\n\n\nWeaknesses\n\n-no results are reported on real tasks with large training set\n\n-not clear exploration on the scalability of the learning methods when training data becomes larger\n\n-when the hidden layers become stochastic, the model shares uncertainty representation with deep Bayes networks or deep generative models (Deep Discriminative and Generative Models for Pattern Recognition , book chapter in \u201cPattern Recognition and Computer Vision\u201d, November 2015, Download PDF). Such connections should be discussed, especially wrt the use of uncertainty representation to benefit pattern recognition (i.e. supervised learning via Bayes rule) and to benefit the use of domain knowledge such as \u201cexplaining away\u201d.\n\n-would like to see connections with variational autoencoder models and training, which is also stochastic with hidden layers\n"
  },
  {
    "people": [
      "Neal",
      "Neal"
    ],
    "review": "Update: Because no revision of the paper has been provided by the authors, I am reducing my rating to \"marginally below acceptance\".\n\n----------\n\nThis paper addresses the problem of training stochastic feedforward neural networks.  It proposes to transfer weights from a deterministic deep neural network trained using standard procedures (including techniques such as dropout and batch normalization) to a stochastic network having the same topology.  The initial mechanism described for performing the transfer involves a rescaling of unit inputs and layer weights, and appropriate specification of the stochastic latent units if the DNN used for pretraining employs ReLU nonlinearities.  Initial experiments on MNIST classification and a toy generative task with a multimodal target distribution show that the simple transfer process works well if the DNN used for pretraining uses sigmoid nonlinearities, but not if the pretraining DNN uses ReLUs.  To tackle this problem, the paper introduces the \"simplified stochastic feedforward neural network,\" in which every stochastic layer is followed by a layer that takes an expectation over samples from its input, thus limiting the propagation of stochasticity in the network.  A modified process for transferring weights from a pretraining DNN to the simplified SFNN is described and justified.  The training process then occurs in three steps:  (1) pretrain a DNN, (2) transfer weights from the DNN to a simplified SFNN and continue training, and (3) optionally transfer the weights to a full SFNN and continue training or transfer them to a deterministic model (called DNN*) and continue training.  The third step can be skipped and the simplified SFNN may also be used directly as an inference model.  Experimental results on MNIST classification show that the use of simplified SFNN training can improve a deterministic DNN* model over a DNN baseline trained with batch normalization and dropout.  Experiments on two generative tasks (MNIST-half and the Toronto Faces Database) show that the proposed pretraining process improves test set negative log-likelihoods.  Finally, experiments on CIFAR-10, CIFAR-100, and SVHN with the LeNet-5, network-in-network, and wide residual network architectures show that use of a stochastic training step can improve performance of a deterministic (DNN*) model.\n\nIt is a bit confusing to refer to \"multi-modal\" tasks, when what is meant is \"generative tasks with a multimodal target distribution\" because \"multi-modal\" task can also refer to a learning task that crosses sensory modalities such as audio-visual speech recognition, text-based image retrieval, or image captioning.  I recommend that you use the more precise term (\"generative tasks with a multimodal target distribution\") early in the introduction and then say that you will refer to such tasks as \"multi-modal tasks\" in the rest of the paper for the sake of brevity.\n\nThe paper would be easier to read if \"SFNN\" were not used to refer to both the singular (\"stochastic feedforward neural network\") and plural (\"stochastic feedforward neural networks\") cases.  When the plural is meant, write \"SFNNs\".\n\nIn Table 1, why does the 3 hidden layer SFNN initialized from a ReLU DNN have so much worse of a test NLL than the 2 hidden layer SFNN initialized from a ReLU DNN?\n\nThe notation that uses superscripts to indicate layer indexes is confusing.  The reader naturally parses N\u00b2 as \"N squared\" and not as \"the number of units in the second layer.\"\n\nWhen you transfer weights back from the simplified SFNN to the DNN* model, do you need to perform some sort of rescaling that undoes the operations in Equation (8) in the paper?\n\nWhat does NCSFNN stand for in the supplementary material?\n\nPros\n+ The proposed model is easy to implement and apply to other tasks.\n+ The MNIST results showing that the stochastic model training can produce a deterministic model (called DNN* in the paper) that generalizes better than a DNN trained with batch normalization and dropout is quite exciting.\n\nCons\n- For the reasons outlined above, the paper is at times a bit hard to follow.\n- The results CIFAR-10, CIFAR-100, and SVHN would be more convincing if the baselines used dropout and batch normalization.  While this is shown on MINST, demonstration of a similar result on a more challenging task would strengthen the paper.\n\nMinor issues\n\nIt has been believed that stochastic \u2192 It is believed that stochastic\n\nunderlying these successes is on the efficient training methods \u2192 underlying these successes is efficient training methods\n\nnecessary in order to model complex stochastic natures in many real-world tasks \u2192 necessary in to model the complex stochastic nature of many real-world tasks\n\nstructured prediction, image generation and memory networks : memory networks are models, not tasks.\n\nFurthermore, it has been believed that SFNN \u2192 Furthermore, it is believed that SFNN\n\nusing backpropagation under the variational techniques and the reparameterization tricks  \u2192 using backpropagation with variational techniques and reparameterization tricks\n\nThere have been several efforts developing efficient training methods \u2192 There have been several efforts toward developing efficient training methods\n\nHowever, training SFNN is still significantly slower than doing DNN \u2192 However, training a SFNN is still significantly slower than training a DNN\n\ne.g., most prior works on this line have considered a \u2192 consequently most prior works in this area have considered a\n\nInstead of training SFNN directly \u2192 Instead of training a SFNN directly\n\nwhether pre-trained parameters of DNN \u2192 whether pre-trained parameters from a DNN\n\nwith further fine-tuning of light cost \u2192 with further low-cost fine-tuning\n\nrecent advances in DNN on its design and training \u2192 recent advances in DNN design and training\n\nit is rather believed that transferring parameters \u2192  it is believed that transferring parameters\n\nbut the opposite direction is unlikely possible \u2192 but the opposite is unlikely\n\nTo address the issues, we propose \u2192 To address these issues, we propose\n\nwhich intermediates between SFNN and DNN, \u2192 which is intermediate between SFNN and DNN,\n\nin forward pass and computing gradients in backward pass \u2192 in the forward pass and computing gradients in the backward pass\n\nin order to handle the issue in forward pass \u2192  in order to handle the issue in the forward pass\n\nNeal (1990) proposed a Gibbs sampling \u2192 Neal (1990) proposed Gibbs sampling\n\nfor making DNN and SFNN are equivalent \u2192 for making the DNN and SFNN equivalent\n\nin the case when DNN uses the unbounded ReLU \u2192 in the case when the DNN uses the unbounded ReLU\n\nare of ReLU-DNN type due to the gradient vanishing problem \u2192 are of the ReLU-DNN type because they mitigate the gradient vanishing problem\n\nmultiple modes in outupt space y \u2192 multiple modes in output space y\n\nThe only first hidden layer of DNN \u2192 Only the first hidden layer of the DNN\n\nis replaced by stochastic one, \u2192 is replaced by a stochastic layer,\n\nthe former significantly outperforms for the latter for the \u2192 the former significantly outperforms the latter for the\n\nsimple parameter transformations from DNN to SFNN are not clear to work in general, \u2192 simple parameter transformations from DNN to SFNN do not clearly work in general,\n\nis a special form of stochastic neural networks \u2192 is a special form of stochastic neural network\n\nAs like (3), the first layer is \u2192 As in (3), the first layer is\n\nThis connection naturally leads an efficient training procedure \u2192 This connection naturally leads to an efficient training procedure\n"
  },
  {
    "people": [
      "Hinton",
      "Hinton"
    ],
    "review": "In this paper, citations are appearing with the authors' first initials and last names, e.g. (Hinton, G. et al., 2012a) instead of the authors last names and no initials, e.g. (Hinton et al., 2012a).  I find the first initials to be very distracting.  Please reformat the paper to match the citation style of the ICLR 2017 template.\n"
  },
  {
    "people": [
      "Bayes"
    ],
    "review": "Strengths\n\n- interesting to explore the connection between ReLU DNN and simplified SFNN\n- small task (MNIST)  is used to demonstrate the usefulness of the proposed training methods experimentally\n- the proposed, multi-stage training methods are simple to implement (despite lacking theoretical rigor)\n\n\nWeaknesses\n\n-no results are reported on real tasks with large training set\n\n-not clear exploration on the scalability of the learning methods when training data becomes larger\n\n-when the hidden layers become stochastic, the model shares uncertainty representation with deep Bayes networks or deep generative models (Deep Discriminative and Generative Models for Pattern Recognition , book chapter in \u201cPattern Recognition and Computer Vision\u201d, November 2015, Download PDF). Such connections should be discussed, especially wrt the use of uncertainty representation to benefit pattern recognition (i.e. supervised learning via Bayes rule) and to benefit the use of domain knowledge such as \u201cexplaining away\u201d.\n\n-would like to see connections with variational autoencoder models and training, which is also stochastic with hidden layers"
  },
  {
    "people": [
      "Bayes"
    ],
    "review": "Strengths\n\n- interesting to explore the connection between ReLU DNN and simplified SFNN\n- small task (MNIST)  is used to demonstrate the usefulness of the proposed training methods experimentally\n- the proposed, multi-stage training methods are simple to implement (despite lacking theoretical rigor)\n\n\nWeaknesses\n\n-no results are reported on real tasks with large training set\n\n-not clear exploration on the scalability of the learning methods when training data becomes larger\n\n-when the hidden layers become stochastic, the model shares uncertainty representation with deep Bayes networks or deep generative models (Deep Discriminative and Generative Models for Pattern Recognition , book chapter in \u201cPattern Recognition and Computer Vision\u201d, November 2015, Download PDF). Such connections should be discussed, especially wrt the use of uncertainty representation to benefit pattern recognition (i.e. supervised learning via Bayes rule) and to benefit the use of domain knowledge such as \u201cexplaining away\u201d.\n\n-would like to see connections with variational autoencoder models and training, which is also stochastic with hidden layers\n"
  },
  {
    "people": [
      "Neal",
      "Neal"
    ],
    "review": "Update: Because no revision of the paper has been provided by the authors, I am reducing my rating to \"marginally below acceptance\".\n\n----------\n\nThis paper addresses the problem of training stochastic feedforward neural networks.  It proposes to transfer weights from a deterministic deep neural network trained using standard procedures (including techniques such as dropout and batch normalization) to a stochastic network having the same topology.  The initial mechanism described for performing the transfer involves a rescaling of unit inputs and layer weights, and appropriate specification of the stochastic latent units if the DNN used for pretraining employs ReLU nonlinearities.  Initial experiments on MNIST classification and a toy generative task with a multimodal target distribution show that the simple transfer process works well if the DNN used for pretraining uses sigmoid nonlinearities, but not if the pretraining DNN uses ReLUs.  To tackle this problem, the paper introduces the \"simplified stochastic feedforward neural network,\" in which every stochastic layer is followed by a layer that takes an expectation over samples from its input, thus limiting the propagation of stochasticity in the network.  A modified process for transferring weights from a pretraining DNN to the simplified SFNN is described and justified.  The training process then occurs in three steps:  (1) pretrain a DNN, (2) transfer weights from the DNN to a simplified SFNN and continue training, and (3) optionally transfer the weights to a full SFNN and continue training or transfer them to a deterministic model (called DNN*) and continue training.  The third step can be skipped and the simplified SFNN may also be used directly as an inference model.  Experimental results on MNIST classification show that the use of simplified SFNN training can improve a deterministic DNN* model over a DNN baseline trained with batch normalization and dropout.  Experiments on two generative tasks (MNIST-half and the Toronto Faces Database) show that the proposed pretraining process improves test set negative log-likelihoods.  Finally, experiments on CIFAR-10, CIFAR-100, and SVHN with the LeNet-5, network-in-network, and wide residual network architectures show that use of a stochastic training step can improve performance of a deterministic (DNN*) model.\n\nIt is a bit confusing to refer to \"multi-modal\" tasks, when what is meant is \"generative tasks with a multimodal target distribution\" because \"multi-modal\" task can also refer to a learning task that crosses sensory modalities such as audio-visual speech recognition, text-based image retrieval, or image captioning.  I recommend that you use the more precise term (\"generative tasks with a multimodal target distribution\") early in the introduction and then say that you will refer to such tasks as \"multi-modal tasks\" in the rest of the paper for the sake of brevity.\n\nThe paper would be easier to read if \"SFNN\" were not used to refer to both the singular (\"stochastic feedforward neural network\") and plural (\"stochastic feedforward neural networks\") cases.  When the plural is meant, write \"SFNNs\".\n\nIn Table 1, why does the 3 hidden layer SFNN initialized from a ReLU DNN have so much worse of a test NLL than the 2 hidden layer SFNN initialized from a ReLU DNN?\n\nThe notation that uses superscripts to indicate layer indexes is confusing.  The reader naturally parses N\u00b2 as \"N squared\" and not as \"the number of units in the second layer.\"\n\nWhen you transfer weights back from the simplified SFNN to the DNN* model, do you need to perform some sort of rescaling that undoes the operations in Equation (8) in the paper?\n\nWhat does NCSFNN stand for in the supplementary material?\n\nPros\n+ The proposed model is easy to implement and apply to other tasks.\n+ The MNIST results showing that the stochastic model training can produce a deterministic model (called DNN* in the paper) that generalizes better than a DNN trained with batch normalization and dropout is quite exciting.\n\nCons\n- For the reasons outlined above, the paper is at times a bit hard to follow.\n- The results CIFAR-10, CIFAR-100, and SVHN would be more convincing if the baselines used dropout and batch normalization.  While this is shown on MINST, demonstration of a similar result on a more challenging task would strengthen the paper.\n\nMinor issues\n\nIt has been believed that stochastic \u2192 It is believed that stochastic\n\nunderlying these successes is on the efficient training methods \u2192 underlying these successes is efficient training methods\n\nnecessary in order to model complex stochastic natures in many real-world tasks \u2192 necessary in to model the complex stochastic nature of many real-world tasks\n\nstructured prediction, image generation and memory networks : memory networks are models, not tasks.\n\nFurthermore, it has been believed that SFNN \u2192 Furthermore, it is believed that SFNN\n\nusing backpropagation under the variational techniques and the reparameterization tricks  \u2192 using backpropagation with variational techniques and reparameterization tricks\n\nThere have been several efforts developing efficient training methods \u2192 There have been several efforts toward developing efficient training methods\n\nHowever, training SFNN is still significantly slower than doing DNN \u2192 However, training a SFNN is still significantly slower than training a DNN\n\ne.g., most prior works on this line have considered a \u2192 consequently most prior works in this area have considered a\n\nInstead of training SFNN directly \u2192 Instead of training a SFNN directly\n\nwhether pre-trained parameters of DNN \u2192 whether pre-trained parameters from a DNN\n\nwith further fine-tuning of light cost \u2192 with further low-cost fine-tuning\n\nrecent advances in DNN on its design and training \u2192 recent advances in DNN design and training\n\nit is rather believed that transferring parameters \u2192  it is believed that transferring parameters\n\nbut the opposite direction is unlikely possible \u2192 but the opposite is unlikely\n\nTo address the issues, we propose \u2192 To address these issues, we propose\n\nwhich intermediates between SFNN and DNN, \u2192 which is intermediate between SFNN and DNN,\n\nin forward pass and computing gradients in backward pass \u2192 in the forward pass and computing gradients in the backward pass\n\nin order to handle the issue in forward pass \u2192  in order to handle the issue in the forward pass\n\nNeal (1990) proposed a Gibbs sampling \u2192 Neal (1990) proposed Gibbs sampling\n\nfor making DNN and SFNN are equivalent \u2192 for making the DNN and SFNN equivalent\n\nin the case when DNN uses the unbounded ReLU \u2192 in the case when the DNN uses the unbounded ReLU\n\nare of ReLU-DNN type due to the gradient vanishing problem \u2192 are of the ReLU-DNN type because they mitigate the gradient vanishing problem\n\nmultiple modes in outupt space y \u2192 multiple modes in output space y\n\nThe only first hidden layer of DNN \u2192 Only the first hidden layer of the DNN\n\nis replaced by stochastic one, \u2192 is replaced by a stochastic layer,\n\nthe former significantly outperforms for the latter for the \u2192 the former significantly outperforms the latter for the\n\nsimple parameter transformations from DNN to SFNN are not clear to work in general, \u2192 simple parameter transformations from DNN to SFNN do not clearly work in general,\n\nis a special form of stochastic neural networks \u2192 is a special form of stochastic neural network\n\nAs like (3), the first layer is \u2192 As in (3), the first layer is\n\nThis connection naturally leads an efficient training procedure \u2192 This connection naturally leads to an efficient training procedure\n"
  },
  {
    "people": [
      "Hinton",
      "Hinton"
    ],
    "review": "In this paper, citations are appearing with the authors' first initials and last names, e.g. (Hinton, G. et al., 2012a) instead of the authors last names and no initials, e.g. (Hinton et al., 2012a).  I find the first initials to be very distracting.  Please reformat the paper to match the citation style of the ICLR 2017 template.\n"
  },
  {
    "people": [
      "Eve",
      "Eve",
      "Adam",
      "Eve"
    ],
    "review": "As you noted for Figure 5 Left, sometimes it seems sufficient to tune learning rates. I see your argument for Figure 6 Right, \nbut \n1) not for all good learning rates make Adam fail, I guess you selected the one where it did (note that Adam was several times faster than Eve in the beginning)\n2) I don't buy \"Eve always converges\" because you show it only for 0.1 and since Eve is not Adam, 0.1 of Adam is not 0.1 of Eve because of d_t. \n\nTo my understanding, you define d_t over time with 3 hyperparameters. Similarly, one can define d_t directly. The behaviour of d_t that you show is not extraordinary and can be parameterized. If Eve is better than Adam, then looking at d_t we can directly see whether we underestimated or overestimated learning rates. You could argue that Eve does it automatically but you do tune learning rates for each problem individually anyway."
  },
  {
    "people": [
      "Adam"
    ],
    "review": "The authors propose a simple strategy that uses function values to improve the performance of Adam. There is no theoretical analysis of this variant, but there is an extensive empirical evaluation. A disadvantage of the proposed approach is that it has 3 parameters to tune, but the same parameters are used across experiments. Overall however, the PCs believe that this paper doesn't quite reach the level expected for ICLR and thus cannot be accepted."
  },
  {
    "people": [
      "Adam",
      "Eve",
      "Adam",
      "Adam"
    ],
    "review": "The paper demonstrates a semi-automatic learning rate schedule for the Adam optimizer, called Eve. Originality is somehow limited but the method appears to have a positive effect on neural network training. The paper is well written and illustrations are appropriate.\n\nPros:\n\n- probably a more sophisticated scheduling technique than a simple decay term\n- reasonable results on the CIFAR dataset (although with comparably small neural network)\n\nCons:\n\n- effect of momentum term would be of interest\n- the Adam reference doesn't point to the conference publications but only to arxiv\n- comparison to Adam not entirely conclusive"
  },
  {
    "people": [
      "Eve",
      "Eve",
      "Adam",
      "Eve"
    ],
    "review": "As you noted for Figure 5 Left, sometimes it seems sufficient to tune learning rates. I see your argument for Figure 6 Right, \nbut \n1) not for all good learning rates make Adam fail, I guess you selected the one where it did (note that Adam was several times faster than Eve in the beginning)\n2) I don't buy \"Eve always converges\" because you show it only for 0.1 and since Eve is not Adam, 0.1 of Adam is not 0.1 of Eve because of d_t. \n\nTo my understanding, you define d_t over time with 3 hyperparameters. Similarly, one can define d_t directly. The behaviour of d_t that you show is not extraordinary and can be parameterized. If Eve is better than Adam, then looking at d_t we can directly see whether we underestimated or overestimated learning rates. You could argue that Eve does it automatically but you do tune learning rates for each problem individually anyway. "
  },
  {
    "people": [
      "Adam"
    ],
    "review": "I think there is a bug in Algorithm 1. The comparison between f and \\hat{f} should be the other way around.\n\nWith that fix, I re-implemented the algorithm and indeed it was slightly faster than Adam in training a complex autoencoder :)"
  },
  {
    "people": [
      "Eve",
      "Eve",
      "Adam",
      "Eve"
    ],
    "review": "As you noted for Figure 5 Left, sometimes it seems sufficient to tune learning rates. I see your argument for Figure 6 Right, \nbut \n1) not for all good learning rates make Adam fail, I guess you selected the one where it did (note that Adam was several times faster than Eve in the beginning)\n2) I don't buy \"Eve always converges\" because you show it only for 0.1 and since Eve is not Adam, 0.1 of Adam is not 0.1 of Eve because of d_t. \n\nTo my understanding, you define d_t over time with 3 hyperparameters. Similarly, one can define d_t directly. The behaviour of d_t that you show is not extraordinary and can be parameterized. If Eve is better than Adam, then looking at d_t we can directly see whether we underestimated or overestimated learning rates. You could argue that Eve does it automatically but you do tune learning rates for each problem individually anyway."
  },
  {
    "people": [
      "Adam"
    ],
    "review": "The authors propose a simple strategy that uses function values to improve the performance of Adam. There is no theoretical analysis of this variant, but there is an extensive empirical evaluation. A disadvantage of the proposed approach is that it has 3 parameters to tune, but the same parameters are used across experiments. Overall however, the PCs believe that this paper doesn't quite reach the level expected for ICLR and thus cannot be accepted."
  },
  {
    "people": [
      "Adam",
      "Eve",
      "Adam",
      "Adam"
    ],
    "review": "The paper demonstrates a semi-automatic learning rate schedule for the Adam optimizer, called Eve. Originality is somehow limited but the method appears to have a positive effect on neural network training. The paper is well written and illustrations are appropriate.\n\nPros:\n\n- probably a more sophisticated scheduling technique than a simple decay term\n- reasonable results on the CIFAR dataset (although with comparably small neural network)\n\nCons:\n\n- effect of momentum term would be of interest\n- the Adam reference doesn't point to the conference publications but only to arxiv\n- comparison to Adam not entirely conclusive"
  },
  {
    "people": [
      "Eve",
      "Eve",
      "Adam",
      "Eve"
    ],
    "review": "As you noted for Figure 5 Left, sometimes it seems sufficient to tune learning rates. I see your argument for Figure 6 Right, \nbut \n1) not for all good learning rates make Adam fail, I guess you selected the one where it did (note that Adam was several times faster than Eve in the beginning)\n2) I don't buy \"Eve always converges\" because you show it only for 0.1 and since Eve is not Adam, 0.1 of Adam is not 0.1 of Eve because of d_t. \n\nTo my understanding, you define d_t over time with 3 hyperparameters. Similarly, one can define d_t directly. The behaviour of d_t that you show is not extraordinary and can be parameterized. If Eve is better than Adam, then looking at d_t we can directly see whether we underestimated or overestimated learning rates. You could argue that Eve does it automatically but you do tune learning rates for each problem individually anyway. "
  },
  {
    "people": [
      "Adam"
    ],
    "review": "I think there is a bug in Algorithm 1. The comparison between f and \\hat{f} should be the other way around.\n\nWith that fix, I re-implemented the algorithm and indeed it was slightly faster than Adam in training a complex autoencoder :)"
  },
  {
    "people": [
      "Radenovi\u0107",
      "Gordo",
      "Tolias"
    ],
    "review": "This paper explores different strategies for instance-level image retrieval with deep CNNs. The approach consists of extracting features from a network pre-trained for image classification (e.g. VGG), and post-process them for image retrieval. In other words, the network is off-the-shelf and solely acts as a feature extractor. The post-processing strategies are borrowed from traditional retrieval pipelines relying on hand-crafted features (e.g. SIFT + Fisher Vectors), denoted by the authors as \"traditional wisdom\".\n\nSpecifically, the authors examine where to extract features in the network (i.e. features are neurons activations of a convolution layer), which type of feature aggregation and normalization performs best, whether resizing images helps, whether combining multiple scales helps, and so on. \n\nWhile this type of experimental study is reasonable and well motivated, it suffers from a huge problem. Namely it \"ignores\" 2 major recent works that are in direct contradictions with many claims of the paper ([a] \"End-to-end Learning of Deep Visual Representations for Image Retrieval\" by  Gordo et al. and [b] \"CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples\" by Radenovi\u0107 et al., both ECCV'16 papers). These works have shown that training for retrieval can be achieved with a siamese architectures and have demonstrated outstanding performance. As a result, many claims and findings of the paper are either outdated, questionable or just wrong.\n\nHere are some of the misleading claims: \n\n  - \"Features aggregated from these feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in recent years.\"\n  Until [a] (not cited), the state-of-the-art was still largely dominated by sparse invariant features based methods (see last Table in [a]).\n  \n  - \"the proposed method [...] outperforms the state-of-the-art methods on four typical datasets\"\n  That is not true, for the same reasons than above, and also because the state-of-the-art is now dominated by [a] and [b].\n  \n  - \"Also in situations where a large numbers of training samples are not available, instance retrieval using unsupervised method is still preferable and may be the only option.\".\n  This is a questionable opinion. The method exposed in \"End-to-end Learning of Deep Visual Representations for Image Retrieval\" by Gordo et al. outperforms the state-of-the-art on the UKB dataset (3.84 without QE or DBA) whereas it was trained for landmarks retrieval and not objects, i.e. in a different retrieval context. This demonstrates that in spite of insufficient training data, training is still possible and beneficial.\n\n  - Finally, most findings are not even new or surprising (e.g. aggregate several regions in a multi-scale manner was already achieved by Tolias at al, etc.). So the interest of the paper is limited overall.\n\nIn addition, there are some problems in the experiments. For instance, the tuning experiments are only conducted on the Oxford dataset and using a single network (VGG-19), whereas it is not clear whether these conditions are well representative of all datasets and all networks (it is well known that the Oxford dataset behaves very differently than the Holidays dataset, for instance). In addition, tuning is performed very aggressively, making it look like the authors are tuning on the test set (e.g. see Table 3). \n\nTo conclude, the paper is one year too late with respect to recent developments in the state of the art."
  },
  {
    "people": [
      "Gordo",
      "Paulin"
    ],
    "review": "The paper conducts a detailed evaluation of different CNN architectures applied to visual instance retrieval. The authors consider various deep neural network architectures, with a focus on architectures pre-trained for image classification. \n \n An important concern of the reviewers is the relevance of the evaluation given the recent impressive experimental results of deep neural networks trained end-to-end for visual instance retrieval by Gordo et al. \"End-to-end Learning of Deep Visual Representations for Image Retrieval\". Another concern is the novelty of the proposed evaluation given the evaluation of the performance for visual instance retrieval of deep neural network pre-trained for image classification performed in Paulin et al. \"Convolutional Patch Representations for Image Retrieval: An Unsupervised Approach\". \n \n A revision of the paper, following the reviewers' suggestions, will generate a stronger submission to a future venue."
  },
  {
    "people": [
      "Gordo",
      "Gordo"
    ],
    "review": "The paper conducts a detailed evaluation of different CNN architectures applied to image retrieval. The authors focus on testing various architectural choices, but do not propose or compare to end-to-end learning frameworks.\n\nTechnically, the contribution is clear, particularly with the promised clarifications on how multiple scales are handled in the representation. However, I am still not entirely clear whether there would be a difference in the multi-scale settting for full and cropped queries.\n\nWhile the paper focuses on comparing different baseline architectures for CNN-based image retrieval, several recent papers have proposed to learn end-to-end representations specific for this task, with very good result (see for instance the recent work by Gordo et al. \"End-to-end Learning of Deep Visual Representations for Image Retrieval\"). The authors clarify that their work is orthogonal to papers such as Gordo et al. as they assess instead the performance of networks pre-trained from image classification. In fact, they also indicate that image retrieval is more difficult than image classification -- this is because it is performed by using features originally trained for classification. I can partially accept this argument. However, given the results in recent papers, it is clear than end-to-end training is far superior in practice and it is not clear the analysis developed by the authors in this work would transfer or be useful for that case as well.\n"
  },
  {
    "people": [
      "Tolias"
    ],
    "review": "Authors investigate how to use pretrained CNNs for retrieval and perform an extensive evaluation of the influence of various parameters. For detailed comments on everything see the questions I posted earlier. The summary is here:\n\nI don't think we learn much from this paper: we already knew that we should use the last conv layer, we knew we should use PCA with whitening, we knew we should use original size images (authors say Tolias didn't do this as they resized the images, but they did this exactly for the same reason as authors didn't evaluate on Holidays - the images are too big. So they basically used \"as large as possible\" image sizes, which is what this paper effectively suggests as well), etc. This paper essentially concatenates methods that people have already used, and performs some more parameter tweaking to achieve the state-of-the-art (while the tweaking is actually performed on the test set of some of the tests).\n\nThe setting of the state-of-the-art results is quite misleading as it doesn't really come from the good choice of parameters, but mainly due to the usage of the deeper VGG-19 network. \n\nFurthermore, I don't think it's sufficient to just try one network and claim these are the best practices for using CNNs for instance retrieval - what about ResNet, what about Inception, I don't know how to apply any of these conclusions for those networks, and would these conclusions even hold for them. Furthermore the parameter tweaking was done on Oxford, I really can't tell what conclusions would we get if we tuned on UKB for example. So a more appropriate paper title would be \"What are the best parameter values for VGG-19 on Oxford/Paris benchmarks?\" - I don't think this is sufficiently novel nor interesting for the community.\n"
  },
  {
    "people": [
      "Radenovi\u0107",
      "Gordo",
      "Tolias"
    ],
    "review": "This paper explores different strategies for instance-level image retrieval with deep CNNs. The approach consists of extracting features from a network pre-trained for image classification (e.g. VGG), and post-process them for image retrieval. In other words, the network is off-the-shelf and solely acts as a feature extractor. The post-processing strategies are borrowed from traditional retrieval pipelines relying on hand-crafted features (e.g. SIFT + Fisher Vectors), denoted by the authors as \"traditional wisdom\".\n\nSpecifically, the authors examine where to extract features in the network (i.e. features are neurons activations of a convolution layer), which type of feature aggregation and normalization performs best, whether resizing images helps, whether combining multiple scales helps, and so on. \n\nWhile this type of experimental study is reasonable and well motivated, it suffers from a huge problem. Namely it \"ignores\" 2 major recent works that are in direct contradictions with many claims of the paper ([a] \"End-to-end Learning of Deep Visual Representations for Image Retrieval\" by  Gordo et al. and [b] \"CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples\" by Radenovi\u0107 et al., both ECCV'16 papers). These works have shown that training for retrieval can be achieved with a siamese architectures and have demonstrated outstanding performance. As a result, many claims and findings of the paper are either outdated, questionable or just wrong.\n\nHere are some of the misleading claims: \n\n  - \"Features aggregated from these feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in recent years.\"\n  Until [a] (not cited), the state-of-the-art was still largely dominated by sparse invariant features based methods (see last Table in [a]).\n  \n  - \"the proposed method [...] outperforms the state-of-the-art methods on four typical datasets\"\n  That is not true, for the same reasons than above, and also because the state-of-the-art is now dominated by [a] and [b].\n  \n  - \"Also in situations where a large numbers of training samples are not available, instance retrieval using unsupervised method is still preferable and may be the only option.\".\n  This is a questionable opinion. The method exposed in \"End-to-end Learning of Deep Visual Representations for Image Retrieval\" by Gordo et al. outperforms the state-of-the-art on the UKB dataset (3.84 without QE or DBA) whereas it was trained for landmarks retrieval and not objects, i.e. in a different retrieval context. This demonstrates that in spite of insufficient training data, training is still possible and beneficial.\n\n  - Finally, most findings are not even new or surprising (e.g. aggregate several regions in a multi-scale manner was already achieved by Tolias at al, etc.). So the interest of the paper is limited overall.\n\nIn addition, there are some problems in the experiments. For instance, the tuning experiments are only conducted on the Oxford dataset and using a single network (VGG-19), whereas it is not clear whether these conditions are well representative of all datasets and all networks (it is well known that the Oxford dataset behaves very differently than the Holidays dataset, for instance). In addition, tuning is performed very aggressively, making it look like the authors are tuning on the test set (e.g. see Table 3). \n\nTo conclude, the paper is one year too late with respect to recent developments in the state of the art."
  },
  {
    "people": [
      "Gordo",
      "Radenovic",
      "Arandjelovic",
      "Arandjelovic",
      "Tolias",
      "Gordo",
      "Radenovic",
      "Babenko",
      "Lempitsky",
      "Babenko",
      "Tolias",
      "Arandjelovic",
      "Gordo",
      "Radenovic",
      "Ross Girshick"
    ],
    "review": "\n5)\nI agree with other reviewers on the lack of comparison with Gordo et al and Radenovic et al, though I understand that authors' arguments are that they do not want to train their networks (though then comparing with Arandjelovic et al. also doesn't make sense). It's still worth citing these papers and commenting on them. Also, with such an extensive set of experiments, it's a bit arguable if authors don't really do training - they don't do the canonical SGD, but they essentially perform grid search for parameters on the test (see question 3).\n\n6)\nI'm not sure what did we actually learn from this paper. To use the last conv? We knew that before as all recent papers do this (Arandjelovic et al, Tolias et al, Gordo et al, Radenovic et al, Babenko and Lempitsky, ..). That using original image sizes is important? We knew this as well, early works (Babenko et al 2014, etc) used smaller images while all recent works apply the networks convolutionally over original size images (e.g. Tolias et al have this experiment in table 1). That one should use PCA with whitening (and if possible learn whitening on the test set)? We knew this already as well. So the only two things that haven't been done in exactly the same way as people did it before is the multi-scale pooling (though obviously various other similar versions exist), and the exploration of max/sum pooling with l1 or l2 normalization (though the experiments in table 1 are basically ignored as sum-l1 works the best there, but authors then say that actually later they notice that for multiscale max-l2 works best). Actually the most interesting part for me, one that I can actually say I didn't know and don't think anyone knew, is figure 3.\n\n7)\nI think it's a bit of an overstatement to call this paper 'best practice for CNNs' when only a single CNN architecture, VGG-19, is considered. What is the best practice for other models, e.g. ResNet, Inception? Presumably the last conv is likely to be best though for ResNet it's not that clear, and I'm not sure if sum vs max pooling would change as those two networks were trained with sum pooling, and I'm not sure if any of the other conclusions hold either. This is more of a surgery of VGG-19 than best practices for CNNs in general.\n\n8)\nOn a more philosophical level, and not only aimed at authors but also at others who are potentially reading this - this conference is about learning representations, while no learning is being performed. Taking CNNs as black boxes and tweaking the inputs and outputs in different ways with different normalizations is much more like using hand-engineered features like SIFT (replace black-box SIFT extractor with black-box CNN) than actually doing Deep Learning. I'm not saying this type of paper shouldn't exist as it's good to know what works best, but my preference in terms of what papers I would like to see in the future is:\na) There have been too many papers for using CNNs as black-boxes, I hoped we are finally over with this\nb) For ICLR I think one should actually do some training, e.g. after we figure out the best image representation, now train the whole system end-to-end and see if you can improve the performance.\nc) Design architectures which are specifically aimed at image retrieval - maybe something different than CNNs for classification pops up?\nd) Figure out ways to train CNNs for retrieval, we know how to do it for classification by paying people to label millions of images, can we do something better for retrieval? (though this is to some extend addressed now by Arandjelovic et al, Gordo et al and Radenovic et al).\n\n\nOther minor comments:\n\n- I was also surprised by the \"harder than category retrieval\" statement, as reviewer 3. I wouldn't go as far as saying that the opposite is true either, the two just cannot be compared so easily.\n- Inconsistencies of references (e.g. \"Y. Lecun\" vs \"Ross Girshick\", \"CVPR\" versus \"Computer\nVision and Pattern Recognition\", ..\n"
  },
  {
    "people": [
      "Radenovi\u0107",
      "Gordo",
      "Tolias"
    ],
    "review": "This paper explores different strategies for instance-level image retrieval with deep CNNs. The approach consists of extracting features from a network pre-trained for image classification (e.g. VGG), and post-process them for image retrieval. In other words, the network is off-the-shelf and solely acts as a feature extractor. The post-processing strategies are borrowed from traditional retrieval pipelines relying on hand-crafted features (e.g. SIFT + Fisher Vectors), denoted by the authors as \"traditional wisdom\".\n\nSpecifically, the authors examine where to extract features in the network (i.e. features are neurons activations of a convolution layer), which type of feature aggregation and normalization performs best, whether resizing images helps, whether combining multiple scales helps, and so on. \n\nWhile this type of experimental study is reasonable and well motivated, it suffers from a huge problem. Namely it \"ignores\" 2 major recent works that are in direct contradictions with many claims of the paper ([a] \"End-to-end Learning of Deep Visual Representations for Image Retrieval\" by  Gordo et al. and [b] \"CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples\" by Radenovi\u0107 et al., both ECCV'16 papers). These works have shown that training for retrieval can be achieved with a siamese architectures and have demonstrated outstanding performance. As a result, many claims and findings of the paper are either outdated, questionable or just wrong.\n\nHere are some of the misleading claims: \n\n  - \"Features aggregated from these feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in recent years.\"\n  Until [a] (not cited), the state-of-the-art was still largely dominated by sparse invariant features based methods (see last Table in [a]).\n  \n  - \"the proposed method [...] outperforms the state-of-the-art methods on four typical datasets\"\n  That is not true, for the same reasons than above, and also because the state-of-the-art is now dominated by [a] and [b].\n  \n  - \"Also in situations where a large numbers of training samples are not available, instance retrieval using unsupervised method is still preferable and may be the only option.\".\n  This is a questionable opinion. The method exposed in \"End-to-end Learning of Deep Visual Representations for Image Retrieval\" by Gordo et al. outperforms the state-of-the-art on the UKB dataset (3.84 without QE or DBA) whereas it was trained for landmarks retrieval and not objects, i.e. in a different retrieval context. This demonstrates that in spite of insufficient training data, training is still possible and beneficial.\n\n  - Finally, most findings are not even new or surprising (e.g. aggregate several regions in a multi-scale manner was already achieved by Tolias at al, etc.). So the interest of the paper is limited overall.\n\nIn addition, there are some problems in the experiments. For instance, the tuning experiments are only conducted on the Oxford dataset and using a single network (VGG-19), whereas it is not clear whether these conditions are well representative of all datasets and all networks (it is well known that the Oxford dataset behaves very differently than the Holidays dataset, for instance). In addition, tuning is performed very aggressively, making it look like the authors are tuning on the test set (e.g. see Table 3). \n\nTo conclude, the paper is one year too late with respect to recent developments in the state of the art."
  },
  {
    "people": [
      "Gordo",
      "Paulin"
    ],
    "review": "The paper conducts a detailed evaluation of different CNN architectures applied to visual instance retrieval. The authors consider various deep neural network architectures, with a focus on architectures pre-trained for image classification. \n \n An important concern of the reviewers is the relevance of the evaluation given the recent impressive experimental results of deep neural networks trained end-to-end for visual instance retrieval by Gordo et al. \"End-to-end Learning of Deep Visual Representations for Image Retrieval\". Another concern is the novelty of the proposed evaluation given the evaluation of the performance for visual instance retrieval of deep neural network pre-trained for image classification performed in Paulin et al. \"Convolutional Patch Representations for Image Retrieval: An Unsupervised Approach\". \n \n A revision of the paper, following the reviewers' suggestions, will generate a stronger submission to a future venue."
  },
  {
    "people": [
      "Gordo",
      "Gordo"
    ],
    "review": "The paper conducts a detailed evaluation of different CNN architectures applied to image retrieval. The authors focus on testing various architectural choices, but do not propose or compare to end-to-end learning frameworks.\n\nTechnically, the contribution is clear, particularly with the promised clarifications on how multiple scales are handled in the representation. However, I am still not entirely clear whether there would be a difference in the multi-scale settting for full and cropped queries.\n\nWhile the paper focuses on comparing different baseline architectures for CNN-based image retrieval, several recent papers have proposed to learn end-to-end representations specific for this task, with very good result (see for instance the recent work by Gordo et al. \"End-to-end Learning of Deep Visual Representations for Image Retrieval\"). The authors clarify that their work is orthogonal to papers such as Gordo et al. as they assess instead the performance of networks pre-trained from image classification. In fact, they also indicate that image retrieval is more difficult than image classification -- this is because it is performed by using features originally trained for classification. I can partially accept this argument. However, given the results in recent papers, it is clear than end-to-end training is far superior in practice and it is not clear the analysis developed by the authors in this work would transfer or be useful for that case as well.\n"
  },
  {
    "people": [
      "Tolias"
    ],
    "review": "Authors investigate how to use pretrained CNNs for retrieval and perform an extensive evaluation of the influence of various parameters. For detailed comments on everything see the questions I posted earlier. The summary is here:\n\nI don't think we learn much from this paper: we already knew that we should use the last conv layer, we knew we should use PCA with whitening, we knew we should use original size images (authors say Tolias didn't do this as they resized the images, but they did this exactly for the same reason as authors didn't evaluate on Holidays - the images are too big. So they basically used \"as large as possible\" image sizes, which is what this paper effectively suggests as well), etc. This paper essentially concatenates methods that people have already used, and performs some more parameter tweaking to achieve the state-of-the-art (while the tweaking is actually performed on the test set of some of the tests).\n\nThe setting of the state-of-the-art results is quite misleading as it doesn't really come from the good choice of parameters, but mainly due to the usage of the deeper VGG-19 network. \n\nFurthermore, I don't think it's sufficient to just try one network and claim these are the best practices for using CNNs for instance retrieval - what about ResNet, what about Inception, I don't know how to apply any of these conclusions for those networks, and would these conclusions even hold for them. Furthermore the parameter tweaking was done on Oxford, I really can't tell what conclusions would we get if we tuned on UKB for example. So a more appropriate paper title would be \"What are the best parameter values for VGG-19 on Oxford/Paris benchmarks?\" - I don't think this is sufficiently novel nor interesting for the community.\n"
  },
  {
    "people": [
      "Radenovi\u0107",
      "Gordo",
      "Tolias"
    ],
    "review": "This paper explores different strategies for instance-level image retrieval with deep CNNs. The approach consists of extracting features from a network pre-trained for image classification (e.g. VGG), and post-process them for image retrieval. In other words, the network is off-the-shelf and solely acts as a feature extractor. The post-processing strategies are borrowed from traditional retrieval pipelines relying on hand-crafted features (e.g. SIFT + Fisher Vectors), denoted by the authors as \"traditional wisdom\".\n\nSpecifically, the authors examine where to extract features in the network (i.e. features are neurons activations of a convolution layer), which type of feature aggregation and normalization performs best, whether resizing images helps, whether combining multiple scales helps, and so on. \n\nWhile this type of experimental study is reasonable and well motivated, it suffers from a huge problem. Namely it \"ignores\" 2 major recent works that are in direct contradictions with many claims of the paper ([a] \"End-to-end Learning of Deep Visual Representations for Image Retrieval\" by  Gordo et al. and [b] \"CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples\" by Radenovi\u0107 et al., both ECCV'16 papers). These works have shown that training for retrieval can be achieved with a siamese architectures and have demonstrated outstanding performance. As a result, many claims and findings of the paper are either outdated, questionable or just wrong.\n\nHere are some of the misleading claims: \n\n  - \"Features aggregated from these feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in recent years.\"\n  Until [a] (not cited), the state-of-the-art was still largely dominated by sparse invariant features based methods (see last Table in [a]).\n  \n  - \"the proposed method [...] outperforms the state-of-the-art methods on four typical datasets\"\n  That is not true, for the same reasons than above, and also because the state-of-the-art is now dominated by [a] and [b].\n  \n  - \"Also in situations where a large numbers of training samples are not available, instance retrieval using unsupervised method is still preferable and may be the only option.\".\n  This is a questionable opinion. The method exposed in \"End-to-end Learning of Deep Visual Representations for Image Retrieval\" by Gordo et al. outperforms the state-of-the-art on the UKB dataset (3.84 without QE or DBA) whereas it was trained for landmarks retrieval and not objects, i.e. in a different retrieval context. This demonstrates that in spite of insufficient training data, training is still possible and beneficial.\n\n  - Finally, most findings are not even new or surprising (e.g. aggregate several regions in a multi-scale manner was already achieved by Tolias at al, etc.). So the interest of the paper is limited overall.\n\nIn addition, there are some problems in the experiments. For instance, the tuning experiments are only conducted on the Oxford dataset and using a single network (VGG-19), whereas it is not clear whether these conditions are well representative of all datasets and all networks (it is well known that the Oxford dataset behaves very differently than the Holidays dataset, for instance). In addition, tuning is performed very aggressively, making it look like the authors are tuning on the test set (e.g. see Table 3). \n\nTo conclude, the paper is one year too late with respect to recent developments in the state of the art."
  },
  {
    "people": [
      "Gordo",
      "Radenovic",
      "Arandjelovic",
      "Arandjelovic",
      "Tolias",
      "Gordo",
      "Radenovic",
      "Babenko",
      "Lempitsky",
      "Babenko",
      "Tolias",
      "Arandjelovic",
      "Gordo",
      "Radenovic",
      "Ross Girshick"
    ],
    "review": "\n5)\nI agree with other reviewers on the lack of comparison with Gordo et al and Radenovic et al, though I understand that authors' arguments are that they do not want to train their networks (though then comparing with Arandjelovic et al. also doesn't make sense). It's still worth citing these papers and commenting on them. Also, with such an extensive set of experiments, it's a bit arguable if authors don't really do training - they don't do the canonical SGD, but they essentially perform grid search for parameters on the test (see question 3).\n\n6)\nI'm not sure what did we actually learn from this paper. To use the last conv? We knew that before as all recent papers do this (Arandjelovic et al, Tolias et al, Gordo et al, Radenovic et al, Babenko and Lempitsky, ..). That using original image sizes is important? We knew this as well, early works (Babenko et al 2014, etc) used smaller images while all recent works apply the networks convolutionally over original size images (e.g. Tolias et al have this experiment in table 1). That one should use PCA with whitening (and if possible learn whitening on the test set)? We knew this already as well. So the only two things that haven't been done in exactly the same way as people did it before is the multi-scale pooling (though obviously various other similar versions exist), and the exploration of max/sum pooling with l1 or l2 normalization (though the experiments in table 1 are basically ignored as sum-l1 works the best there, but authors then say that actually later they notice that for multiscale max-l2 works best). Actually the most interesting part for me, one that I can actually say I didn't know and don't think anyone knew, is figure 3.\n\n7)\nI think it's a bit of an overstatement to call this paper 'best practice for CNNs' when only a single CNN architecture, VGG-19, is considered. What is the best practice for other models, e.g. ResNet, Inception? Presumably the last conv is likely to be best though for ResNet it's not that clear, and I'm not sure if sum vs max pooling would change as those two networks were trained with sum pooling, and I'm not sure if any of the other conclusions hold either. This is more of a surgery of VGG-19 than best practices for CNNs in general.\n\n8)\nOn a more philosophical level, and not only aimed at authors but also at others who are potentially reading this - this conference is about learning representations, while no learning is being performed. Taking CNNs as black boxes and tweaking the inputs and outputs in different ways with different normalizations is much more like using hand-engineered features like SIFT (replace black-box SIFT extractor with black-box CNN) than actually doing Deep Learning. I'm not saying this type of paper shouldn't exist as it's good to know what works best, but my preference in terms of what papers I would like to see in the future is:\na) There have been too many papers for using CNNs as black-boxes, I hoped we are finally over with this\nb) For ICLR I think one should actually do some training, e.g. after we figure out the best image representation, now train the whole system end-to-end and see if you can improve the performance.\nc) Design architectures which are specifically aimed at image retrieval - maybe something different than CNNs for classification pops up?\nd) Figure out ways to train CNNs for retrieval, we know how to do it for classification by paying people to label millions of images, can we do something better for retrieval? (though this is to some extend addressed now by Arandjelovic et al, Gordo et al and Radenovic et al).\n\n\nOther minor comments:\n\n- I was also surprised by the \"harder than category retrieval\" statement, as reviewer 3. I wouldn't go as far as saying that the opposite is true either, the two just cannot be compared so easily.\n- Inconsistencies of references (e.g. \"Y. Lecun\" vs \"Ross Girshick\", \"CVPR\" versus \"Computer\nVision and Pattern Recognition\", ..\n"
  },
  {
    "people": [
      "RMSprop"
    ],
    "review": "Dear all,\n\nBelow are some more responses to two points raised. Both responses are reflected in a new version of the paper I just uploaded.\n\nThe following has changed in the paper:\n\n - section 3.1 (self-similar nonlinearities) is new\n - the final paragraph of section 3 is new\n - section 7.2 (proof of new proposition) is new\n\nThe paper is now longer than 8 pages. Having looked at many other submitted papers, it appears that the 8 page requirement is not very serious. If the area chair would prefer an 8 page paper, I can move some more stuff to the appendix.\n\n** Reducing the number of hyperparameters **\n\nTwo questions that came up throughout the reviews is whether our method reduces the number of hyperparameters and whether there is an automatic way to set lambda. \n\nOne advantage of nonparametric networks is that instead of having one hyperparameter per layer (size) there is one hyperparameter for the entire network (lambda) that controls size. It is worth pointing out that this reduction in complexity is not arbitrary. In fact, one can prove that in a ReLU network, one regularization parameter lambda captures all the complexity of having one regularization parameter per layer because we could replace all of these regularization parameters with their geometric mean without changing the objective. (See the newly included-in-the-paper proposition 1.) Hence, nonparametric networks apportion regularization to each layer automatically.\n\nWhile we don't (yet) have a great way of efficiently picking the single remaining lambda, this reduction in complexity certainly contributes to reducing hyperparameter complexity.\n\n** Computational cost of AdaRad (in response to reviewer 1) **\n\n(From the paper:) Using AdaRad over SGD incurs additional computational cost. However, that cost scales more gracefully than the cost of, for example, RMSprop. AdaRad normalizes each fan-in instead of each individual weight, so many of its operations scale only with the number of units and not with the number of weights in the network. In Table \\ref{costTable}, we compare the costs of SGD, AdaRad and RMSprop. Further, RMSprop has a larger memory footprint than AdaRad. It requires a cache of size equal to the number of weights, whereas AdaRad only requires 2 caches of size equal to the number of neurons.\n\nCosts (per minibatch and weight)\n\nSGD, no $\\ell_2$ shrinkage: 1 multiplication\nSGD with $\\ell_2$ shrinkage: 3 multiplications\nAdaRad, no $\\ell_2$ shrinkage: 4 multiplications\nAdaRad with $\\ell_2$ shrinkage: 4 multiplications\nRMSprop, no $\\ell_2$ shrinkage: 4 multiplications, 1 division, 1 square root\nRMSprop with $\\ell_2$ shrinkage: 6 multiplications, 1 division, 1 square root\n\nBest,\nGeorge\n\n"
  },
  {
    "people": [
      "RMSprop"
    ],
    "review": "Dear all,\n\nBelow are some more responses to two points raised. Both responses are reflected in a new version of the paper I just uploaded.\n\nThe following has changed in the paper:\n\n - section 3.1 (self-similar nonlinearities) is new\n - the final paragraph of section 3 is new\n - section 7.2 (proof of new proposition) is new\n\nThe paper is now longer than 8 pages. Having looked at many other submitted papers, it appears that the 8 page requirement is not very serious. If the area chair would prefer an 8 page paper, I can move some more stuff to the appendix.\n\n** Reducing the number of hyperparameters **\n\nTwo questions that came up throughout the reviews is whether our method reduces the number of hyperparameters and whether there is an automatic way to set lambda. \n\nOne advantage of nonparametric networks is that instead of having one hyperparameter per layer (size) there is one hyperparameter for the entire network (lambda) that controls size. It is worth pointing out that this reduction in complexity is not arbitrary. In fact, one can prove that in a ReLU network, one regularization parameter lambda captures all the complexity of having one regularization parameter per layer because we could replace all of these regularization parameters with their geometric mean without changing the objective. (See the newly included-in-the-paper proposition 1.) Hence, nonparametric networks apportion regularization to each layer automatically.\n\nWhile we don't (yet) have a great way of efficiently picking the single remaining lambda, this reduction in complexity certainly contributes to reducing hyperparameter complexity.\n\n** Computational cost of AdaRad (in response to reviewer 1) **\n\n(From the paper:) Using AdaRad over SGD incurs additional computational cost. However, that cost scales more gracefully than the cost of, for example, RMSprop. AdaRad normalizes each fan-in instead of each individual weight, so many of its operations scale only with the number of units and not with the number of weights in the network. In Table \\ref{costTable}, we compare the costs of SGD, AdaRad and RMSprop. Further, RMSprop has a larger memory footprint than AdaRad. It requires a cache of size equal to the number of weights, whereas AdaRad only requires 2 caches of size equal to the number of neurons.\n\nCosts (per minibatch and weight)\n\nSGD, no $\\ell_2$ shrinkage: 1 multiplication\nSGD with $\\ell_2$ shrinkage: 3 multiplications\nAdaRad, no $\\ell_2$ shrinkage: 4 multiplications\nAdaRad with $\\ell_2$ shrinkage: 4 multiplications\nRMSprop, no $\\ell_2$ shrinkage: 4 multiplications, 1 division, 1 square root\nRMSprop with $\\ell_2$ shrinkage: 6 multiplications, 1 division, 1 square root\n\nBest,\nGeorge\n\n"
  },
  {
    "people": [
      "Niepert"
    ],
    "review": "the paper proposed a method mainly for graph classification. The proposal is to decompose graphs objects into hierarchies of small graphs followed by generating vector embeddings and aggregation using deep networks. \nThe approach is reasonable and intuitive however, experiments do not show superiority of their approach. \n\nThe proposed method outperforms Yanardag et al. 2015 and Niepert et al., 2016 on social networks graphs but are quite inferior to Niepert et al., 2016 on bio-informatics datasets. the authors did not report acccuracy for Yanardag et al. 2015 which on similar bio-ddatasets for example NCI1 is 80%, significantly better than achieved by the proposed method. The authors claim that their method is tailored for social networks graph more is not supported by good arguments? what models of graphs is this method more suitable?  "
  },
  {
    "people": [
      "Niepert"
    ],
    "review": "the paper proposed a method mainly for graph classification. The proposal is to decompose graphs objects into hierarchies of small graphs followed by generating vector embeddings and aggregation using deep networks. \nThe approach is reasonable and intuitive however, experiments do not show superiority of their approach. \n\nThe proposed method outperforms Yanardag et al. 2015 and Niepert et al., 2016 on social networks graphs but are quite inferior to Niepert et al., 2016 on bio-informatics datasets. the authors did not report acccuracy for Yanardag et al. 2015 which on similar bio-ddatasets for example NCI1 is 80%, significantly better than achieved by the proposed method. The authors claim that their method is tailored for social networks graph more is not supported by good arguments? what models of graphs is this method more suitable?  "
  },
  {
    "people": [
      "Jaderberg"
    ],
    "review": "This paper proposed a neural attention model which has a learnable and differentiable sampling lattice. The work is well motivated as few previous work focus on learning the sampling lattice but with a fixed lattice. This work is quite similar to Spatial Transformer Networks (Jaderberg 2015), but the sampling lattice is learned by the model. The experiments showed that the model can learn a meaning lattice to the visual search task where the sampling lattice looks similar to human being's. \n\nThe main concern of the paper is that experiments are not sufficient. The paper only reports the results on a modified clustered MNIST dataset. It would be more interesting if the authors could conduct  the model on real datasets, such as Toronto Face dataset, CUB bird dataset and SVHN. For example, for the Face dataset, it would be nice if the model can learn to attend different parts of the face for expression recognition, or attend different part of birds for fine-grained classification. Since the authors replied in the pre-review question that the model can learn meaningful lattice on MSCOCO dataset, I think it would be better to add that results into the paper.\n\nAnother drawback of the model is that the paper only compare with different variants of itselves. I suggest that this paper should compare with  Spatial Transformer Networks, DRAW, etc., on the same dataset to show the advantage of the learned sampling lattice."
  },
  {
    "people": [
      "Jaderberg"
    ],
    "review": "This paper proposed a neural attention model which has a learnable and differentiable sampling lattice. The work is well motivated as few previous work focus on learning the sampling lattice but with a fixed lattice. This work is quite similar to Spatial Transformer Networks (Jaderberg 2015), but the sampling lattice is learned by the model. The experiments showed that the model can learn a meaning lattice to the visual search task where the sampling lattice looks similar to human being's. \n\nThe main concern of the paper is that experiments are not sufficient. The paper only reports the results on a modified clustered MNIST dataset. It would be more interesting if the authors could conduct  the model on real datasets, such as Toronto Face dataset, CUB bird dataset and SVHN. For example, for the Face dataset, it would be nice if the model can learn to attend different parts of the face for expression recognition, or attend different part of birds for fine-grained classification. Since the authors replied in the pre-review question that the model can learn meaningful lattice on MSCOCO dataset, I think it would be better to add that results into the paper.\n\nAnother drawback of the model is that the paper only compare with different variants of itselves. I suggest that this paper should compare with  Spatial Transformer Networks, DRAW, etc., on the same dataset to show the advantage of the learned sampling lattice."
  },
  {
    "people": [
      "Jaderberg"
    ],
    "review": "This paper proposed a neural attention model which has a learnable and differentiable sampling lattice. The work is well motivated as few previous work focus on learning the sampling lattice but with a fixed lattice. This work is quite similar to Spatial Transformer Networks (Jaderberg 2015), but the sampling lattice is learned by the model. The experiments showed that the model can learn a meaning lattice to the visual search task where the sampling lattice looks similar to human being's. \n\nThe main concern of the paper is that experiments are not sufficient. The paper only reports the results on a modified clustered MNIST dataset. It would be more interesting if the authors could conduct  the model on real datasets, such as Toronto Face dataset, CUB bird dataset and SVHN. For example, for the Face dataset, it would be nice if the model can learn to attend different parts of the face for expression recognition, or attend different part of birds for fine-grained classification. Since the authors replied in the pre-review question that the model can learn meaningful lattice on MSCOCO dataset, I think it would be better to add that results into the paper.\n\nAnother drawback of the model is that the paper only compare with different variants of itselves. I suggest that this paper should compare with  Spatial Transformer Networks, DRAW, etc., on the same dataset to show the advantage of the learned sampling lattice."
  },
  {
    "people": [
      "Jaderberg"
    ],
    "review": "This paper proposed a neural attention model which has a learnable and differentiable sampling lattice. The work is well motivated as few previous work focus on learning the sampling lattice but with a fixed lattice. This work is quite similar to Spatial Transformer Networks (Jaderberg 2015), but the sampling lattice is learned by the model. The experiments showed that the model can learn a meaning lattice to the visual search task where the sampling lattice looks similar to human being's. \n\nThe main concern of the paper is that experiments are not sufficient. The paper only reports the results on a modified clustered MNIST dataset. It would be more interesting if the authors could conduct  the model on real datasets, such as Toronto Face dataset, CUB bird dataset and SVHN. For example, for the Face dataset, it would be nice if the model can learn to attend different parts of the face for expression recognition, or attend different part of birds for fine-grained classification. Since the authors replied in the pre-review question that the model can learn meaningful lattice on MSCOCO dataset, I think it would be better to add that results into the paper.\n\nAnother drawback of the model is that the paper only compare with different variants of itselves. I suggest that this paper should compare with  Spatial Transformer Networks, DRAW, etc., on the same dataset to show the advantage of the learned sampling lattice."
  },
  {
    "people": [
      "Sajjadi"
    ],
    "review": "It has come to our attention that a recent paper titled \"Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning\" by Sajjadi et al., presented at NIPS 2016, builds on the same core principle as our work. We have therefore uploaded a new revision of our paper to cite this related work and to contrast our contributions against it. We also added discussion about our current understanding of the reasons why our methods help in the corrupted labels test. To address the points raised by Reviewer #2, we clarified that the main reason for not using SVHN extra data was to be comparable to the previous best results in semi-supervised learning, bolded the best-in-category results in Tables 1 and 2, and added a bit of additional detail about function w(t) where it is first introduced.\n \nWe look forward to augmenting the paper further with results from CIFAR-100 in the near future, as mentioned in our previous response to Reviewer #1.\n"
  },
  {
    "people": [
      "Sajjadi"
    ],
    "review": "It has come to our attention that a recent paper titled \"Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning\" by Sajjadi et al., presented at NIPS 2016, builds on the same core principle as our work. We have therefore uploaded a new revision of our paper to cite this related work and to contrast our contributions against it. We also added discussion about our current understanding of the reasons why our methods help in the corrupted labels test. To address the points raised by Reviewer #2, we clarified that the main reason for not using SVHN extra data was to be comparable to the previous best results in semi-supervised learning, bolded the best-in-category results in Tables 1 and 2, and added a bit of additional detail about function w(t) where it is first introduced.\n \nWe look forward to augmenting the paper further with results from CIFAR-100 in the near future, as mentioned in our previous response to Reviewer #1.\n"
  },
  {
    "people": [
      "Dai",
      "Dai",
      "Dai",
      "Bahandau",
      "Adam"
    ],
    "review": "TDLR: The authors present a regularization method wherein they add noise to some representation space. The paper mainly applies the technique w/ sequence autoencoders (Dai et al., 2015) without the usage of attention (i.e., only using the context vector). Experimental results show improvement from author's baseline on some toy tasks.\n\n=== Augmentation ===\nThe augmentation process is simple enough, take the seq2seq context vector and add noise/interpolate/extrapolate to it (Section 3.2). This reviewer is very curious whether this process will also work in non seq2seq applications. \n\nThis reviewer would have liked to see comparison with dropout on the context vector.\n\n=== Experiments ===\nSince the authors are experimenting w/ seq2seq architectures, its a little bit disappointing they didn't compare it w/ Machine Translation (MT), where there are many published papers to compare to.\n\nThe authors did compare their method on several toy datasets (that are less commonly used in DL literature) and MNIST/CIFAR. The authors show improvement over their own baselines on several toy datasets. The improvement on MNIST/CIFAR over the author's baseline seems marginal at best. The author also didn't cite/compare to the baseline published by Dai et al., 2015 for CIFAR -- here they have a much better LSTM baseline of 25% for CIFAR which beats the author's baseline of 32.35% and the author's method of 31.93%.\n\nThe experiments would be much more convincing if they did it on seq2seq+MT on say EN-FR or EN-DE. There is almost no excuse why the experiments wasn't run on the MT task, given this is the first application of seq2seq was born from. Even if not MT, then at least the sentiment analysis tasks (IMDB/Rotten Tomatoes) of the Dai et al., 2015 paper which this paper is so heavily based on for the sequence autoencoder.\n\n=== References ===\nSomething is wrong w/ your references latex setting? Seems like a lot of the conference/journal names are omitted. Additionally, you should update many cites to use the conference/journal name rather than just \"arxiv\".\n\nListen, attend and spell (should be Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition) -> ICASSP\nif citing ICASSP paper above, should also cite Bahandau paper \"End-to-End Attention-based Large Vocabulary Speech Recognition\" which was published in parallel (also in ICASSP).\n\nAdam: A method for stochastic optimization -> ICLR\nAuto-encoding variational bayes -> ICLR\nAddressing the rare word problem in neural machine translation -> ACL\nPixel recurrent neural networks -> ICML\nA neural conversational model -> ICML Workshop\n"
  },
  {
    "people": [
      "Dai",
      "Dai",
      "Dai",
      "Bahandau",
      "Adam"
    ],
    "review": "TDLR: The authors present a regularization method wherein they add noise to some representation space. The paper mainly applies the technique w/ sequence autoencoders (Dai et al., 2015) without the usage of attention (i.e., only using the context vector). Experimental results show improvement from author's baseline on some toy tasks.\n\n=== Augmentation ===\nThe augmentation process is simple enough, take the seq2seq context vector and add noise/interpolate/extrapolate to it (Section 3.2). This reviewer is very curious whether this process will also work in non seq2seq applications. \n\nThis reviewer would have liked to see comparison with dropout on the context vector.\n\n=== Experiments ===\nSince the authors are experimenting w/ seq2seq architectures, its a little bit disappointing they didn't compare it w/ Machine Translation (MT), where there are many published papers to compare to.\n\nThe authors did compare their method on several toy datasets (that are less commonly used in DL literature) and MNIST/CIFAR. The authors show improvement over their own baselines on several toy datasets. The improvement on MNIST/CIFAR over the author's baseline seems marginal at best. The author also didn't cite/compare to the baseline published by Dai et al., 2015 for CIFAR -- here they have a much better LSTM baseline of 25% for CIFAR which beats the author's baseline of 32.35% and the author's method of 31.93%.\n\nThe experiments would be much more convincing if they did it on seq2seq+MT on say EN-FR or EN-DE. There is almost no excuse why the experiments wasn't run on the MT task, given this is the first application of seq2seq was born from. Even if not MT, then at least the sentiment analysis tasks (IMDB/Rotten Tomatoes) of the Dai et al., 2015 paper which this paper is so heavily based on for the sequence autoencoder.\n\n=== References ===\nSomething is wrong w/ your references latex setting? Seems like a lot of the conference/journal names are omitted. Additionally, you should update many cites to use the conference/journal name rather than just \"arxiv\".\n\nListen, attend and spell (should be Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition) -> ICASSP\nif citing ICASSP paper above, should also cite Bahandau paper \"End-to-End Attention-based Large Vocabulary Speech Recognition\" which was published in parallel (also in ICASSP).\n\nAdam: A method for stochastic optimization -> ICLR\nAuto-encoding variational bayes -> ICLR\nAddressing the rare word problem in neural machine translation -> ACL\nPixel recurrent neural networks -> ICML\nA neural conversational model -> ICML Workshop\n"
  },
  {
    "people": [
      "Moses",
      "Sennrich"
    ],
    "review": "I want to draw attention to the fact that you compare tokenized BLEU results (with multi-bleu.perl) and detokenized BLEU results (with mteval-v13a.pl). The two should *never* be mixed in the same table, as the tokenization will have a big effect on results. Even when comparing systems that all have tokenized BLEU, and all use the Moses tokenizer, using different parameters (such as the \"-a\" option for aggressive hyphen splitting) will skew the results.\n\nDetokenized BLEU is standard for WMT, and reported by Sennrich et al. (2016a,b).\n\nI re-ran BLEU on our EN-RO system for comparison:\n\ndetokenized BLEU, mteval-v13a.pl: 28.1 BLEU\ntokenized BLEU, multi-bleu.perl: 29.4 BLEU\n\nyour reported result (multi-bleu.perl): 28.5"
  },
  {
    "people": [
      "Moses",
      "Sennrich"
    ],
    "review": "I want to draw attention to the fact that you compare tokenized BLEU results (with multi-bleu.perl) and detokenized BLEU results (with mteval-v13a.pl). The two should *never* be mixed in the same table, as the tokenization will have a big effect on results. Even when comparing systems that all have tokenized BLEU, and all use the Moses tokenizer, using different parameters (such as the \"-a\" option for aggressive hyphen splitting) will skew the results.\n\nDetokenized BLEU is standard for WMT, and reported by Sennrich et al. (2016a,b).\n\nI re-ran BLEU on our EN-RO system for comparison:\n\ndetokenized BLEU, mteval-v13a.pl: 28.1 BLEU\ntokenized BLEU, multi-bleu.perl: 29.4 BLEU\n\nyour reported result (multi-bleu.perl): 28.5"
  },
  {
    "people": [
      "Giles"
    ],
    "review": "The paper has been updated with the following changes:\n\n- Fixed typo in the equations given in B.2 and B.2.1.\n- Added a reference to work done by Giles et al. in Section 6.\n- Clarified the operation of direct reference in Sections 3 and 4.\n- Added a link to the source code for the model (in Appendix B).\n- Minor wording changes in the Abstract, Introduction, and Sections 3 and 4."
  },
  {
    "people": [
      "Giles"
    ],
    "review": "The paper has been updated with the following changes:\n\n- Fixed typo in the equations given in B.2 and B.2.1.\n- Added a reference to work done by Giles et al. in Section 6.\n- Clarified the operation of direct reference in Sections 3 and 4.\n- Added a link to the source code for the model (in Appendix B).\n- Minor wording changes in the Abstract, Introduction, and Sections 3 and 4."
  },
  {
    "people": [
      "Mnih"
    ],
    "review": "This paper proposed to use Generalized Advantage Estimation (GAE) to optimize DNNs for information seeking tasks. The task is posed as a reinforcement learning problem and the proposed method explicitly promotes information gain to encourage exploration.\n\nBoth GAE and DNN have been used for RL before. The novelty in this paper seems to be the explicit modeling of information gain. However, there is insufficient empirical evidence to demonstrate the benefit and generality of the proposed method. An apple to apple comparison to previous RL framework that doesn't model information gain is missing. For example, the cluttered MNIST experiment tried to compare against Mnih et al. (2014) (which is a little out dated) with two settings. But in both setting the input to the two methods are different. Thus it is unclear what contributed to the performance difference.\n\nThe experiment section is cluttered and hard to read. A table that summarizes the numbers would be much better."
  },
  {
    "people": [
      "Navarro",
      "Hangman"
    ],
    "review": "This paper proposes a setting to learn models that will seek information (e.g., by asking question) in order to solve a given task. They introduce a set of tasks that were designed for that goal. They show that it is possible to train models to solve these tasks with reinforcement learning.\n\nOne key motivation for the tasks proposed in this work are the existence of games like 20Q or battleships where an agent needs to ask questions to solve a given task. It is quite surprising that the authors do not actually consider these games as potential tasks to explore (beside the Hangman). It is also not completely clear how the tasks have been selected. A significant amount of work has been dedicated in the past to understand the property of games like 20Q (e.g., Navarro et al., 2010) and how humans solve them.  It would interesting to see how the tasks proposed in this work distinguish themselves from the ones studied in the existing literature, and how humans would perform on them.  In particular, Cohen & Lake, 2016m have recently studied the 20 questions games in their paper \u201cSearching large hypothesis spaces by asking questions\u201d where they both evaluate the performance of humans and computer. I believe that this paper would really benefits from a similar study.\n\nDeveloping the ability of models to actively seek for information to solve a task is a very interesting but challenging problem. In this paper, all of the  tasks require the agent to select a questions from a finite set of clean and informative possibilities. This allows a simpler analysis of how a given agent may perform but at the cost of a reducing the level of noise that would appear in more realistic settings.\n\nThis paper also show that by using a relatively standard mix of deep learning models and reinforcement learning, they are able to train agents that can solve these tasks in the way it was intended to. This validates their empirical setting but also may exhibit some of the limitation of their approach; using relatively toy-ish settings with perfect information and a fixed number of questions may be too simple. \n\nWhile it is interesting to see that their agent are able to perform well on all of their tasks, the absence of baselines limit the conclusions we can draw from these experiments. For example in the Hangman experiment, it seems that the frequency based model obtains promising performance. It would interesting to see how good are baselines that may use the co-occurrence of letters or the frequency of character n-grams.\n\n\nOverall, this paper explores a very interesting direction of research and propose a set of promising tasks to test the capability of a model to learn from asking question. However, the current analysis of the tasks is a bit limited, and it is hard to draw any conclusion from them. It would be good if the paper would focus more on how humans perform on these tasks, on strong simple baselines and on more tasks related to natural language (since it is one of the motivation of this work) rather than on solving them with relatively sophisticated models.\n"
  },
  {
    "people": [
      "Mnih"
    ],
    "review": "This paper proposed to use Generalized Advantage Estimation (GAE) to optimize DNNs for information seeking tasks. The task is posed as a reinforcement learning problem and the proposed method explicitly promotes information gain to encourage exploration.\n\nBoth GAE and DNN have been used for RL before. The novelty in this paper seems to be the explicit modeling of information gain. However, there is insufficient empirical evidence to demonstrate the benefit and generality of the proposed method. An apple to apple comparison to previous RL framework that doesn't model information gain is missing. For example, the cluttered MNIST experiment tried to compare against Mnih et al. (2014) (which is a little out dated) with two settings. But in both setting the input to the two methods are different. Thus it is unclear what contributed to the performance difference.\n\nThe experiment section is cluttered and hard to read. A table that summarizes the numbers would be much better."
  },
  {
    "people": [
      "Mnih"
    ],
    "review": "This paper proposed to use Generalized Advantage Estimation (GAE) to optimize DNNs for information seeking tasks. The task is posed as a reinforcement learning problem and the proposed method explicitly promotes information gain to encourage exploration.\n\nBoth GAE and DNN have been used for RL before. The novelty in this paper seems to be the explicit modeling of information gain. However, there is insufficient empirical evidence to demonstrate the benefit and generality of the proposed method. An apple to apple comparison to previous RL framework that doesn't model information gain is missing. For example, the cluttered MNIST experiment tried to compare against Mnih et al. (2014) (which is a little out dated) with two settings. But in both setting the input to the two methods are different. Thus it is unclear what contributed to the performance difference.\n\nThe experiment section is cluttered and hard to read. A table that summarizes the numbers would be much better."
  },
  {
    "people": [
      "Navarro",
      "Hangman"
    ],
    "review": "This paper proposes a setting to learn models that will seek information (e.g., by asking question) in order to solve a given task. They introduce a set of tasks that were designed for that goal. They show that it is possible to train models to solve these tasks with reinforcement learning.\n\nOne key motivation for the tasks proposed in this work are the existence of games like 20Q or battleships where an agent needs to ask questions to solve a given task. It is quite surprising that the authors do not actually consider these games as potential tasks to explore (beside the Hangman). It is also not completely clear how the tasks have been selected. A significant amount of work has been dedicated in the past to understand the property of games like 20Q (e.g., Navarro et al., 2010) and how humans solve them.  It would interesting to see how the tasks proposed in this work distinguish themselves from the ones studied in the existing literature, and how humans would perform on them.  In particular, Cohen & Lake, 2016m have recently studied the 20 questions games in their paper \u201cSearching large hypothesis spaces by asking questions\u201d where they both evaluate the performance of humans and computer. I believe that this paper would really benefits from a similar study.\n\nDeveloping the ability of models to actively seek for information to solve a task is a very interesting but challenging problem. In this paper, all of the  tasks require the agent to select a questions from a finite set of clean and informative possibilities. This allows a simpler analysis of how a given agent may perform but at the cost of a reducing the level of noise that would appear in more realistic settings.\n\nThis paper also show that by using a relatively standard mix of deep learning models and reinforcement learning, they are able to train agents that can solve these tasks in the way it was intended to. This validates their empirical setting but also may exhibit some of the limitation of their approach; using relatively toy-ish settings with perfect information and a fixed number of questions may be too simple. \n\nWhile it is interesting to see that their agent are able to perform well on all of their tasks, the absence of baselines limit the conclusions we can draw from these experiments. For example in the Hangman experiment, it seems that the frequency based model obtains promising performance. It would interesting to see how good are baselines that may use the co-occurrence of letters or the frequency of character n-grams.\n\n\nOverall, this paper explores a very interesting direction of research and propose a set of promising tasks to test the capability of a model to learn from asking question. However, the current analysis of the tasks is a bit limited, and it is hard to draw any conclusion from them. It would be good if the paper would focus more on how humans perform on these tasks, on strong simple baselines and on more tasks related to natural language (since it is one of the motivation of this work) rather than on solving them with relatively sophisticated models.\n"
  },
  {
    "people": [
      "Mnih"
    ],
    "review": "This paper proposed to use Generalized Advantage Estimation (GAE) to optimize DNNs for information seeking tasks. The task is posed as a reinforcement learning problem and the proposed method explicitly promotes information gain to encourage exploration.\n\nBoth GAE and DNN have been used for RL before. The novelty in this paper seems to be the explicit modeling of information gain. However, there is insufficient empirical evidence to demonstrate the benefit and generality of the proposed method. An apple to apple comparison to previous RL framework that doesn't model information gain is missing. For example, the cluttered MNIST experiment tried to compare against Mnih et al. (2014) (which is a little out dated) with two settings. But in both setting the input to the two methods are different. Thus it is unclear what contributed to the performance difference.\n\nThe experiment section is cluttered and hard to read. A table that summarizes the numbers would be much better."
  },
  {
    "people": [
      "Ba",
      "Gulcehre"
    ],
    "review": "This paper focusses on attention for neural language modeling and has two major contributions:\n\n1. Authors propose to use separate key, value, and predict vectors for attention mechanism instead of a single vector doing all the 3 functions. This is an interesting extension to standard attention mechanism which can be used in other applications as well.\n2. Authors report that very short attention span is sufficient for language models (which is not very surprising) and propose an n-gram RNN which exploits this fact.\n\nThe paper has novel models for neural language modeling and some interesting messages. Authors have done a thorough experimental analysis of the proposed ideas on a language modeling task and CBT task.\n\nI am convinced with authors\u2019 responses for my pre-review questions.\n\nMinor comment: Ba et al., Reed & de Freitas, and Gulcehre et al. should be added to the related work section as well.\n"
  },
  {
    "people": [
      "George Abbot",
      "Abbot",
      "Abbot",
      "Abbot"
    ],
    "review": "This paper explores a variety of memory augmented architectures (key, key-value, key-predict-value) and additionally simpler near memory-less RNN architectures. Using an attention model that has access to the various decompositions is an interesting idea and one worth future explorations, potentially in different tasks where this type of model could excel even more. The results over the Wikipedia corpus are interesting and feature a wide variety of different model types. This is where the models suggested in the paper are strongest. The same models run over the CBT dataset show a comparable but less convincing demonstration of the variations between the models.\n\nThe authors also released their Wikipedia corpus already. Having inspected it I consider it a positive and interesting contribution. I still believe that, if a model was found that could better handle longer term dependencies, it would do better on this Wikipedia dataset, but at least within the realm of what . As an example, the first article in train.txt is about a person named \"George Abbot\", yet \"Abbot\" isn't mentioned again until the next sentence 40 tokens later, and then the next \"Abbot\" is 15 tokens from there. Most gaps between occurrences of \"Abbot\" are dozens of timesteps. Performing an analysis based upon easily accessed information, such as when the same token reappears again or average sentence length, may be useful as an approximation for the length that an attention window may prefer.\n\nThis is a well explained paper that raises interesting questions regarding the spans used in existing language modeling approaches and serves as a potential springboard for future directions."
  },
  {
    "people": [
      "Ba",
      "Gulcehre"
    ],
    "review": "This paper focusses on attention for neural language modeling and has two major contributions:\n\n1. Authors propose to use separate key, value, and predict vectors for attention mechanism instead of a single vector doing all the 3 functions. This is an interesting extension to standard attention mechanism which can be used in other applications as well.\n2. Authors report that very short attention span is sufficient for language models (which is not very surprising) and propose an n-gram RNN which exploits this fact.\n\nThe paper has novel models for neural language modeling and some interesting messages. Authors have done a thorough experimental analysis of the proposed ideas on a language modeling task and CBT task.\n\nI am convinced with authors\u2019 responses for my pre-review questions.\n\nMinor comment: Ba et al., Reed & de Freitas, and Gulcehre et al. should be added to the related work section as well.\n"
  },
  {
    "people": [
      "George Abbot",
      "Abbot",
      "Abbot",
      "Abbot"
    ],
    "review": "This paper explores a variety of memory augmented architectures (key, key-value, key-predict-value) and additionally simpler near memory-less RNN architectures. Using an attention model that has access to the various decompositions is an interesting idea and one worth future explorations, potentially in different tasks where this type of model could excel even more. The results over the Wikipedia corpus are interesting and feature a wide variety of different model types. This is where the models suggested in the paper are strongest. The same models run over the CBT dataset show a comparable but less convincing demonstration of the variations between the models.\n\nThe authors also released their Wikipedia corpus already. Having inspected it I consider it a positive and interesting contribution. I still believe that, if a model was found that could better handle longer term dependencies, it would do better on this Wikipedia dataset, but at least within the realm of what . As an example, the first article in train.txt is about a person named \"George Abbot\", yet \"Abbot\" isn't mentioned again until the next sentence 40 tokens later, and then the next \"Abbot\" is 15 tokens from there. Most gaps between occurrences of \"Abbot\" are dozens of timesteps. Performing an analysis based upon easily accessed information, such as when the same token reappears again or average sentence length, may be useful as an approximation for the length that an attention window may prefer.\n\nThis is a well explained paper that raises interesting questions regarding the spans used in existing language modeling approaches and serves as a potential springboard for future directions."
  },
  {
    "people": [
      "Zoph",
      "Dai",
      "Le"
    ],
    "review": "In this paper, the authors propose to pretrain the encoder/decoder of seq2seq models on a large amount of unlabeled data using a LM objective. They obtain improvements using this technique on machine translation and abstractive summarization.\n\nWhile the effectiveness of pretraining seq2seq models has been known among researchers and explored in a few papers (e.g. Zoph et al. 2016,  Dai and Le 2015), I believe this is the first paper to pretrain using a LM for both the encoder/decoder. The technique is simple, but the gains are large (e.g. +2.7 BLEU on NMT). In addition, the authors perform extensive ablation studies to analyze where the performance is coming from. Hence, I think this paper should be accepted.\n"
  },
  {
    "people": [
      "Chen",
      "Dahl"
    ],
    "review": "strengths:\n\nA method is proposed in this paper to initialize the encoder and decoder of the seq2seq model using the trained weights of language models with no parallel data. After such pretraining, all weights are jointly fine-tuned with parallel labeled data with an additional language modeling loss.\n\nIt is shown that pretraining accelerates training and improves generalization of seq2seq models.\n\nThe main value of the proposed method is to leverage separate source and target corpora, contrasting the common methods of using large amounts of parallel training corpora.\n\n\nweaknesses:\n\nThe objective function shown in the middle of pg 3 is highly empirical, not directly linked to how non-parallel data helps to improve the final prediction results. The paper should compare with and discuss the objective function based on expectation of cross entropy which is directly linked to improving prediction results as proposed in arXiv:1606.04646, Chen et al.:  Unsupervised Learning of Predictors from Unpaired Input-Output Samples, 2016.\n\nThe pre-training procedure proposed in this paper is also closely connected with the DNN pretraining method presented in Dahl et al. 2011, 2012. Comparisons should be made in the paper, highlighting why the proposed one is conceptually superior if the authors believe so. \n"
  },
  {
    "people": [
      "Jean"
    ],
    "review": "what's your own baseline for NMT without pre-training? Jean et al. (2015) uses a more shallow architecture than this paper, so I presume your baseline would be higher (this is also corrobated in figure 3).\n\nyou should make it clear how much of your improvement over related work comes from pretraining, and how much from having a deeper architecture."
  },
  {
    "people": [
      "Zoph",
      "Dai",
      "Le"
    ],
    "review": "In this paper, the authors propose to pretrain the encoder/decoder of seq2seq models on a large amount of unlabeled data using a LM objective. They obtain improvements using this technique on machine translation and abstractive summarization.\n\nWhile the effectiveness of pretraining seq2seq models has been known among researchers and explored in a few papers (e.g. Zoph et al. 2016,  Dai and Le 2015), I believe this is the first paper to pretrain using a LM for both the encoder/decoder. The technique is simple, but the gains are large (e.g. +2.7 BLEU on NMT). In addition, the authors perform extensive ablation studies to analyze where the performance is coming from. Hence, I think this paper should be accepted.\n"
  },
  {
    "people": [
      "Chen",
      "Dahl"
    ],
    "review": "strengths:\n\nA method is proposed in this paper to initialize the encoder and decoder of the seq2seq model using the trained weights of language models with no parallel data. After such pretraining, all weights are jointly fine-tuned with parallel labeled data with an additional language modeling loss.\n\nIt is shown that pretraining accelerates training and improves generalization of seq2seq models.\n\nThe main value of the proposed method is to leverage separate source and target corpora, contrasting the common methods of using large amounts of parallel training corpora.\n\n\nweaknesses:\n\nThe objective function shown in the middle of pg 3 is highly empirical, not directly linked to how non-parallel data helps to improve the final prediction results. The paper should compare with and discuss the objective function based on expectation of cross entropy which is directly linked to improving prediction results as proposed in arXiv:1606.04646, Chen et al.:  Unsupervised Learning of Predictors from Unpaired Input-Output Samples, 2016.\n\nThe pre-training procedure proposed in this paper is also closely connected with the DNN pretraining method presented in Dahl et al. 2011, 2012. Comparisons should be made in the paper, highlighting why the proposed one is conceptually superior if the authors believe so. \n"
  },
  {
    "people": [
      "Jean"
    ],
    "review": "what's your own baseline for NMT without pre-training? Jean et al. (2015) uses a more shallow architecture than this paper, so I presume your baseline would be higher (this is also corrobated in figure 3).\n\nyou should make it clear how much of your improvement over related work comes from pretraining, and how much from having a deeper architecture."
  },
  {
    "people": [
      "Adagrad Duchi",
      "Adagrad",
      "Andrychowicz"
    ],
    "review": "This work proposes to augment normal gradient descent algorithms with a \"Data Filter\", that acts as a curriculum teacher by selecting which examples the trained target network should see to learn optimally. Such a filter is learned simultaneously to the target network, and trained via Reinforcement Learning algorithms receiving rewards based on the state of training with respect to some pseudo-validation set.\n\n\nStylistic comment, please use the more common style of \"(Author, year)\" rather than \"Author (year)\" when the Author is *not* referred to or used in the sentence.\nE.g. \"and its variants such as Adagrad Duchi et al. (2011)\" should be \"such as Adagrad (Duchi et al., 2011)\", and  \"proposed in Andrychowicz et al. (2016),\" should remain so.\n\nI think the paragraph containing \"What we need to do is, after seeing the mini-batch Dt of M training instances, we dynamically determine which instances in Dt are used for training and which are filtered.\" should be clarified. What is \"seeing\"? That is, you should mention explicitly that you do the forward-pass first, then compute features from that, and then decide for which examples to perform the backwards pass.\n\n\nThere are a few choices in this work which I do not understand:\n\nWhy wait until the end of the episode to update your reinforce policy (algorithm 2), but train your actor critic at each step (algorithm 3)? You say REINFORCE has high variance, which is true, but does not mean it cannot be trained at each step (unless you have some experiments that suggest otherwise, and if so they should be included or mentionned in the paper).\n\nSimilarly, why not train REINFORCE with the same reward as your Actor-Critic model? And vice-versa? You claim several times that a limitation of REINFORCE is that you need to wait for the episode to be over, but considering your data is i.i.d., you can make your episode be anything from a single training step, one D_t, to the whole multi-epoch training procedure.\n\n\nI have a few qualms with the experimental setting:\n- is Figure 2 obtained from a single (i.e. one per setup) experiment? From different initial weights? If so, there is no proper way of knowing whether results are chance or not! This is a serious concern for me.\n- with most state-of-the-art work using optimization methods such as Adam and RMSProp, is it surprising that they were not experimented with.\n- it is not clear what the learning rates are; how fast should the RL part adapt to the SL part? Its not clear that this was experimented with at all.\n- the environment, i.e. the target network being trained, is not stationnary at all. It would have been interesting to measure how much the policy changes as a function of time. Figure 3, could both be the result of the policy adapting, or of the policy remaining fixed and the features changing (which could indicate a failure of the policy to adapt).\n- in fact it is not really adressed in the paper that the environment is non-stationary, given the current setup, the distribution of features will change as the target network progresses. This has an impact on optimization.\n- how is the \"pseudo-validation\" data, target to the policy, chosen? It should be a subset of the training data. The second paragraph of section 3.2 suggests something of the sort, but then your algorithms suggest that the same data is used to train both the policies and the networks, so I am unsure of which is what.\n\n\nOverall the idea is novel and interesting, the paper is well written for the most part, but the methodology has some flaws. Clearer explanations and either more justification of the experimental choices or more experiments are needed to make this paper complete. Unless the authors convince me otherwise, I think it would be worth waiting for more experiments and submitting a very strong paper rather than presenting this (potentially powerful!) idea with weak results."
  },
  {
    "people": [
      "Fei"
    ],
    "review": "Dear All Reviewers,\n\nWe have updated a new version, which: 1) adds a new experiment on mnist dataset to further verify the effectiveness of our proposed NDF algorithm; 2) makes a more dedicated and clearer description towards baseline method, REINFORCE v.s. Actor-Critic and validation setup.\n\nWe hope the new version can remove your previous concerns towards this work, even it is approaching the end of rebuttal period (we feel it sorry to submit it late).\n\nThanks again for all of your valuable comments and suggestions to this work!\n\nBest,\nFei"
  },
  {
    "people": [
      "Adam",
      "Adam",
      "Adam"
    ],
    "review": "Final review: The writers were very responsive and I agree the reviewer2 that their experimental setup is not wrong after all and increased the score by one.  But I still think there is lack of experiments and the results are not conclusive. As a reader I am interested in two things, either getting a new insight and understanding something better, or learn a method for a better performance. This paper falls in the category two, but fails to prove it with more throughout and rigorous experiments. In summary the paper lacks experiments and results are inconclusive and I do not believe the proposed method would be quite useful and hence not a conference level publication. \n\n--\nThe paper proposes to train a policy network along the main network for selecting subset of data during training for achieving faster convergence with less data.\n\nPros:\nIt's well written and straightforward to follow\nThe algorithm has been explained clearly.\n\nCons:\nSection 2 mentions that the validation accuracy is used as one of the feature vectors for training the NDF. This invalidates the experiments, as the training procedure is using some data from the validation set.\n\nOnly one dataset has been tested on. Papers such as this one that claim faster convergence rate should be tested on multiple datasets and network architectures to show consistency of results. Especially larger datasets as the proposed methods is going to use less training data at each iteration, it has to be shown in much larger scaler datasets such as Imagenet.\n\nAs discussed more in detail in the pre-reviews question, if the paper is claiming faster convergence then it has to compare the learning curves with other baselines such Adam. Plain SGD is very unfair comparison as it is almost never used in practice. And this is regardless of what is the black box optimizer they use. The case could be that Adam alone as black box optimizer works as well or better than Adam as black box + NDF."
  },
  {
    "people": [
      "Adagrad Duchi",
      "Adagrad",
      "Andrychowicz"
    ],
    "review": "This work proposes to augment normal gradient descent algorithms with a \"Data Filter\", that acts as a curriculum teacher by selecting which examples the trained target network should see to learn optimally. Such a filter is learned simultaneously to the target network, and trained via Reinforcement Learning algorithms receiving rewards based on the state of training with respect to some pseudo-validation set.\n\n\nStylistic comment, please use the more common style of \"(Author, year)\" rather than \"Author (year)\" when the Author is *not* referred to or used in the sentence.\nE.g. \"and its variants such as Adagrad Duchi et al. (2011)\" should be \"such as Adagrad (Duchi et al., 2011)\", and  \"proposed in Andrychowicz et al. (2016),\" should remain so.\n\nI think the paragraph containing \"What we need to do is, after seeing the mini-batch Dt of M training instances, we dynamically determine which instances in Dt are used for training and which are filtered.\" should be clarified. What is \"seeing\"? That is, you should mention explicitly that you do the forward-pass first, then compute features from that, and then decide for which examples to perform the backwards pass.\n\n\nThere are a few choices in this work which I do not understand:\n\nWhy wait until the end of the episode to update your reinforce policy (algorithm 2), but train your actor critic at each step (algorithm 3)? You say REINFORCE has high variance, which is true, but does not mean it cannot be trained at each step (unless you have some experiments that suggest otherwise, and if so they should be included or mentionned in the paper).\n\nSimilarly, why not train REINFORCE with the same reward as your Actor-Critic model? And vice-versa? You claim several times that a limitation of REINFORCE is that you need to wait for the episode to be over, but considering your data is i.i.d., you can make your episode be anything from a single training step, one D_t, to the whole multi-epoch training procedure.\n\n\nI have a few qualms with the experimental setting:\n- is Figure 2 obtained from a single (i.e. one per setup) experiment? From different initial weights? If so, there is no proper way of knowing whether results are chance or not! This is a serious concern for me.\n- with most state-of-the-art work using optimization methods such as Adam and RMSProp, is it surprising that they were not experimented with.\n- it is not clear what the learning rates are; how fast should the RL part adapt to the SL part? Its not clear that this was experimented with at all.\n- the environment, i.e. the target network being trained, is not stationnary at all. It would have been interesting to measure how much the policy changes as a function of time. Figure 3, could both be the result of the policy adapting, or of the policy remaining fixed and the features changing (which could indicate a failure of the policy to adapt).\n- in fact it is not really adressed in the paper that the environment is non-stationary, given the current setup, the distribution of features will change as the target network progresses. This has an impact on optimization.\n- how is the \"pseudo-validation\" data, target to the policy, chosen? It should be a subset of the training data. The second paragraph of section 3.2 suggests something of the sort, but then your algorithms suggest that the same data is used to train both the policies and the networks, so I am unsure of which is what.\n\n\nOverall the idea is novel and interesting, the paper is well written for the most part, but the methodology has some flaws. Clearer explanations and either more justification of the experimental choices or more experiments are needed to make this paper complete. Unless the authors convince me otherwise, I think it would be worth waiting for more experiments and submitting a very strong paper rather than presenting this (potentially powerful!) idea with weak results.\n"
  },
  {
    "people": [
      "Adagrad Duchi",
      "Adagrad",
      "Andrychowicz"
    ],
    "review": "This work proposes to augment normal gradient descent algorithms with a \"Data Filter\", that acts as a curriculum teacher by selecting which examples the trained target network should see to learn optimally. Such a filter is learned simultaneously to the target network, and trained via Reinforcement Learning algorithms receiving rewards based on the state of training with respect to some pseudo-validation set.\n\n\nStylistic comment, please use the more common style of \"(Author, year)\" rather than \"Author (year)\" when the Author is *not* referred to or used in the sentence.\nE.g. \"and its variants such as Adagrad Duchi et al. (2011)\" should be \"such as Adagrad (Duchi et al., 2011)\", and  \"proposed in Andrychowicz et al. (2016),\" should remain so.\n\nI think the paragraph containing \"What we need to do is, after seeing the mini-batch Dt of M training instances, we dynamically determine which instances in Dt are used for training and which are filtered.\" should be clarified. What is \"seeing\"? That is, you should mention explicitly that you do the forward-pass first, then compute features from that, and then decide for which examples to perform the backwards pass.\n\n\nThere are a few choices in this work which I do not understand:\n\nWhy wait until the end of the episode to update your reinforce policy (algorithm 2), but train your actor critic at each step (algorithm 3)? You say REINFORCE has high variance, which is true, but does not mean it cannot be trained at each step (unless you have some experiments that suggest otherwise, and if so they should be included or mentionned in the paper).\n\nSimilarly, why not train REINFORCE with the same reward as your Actor-Critic model? And vice-versa? You claim several times that a limitation of REINFORCE is that you need to wait for the episode to be over, but considering your data is i.i.d., you can make your episode be anything from a single training step, one D_t, to the whole multi-epoch training procedure.\n\n\nI have a few qualms with the experimental setting:\n- is Figure 2 obtained from a single (i.e. one per setup) experiment? From different initial weights? If so, there is no proper way of knowing whether results are chance or not! This is a serious concern for me.\n- with most state-of-the-art work using optimization methods such as Adam and RMSProp, is it surprising that they were not experimented with.\n- it is not clear what the learning rates are; how fast should the RL part adapt to the SL part? Its not clear that this was experimented with at all.\n- the environment, i.e. the target network being trained, is not stationnary at all. It would have been interesting to measure how much the policy changes as a function of time. Figure 3, could both be the result of the policy adapting, or of the policy remaining fixed and the features changing (which could indicate a failure of the policy to adapt).\n- in fact it is not really adressed in the paper that the environment is non-stationary, given the current setup, the distribution of features will change as the target network progresses. This has an impact on optimization.\n- how is the \"pseudo-validation\" data, target to the policy, chosen? It should be a subset of the training data. The second paragraph of section 3.2 suggests something of the sort, but then your algorithms suggest that the same data is used to train both the policies and the networks, so I am unsure of which is what.\n\n\nOverall the idea is novel and interesting, the paper is well written for the most part, but the methodology has some flaws. Clearer explanations and either more justification of the experimental choices or more experiments are needed to make this paper complete. Unless the authors convince me otherwise, I think it would be worth waiting for more experiments and submitting a very strong paper rather than presenting this (potentially powerful!) idea with weak results."
  },
  {
    "people": [
      "Fei"
    ],
    "review": "Dear All Reviewers,\n\nWe have updated a new version, which: 1) adds a new experiment on mnist dataset to further verify the effectiveness of our proposed NDF algorithm; 2) makes a more dedicated and clearer description towards baseline method, REINFORCE v.s. Actor-Critic and validation setup.\n\nWe hope the new version can remove your previous concerns towards this work, even it is approaching the end of rebuttal period (we feel it sorry to submit it late).\n\nThanks again for all of your valuable comments and suggestions to this work!\n\nBest,\nFei"
  },
  {
    "people": [
      "Adam",
      "Adam",
      "Adam"
    ],
    "review": "Final review: The writers were very responsive and I agree the reviewer2 that their experimental setup is not wrong after all and increased the score by one.  But I still think there is lack of experiments and the results are not conclusive. As a reader I am interested in two things, either getting a new insight and understanding something better, or learn a method for a better performance. This paper falls in the category two, but fails to prove it with more throughout and rigorous experiments. In summary the paper lacks experiments and results are inconclusive and I do not believe the proposed method would be quite useful and hence not a conference level publication. \n\n--\nThe paper proposes to train a policy network along the main network for selecting subset of data during training for achieving faster convergence with less data.\n\nPros:\nIt's well written and straightforward to follow\nThe algorithm has been explained clearly.\n\nCons:\nSection 2 mentions that the validation accuracy is used as one of the feature vectors for training the NDF. This invalidates the experiments, as the training procedure is using some data from the validation set.\n\nOnly one dataset has been tested on. Papers such as this one that claim faster convergence rate should be tested on multiple datasets and network architectures to show consistency of results. Especially larger datasets as the proposed methods is going to use less training data at each iteration, it has to be shown in much larger scaler datasets such as Imagenet.\n\nAs discussed more in detail in the pre-reviews question, if the paper is claiming faster convergence then it has to compare the learning curves with other baselines such Adam. Plain SGD is very unfair comparison as it is almost never used in practice. And this is regardless of what is the black box optimizer they use. The case could be that Adam alone as black box optimizer works as well or better than Adam as black box + NDF."
  },
  {
    "people": [
      "Adagrad Duchi",
      "Adagrad",
      "Andrychowicz"
    ],
    "review": "This work proposes to augment normal gradient descent algorithms with a \"Data Filter\", that acts as a curriculum teacher by selecting which examples the trained target network should see to learn optimally. Such a filter is learned simultaneously to the target network, and trained via Reinforcement Learning algorithms receiving rewards based on the state of training with respect to some pseudo-validation set.\n\n\nStylistic comment, please use the more common style of \"(Author, year)\" rather than \"Author (year)\" when the Author is *not* referred to or used in the sentence.\nE.g. \"and its variants such as Adagrad Duchi et al. (2011)\" should be \"such as Adagrad (Duchi et al., 2011)\", and  \"proposed in Andrychowicz et al. (2016),\" should remain so.\n\nI think the paragraph containing \"What we need to do is, after seeing the mini-batch Dt of M training instances, we dynamically determine which instances in Dt are used for training and which are filtered.\" should be clarified. What is \"seeing\"? That is, you should mention explicitly that you do the forward-pass first, then compute features from that, and then decide for which examples to perform the backwards pass.\n\n\nThere are a few choices in this work which I do not understand:\n\nWhy wait until the end of the episode to update your reinforce policy (algorithm 2), but train your actor critic at each step (algorithm 3)? You say REINFORCE has high variance, which is true, but does not mean it cannot be trained at each step (unless you have some experiments that suggest otherwise, and if so they should be included or mentionned in the paper).\n\nSimilarly, why not train REINFORCE with the same reward as your Actor-Critic model? And vice-versa? You claim several times that a limitation of REINFORCE is that you need to wait for the episode to be over, but considering your data is i.i.d., you can make your episode be anything from a single training step, one D_t, to the whole multi-epoch training procedure.\n\n\nI have a few qualms with the experimental setting:\n- is Figure 2 obtained from a single (i.e. one per setup) experiment? From different initial weights? If so, there is no proper way of knowing whether results are chance or not! This is a serious concern for me.\n- with most state-of-the-art work using optimization methods such as Adam and RMSProp, is it surprising that they were not experimented with.\n- it is not clear what the learning rates are; how fast should the RL part adapt to the SL part? Its not clear that this was experimented with at all.\n- the environment, i.e. the target network being trained, is not stationnary at all. It would have been interesting to measure how much the policy changes as a function of time. Figure 3, could both be the result of the policy adapting, or of the policy remaining fixed and the features changing (which could indicate a failure of the policy to adapt).\n- in fact it is not really adressed in the paper that the environment is non-stationary, given the current setup, the distribution of features will change as the target network progresses. This has an impact on optimization.\n- how is the \"pseudo-validation\" data, target to the policy, chosen? It should be a subset of the training data. The second paragraph of section 3.2 suggests something of the sort, but then your algorithms suggest that the same data is used to train both the policies and the networks, so I am unsure of which is what.\n\n\nOverall the idea is novel and interesting, the paper is well written for the most part, but the methodology has some flaws. Clearer explanations and either more justification of the experimental choices or more experiments are needed to make this paper complete. Unless the authors convince me otherwise, I think it would be worth waiting for more experiments and submitting a very strong paper rather than presenting this (potentially powerful!) idea with weak results.\n"
  },
  {
    "people": [
      "Hinton"
    ],
    "review": "This paper describes a careful experimental study on the CIFAR-10 task that uses data augmentation and Bayesian hyperparameter optimization to train a large number of high-quality, deep convolutional network classification models from hard (0-1) targets.  An ensemble of the 16 best models is then used as a teacher model in the distillation framework, where student models are trained to match the averaged logits from the teacher ensemble.  Data augmentation and Bayesian hyperparameter optimization is also applied in the training of the student models.  Both non-convolutional (MLP) and convolutional student models of varying depths and parameter counts are trained.  Convolutional models with the same architecture and parameter count as some of the convolutional students are also trained using hard targets and cross-entropy loss.  The experimental results show that convolutional students with only one or two convolutional layers are unable to match the results of students having more convolutional layers under the constraint that the number of parameters in all students is kept constant.\n\nPros\n+ This is a very thorough and well designed study that make use of the best existing tools to try to answer the question of whether or not deep convolutional models need both depth and convolution.\n+ It builds nicely on the preliminary results in Ba & Caruana, 2014.\n\nCons\n- It is difficult to prove a negative, as the authors admit.  That said, this study is as convincing as possible given current theory and practice in deep learning.\n\nSection 2.2 should state that the logits are unnormalized log-probabilities (they don't include the log partition function).\n\nThe paper does not follow the ICLR citation style.  Quoting from the template:  \"When the authors or the publication are included in the sentence, the citation should not be in parenthesis (as in \u201cSee Hinton et al. (2006) for more information.\u201d). Otherwise, the citation should be in parenthesis (as in \u201cDeep learning shows promise to make progress towards AI (Bengio & LeCun, 2007).\u201d).\"\n\nThere are a few minor issues with English usage and typos that should be cleaned up in the final manuscript.\n\nnecessary when training student models with more than 1 convolutional layers \u2192 necessary when training student models with more than 1 convolutional layer\n\nremaining 10,000 images as validation set \u2192 remaining 10,000 images as the validation set\n\nevaluate the ensemble\u2019s predictions (logits) on these samples, and save all data \u2192 evaluated the ensemble\u2019s predictions (logits) on these samples, and saved all data\n\nmore detail about hyperparamter optimization \u2192 more detail about hyperparameter optimization\n\nWe trained 129 deep CNN models with spearmint \u2192 We trained 129 deep CNN models with Spearmint\n\nThe best model obtained an accuracy of 92.78%, the fifth best achieved 92.67%. \u2192 The best model obtained an accuracy of 92.78%; the fifth best achieved 92.67%.\n\nthe sizes and architectures of three best models \u2192 the sizes and architectures of the three best models\n\nclearly suggests that convolutional is critical \u2192  clearly suggests that convolution is critical\n\nsimilarly from the hyperparameter-opimizer\u2019s point of view \u2192 similarly from the hyperparameter-optimizer\u2019s point of view"
  },
  {
    "people": [
      "Ba",
      "Caruana"
    ],
    "review": "Description.\nThis paper describes experiments testing whether deep convolutional networks can be replaced with shallow networks with the same number of parameters without loss of accuracy. The experiments are performed on he CIFAR 10 dataset where deep convolutional teacher networks are used to train shallow student networks using L2 regression on logit outputs.  The results show that similar accuracy on the same parameter budget can be only obtained when multiple layers of convolution are used. \n\nStrong  points.\n- The experiments are carefully done with thorough selection of hyperparameters. \n- The paper shows interesting results that go partially against conclusions from the previous work in this area (Ba and Caruana 2014).\n- The paper is well and clearly written.\n\nWeak points:\n- CIFAR is still somewhat toy dataset with only 10 classes. It would be interesting to see some results on a more challenging problem such as ImageNet. Would the results for a large number of classes be similar?\n\nOriginality:\n- This is mainly an experimental paper, but the question it asks is interesting and worth investigation. The experimental results are solid and provide new insights.\n\nQuality:\n- The experiments are well done.\n\nClarity:\n- The paper is well written and clear.\n\nSignificance:\n- The results go against some of the conclusions from previous work, so should be published and discussed.\n\nOverall:\nExperimental paper with interesting results. Well written. Solid experiments. \n"
  },
  {
    "people": [
      "Hinton"
    ],
    "review": "This paper describes a careful experimental study on the CIFAR-10 task that uses data augmentation and Bayesian hyperparameter optimization to train a large number of high-quality, deep convolutional network classification models from hard (0-1) targets.  An ensemble of the 16 best models is then used as a teacher model in the distillation framework, where student models are trained to match the averaged logits from the teacher ensemble.  Data augmentation and Bayesian hyperparameter optimization is also applied in the training of the student models.  Both non-convolutional (MLP) and convolutional student models of varying depths and parameter counts are trained.  Convolutional models with the same architecture and parameter count as some of the convolutional students are also trained using hard targets and cross-entropy loss.  The experimental results show that convolutional students with only one or two convolutional layers are unable to match the results of students having more convolutional layers under the constraint that the number of parameters in all students is kept constant.\n\nPros\n+ This is a very thorough and well designed study that make use of the best existing tools to try to answer the question of whether or not deep convolutional models need both depth and convolution.\n+ It builds nicely on the preliminary results in Ba & Caruana, 2014.\n\nCons\n- It is difficult to prove a negative, as the authors admit.  That said, this study is as convincing as possible given current theory and practice in deep learning.\n\nSection 2.2 should state that the logits are unnormalized log-probabilities (they don't include the log partition function).\n\nThe paper does not follow the ICLR citation style.  Quoting from the template:  \"When the authors or the publication are included in the sentence, the citation should not be in parenthesis (as in \u201cSee Hinton et al. (2006) for more information.\u201d). Otherwise, the citation should be in parenthesis (as in \u201cDeep learning shows promise to make progress towards AI (Bengio & LeCun, 2007).\u201d).\"\n\nThere are a few minor issues with English usage and typos that should be cleaned up in the final manuscript.\n\nnecessary when training student models with more than 1 convolutional layers \u2192 necessary when training student models with more than 1 convolutional layer\n\nremaining 10,000 images as validation set \u2192 remaining 10,000 images as the validation set\n\nevaluate the ensemble\u2019s predictions (logits) on these samples, and save all data \u2192 evaluated the ensemble\u2019s predictions (logits) on these samples, and saved all data\n\nmore detail about hyperparamter optimization \u2192 more detail about hyperparameter optimization\n\nWe trained 129 deep CNN models with spearmint \u2192 We trained 129 deep CNN models with Spearmint\n\nThe best model obtained an accuracy of 92.78%, the fifth best achieved 92.67%. \u2192 The best model obtained an accuracy of 92.78%; the fifth best achieved 92.67%.\n\nthe sizes and architectures of three best models \u2192 the sizes and architectures of the three best models\n\nclearly suggests that convolutional is critical \u2192  clearly suggests that convolution is critical\n\nsimilarly from the hyperparameter-opimizer\u2019s point of view \u2192 similarly from the hyperparameter-optimizer\u2019s point of view\n\n"
  },
  {
    "people": [
      "Hinton"
    ],
    "review": "This paper describes a careful experimental study on the CIFAR-10 task that uses data augmentation and Bayesian hyperparameter optimization to train a large number of high-quality, deep convolutional network classification models from hard (0-1) targets.  An ensemble of the 16 best models is then used as a teacher model in the distillation framework, where student models are trained to match the averaged logits from the teacher ensemble.  Data augmentation and Bayesian hyperparameter optimization is also applied in the training of the student models.  Both non-convolutional (MLP) and convolutional student models of varying depths and parameter counts are trained.  Convolutional models with the same architecture and parameter count as some of the convolutional students are also trained using hard targets and cross-entropy loss.  The experimental results show that convolutional students with only one or two convolutional layers are unable to match the results of students having more convolutional layers under the constraint that the number of parameters in all students is kept constant.\n\nPros\n+ This is a very thorough and well designed study that make use of the best existing tools to try to answer the question of whether or not deep convolutional models need both depth and convolution.\n+ It builds nicely on the preliminary results in Ba & Caruana, 2014.\n\nCons\n- It is difficult to prove a negative, as the authors admit.  That said, this study is as convincing as possible given current theory and practice in deep learning.\n\nSection 2.2 should state that the logits are unnormalized log-probabilities (they don't include the log partition function).\n\nThe paper does not follow the ICLR citation style.  Quoting from the template:  \"When the authors or the publication are included in the sentence, the citation should not be in parenthesis (as in \u201cSee Hinton et al. (2006) for more information.\u201d). Otherwise, the citation should be in parenthesis (as in \u201cDeep learning shows promise to make progress towards AI (Bengio & LeCun, 2007).\u201d).\"\n\nThere are a few minor issues with English usage and typos that should be cleaned up in the final manuscript.\n\nnecessary when training student models with more than 1 convolutional layers \u2192 necessary when training student models with more than 1 convolutional layer\n\nremaining 10,000 images as validation set \u2192 remaining 10,000 images as the validation set\n\nevaluate the ensemble\u2019s predictions (logits) on these samples, and save all data \u2192 evaluated the ensemble\u2019s predictions (logits) on these samples, and saved all data\n\nmore detail about hyperparamter optimization \u2192 more detail about hyperparameter optimization\n\nWe trained 129 deep CNN models with spearmint \u2192 We trained 129 deep CNN models with Spearmint\n\nThe best model obtained an accuracy of 92.78%, the fifth best achieved 92.67%. \u2192 The best model obtained an accuracy of 92.78%; the fifth best achieved 92.67%.\n\nthe sizes and architectures of three best models \u2192 the sizes and architectures of the three best models\n\nclearly suggests that convolutional is critical \u2192  clearly suggests that convolution is critical\n\nsimilarly from the hyperparameter-opimizer\u2019s point of view \u2192 similarly from the hyperparameter-optimizer\u2019s point of view"
  },
  {
    "people": [
      "Ba",
      "Caruana"
    ],
    "review": "Description.\nThis paper describes experiments testing whether deep convolutional networks can be replaced with shallow networks with the same number of parameters without loss of accuracy. The experiments are performed on he CIFAR 10 dataset where deep convolutional teacher networks are used to train shallow student networks using L2 regression on logit outputs.  The results show that similar accuracy on the same parameter budget can be only obtained when multiple layers of convolution are used. \n\nStrong  points.\n- The experiments are carefully done with thorough selection of hyperparameters. \n- The paper shows interesting results that go partially against conclusions from the previous work in this area (Ba and Caruana 2014).\n- The paper is well and clearly written.\n\nWeak points:\n- CIFAR is still somewhat toy dataset with only 10 classes. It would be interesting to see some results on a more challenging problem such as ImageNet. Would the results for a large number of classes be similar?\n\nOriginality:\n- This is mainly an experimental paper, but the question it asks is interesting and worth investigation. The experimental results are solid and provide new insights.\n\nQuality:\n- The experiments are well done.\n\nClarity:\n- The paper is well written and clear.\n\nSignificance:\n- The results go against some of the conclusions from previous work, so should be published and discussed.\n\nOverall:\nExperimental paper with interesting results. Well written. Solid experiments. \n"
  },
  {
    "people": [
      "Hinton"
    ],
    "review": "This paper describes a careful experimental study on the CIFAR-10 task that uses data augmentation and Bayesian hyperparameter optimization to train a large number of high-quality, deep convolutional network classification models from hard (0-1) targets.  An ensemble of the 16 best models is then used as a teacher model in the distillation framework, where student models are trained to match the averaged logits from the teacher ensemble.  Data augmentation and Bayesian hyperparameter optimization is also applied in the training of the student models.  Both non-convolutional (MLP) and convolutional student models of varying depths and parameter counts are trained.  Convolutional models with the same architecture and parameter count as some of the convolutional students are also trained using hard targets and cross-entropy loss.  The experimental results show that convolutional students with only one or two convolutional layers are unable to match the results of students having more convolutional layers under the constraint that the number of parameters in all students is kept constant.\n\nPros\n+ This is a very thorough and well designed study that make use of the best existing tools to try to answer the question of whether or not deep convolutional models need both depth and convolution.\n+ It builds nicely on the preliminary results in Ba & Caruana, 2014.\n\nCons\n- It is difficult to prove a negative, as the authors admit.  That said, this study is as convincing as possible given current theory and practice in deep learning.\n\nSection 2.2 should state that the logits are unnormalized log-probabilities (they don't include the log partition function).\n\nThe paper does not follow the ICLR citation style.  Quoting from the template:  \"When the authors or the publication are included in the sentence, the citation should not be in parenthesis (as in \u201cSee Hinton et al. (2006) for more information.\u201d). Otherwise, the citation should be in parenthesis (as in \u201cDeep learning shows promise to make progress towards AI (Bengio & LeCun, 2007).\u201d).\"\n\nThere are a few minor issues with English usage and typos that should be cleaned up in the final manuscript.\n\nnecessary when training student models with more than 1 convolutional layers \u2192 necessary when training student models with more than 1 convolutional layer\n\nremaining 10,000 images as validation set \u2192 remaining 10,000 images as the validation set\n\nevaluate the ensemble\u2019s predictions (logits) on these samples, and save all data \u2192 evaluated the ensemble\u2019s predictions (logits) on these samples, and saved all data\n\nmore detail about hyperparamter optimization \u2192 more detail about hyperparameter optimization\n\nWe trained 129 deep CNN models with spearmint \u2192 We trained 129 deep CNN models with Spearmint\n\nThe best model obtained an accuracy of 92.78%, the fifth best achieved 92.67%. \u2192 The best model obtained an accuracy of 92.78%; the fifth best achieved 92.67%.\n\nthe sizes and architectures of three best models \u2192 the sizes and architectures of the three best models\n\nclearly suggests that convolutional is critical \u2192  clearly suggests that convolution is critical\n\nsimilarly from the hyperparameter-opimizer\u2019s point of view \u2192 similarly from the hyperparameter-optimizer\u2019s point of view\n\n"
  },
  {
    "people": [
      "C. Lee Giles",
      "Caruana",
      "Hinton",
      "Chen",
      "M.W. Goudreau",
      "C.L. Giles",
      "S.T. Chakradhar",
      "D. Chen",
      "Cristian Bucila",
      "Rich Caruana",
      "Alexandru Niculescu-Mizil",
      "O Vinyals",
      "J Dean",
      "W. Chen",
      "J. Wilson",
      "S. Tyree",
      "K. Weinberger",
      "Y. Chen"
    ],
    "review": "*** Paper Summary ***\n\nThe paper proposes to a new neural network architecture. The layer weights of a classical network are computed as a function of a latent representation associated with the layer. Two instances are presented (i) a CNN where each layer weight is computed from a lower dimensional layer embedding vector; (ii) an RNN where each layer weight is computed from a secondary RNN state.\n\n*** Review Summary ***\n\nPros: \n- I like the idea of bringing multiplicative RNNs and their predecessors back into the spotlight. \n- LM and MT results are excellent.\n\nCons:  \n- The paper could be better written. It is too long for the conference format and need refocussing. \n- On related work, the relation with multiplicative RNN and their generic tensor product predecessor (Order 2 networks, wrt C. Lee Giles definition) should be mentioned in the related work section and the differences with earlier research need to be explained and motivated (by the way it is better to say that something is revisiting an old idea or training it at modern scale/on modern tasks than ommitting it).\n- on focus, it is not clear if your goal is to achieve better performance or more compact networks. In the RNN section you lean toward the former, in the CNN section you seem to lean toward the latter.\n\nI would suggest to make the paper shorter and clearer possibly leaving the CNN results for latter publication. The relation with multiplicative/order 2 networks and eventual differences need to be explained.\n\n*** Detailed Review ***\n\nMultiplicative networks are an extremely powerfull architecture and bringing them back into the spotlight is excellent. This paper has excellent results but suffer poor presentation, lack of a clear focus. It spends time on details and ommit important points. In its current form, it is much too long to long and his not self contained without the appendices.\n\nSpending more time on multiplicative RNNs, order 2 networks at the begining of the paper would be excellent. This will let you highlight the difference between this paper and earlier work. It would also be necessary to spend a little time on why multiplicative RNN were less used than gated RNN: it seems that the optimization problem their training involve is tricker and it would be helpful to explain whether you had a harder time tweaking optimization parameters or whether you needed longer training sessions compared to LSTMs, regular CNN. On name, I am not sure that \"hypernetwork\" help the reader understand better what the proposed architecture compared to multiplicative interactions.\n\nIn section 3.2, you seem to imply that there are different settings of hypernetworks that allow to vary from an RNN to a CNN, this is not clear to me, maybe you could show how this would work on a simple temporal problem with equations. \n\nThe work on CNN and RNN are rather disconnected to me: for CNN, you seem to be interested in a low rank structure of the weights, showing that similar performance can be achieved with less weights. It is not clear to me why to pursue that goal. Do you expect speedups? less memory for embedded applications? In that case you should compare with alternative strategies, e.g. model compression (Caruana et al 2006, aka Dark Knowledge, Hinton et al 2014) or hashed networks (Chen et al 2015). \n\nFor RNN, you seem to target better perplexity/BLEU and model compactness is not a priority. Instead of making the weights have a simpler structure, you make them richer, i.e. dependent over time. It seems in that case models might be bigger and take longer to train. You might want to comment on training time, inference time, memory requirement in that case, as you highlight it might be an important goal in the CNN section. Overall, I am not sure it helps to have this mixed message. I would rather see the paper fit in the conference format with the RNN results alone and a clearer explanation and defers the publications of the CNN results when a proper comparison with memory concerned methods is performed.\n\nSome of the discussions are not clear to me, I am not sure what message the reader should get from Figure 2 or from the discussion on saturation statistics (p10, Figure 5). Similarly, I am not sure if Figure 4 is showing anything: everything should change more drastically at word boundaries even in a regular LSTM (states, gates units should look very different before/after a space); without such a comparison it is hard to see if this is unique to your network.\n\nThe results on handwriting generation are harder to compare for me. Log-loss are hard to understand, I have no sense whether the difference between models is significant (what would be the variance in this metric under boostrap sampling of the training set?). I am not sold either on qualitative metric were human can assess quality but human cannot evaluate if the network is repeating the training set. Did you thing at precision/recall metric for ink, possibly with some spatial tolerance ? (e.g. evaluation of segmentation tasks in vision).\n\nThe MT experiments are insufficiently discussed in the main text.\n\nOverall, I would suggest to make the paper shorter and clearer possibly leaving the CNN results for latter publication. You need to properly discuss the relation to multiplicative/order 2 networks and highlight the differences. Unclear discussion can be eliminated to make the experimental setup and the results presentation clearer in the main text.\n\n*** References ***\n\nM.W. Goudreau, C.L. Giles, S.T. Chakradhar, D. Chen, \"First-Order Vs. Second-Order Single Layer Recurrent Neural Networks,\"IEEE Trans. on Neural Networks, 5 (3), p. 511, 1994.\n\nCristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil, \"Model Compression,\" The Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2006), August 2006, pp. 535-541.\n\nDark knowledge, G Hinton, O Vinyals, J Dean 2014\n\nW. Chen, J. Wilson, S. Tyree, K. Weinberger and Y. Chen, Compressing Neural Networks with the Hashing Trick, Proc. International Conference on Machine Learning (ICML-15)"
  },
  {
    "people": [
      "C. Lee Giles",
      "Caruana",
      "Hinton",
      "Chen",
      "M.W. Goudreau",
      "C.L. Giles",
      "S.T. Chakradhar",
      "D. Chen",
      "Cristian Bucila",
      "Rich Caruana",
      "Alexandru Niculescu-Mizil",
      "O Vinyals",
      "J Dean",
      "W. Chen",
      "J. Wilson",
      "S. Tyree",
      "K. Weinberger",
      "Y. Chen"
    ],
    "review": "*** Paper Summary ***\n\nThe paper proposes to a new neural network architecture. The layer weights of a classical network are computed as a function of a latent representation associated with the layer. Two instances are presented (i) a CNN where each layer weight is computed from a lower dimensional layer embedding vector; (ii) an RNN where each layer weight is computed from a secondary RNN state.\n\n*** Review Summary ***\n\nPros: \n- I like the idea of bringing multiplicative RNNs and their predecessors back into the spotlight. \n- LM and MT results are excellent.\n\nCons:  \n- The paper could be better written. It is too long for the conference format and need refocussing. \n- On related work, the relation with multiplicative RNN and their generic tensor product predecessor (Order 2 networks, wrt C. Lee Giles definition) should be mentioned in the related work section and the differences with earlier research need to be explained and motivated (by the way it is better to say that something is revisiting an old idea or training it at modern scale/on modern tasks than ommitting it).\n- on focus, it is not clear if your goal is to achieve better performance or more compact networks. In the RNN section you lean toward the former, in the CNN section you seem to lean toward the latter.\n\nI would suggest to make the paper shorter and clearer possibly leaving the CNN results for latter publication. The relation with multiplicative/order 2 networks and eventual differences need to be explained.\n\n*** Detailed Review ***\n\nMultiplicative networks are an extremely powerfull architecture and bringing them back into the spotlight is excellent. This paper has excellent results but suffer poor presentation, lack of a clear focus. It spends time on details and ommit important points. In its current form, it is much too long to long and his not self contained without the appendices.\n\nSpending more time on multiplicative RNNs, order 2 networks at the begining of the paper would be excellent. This will let you highlight the difference between this paper and earlier work. It would also be necessary to spend a little time on why multiplicative RNN were less used than gated RNN: it seems that the optimization problem their training involve is tricker and it would be helpful to explain whether you had a harder time tweaking optimization parameters or whether you needed longer training sessions compared to LSTMs, regular CNN. On name, I am not sure that \"hypernetwork\" help the reader understand better what the proposed architecture compared to multiplicative interactions.\n\nIn section 3.2, you seem to imply that there are different settings of hypernetworks that allow to vary from an RNN to a CNN, this is not clear to me, maybe you could show how this would work on a simple temporal problem with equations. \n\nThe work on CNN and RNN are rather disconnected to me: for CNN, you seem to be interested in a low rank structure of the weights, showing that similar performance can be achieved with less weights. It is not clear to me why to pursue that goal. Do you expect speedups? less memory for embedded applications? In that case you should compare with alternative strategies, e.g. model compression (Caruana et al 2006, aka Dark Knowledge, Hinton et al 2014) or hashed networks (Chen et al 2015). \n\nFor RNN, you seem to target better perplexity/BLEU and model compactness is not a priority. Instead of making the weights have a simpler structure, you make them richer, i.e. dependent over time. It seems in that case models might be bigger and take longer to train. You might want to comment on training time, inference time, memory requirement in that case, as you highlight it might be an important goal in the CNN section. Overall, I am not sure it helps to have this mixed message. I would rather see the paper fit in the conference format with the RNN results alone and a clearer explanation and defers the publications of the CNN results when a proper comparison with memory concerned methods is performed.\n\nSome of the discussions are not clear to me, I am not sure what message the reader should get from Figure 2 or from the discussion on saturation statistics (p10, Figure 5). Similarly, I am not sure if Figure 4 is showing anything: everything should change more drastically at word boundaries even in a regular LSTM (states, gates units should look very different before/after a space); without such a comparison it is hard to see if this is unique to your network.\n\nThe results on handwriting generation are harder to compare for me. Log-loss are hard to understand, I have no sense whether the difference between models is significant (what would be the variance in this metric under boostrap sampling of the training set?). I am not sold either on qualitative metric were human can assess quality but human cannot evaluate if the network is repeating the training set. Did you thing at precision/recall metric for ink, possibly with some spatial tolerance ? (e.g. evaluation of segmentation tasks in vision).\n\nThe MT experiments are insufficiently discussed in the main text.\n\nOverall, I would suggest to make the paper shorter and clearer possibly leaving the CNN results for latter publication. You need to properly discuss the relation to multiplicative/order 2 networks and highlight the differences. Unclear discussion can be eliminated to make the experimental setup and the results presentation clearer in the main text.\n\n*** References ***\n\nM.W. Goudreau, C.L. Giles, S.T. Chakradhar, D. Chen, \"First-Order Vs. Second-Order Single Layer Recurrent Neural Networks,\"IEEE Trans. on Neural Networks, 5 (3), p. 511, 1994.\n\nCristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil, \"Model Compression,\" The Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2006), August 2006, pp. 535-541.\n\nDark knowledge, G Hinton, O Vinyals, J Dean 2014\n\nW. Chen, J. Wilson, S. Tyree, K. Weinberger and Y. Chen, Compressing Neural Networks with the Hashing Trick, Proc. International Conference on Machine Learning (ICML-15)"
  },
  {
    "people": [
      "C. Lee Giles",
      "Caruana",
      "Hinton",
      "Chen",
      "M.W. Goudreau",
      "C.L. Giles",
      "S.T. Chakradhar",
      "D. Chen",
      "Cristian Bucila",
      "Rich Caruana",
      "Alexandru Niculescu-Mizil",
      "O Vinyals",
      "J Dean",
      "W. Chen",
      "J. Wilson",
      "S. Tyree",
      "K. Weinberger",
      "Y. Chen"
    ],
    "review": "*** Paper Summary ***\n\nThe paper proposes to a new neural network architecture. The layer weights of a classical network are computed as a function of a latent representation associated with the layer. Two instances are presented (i) a CNN where each layer weight is computed from a lower dimensional layer embedding vector; (ii) an RNN where each layer weight is computed from a secondary RNN state.\n\n*** Review Summary ***\n\nPros: \n- I like the idea of bringing multiplicative RNNs and their predecessors back into the spotlight. \n- LM and MT results are excellent.\n\nCons:  \n- The paper could be better written. It is too long for the conference format and need refocussing. \n- On related work, the relation with multiplicative RNN and their generic tensor product predecessor (Order 2 networks, wrt C. Lee Giles definition) should be mentioned in the related work section and the differences with earlier research need to be explained and motivated (by the way it is better to say that something is revisiting an old idea or training it at modern scale/on modern tasks than ommitting it).\n- on focus, it is not clear if your goal is to achieve better performance or more compact networks. In the RNN section you lean toward the former, in the CNN section you seem to lean toward the latter.\n\nI would suggest to make the paper shorter and clearer possibly leaving the CNN results for latter publication. The relation with multiplicative/order 2 networks and eventual differences need to be explained.\n\n*** Detailed Review ***\n\nMultiplicative networks are an extremely powerfull architecture and bringing them back into the spotlight is excellent. This paper has excellent results but suffer poor presentation, lack of a clear focus. It spends time on details and ommit important points. In its current form, it is much too long to long and his not self contained without the appendices.\n\nSpending more time on multiplicative RNNs, order 2 networks at the begining of the paper would be excellent. This will let you highlight the difference between this paper and earlier work. It would also be necessary to spend a little time on why multiplicative RNN were less used than gated RNN: it seems that the optimization problem their training involve is tricker and it would be helpful to explain whether you had a harder time tweaking optimization parameters or whether you needed longer training sessions compared to LSTMs, regular CNN. On name, I am not sure that \"hypernetwork\" help the reader understand better what the proposed architecture compared to multiplicative interactions.\n\nIn section 3.2, you seem to imply that there are different settings of hypernetworks that allow to vary from an RNN to a CNN, this is not clear to me, maybe you could show how this would work on a simple temporal problem with equations. \n\nThe work on CNN and RNN are rather disconnected to me: for CNN, you seem to be interested in a low rank structure of the weights, showing that similar performance can be achieved with less weights. It is not clear to me why to pursue that goal. Do you expect speedups? less memory for embedded applications? In that case you should compare with alternative strategies, e.g. model compression (Caruana et al 2006, aka Dark Knowledge, Hinton et al 2014) or hashed networks (Chen et al 2015). \n\nFor RNN, you seem to target better perplexity/BLEU and model compactness is not a priority. Instead of making the weights have a simpler structure, you make them richer, i.e. dependent over time. It seems in that case models might be bigger and take longer to train. You might want to comment on training time, inference time, memory requirement in that case, as you highlight it might be an important goal in the CNN section. Overall, I am not sure it helps to have this mixed message. I would rather see the paper fit in the conference format with the RNN results alone and a clearer explanation and defers the publications of the CNN results when a proper comparison with memory concerned methods is performed.\n\nSome of the discussions are not clear to me, I am not sure what message the reader should get from Figure 2 or from the discussion on saturation statistics (p10, Figure 5). Similarly, I am not sure if Figure 4 is showing anything: everything should change more drastically at word boundaries even in a regular LSTM (states, gates units should look very different before/after a space); without such a comparison it is hard to see if this is unique to your network.\n\nThe results on handwriting generation are harder to compare for me. Log-loss are hard to understand, I have no sense whether the difference between models is significant (what would be the variance in this metric under boostrap sampling of the training set?). I am not sold either on qualitative metric were human can assess quality but human cannot evaluate if the network is repeating the training set. Did you thing at precision/recall metric for ink, possibly with some spatial tolerance ? (e.g. evaluation of segmentation tasks in vision).\n\nThe MT experiments are insufficiently discussed in the main text.\n\nOverall, I would suggest to make the paper shorter and clearer possibly leaving the CNN results for latter publication. You need to properly discuss the relation to multiplicative/order 2 networks and highlight the differences. Unclear discussion can be eliminated to make the experimental setup and the results presentation clearer in the main text.\n\n*** References ***\n\nM.W. Goudreau, C.L. Giles, S.T. Chakradhar, D. Chen, \"First-Order Vs. Second-Order Single Layer Recurrent Neural Networks,\"IEEE Trans. on Neural Networks, 5 (3), p. 511, 1994.\n\nCristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil, \"Model Compression,\" The Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2006), August 2006, pp. 535-541.\n\nDark knowledge, G Hinton, O Vinyals, J Dean 2014\n\nW. Chen, J. Wilson, S. Tyree, K. Weinberger and Y. Chen, Compressing Neural Networks with the Hashing Trick, Proc. International Conference on Machine Learning (ICML-15)"
  },
  {
    "people": [
      "C. Lee Giles",
      "Caruana",
      "Hinton",
      "Chen",
      "M.W. Goudreau",
      "C.L. Giles",
      "S.T. Chakradhar",
      "D. Chen",
      "Cristian Bucila",
      "Rich Caruana",
      "Alexandru Niculescu-Mizil",
      "O Vinyals",
      "J Dean",
      "W. Chen",
      "J. Wilson",
      "S. Tyree",
      "K. Weinberger",
      "Y. Chen"
    ],
    "review": "*** Paper Summary ***\n\nThe paper proposes to a new neural network architecture. The layer weights of a classical network are computed as a function of a latent representation associated with the layer. Two instances are presented (i) a CNN where each layer weight is computed from a lower dimensional layer embedding vector; (ii) an RNN where each layer weight is computed from a secondary RNN state.\n\n*** Review Summary ***\n\nPros: \n- I like the idea of bringing multiplicative RNNs and their predecessors back into the spotlight. \n- LM and MT results are excellent.\n\nCons:  \n- The paper could be better written. It is too long for the conference format and need refocussing. \n- On related work, the relation with multiplicative RNN and their generic tensor product predecessor (Order 2 networks, wrt C. Lee Giles definition) should be mentioned in the related work section and the differences with earlier research need to be explained and motivated (by the way it is better to say that something is revisiting an old idea or training it at modern scale/on modern tasks than ommitting it).\n- on focus, it is not clear if your goal is to achieve better performance or more compact networks. In the RNN section you lean toward the former, in the CNN section you seem to lean toward the latter.\n\nI would suggest to make the paper shorter and clearer possibly leaving the CNN results for latter publication. The relation with multiplicative/order 2 networks and eventual differences need to be explained.\n\n*** Detailed Review ***\n\nMultiplicative networks are an extremely powerfull architecture and bringing them back into the spotlight is excellent. This paper has excellent results but suffer poor presentation, lack of a clear focus. It spends time on details and ommit important points. In its current form, it is much too long to long and his not self contained without the appendices.\n\nSpending more time on multiplicative RNNs, order 2 networks at the begining of the paper would be excellent. This will let you highlight the difference between this paper and earlier work. It would also be necessary to spend a little time on why multiplicative RNN were less used than gated RNN: it seems that the optimization problem their training involve is tricker and it would be helpful to explain whether you had a harder time tweaking optimization parameters or whether you needed longer training sessions compared to LSTMs, regular CNN. On name, I am not sure that \"hypernetwork\" help the reader understand better what the proposed architecture compared to multiplicative interactions.\n\nIn section 3.2, you seem to imply that there are different settings of hypernetworks that allow to vary from an RNN to a CNN, this is not clear to me, maybe you could show how this would work on a simple temporal problem with equations. \n\nThe work on CNN and RNN are rather disconnected to me: for CNN, you seem to be interested in a low rank structure of the weights, showing that similar performance can be achieved with less weights. It is not clear to me why to pursue that goal. Do you expect speedups? less memory for embedded applications? In that case you should compare with alternative strategies, e.g. model compression (Caruana et al 2006, aka Dark Knowledge, Hinton et al 2014) or hashed networks (Chen et al 2015). \n\nFor RNN, you seem to target better perplexity/BLEU and model compactness is not a priority. Instead of making the weights have a simpler structure, you make them richer, i.e. dependent over time. It seems in that case models might be bigger and take longer to train. You might want to comment on training time, inference time, memory requirement in that case, as you highlight it might be an important goal in the CNN section. Overall, I am not sure it helps to have this mixed message. I would rather see the paper fit in the conference format with the RNN results alone and a clearer explanation and defers the publications of the CNN results when a proper comparison with memory concerned methods is performed.\n\nSome of the discussions are not clear to me, I am not sure what message the reader should get from Figure 2 or from the discussion on saturation statistics (p10, Figure 5). Similarly, I am not sure if Figure 4 is showing anything: everything should change more drastically at word boundaries even in a regular LSTM (states, gates units should look very different before/after a space); without such a comparison it is hard to see if this is unique to your network.\n\nThe results on handwriting generation are harder to compare for me. Log-loss are hard to understand, I have no sense whether the difference between models is significant (what would be the variance in this metric under boostrap sampling of the training set?). I am not sold either on qualitative metric were human can assess quality but human cannot evaluate if the network is repeating the training set. Did you thing at precision/recall metric for ink, possibly with some spatial tolerance ? (e.g. evaluation of segmentation tasks in vision).\n\nThe MT experiments are insufficiently discussed in the main text.\n\nOverall, I would suggest to make the paper shorter and clearer possibly leaving the CNN results for latter publication. You need to properly discuss the relation to multiplicative/order 2 networks and highlight the differences. Unclear discussion can be eliminated to make the experimental setup and the results presentation clearer in the main text.\n\n*** References ***\n\nM.W. Goudreau, C.L. Giles, S.T. Chakradhar, D. Chen, \"First-Order Vs. Second-Order Single Layer Recurrent Neural Networks,\"IEEE Trans. on Neural Networks, 5 (3), p. 511, 1994.\n\nCristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil, \"Model Compression,\" The Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2006), August 2006, pp. 535-541.\n\nDark knowledge, G Hinton, O Vinyals, J Dean 2014\n\nW. Chen, J. Wilson, S. Tyree, K. Weinberger and Y. Chen, Compressing Neural Networks with the Hashing Trick, Proc. International Conference on Machine Learning (ICML-15)"
  },
  {
    "people": [
      "Papernot"
    ],
    "review": "The paper presents a method for generating adversarial input images for a convolutional neural network given only black box access (ability to obtain outputs for chosen inputs, but no access to the network parameters).  However, the notion of adversarial example is somewhat weakened in this setting: it is k-misclassification (ensuring the true label is not a top-k output), instead of misclassification to any desired target label.\n\nA similar black-box setting is examined in Papernot et al. (2016c).  There, black-box access is used to train a substitute for the network, which is then attacked.  Here, black-box access in instead exploited via local search.  The input is perturbed, the resulting change in output scores is examined, and perturbations that push the scores towards k-misclassification are kept.\n\nA major concern with regard to novelty is that this greedy local search procedure is analogous to gradient descent; a numeric approximation (observe change in output for corresponding change in input) is used instead of backpropagation, since one does not have access to the network parameters.  As such, the greedy local search algorithm itself, to which the paper devotes a large amount of discussion, is not surprising and the paper is fairly incremental in terms of technical novelty.\n"
  },
  {
    "people": [
      "Papernot"
    ],
    "review": "The paper presents a method for generating adversarial input images for a convolutional neural network given only black box access (ability to obtain outputs for chosen inputs, but no access to the network parameters).  However, the notion of adversarial example is somewhat weakened in this setting: it is k-misclassification (ensuring the true label is not a top-k output), instead of misclassification to any desired target label.\n\nA similar black-box setting is examined in Papernot et al. (2016c).  There, black-box access is used to train a substitute for the network, which is then attacked.  Here, black-box access in instead exploited via local search.  The input is perturbed, the resulting change in output scores is examined, and perturbations that push the scores towards k-misclassification are kept.\n\nA major concern with regard to novelty is that this greedy local search procedure is analogous to gradient descent; a numeric approximation (observe change in output for corresponding change in input) is used instead of backpropagation, since one does not have access to the network parameters.  As such, the greedy local search algorithm itself, to which the paper devotes a large amount of discussion, is not surprising and the paper is fairly incremental in terms of technical novelty.\n"
  },
  {
    "people": [
      "Johnson"
    ],
    "review": "This paper is a by-the-numbers extension of the hidden semi-Markov model to include nonlinear observations, and neural network-based inference. The paper is fairly clear, although the English isn't great. The experiments are thorough.\n \n Where this paper really falls down is on originality. In particular, in the last two years there have been related works that aren't cited (and unfortunately weren't mentioned by the reviewers) that produce similar models. In particular, Johnson et al's 2016 NIPS paper develops almost the same inference strategy in almost the same model class. \n \n "
  },
  {
    "people": [
      "Ammar",
      "Dyer"
    ],
    "review": "This paper presents a novel model for unsupervised segmentation and classification of time series data.  A recurrent hidden semi-markov model is proposed.  This extends regular hidden semi-markov models to include a recurrent neural network (RNN) for observations.  Each latent class has its own RNN for modeling observations for that category.  Further, an efficient training procedure based on a variational approximation.  Experiments demonstrate the effectiveness of the approach for modeling synthetic and real time series data.\n\nThis is an interesting and novel paper.  The proposed method is a well-motivated combination of duration modeling HMMs with state of the art observation models based on RNNs.  The combination alleviates shortcomings of standard HSMM variants in terms of the simplicity of the emission probability.  The method is technically sound and demonstrated to be effective.\n\nIt would be interesting to see how this method compares quantitatively against CRF-based methods (e.g. Ammar, Dyer, and Smith NIPS 2014).  CRFs can model more complex data likelihoods, though as noted in the response phase there are still limitations.  Regardless, I think the merits of using RNNs for the class-specific generative models are clear.\n"
  },
  {
    "people": [
      "Johnson"
    ],
    "review": "This paper is a by-the-numbers extension of the hidden semi-Markov model to include nonlinear observations, and neural network-based inference. The paper is fairly clear, although the English isn't great. The experiments are thorough.\n \n Where this paper really falls down is on originality. In particular, in the last two years there have been related works that aren't cited (and unfortunately weren't mentioned by the reviewers) that produce similar models. In particular, Johnson et al's 2016 NIPS paper develops almost the same inference strategy in almost the same model class. \n \n "
  },
  {
    "people": [
      "Ammar",
      "Dyer"
    ],
    "review": "This paper presents a novel model for unsupervised segmentation and classification of time series data.  A recurrent hidden semi-markov model is proposed.  This extends regular hidden semi-markov models to include a recurrent neural network (RNN) for observations.  Each latent class has its own RNN for modeling observations for that category.  Further, an efficient training procedure based on a variational approximation.  Experiments demonstrate the effectiveness of the approach for modeling synthetic and real time series data.\n\nThis is an interesting and novel paper.  The proposed method is a well-motivated combination of duration modeling HMMs with state of the art observation models based on RNNs.  The combination alleviates shortcomings of standard HSMM variants in terms of the simplicity of the emission probability.  The method is technically sound and demonstrated to be effective.\n\nIt would be interesting to see how this method compares quantitatively against CRF-based methods (e.g. Ammar, Dyer, and Smith NIPS 2014).  CRFs can model more complex data likelihoods, though as noted in the response phase there are still limitations.  Regardless, I think the merits of using RNNs for the class-specific generative models are clear.\n"
  },
  {
    "people": [
      "Mnih",
      "Mnih",
      "Mnih",
      "Mnih",
      "Mnih",
      "Mnih",
      "Mnih",
      "Mnih"
    ],
    "review": "We first thank all the reviewers for the interesting comments they provided. All three reviewers agree on the potential importance of the proposed algorithm, but they also suggest that more experimental data are needed to demonstrate that GA3C achieves results in the same ballpark as the original A3C. This turns out to be particularly important in the light of the fact that the policy lag in GA3C can potentially introduce instabilities, as noted by Reviewer #1.\n\nWe have included a better description of this in Section 4.4 of the revised paper, as well as analyzed it in a newly added Section 5.3, while providing additional experiments to verify the game performance of GA3C and its stability. As pointed out by the reviewers and already highlighted in the paper, establishing a validation protocol that allows comparing the final scores achieved by GA3C with the scores achieved by other training algorithms is complex or even infeasible, as many implementation details are not shared, as well as validation protocols are poorly reproducible. Beyond that, the final score (still reported in Table 3) tells only part of the story, and therefore we espouse the idea shared by Reviewer #1 and #3 of reporting learning curves as a function of the time. The newly added Fig. 6 and 7 show the learning curves for GA3C and several games as a function of the time. The reader can easily compare these graphs with those reported in Mnih\u2019s paper, as well as in other venues mentioned by the reviewers (e.g. github). The score vs. time curves show a similar dynamic for GA3C and A3C, although it is also important mentioning that curves reported in Mnih\u2019s paper represent the average of the top 5 curves in a set of 50 with various learning rates, which we are unable to match due limitations in available computational resources. Since systems, system loads, environments and hyper-parameters are so widely varying, we also believe that releasing the GA3C code (which we plan to do in the next days) will allow other researchers to systematically evaluate the performances of GA3C and facilitate a fair comparison with new techniques.\n\nReviewer #1\u2019s main concern is about instabilities introduced in GA3C. The training curves reported in the paper indeed show a few instabilities occurring in the training process, but these represent a common phenomenon in RL. Beyond analyzing in Section 4.4 how the policy lag in GA3C can generate these instabilities, we now show experimentally that such instabilities do not affect some games (e.g. Pong), while they can invalidate the learning process in other ones (e.g. Breakout), especially in the case of a large learning rate (Fig. 7 and 8). We found that properly tuning the algorithm hyper-parameters (and in particular the learning rate) dramatically limits the effect of instabilities on the learning process. GA3C would likely benefit from a thorough learning rate search as presented in Mnih\u2019s paper over a set of 50 different learning rates for A3C. Beyond this, we also show how batching on training data (see the newly introduced Section 5.3, and Fig. 7 and 8 in particular) can be used to limit the effect of these instabilities, at the price of a slower convergence rate.\n\nSpecific answers to the individual reviewers follow.\n\n===============================\nReviewer 1\n===============================\n\nWe agree with the reviewer that instabilities may affect the learning process. This is indeed a commonly observed phenomenon in RL. Given the fact that GA3C and A3C are indeed (slightly) different algorithms (as now clearly explained in Section 4.4 of the paper), we followed the reviewer\u2019s suggestion and performed a thorough evaluation of the instabilities of the learning process on several games. Experimental results are reported in Section 5.3 and in Fig. 7 and 8 in particular. These show that, for some games (e.g. Pong), the delayed computation of the gradient terms in GA3C (see Section 4.4) does not introduce convergence issues, that are however present in other games (e.g. Breakout). However, a proper selection of the algorithm hyper-parameters, and of the learning rate in particular, is sufficient to achieve results that are comparable to those achieved by A3C (see Table 3). Quite interestingly, the optimal learning rate for GA3C results to be smaller than the one reported in Mnih\u2019s paper for A3C (as it can be seen in Fig. 6) - this somehow confirms that A3C and GA3C are indeed slightly different algorithms, and GA3C requires careful parameter tuning to obtain optimal results. We also demonstrated that batching of the training data can be used to limit the instabilities, at the price of a slower convergence rate.\n\n===============================\nReviewer 2\n===============================\n\nFollowing the suggestion of the other two reviewers, and sharing the conviction that replicating the Deep Mind evaluation protocol is non-trivial, we now report in the paper more training curves for several games (Fig. 6) as a function of the training time. This allows the reader to perform a direct comparison between our training curves and the ones reported in Mnih\u2019s paper, independently from the hardware system used for training. Although, the reader should be careful about this comparison since Mnih\u2019s graphs show top 5 out of 50 runs which is hard to compare with the small number of runs considered in Fig. 6. These curves demonstrate that training time is comparable while achieving a similar training score. The analysis of the theoretical discrepancies between A3C and GA3C (Section 4.4) and its experimental evaluation in terms of instabilities (Section 5.3) should also serve to clarify that, once the hyper-parameters are properly set, GA3C is comparable to A3C in terms of performance.\nAs for the question about the protocol used for evaluation the score of our trained agents, we now better specify in the caption of table 3 and in the text that we are reporting the average score over 30 different runs for the best agent we trained with GA3C. A direct comparison using the protocol followed in Mnih\u2019s paper is not feasible, as the authors did not share the \u201chuman start condition\u201d for each game.\n\n===============================\nReviewer 3\n===============================\n\nWe agree with the reviewer that showing score vs. time plot would be more informative than reporting only the final score as in Table 3. Therefore, we added Fig. 6 to the paper, showing that, with the proper selection of the hyper-parameters, the dynamic of the learning curves of GA3C is similar to that shown in the original A3C paper by Mnih et al. We believe that other, more precise comparisons in terms of learning speed require an optimized implementation as well as a broad hyper-parameter search (in the style of the original A3C plot, showing top 5 results among a set of 50 learners trained with different learning rates), which is out of the scope for this paper. We also decided to keep Table 3 in the paper - although, as explicitly mentioned in the paper, this information is approximate, it gives an idea that GA3C and A3C achieves final score in the same ballpark.\nWe also suggest that the contribution of our paper is not limited to the implementation of GA3C, but it also includes the analysis of the computational aspects of GA3C and other RL algorithms that can be implemented through a similar architecture. This contribution stands independently from the potential stability issues for GA3C, while the proposed architecture (and the freely available code) can be modified or extended to address these issues, improve the convergence speed, or attack real world problems."
  },
  {
    "people": [
      "Mnih",
      "Mnih",
      "Mnih",
      "Mnih",
      "Mnih",
      "Mnih",
      "Mnih",
      "Mnih"
    ],
    "review": "We first thank all the reviewers for the interesting comments they provided. All three reviewers agree on the potential importance of the proposed algorithm, but they also suggest that more experimental data are needed to demonstrate that GA3C achieves results in the same ballpark as the original A3C. This turns out to be particularly important in the light of the fact that the policy lag in GA3C can potentially introduce instabilities, as noted by Reviewer #1.\n\nWe have included a better description of this in Section 4.4 of the revised paper, as well as analyzed it in a newly added Section 5.3, while providing additional experiments to verify the game performance of GA3C and its stability. As pointed out by the reviewers and already highlighted in the paper, establishing a validation protocol that allows comparing the final scores achieved by GA3C with the scores achieved by other training algorithms is complex or even infeasible, as many implementation details are not shared, as well as validation protocols are poorly reproducible. Beyond that, the final score (still reported in Table 3) tells only part of the story, and therefore we espouse the idea shared by Reviewer #1 and #3 of reporting learning curves as a function of the time. The newly added Fig. 6 and 7 show the learning curves for GA3C and several games as a function of the time. The reader can easily compare these graphs with those reported in Mnih\u2019s paper, as well as in other venues mentioned by the reviewers (e.g. github). The score vs. time curves show a similar dynamic for GA3C and A3C, although it is also important mentioning that curves reported in Mnih\u2019s paper represent the average of the top 5 curves in a set of 50 with various learning rates, which we are unable to match due limitations in available computational resources. Since systems, system loads, environments and hyper-parameters are so widely varying, we also believe that releasing the GA3C code (which we plan to do in the next days) will allow other researchers to systematically evaluate the performances of GA3C and facilitate a fair comparison with new techniques.\n\nReviewer #1\u2019s main concern is about instabilities introduced in GA3C. The training curves reported in the paper indeed show a few instabilities occurring in the training process, but these represent a common phenomenon in RL. Beyond analyzing in Section 4.4 how the policy lag in GA3C can generate these instabilities, we now show experimentally that such instabilities do not affect some games (e.g. Pong), while they can invalidate the learning process in other ones (e.g. Breakout), especially in the case of a large learning rate (Fig. 7 and 8). We found that properly tuning the algorithm hyper-parameters (and in particular the learning rate) dramatically limits the effect of instabilities on the learning process. GA3C would likely benefit from a thorough learning rate search as presented in Mnih\u2019s paper over a set of 50 different learning rates for A3C. Beyond this, we also show how batching on training data (see the newly introduced Section 5.3, and Fig. 7 and 8 in particular) can be used to limit the effect of these instabilities, at the price of a slower convergence rate.\n\nSpecific answers to the individual reviewers follow.\n\n===============================\nReviewer 1\n===============================\n\nWe agree with the reviewer that instabilities may affect the learning process. This is indeed a commonly observed phenomenon in RL. Given the fact that GA3C and A3C are indeed (slightly) different algorithms (as now clearly explained in Section 4.4 of the paper), we followed the reviewer\u2019s suggestion and performed a thorough evaluation of the instabilities of the learning process on several games. Experimental results are reported in Section 5.3 and in Fig. 7 and 8 in particular. These show that, for some games (e.g. Pong), the delayed computation of the gradient terms in GA3C (see Section 4.4) does not introduce convergence issues, that are however present in other games (e.g. Breakout). However, a proper selection of the algorithm hyper-parameters, and of the learning rate in particular, is sufficient to achieve results that are comparable to those achieved by A3C (see Table 3). Quite interestingly, the optimal learning rate for GA3C results to be smaller than the one reported in Mnih\u2019s paper for A3C (as it can be seen in Fig. 6) - this somehow confirms that A3C and GA3C are indeed slightly different algorithms, and GA3C requires careful parameter tuning to obtain optimal results. We also demonstrated that batching of the training data can be used to limit the instabilities, at the price of a slower convergence rate.\n\n===============================\nReviewer 2\n===============================\n\nFollowing the suggestion of the other two reviewers, and sharing the conviction that replicating the Deep Mind evaluation protocol is non-trivial, we now report in the paper more training curves for several games (Fig. 6) as a function of the training time. This allows the reader to perform a direct comparison between our training curves and the ones reported in Mnih\u2019s paper, independently from the hardware system used for training. Although, the reader should be careful about this comparison since Mnih\u2019s graphs show top 5 out of 50 runs which is hard to compare with the small number of runs considered in Fig. 6. These curves demonstrate that training time is comparable while achieving a similar training score. The analysis of the theoretical discrepancies between A3C and GA3C (Section 4.4) and its experimental evaluation in terms of instabilities (Section 5.3) should also serve to clarify that, once the hyper-parameters are properly set, GA3C is comparable to A3C in terms of performance.\nAs for the question about the protocol used for evaluation the score of our trained agents, we now better specify in the caption of table 3 and in the text that we are reporting the average score over 30 different runs for the best agent we trained with GA3C. A direct comparison using the protocol followed in Mnih\u2019s paper is not feasible, as the authors did not share the \u201chuman start condition\u201d for each game.\n\n===============================\nReviewer 3\n===============================\n\nWe agree with the reviewer that showing score vs. time plot would be more informative than reporting only the final score as in Table 3. Therefore, we added Fig. 6 to the paper, showing that, with the proper selection of the hyper-parameters, the dynamic of the learning curves of GA3C is similar to that shown in the original A3C paper by Mnih et al. We believe that other, more precise comparisons in terms of learning speed require an optimized implementation as well as a broad hyper-parameter search (in the style of the original A3C plot, showing top 5 results among a set of 50 learners trained with different learning rates), which is out of the scope for this paper. We also decided to keep Table 3 in the paper - although, as explicitly mentioned in the paper, this information is approximate, it gives an idea that GA3C and A3C achieves final score in the same ballpark.\nWe also suggest that the contribution of our paper is not limited to the implementation of GA3C, but it also includes the analysis of the computational aspects of GA3C and other RL algorithms that can be implemented through a similar architecture. This contribution stands independently from the potential stability issues for GA3C, while the proposed architecture (and the freely available code) can be modified or extended to address these issues, improve the convergence speed, or attack real world problems."
  },
  {
    "people": [
      "Tabacof"
    ],
    "review": "After the rebuttal:\n\nThe paper contains an interesting set of results (mainly produced after the initial submission), but novelty is limited, and presentation is suboptimal. \n\nFor me now the biggest problem now is that the title and the content do not correspond. The authors clearly attack deterministic encoder-decoder models (as described in 3.2), which are not at all the same as generative models, even though many generative models make use of this architecture. A small experiment with sampling is interesting, but does not change the overall focus of the paper. This inconsistency in not acceptable. The whole issue could be resolved for example by simply replacing \"generative models\" by \"encoder-decoder networks\" in the title. Then I would tend towards recommending acceptance.\n\n------\nInitial review:\n\nThe paper describes three approaches to generating adversarial examples for deep encoder-decoder generative networks (trained as VAE or VAE-GAN), and shows a comparative analysis of these. While the phenomenon of adversarial examples in discriminative models is widely known and relatively well studied, I am not aware of previous work on adversarial examples for generative networks, so this work is novel (there is a concurrent work by Tabacof et al. which should be cited, though).  The paper has significantly improved since the initial submission; still, I have a number of remarks on presentation and experimental evaluation. I am in the borderline mode, and may change my rating during the discussion phase.\n\nDetailed comments:\n\n1) The paper is 13 pages long - significantly over the recommended page limit of 8 pages. Reviewers have to read multiple papers, multiple versions of each, it is a lot of work. Large portions of the paper should be shortened and/or moved to the appendix. It is job of the authors to make the paper concise and readable. \"in our attempts to be thorough, we have had a hard time keeping the length down\" is a bad excuse - it may be hard, but has to be done.\n\n2) I intentionally avoided term \"generative model\" above because it is not obvious to me if the attacks described by the authors indeed attack generative models. To clarify, the authors train encoder-decoders as generative models (VAE or VAE-GAN), but then remove all stochasticity (sampling) and prior on the latent variables: that is, they treat the models as deterministic encoders-decoders. It is not a big surprise that a deterministic deep network can be easily tricked; it would be much more interesting to see if the probabilistic aspect of generative models makes them more robust to such attacks. Am I missing something? I would like the authors to clarify their view on this and adjust the claims in the paper if necessary.\n\n3) The paper is motivated by possible attacks on a data channel which uses a generative network for compressing information. Description of the attack scenario in 3.1 does not look convincing to me. It takes a huge amount of space and I do not think it adds much to the paper. First, experiments on natural images are necessary to judge if the proposed attack could succeed in a realistic scenario and second, I am not aware of any existing practical applications of VAEs to image compression: attacking JPEG would be much more practical. \n\n4) Experiments are limited to MNIST and, in the latest version, SVHN (which is very nice). While no good generative models of general natural images exist, it is common to evaluate generative models on datasets of faces, so this would be another very natural domain for testing the proposed approach. \n\nSmaller remarks:\n1) Usage of \"Oracle\" in 3.2 does not look appropriate - oracle typically has access to (part of) ground truth, which is not the case here as far as I understand.\n2) Beginning of section 4: \"All three methods work for any generative architecture that relies on a learned latent representation z\" - \"are technically applicable to\" would be more correct than \"work for\". \n3) 4.1: \"confidentally\""
  },
  {
    "people": [
      "Szegedy"
    ],
    "review": "Comments: \n\n\"This contrasts to adversarial attacks on classifiers, where any inspection of the inputs will reveal the original bytes the adversary supplied,\nwhich often have telltale noise\"\n\nIs this really true?  If it were the case, wouldn't it imply that training \"against\" adversarial examples should easily make a classifier robust to adversarial examples (if they all have a telltale noise)?  \n\nPros: \n  -The question of whether adversarial examples exist in generative models, and indeed how the definition of \"adversarial example\" carries over is an interesting one.  \n  -Finding that a certain type of generative model *doesn't have* adversarial examples would be a really significant result, finding that generative models have adversarial examples would also be a worth negative result.  \n  -The adversarial examples in figures 5 and 6 seem convincing, though they seem much more overt and noisy than the adversarial examples on MNIST shown in (Szegedy 2014).  Is this because it's actually harder to find adversarial examples in these types of generative models?  \n\nIssues: \n  -Paper is significantly over length at 13 pages.  \n  -The beginning of the paper should more clearly motivate its purpose.  \n  -Paper has \"generative models\" in the title but as far as I can tell the whole paper is concerned with autoencoder-type models.  This is kind of annoying because if someone wanted to consider adversarial attacks on, say, autoregressive models, they might be unreasonably burdened by having to explain how they're distinct from a paper called \"adversarial examples for generative models\".  \n   -I think that the introduction contains too much background information - it could be tightened.  \n"
  },
  {
    "people": [
      "Tabacof"
    ],
    "review": "\nAfter the rebuttal:\n\nThe paper contains an interesting set of results (mainly produced after the initial submission), but novelty is limited, and presentation is suboptimal. \n\nFor me now the biggest problem now is that the title and the content do not correspond. The authors clearly attack deterministic encoder-decoder models (as described in 3.2), which are not at all the same as generative models, even though many generative models make use of this architecture. A small experiment with sampling is interesting, but does not change the overall focus of the paper. This inconsistency in not acceptable. The whole issue could be resolved for example by simply replacing \"generative models\" by \"encoder-decoder networks\" in the title. Then I would tend towards recommending acceptance.\n\n------\nInitial review:\n\nThe paper describes three approaches to generating adversarial examples for deep encoder-decoder generative networks (trained as VAE or VAE-GAN), and shows a comparative analysis of these. While the phenomenon of adversarial examples in discriminative models is widely known and relatively well studied, I am not aware of previous work on adversarial examples for generative networks, so this work is novel (there is a concurrent work by Tabacof et al. which should be cited, though).  The paper has significantly improved since the initial submission; still, I have a number of remarks on presentation and experimental evaluation. I am in the borderline mode, and may change my rating during the discussion phase.\n\nDetailed comments:\n\n1) The paper is 13 pages long - significantly over the recommended page limit of 8 pages. Reviewers have to read multiple papers, multiple versions of each, it is a lot of work. Large portions of the paper should be shortened and/or moved to the appendix. It is job of the authors to make the paper concise and readable. \"in our attempts to be thorough, we have had a hard time keeping the length down\" is a bad excuse - it may be hard, but has to be done.\n\n2) I intentionally avoided term \"generative model\" above because it is not obvious to me if the attacks described by the authors indeed attack generative models. To clarify, the authors train encoder-decoders as generative models (VAE or VAE-GAN), but then remove all stochasticity (sampling) and prior on the latent variables: that is, they treat the models as deterministic encoders-decoders. It is not a big surprise that a deterministic deep network can be easily tricked; it would be much more interesting to see if the probabilistic aspect of generative models makes them more robust to such attacks. Am I missing something? I would like the authors to clarify their view on this and adjust the claims in the paper if necessary.\n\n3) The paper is motivated by possible attacks on a data channel which uses a generative network for compressing information. Description of the attack scenario in 3.1 does not look convincing to me. It takes a huge amount of space and I do not think it adds much to the paper. First, experiments on natural images are necessary to judge if the proposed attack could succeed in a realistic scenario and second, I am not aware of any existing practical applications of VAEs to image compression: attacking JPEG would be much more practical. \n\n4) Experiments are limited to MNIST and, in the latest version, SVHN (which is very nice). While no good generative models of general natural images exist, it is common to evaluate generative models on datasets of faces, so this would be another very natural domain for testing the proposed approach. \n\nSmaller remarks:\n1) Usage of \"Oracle\" in 3.2 does not look appropriate - oracle typically has access to (part of) ground truth, which is not the case here as far as I understand.\n2) Beginning of section 4: \"All three methods work for any generative architecture that relies on a learned latent representation z\" - \"are technically applicable to\" would be more correct than \"work for\". \n3) 4.1: \"confidentally\"\n"
  },
  {
    "people": [
      "Tabacof"
    ],
    "review": "After the rebuttal:\n\nThe paper contains an interesting set of results (mainly produced after the initial submission), but novelty is limited, and presentation is suboptimal. \n\nFor me now the biggest problem now is that the title and the content do not correspond. The authors clearly attack deterministic encoder-decoder models (as described in 3.2), which are not at all the same as generative models, even though many generative models make use of this architecture. A small experiment with sampling is interesting, but does not change the overall focus of the paper. This inconsistency in not acceptable. The whole issue could be resolved for example by simply replacing \"generative models\" by \"encoder-decoder networks\" in the title. Then I would tend towards recommending acceptance.\n\n------\nInitial review:\n\nThe paper describes three approaches to generating adversarial examples for deep encoder-decoder generative networks (trained as VAE or VAE-GAN), and shows a comparative analysis of these. While the phenomenon of adversarial examples in discriminative models is widely known and relatively well studied, I am not aware of previous work on adversarial examples for generative networks, so this work is novel (there is a concurrent work by Tabacof et al. which should be cited, though).  The paper has significantly improved since the initial submission; still, I have a number of remarks on presentation and experimental evaluation. I am in the borderline mode, and may change my rating during the discussion phase.\n\nDetailed comments:\n\n1) The paper is 13 pages long - significantly over the recommended page limit of 8 pages. Reviewers have to read multiple papers, multiple versions of each, it is a lot of work. Large portions of the paper should be shortened and/or moved to the appendix. It is job of the authors to make the paper concise and readable. \"in our attempts to be thorough, we have had a hard time keeping the length down\" is a bad excuse - it may be hard, but has to be done.\n\n2) I intentionally avoided term \"generative model\" above because it is not obvious to me if the attacks described by the authors indeed attack generative models. To clarify, the authors train encoder-decoders as generative models (VAE or VAE-GAN), but then remove all stochasticity (sampling) and prior on the latent variables: that is, they treat the models as deterministic encoders-decoders. It is not a big surprise that a deterministic deep network can be easily tricked; it would be much more interesting to see if the probabilistic aspect of generative models makes them more robust to such attacks. Am I missing something? I would like the authors to clarify their view on this and adjust the claims in the paper if necessary.\n\n3) The paper is motivated by possible attacks on a data channel which uses a generative network for compressing information. Description of the attack scenario in 3.1 does not look convincing to me. It takes a huge amount of space and I do not think it adds much to the paper. First, experiments on natural images are necessary to judge if the proposed attack could succeed in a realistic scenario and second, I am not aware of any existing practical applications of VAEs to image compression: attacking JPEG would be much more practical. \n\n4) Experiments are limited to MNIST and, in the latest version, SVHN (which is very nice). While no good generative models of general natural images exist, it is common to evaluate generative models on datasets of faces, so this would be another very natural domain for testing the proposed approach. \n\nSmaller remarks:\n1) Usage of \"Oracle\" in 3.2 does not look appropriate - oracle typically has access to (part of) ground truth, which is not the case here as far as I understand.\n2) Beginning of section 4: \"All three methods work for any generative architecture that relies on a learned latent representation z\" - \"are technically applicable to\" would be more correct than \"work for\". \n3) 4.1: \"confidentally\""
  },
  {
    "people": [
      "Szegedy"
    ],
    "review": "Comments: \n\n\"This contrasts to adversarial attacks on classifiers, where any inspection of the inputs will reveal the original bytes the adversary supplied,\nwhich often have telltale noise\"\n\nIs this really true?  If it were the case, wouldn't it imply that training \"against\" adversarial examples should easily make a classifier robust to adversarial examples (if they all have a telltale noise)?  \n\nPros: \n  -The question of whether adversarial examples exist in generative models, and indeed how the definition of \"adversarial example\" carries over is an interesting one.  \n  -Finding that a certain type of generative model *doesn't have* adversarial examples would be a really significant result, finding that generative models have adversarial examples would also be a worth negative result.  \n  -The adversarial examples in figures 5 and 6 seem convincing, though they seem much more overt and noisy than the adversarial examples on MNIST shown in (Szegedy 2014).  Is this because it's actually harder to find adversarial examples in these types of generative models?  \n\nIssues: \n  -Paper is significantly over length at 13 pages.  \n  -The beginning of the paper should more clearly motivate its purpose.  \n  -Paper has \"generative models\" in the title but as far as I can tell the whole paper is concerned with autoencoder-type models.  This is kind of annoying because if someone wanted to consider adversarial attacks on, say, autoregressive models, they might be unreasonably burdened by having to explain how they're distinct from a paper called \"adversarial examples for generative models\".  \n   -I think that the introduction contains too much background information - it could be tightened.  \n"
  },
  {
    "people": [
      "Tabacof"
    ],
    "review": "\nAfter the rebuttal:\n\nThe paper contains an interesting set of results (mainly produced after the initial submission), but novelty is limited, and presentation is suboptimal. \n\nFor me now the biggest problem now is that the title and the content do not correspond. The authors clearly attack deterministic encoder-decoder models (as described in 3.2), which are not at all the same as generative models, even though many generative models make use of this architecture. A small experiment with sampling is interesting, but does not change the overall focus of the paper. This inconsistency in not acceptable. The whole issue could be resolved for example by simply replacing \"generative models\" by \"encoder-decoder networks\" in the title. Then I would tend towards recommending acceptance.\n\n------\nInitial review:\n\nThe paper describes three approaches to generating adversarial examples for deep encoder-decoder generative networks (trained as VAE or VAE-GAN), and shows a comparative analysis of these. While the phenomenon of adversarial examples in discriminative models is widely known and relatively well studied, I am not aware of previous work on adversarial examples for generative networks, so this work is novel (there is a concurrent work by Tabacof et al. which should be cited, though).  The paper has significantly improved since the initial submission; still, I have a number of remarks on presentation and experimental evaluation. I am in the borderline mode, and may change my rating during the discussion phase.\n\nDetailed comments:\n\n1) The paper is 13 pages long - significantly over the recommended page limit of 8 pages. Reviewers have to read multiple papers, multiple versions of each, it is a lot of work. Large portions of the paper should be shortened and/or moved to the appendix. It is job of the authors to make the paper concise and readable. \"in our attempts to be thorough, we have had a hard time keeping the length down\" is a bad excuse - it may be hard, but has to be done.\n\n2) I intentionally avoided term \"generative model\" above because it is not obvious to me if the attacks described by the authors indeed attack generative models. To clarify, the authors train encoder-decoders as generative models (VAE or VAE-GAN), but then remove all stochasticity (sampling) and prior on the latent variables: that is, they treat the models as deterministic encoders-decoders. It is not a big surprise that a deterministic deep network can be easily tricked; it would be much more interesting to see if the probabilistic aspect of generative models makes them more robust to such attacks. Am I missing something? I would like the authors to clarify their view on this and adjust the claims in the paper if necessary.\n\n3) The paper is motivated by possible attacks on a data channel which uses a generative network for compressing information. Description of the attack scenario in 3.1 does not look convincing to me. It takes a huge amount of space and I do not think it adds much to the paper. First, experiments on natural images are necessary to judge if the proposed attack could succeed in a realistic scenario and second, I am not aware of any existing practical applications of VAEs to image compression: attacking JPEG would be much more practical. \n\n4) Experiments are limited to MNIST and, in the latest version, SVHN (which is very nice). While no good generative models of general natural images exist, it is common to evaluate generative models on datasets of faces, so this would be another very natural domain for testing the proposed approach. \n\nSmaller remarks:\n1) Usage of \"Oracle\" in 3.2 does not look appropriate - oracle typically has access to (part of) ground truth, which is not the case here as far as I understand.\n2) Beginning of section 4: \"All three methods work for any generative architecture that relies on a learned latent representation z\" - \"are technically applicable to\" would be more correct than \"work for\". \n3) 4.1: \"confidentally\"\n"
  },
  {
    "people": [
      "Sergey"
    ],
    "review": "Hi, Sergey.\n\nI am confused about the definition of activation tensor in section 3.1.\n\nIs it obtained before or after ReLU activation function?\n\nIf it's got after ReLu, there is no need adding absolute function.\n\nThanks!"
  },
  {
    "people": [
      "Sergey"
    ],
    "review": "Hi, Sergey.\n\nI am confused about the definition of activation tensor in section 3.1.\n\nIs it obtained before or after ReLU activation function?\n\nIf it's got after ReLu, there is no need adding absolute function.\n\nThanks!"
  },
  {
    "people": [
      "Jaini",
      "Jaini",
      "Jaini",
      "Jaini"
    ],
    "review": "The authors contribute an algorithm for building sum-product networks (SPNs) from data, assuming a Gaussian distribution for all dimensions of the observed data.  Due to the restricted structure of the SPN architecture, building a valid architecture that is tailored to a specific dataset is not an obvious exercise, and so structure-learning algorithms are employed.  For Gaussian distributed observations, the authors state that the previous state of the art is to chose a random SPN that satisfies the completeness and decomposibility constraints that SPNs must observe, and to then learn the parameters (as done in Jaini 2016).  In the contributed manuscript, the algorithm begins with a completely factorized model, and then by passing through the data, builds up more structure, while updating appropriate node statistics to maintain the validity of the SPN.\n\nThe above Jaini reference figures heavily into the reading of the paper because it is (to my limited knowledge) the previous work SOTA on SPNs applied to Gaussian distributed data, and also because the authors of the current manuscript compare performance to datasets studied in Jaini et al.  I personally was unfamiliar with most of these datasets, and so have no basis to judge loglikelihoods, given a particular model, as being either good or poor.  Nevertheless, the current manuscript reports results on these datasets that better (5 / 7) than other methods, such as SPNS (constructed randomly), Stacked Restricted Boltzmann Machines or Generative Moment Matching networks.\n\nOverall: \nFirst let me say, I am not really qualified to make a decision on the acceptance or rejection of this manuscript (yet I am forced to make just such a choice) because I am not an expert in SPNs. I was also unfamiliar with the datasets, so I had no intuitive understanding of the algorithms performance, even when viewed as a black-box.  The algorithm is presented without theoretical inspiration or justification.  These latter are by no means a bad thing, but it again gives me little hold onto when evaluating the manuscript.  The manuscript is clearly written, and to my limited knowledge novel, and their algorithm does a good job (5/7) on selected datasets.  \n\nMy overall impression is that there isn't very much work here (e.g., much of the text is similar to Jaini, and most of the other experiments are repeated verbatim from Jaini), but again I may be missing something (this manuscript DOES mostly Jaini). I say this mostly because I am unfamiliar with the datasets.  Hopefully my reviewing peers will have enough background to know if the results are impressive or not, and my review should be weighted minimally.\n\nSmallish Problems\nI wanted to see nonuniform covariances in the data of the the toy task (Fig 3) for each gaussian component.\n\nThe SPN construction method has two obvious hyper parameters, it is important to see how those parameters affect the graph structure. (I submitted this as a pre-review question, to which the authors responded that they would look into this.)"
  },
  {
    "people": [
      "Wu"
    ],
    "review": "The authors present an online learning method for learning the structure of sum-product networks. The algorithm assumes Gaussian coordinate-wise marginal distributions, and learns both parameters and structure online. The parameters are updated by a recursive procedure that reweights nodes in the network that most contribute to the likelihood of the current data point. The structure learning is done by either merging independent product Gaussian nodes into multivariate leaf nodes, or creating a mixture over the two nodes when the multivariate would be too large.\n\nThe fact that the dataset is scaled to some larger datasets (in terms of the number of datapoints) is promising, although the number of variables is still quite small. Current benchmarks for tractable continuous density modeling with neural networks include the NICE and Real-NVP families of models, which can be scaled to both large numbers of datapoints and variables. Intractable methods like GAN, GenMMN, VAE have the same property. \n\nThe main issue with this work for the ICLR audience is the use of mainly a set of SPN-specific datasets that are not used in the deep learning generative modeling literature. The use of GenMMN as a baseline is also not a good choice to bridge the gap to the neural community, as its Parzen-window based likelihood evaluation is not really meaningful. Better ways to evaluate the likelihood through annealed importance sampling are discussed in \"On the Quantitative Analysis of Decoder-Based Generative Models\" by Wu et al. I would recommend the use of a simple VAE type model to get a lower bound on the likelihood, or something like Real-NVP.\n\nMost neural network density models are scalable to large numbers of observations as well as instances, and it is not clear that this method scales well \"horizontally\" like this. Evaluating the feasibility of modeling something like MNIST would be interesting.\n\nSPNs have the strength that not only marginal but also various type of conditional queries are tractable, but performance on this is not evaluated or compared. One interesting application could be in imputation of unknown pixels or color channels in images, for which there is not currently a high-performing tractable model.\n\nDespite the disconnect from other ICLR generative modeling literature, the algorithm here seems simple and intuitive and convincingly works better than the previous state of the art for online SPN structure learning. I think VAE is a much better baseline for continuous data than GenMMN when attempting to compare to neural network approaches. Further, the sum-product network could actually be combined with such deep latent variable models as an observation model or posterior, which could be a very powerful combination. \n\nI would like it if these SPN models were better known by the ICLR probabilistic modeling community, but I do not know if this paper does enough to make them relevant. As with the other reviewers, I am not an expert on SPNs. However, this seems to be a simple and effective algorithm for online structure induction, and the scalability aspect is something that is important in much recent work in the learning of representations. I think it is good enough for publication, although I would prefer to see many of the above additions to more clearly bridge the gap with other literature in deep generative modeling."
  },
  {
    "people": [
      "Jaini",
      "Jaini",
      "Jaini",
      "Jaini"
    ],
    "review": "The authors contribute an algorithm for building sum-product networks (SPNs) from data, assuming a Gaussian distribution for all dimensions of the observed data.  Due to the restricted structure of the SPN architecture, building a valid architecture that is tailored to a specific dataset is not an obvious exercise, and so structure-learning algorithms are employed.  For Gaussian distributed observations, the authors state that the previous state of the art is to chose a random SPN that satisfies the completeness and decomposibility constraints that SPNs must observe, and to then learn the parameters (as done in Jaini 2016).  In the contributed manuscript, the algorithm begins with a completely factorized model, and then by passing through the data, builds up more structure, while updating appropriate node statistics to maintain the validity of the SPN.\n\nThe above Jaini reference figures heavily into the reading of the paper because it is (to my limited knowledge) the previous work SOTA on SPNs applied to Gaussian distributed data, and also because the authors of the current manuscript compare performance to datasets studied in Jaini et al.  I personally was unfamiliar with most of these datasets, and so have no basis to judge loglikelihoods, given a particular model, as being either good or poor.  Nevertheless, the current manuscript reports results on these datasets that better (5 / 7) than other methods, such as SPNS (constructed randomly), Stacked Restricted Boltzmann Machines or Generative Moment Matching networks.\n\nOverall: \nFirst let me say, I am not really qualified to make a decision on the acceptance or rejection of this manuscript (yet I am forced to make just such a choice) because I am not an expert in SPNs. I was also unfamiliar with the datasets, so I had no intuitive understanding of the algorithms performance, even when viewed as a black-box.  The algorithm is presented without theoretical inspiration or justification.  These latter are by no means a bad thing, but it again gives me little hold onto when evaluating the manuscript.  The manuscript is clearly written, and to my limited knowledge novel, and their algorithm does a good job (5/7) on selected datasets.  \n\nMy overall impression is that there isn't very much work here (e.g., much of the text is similar to Jaini, and most of the other experiments are repeated verbatim from Jaini), but again I may be missing something (this manuscript DOES mostly Jaini). I say this mostly because I am unfamiliar with the datasets.  Hopefully my reviewing peers will have enough background to know if the results are impressive or not, and my review should be weighted minimally.\n\nSmallish Problems\nI wanted to see nonuniform covariances in the data of the the toy task (Fig 3) for each gaussian component.\n\nThe SPN construction method has two obvious hyper parameters, it is important to see how those parameters affect the graph structure. (I submitted this as a pre-review question, to which the authors responded that they would look into this.)"
  },
  {
    "people": [
      "Jaini",
      "Jaini",
      "Jaini",
      "Jaini"
    ],
    "review": "The authors contribute an algorithm for building sum-product networks (SPNs) from data, assuming a Gaussian distribution for all dimensions of the observed data.  Due to the restricted structure of the SPN architecture, building a valid architecture that is tailored to a specific dataset is not an obvious exercise, and so structure-learning algorithms are employed.  For Gaussian distributed observations, the authors state that the previous state of the art is to chose a random SPN that satisfies the completeness and decomposibility constraints that SPNs must observe, and to then learn the parameters (as done in Jaini 2016).  In the contributed manuscript, the algorithm begins with a completely factorized model, and then by passing through the data, builds up more structure, while updating appropriate node statistics to maintain the validity of the SPN.\n\nThe above Jaini reference figures heavily into the reading of the paper because it is (to my limited knowledge) the previous work SOTA on SPNs applied to Gaussian distributed data, and also because the authors of the current manuscript compare performance to datasets studied in Jaini et al.  I personally was unfamiliar with most of these datasets, and so have no basis to judge loglikelihoods, given a particular model, as being either good or poor.  Nevertheless, the current manuscript reports results on these datasets that better (5 / 7) than other methods, such as SPNS (constructed randomly), Stacked Restricted Boltzmann Machines or Generative Moment Matching networks.\n\nOverall: \nFirst let me say, I am not really qualified to make a decision on the acceptance or rejection of this manuscript (yet I am forced to make just such a choice) because I am not an expert in SPNs. I was also unfamiliar with the datasets, so I had no intuitive understanding of the algorithms performance, even when viewed as a black-box.  The algorithm is presented without theoretical inspiration or justification.  These latter are by no means a bad thing, but it again gives me little hold onto when evaluating the manuscript.  The manuscript is clearly written, and to my limited knowledge novel, and their algorithm does a good job (5/7) on selected datasets.  \n\nMy overall impression is that there isn't very much work here (e.g., much of the text is similar to Jaini, and most of the other experiments are repeated verbatim from Jaini), but again I may be missing something (this manuscript DOES mostly Jaini). I say this mostly because I am unfamiliar with the datasets.  Hopefully my reviewing peers will have enough background to know if the results are impressive or not, and my review should be weighted minimally.\n\nSmallish Problems\nI wanted to see nonuniform covariances in the data of the the toy task (Fig 3) for each gaussian component.\n\nThe SPN construction method has two obvious hyper parameters, it is important to see how those parameters affect the graph structure. (I submitted this as a pre-review question, to which the authors responded that they would look into this.)"
  },
  {
    "people": [
      "Wu"
    ],
    "review": "The authors present an online learning method for learning the structure of sum-product networks. The algorithm assumes Gaussian coordinate-wise marginal distributions, and learns both parameters and structure online. The parameters are updated by a recursive procedure that reweights nodes in the network that most contribute to the likelihood of the current data point. The structure learning is done by either merging independent product Gaussian nodes into multivariate leaf nodes, or creating a mixture over the two nodes when the multivariate would be too large.\n\nThe fact that the dataset is scaled to some larger datasets (in terms of the number of datapoints) is promising, although the number of variables is still quite small. Current benchmarks for tractable continuous density modeling with neural networks include the NICE and Real-NVP families of models, which can be scaled to both large numbers of datapoints and variables. Intractable methods like GAN, GenMMN, VAE have the same property. \n\nThe main issue with this work for the ICLR audience is the use of mainly a set of SPN-specific datasets that are not used in the deep learning generative modeling literature. The use of GenMMN as a baseline is also not a good choice to bridge the gap to the neural community, as its Parzen-window based likelihood evaluation is not really meaningful. Better ways to evaluate the likelihood through annealed importance sampling are discussed in \"On the Quantitative Analysis of Decoder-Based Generative Models\" by Wu et al. I would recommend the use of a simple VAE type model to get a lower bound on the likelihood, or something like Real-NVP.\n\nMost neural network density models are scalable to large numbers of observations as well as instances, and it is not clear that this method scales well \"horizontally\" like this. Evaluating the feasibility of modeling something like MNIST would be interesting.\n\nSPNs have the strength that not only marginal but also various type of conditional queries are tractable, but performance on this is not evaluated or compared. One interesting application could be in imputation of unknown pixels or color channels in images, for which there is not currently a high-performing tractable model.\n\nDespite the disconnect from other ICLR generative modeling literature, the algorithm here seems simple and intuitive and convincingly works better than the previous state of the art for online SPN structure learning. I think VAE is a much better baseline for continuous data than GenMMN when attempting to compare to neural network approaches. Further, the sum-product network could actually be combined with such deep latent variable models as an observation model or posterior, which could be a very powerful combination. \n\nI would like it if these SPN models were better known by the ICLR probabilistic modeling community, but I do not know if this paper does enough to make them relevant. As with the other reviewers, I am not an expert on SPNs. However, this seems to be a simple and effective algorithm for online structure induction, and the scalability aspect is something that is important in much recent work in the learning of representations. I think it is good enough for publication, although I would prefer to see many of the above additions to more clearly bridge the gap with other literature in deep generative modeling."
  },
  {
    "people": [
      "Jaini",
      "Jaini",
      "Jaini",
      "Jaini"
    ],
    "review": "The authors contribute an algorithm for building sum-product networks (SPNs) from data, assuming a Gaussian distribution for all dimensions of the observed data.  Due to the restricted structure of the SPN architecture, building a valid architecture that is tailored to a specific dataset is not an obvious exercise, and so structure-learning algorithms are employed.  For Gaussian distributed observations, the authors state that the previous state of the art is to chose a random SPN that satisfies the completeness and decomposibility constraints that SPNs must observe, and to then learn the parameters (as done in Jaini 2016).  In the contributed manuscript, the algorithm begins with a completely factorized model, and then by passing through the data, builds up more structure, while updating appropriate node statistics to maintain the validity of the SPN.\n\nThe above Jaini reference figures heavily into the reading of the paper because it is (to my limited knowledge) the previous work SOTA on SPNs applied to Gaussian distributed data, and also because the authors of the current manuscript compare performance to datasets studied in Jaini et al.  I personally was unfamiliar with most of these datasets, and so have no basis to judge loglikelihoods, given a particular model, as being either good or poor.  Nevertheless, the current manuscript reports results on these datasets that better (5 / 7) than other methods, such as SPNS (constructed randomly), Stacked Restricted Boltzmann Machines or Generative Moment Matching networks.\n\nOverall: \nFirst let me say, I am not really qualified to make a decision on the acceptance or rejection of this manuscript (yet I am forced to make just such a choice) because I am not an expert in SPNs. I was also unfamiliar with the datasets, so I had no intuitive understanding of the algorithms performance, even when viewed as a black-box.  The algorithm is presented without theoretical inspiration or justification.  These latter are by no means a bad thing, but it again gives me little hold onto when evaluating the manuscript.  The manuscript is clearly written, and to my limited knowledge novel, and their algorithm does a good job (5/7) on selected datasets.  \n\nMy overall impression is that there isn't very much work here (e.g., much of the text is similar to Jaini, and most of the other experiments are repeated verbatim from Jaini), but again I may be missing something (this manuscript DOES mostly Jaini). I say this mostly because I am unfamiliar with the datasets.  Hopefully my reviewing peers will have enough background to know if the results are impressive or not, and my review should be weighted minimally.\n\nSmallish Problems\nI wanted to see nonuniform covariances in the data of the the toy task (Fig 3) for each gaussian component.\n\nThe SPN construction method has two obvious hyper parameters, it is important to see how those parameters affect the graph structure. (I submitted this as a pre-review question, to which the authors responded that they would look into this.)"
  },
  {
    "people": [
      "Kiros"
    ],
    "review": "Authors take the skip-graph architecture (Kiros 2015) and apply it to classifying labeled graphs (molecular graphs). They do it by creating many sentences by walking the graph randomly, and asking the model to predict previous part and next part from the middle part. Activations of the decoder part of this model on a walk generated from a new graph are used as features for a binary classifier use to predict whether the molecule has anti-cancer properties.\n\nPaper is well written, except that evaluation section is missing details of how the embedding is used for actual classification (ie, what classifier is used)\n\nUnfortunately I'm not familiar with the dataset and how hard it is to achieve the results they demonstrate, that would be the important factor to weight on the papers acceptance."
  },
  {
    "people": [
      "Kiros"
    ],
    "review": "Authors take the skip-graph architecture (Kiros 2015) and apply it to classifying labeled graphs (molecular graphs). They do it by creating many sentences by walking the graph randomly, and asking the model to predict previous part and next part from the middle part. Activations of the decoder part of this model on a walk generated from a new graph are used as features for a binary classifier use to predict whether the molecule has anti-cancer properties.\n\nPaper is well written, except that evaluation section is missing details of how the embedding is used for actual classification (ie, what classifier is used)\n\nUnfortunately I'm not familiar with the dataset and how hard it is to achieve the results they demonstrate, that would be the important factor to weight on the papers acceptance."
  },
  {
    "people": [
      "Bell",
      "Sejnowski"
    ],
    "review": "This is an 18 page paper plus appendix which presents a mathematical derivation for infomax for an actual neural population with noise.  The original Bell & Sejnowski infomax framework only considered the no noise case.  Results are shown for natural image patches and the mnist dataset, which qualitatively resemble results obtained with other methods.\n\nThis seems like an interesting and potentially more general approach to unsupervised learning.  However the paper is quite long and it was difficult for me to follow all the twists and turns.  For example the introduction of the hierarchical model was confusing and it took several iterations to understand where this was going.  'Hierarchical' is probably not the right terminology here because it's not like a deep net hierarchy, it's just decomposing the tuning curve function into different parts.  I would recommend that the authors try to condense the paper so that the central message and important steps are conveyed in short order, and then put the more complete mathematical development into a supplementary document.\n\nAlso, the authors should look at the work of Karklin & Simoncelli 2011 which is highly related.  They also use an infomax framework for a noisy neural population to derive on and off cells in the retina, and they show the conditions under which orientation selectivity emerges."
  },
  {
    "people": [
      "Wentao"
    ],
    "review": "We would like to thank the reviewers for their conscientious review and comments and for providing us with valuable feedback. We already updated the paper based on the reviewer comments.\n\nThanks!\n\nWentao"
  },
  {
    "people": [
      "Bell",
      "Sejnowski"
    ],
    "review": "This is an 18 page paper plus appendix which presents a mathematical derivation for infomax for an actual neural population with noise.  The original Bell & Sejnowski infomax framework only considered the no noise case.  Results are shown for natural image patches and the mnist dataset, which qualitatively resemble results obtained with other methods.\n\nThis seems like an interesting and potentially more general approach to unsupervised learning.  However the paper is quite long and it was difficult for me to follow all the twists and turns.  For example the introduction of the hierarchical model was confusing and it took several iterations to understand where this was going.  'Hierarchical' is probably not the right terminology here because it's not like a deep net hierarchy, it's just decomposing the tuning curve function into different parts.  I would recommend that the authors try to condense the paper so that the central message and important steps are conveyed in short order, and then put the more complete mathematical development into a supplementary document.\n\nAlso, the authors should look at the work of Karklin & Simoncelli 2011 which is highly related.  They also use an infomax framework for a noisy neural population to derive on and off cells in the retina, and they show the conditions under which orientation selectivity emerges.\n"
  },
  {
    "people": [
      "Horace Barlow"
    ],
    "review": "My main research interests are in computational neuroscience, information theory, machine learning and deep learning. For those of us who engage in AI related research, we all want to learn from the human brain how to process information to achieve intelligence. For example, deep learning is now said to be brain-like, but also some researchers say that nothing to do with the neuroscience. But one thing is for sure, and that is deep nets learning the brain processing information with hierarchical structure. However, the current advances in neuroscience is of limited role on the inspiration for AI research. \n\nHow do we learn from the brain, learn what? We think we should learn the basic design principles for the information processing in our nervous system. What are the fundamental principles to guide the brain to design these complex structures and neural coding. \nIn fact, the principle on energy and information provides a fundamental constraint for the actual nervous systems. The efficient coding hypothesis proposed by Horace Barlow provides a good description of this principle. But how to model this hypothesis, we will encounter great challenges. Because a direct calculation of Shannon's mutual information (MI) is generally a very difficult thing in many cases. \n\nWe first solve the problem of effective approximation for evaluating MI in the context of neural population coding, especially for high-dimensional inputs. Then an fast and robust unsupervised learning algorithm is developed in this paper. With our methods, we have explained some interesting phenomena in neuroscience, which were previously unsolvable in other ways. \nIn machine learning and deep learning, this paper is a more fundamental part  that provides the basis for our other works. We have also got a lot of interesting results from other related works we have done or are doing. For example, we use our method to train CNN, RNN and generative model, etc. We can unify supervised learning and unsupervised learning and can completely without the backpropagation algorithm for supervised learning of deep nets. We can also prove that the cost function of SVM (Hinge loss) is a special case of our neural population infomax. For the current deep nets, we need the big data for training, and in our framework, the small data we also can achieve good results.\n\nThe above is a brief introduction to the background of this paper. Since there is no similar work on this paper before, so there may be relatively more formulas and derivations and not very easy to follow. \n\nThanks!\n"
  },
  {
    "people": [
      "Bell",
      "Sejnowski"
    ],
    "review": "This is an 18 page paper plus appendix which presents a mathematical derivation for infomax for an actual neural population with noise.  The original Bell & Sejnowski infomax framework only considered the no noise case.  Results are shown for natural image patches and the mnist dataset, which qualitatively resemble results obtained with other methods.\n\nThis seems like an interesting and potentially more general approach to unsupervised learning.  However the paper is quite long and it was difficult for me to follow all the twists and turns.  For example the introduction of the hierarchical model was confusing and it took several iterations to understand where this was going.  'Hierarchical' is probably not the right terminology here because it's not like a deep net hierarchy, it's just decomposing the tuning curve function into different parts.  I would recommend that the authors try to condense the paper so that the central message and important steps are conveyed in short order, and then put the more complete mathematical development into a supplementary document.\n\nAlso, the authors should look at the work of Karklin & Simoncelli 2011 which is highly related.  They also use an infomax framework for a noisy neural population to derive on and off cells in the retina, and they show the conditions under which orientation selectivity emerges."
  },
  {
    "people": [
      "Wentao"
    ],
    "review": "We would like to thank the reviewers for their conscientious review and comments and for providing us with valuable feedback. We already updated the paper based on the reviewer comments.\n\nThanks!\n\nWentao"
  },
  {
    "people": [
      "Bell",
      "Sejnowski"
    ],
    "review": "This is an 18 page paper plus appendix which presents a mathematical derivation for infomax for an actual neural population with noise.  The original Bell & Sejnowski infomax framework only considered the no noise case.  Results are shown for natural image patches and the mnist dataset, which qualitatively resemble results obtained with other methods.\n\nThis seems like an interesting and potentially more general approach to unsupervised learning.  However the paper is quite long and it was difficult for me to follow all the twists and turns.  For example the introduction of the hierarchical model was confusing and it took several iterations to understand where this was going.  'Hierarchical' is probably not the right terminology here because it's not like a deep net hierarchy, it's just decomposing the tuning curve function into different parts.  I would recommend that the authors try to condense the paper so that the central message and important steps are conveyed in short order, and then put the more complete mathematical development into a supplementary document.\n\nAlso, the authors should look at the work of Karklin & Simoncelli 2011 which is highly related.  They also use an infomax framework for a noisy neural population to derive on and off cells in the retina, and they show the conditions under which orientation selectivity emerges.\n"
  },
  {
    "people": [
      "Horace Barlow"
    ],
    "review": "My main research interests are in computational neuroscience, information theory, machine learning and deep learning. For those of us who engage in AI related research, we all want to learn from the human brain how to process information to achieve intelligence. For example, deep learning is now said to be brain-like, but also some researchers say that nothing to do with the neuroscience. But one thing is for sure, and that is deep nets learning the brain processing information with hierarchical structure. However, the current advances in neuroscience is of limited role on the inspiration for AI research. \n\nHow do we learn from the brain, learn what? We think we should learn the basic design principles for the information processing in our nervous system. What are the fundamental principles to guide the brain to design these complex structures and neural coding. \nIn fact, the principle on energy and information provides a fundamental constraint for the actual nervous systems. The efficient coding hypothesis proposed by Horace Barlow provides a good description of this principle. But how to model this hypothesis, we will encounter great challenges. Because a direct calculation of Shannon's mutual information (MI) is generally a very difficult thing in many cases. \n\nWe first solve the problem of effective approximation for evaluating MI in the context of neural population coding, especially for high-dimensional inputs. Then an fast and robust unsupervised learning algorithm is developed in this paper. With our methods, we have explained some interesting phenomena in neuroscience, which were previously unsolvable in other ways. \nIn machine learning and deep learning, this paper is a more fundamental part  that provides the basis for our other works. We have also got a lot of interesting results from other related works we have done or are doing. For example, we use our method to train CNN, RNN and generative model, etc. We can unify supervised learning and unsupervised learning and can completely without the backpropagation algorithm for supervised learning of deep nets. We can also prove that the cost function of SVM (Hinge loss) is a special case of our neural population infomax. For the current deep nets, we need the big data for training, and in our framework, the small data we also can achieve good results.\n\nThe above is a brief introduction to the background of this paper. Since there is no similar work on this paper before, so there may be relatively more formulas and derivations and not very easy to follow. \n\nThanks!\n"
  },
  {
    "people": [
      "Oh"
    ],
    "review": "The topic of the paper, model-based RL with a learned model, is important and timely. The paper is well written. I feel that the presented results are too incremental. Augmenting the frame prediction network with another head that predicts the reward is a very sensible thing to do. However neither the methodology not the results are novel / surprising, given that the original method of [Oh et al. 2015] already learns to successfully increment score counters in predicted frames in many games.\n\nI\u2019m very much looking forward to seeing the results of applying the learned joint model of frames and rewards to model-based RL as proposed by the authors."
  },
  {
    "people": [
      "Oh"
    ],
    "review": "The topic of the paper, model-based RL with a learned model, is important and timely. The paper is well written. I feel that the presented results are too incremental. Augmenting the frame prediction network with another head that predicts the reward is a very sensible thing to do. However neither the methodology not the results are novel / surprising, given that the original method of [Oh et al. 2015] already learns to successfully increment score counters in predicted frames in many games.\n\nI\u2019m very much looking forward to seeing the results of applying the learned joint model of frames and rewards to model-based RL as proposed by the authors. "
  },
  {
    "people": [
      "Oh"
    ],
    "review": "The topic of the paper, model-based RL with a learned model, is important and timely. The paper is well written. I feel that the presented results are too incremental. Augmenting the frame prediction network with another head that predicts the reward is a very sensible thing to do. However neither the methodology not the results are novel / surprising, given that the original method of [Oh et al. 2015] already learns to successfully increment score counters in predicted frames in many games.\n\nI\u2019m very much looking forward to seeing the results of applying the learned joint model of frames and rewards to model-based RL as proposed by the authors."
  },
  {
    "people": [
      "Oh"
    ],
    "review": "The topic of the paper, model-based RL with a learned model, is important and timely. The paper is well written. I feel that the presented results are too incremental. Augmenting the frame prediction network with another head that predicts the reward is a very sensible thing to do. However neither the methodology not the results are novel / surprising, given that the original method of [Oh et al. 2015] already learns to successfully increment score counters in predicted frames in many games.\n\nI\u2019m very much looking forward to seeing the results of applying the learned joint model of frames and rewards to model-based RL as proposed by the authors. "
  },
  {
    "people": [
      "Adam",
      "Nesterov",
      "Mishkin",
      "Mishkin"
    ],
    "review": "This paper proposes new initialization for particular architectures and a correction trick to batch normalization to correct variance introduced by dropout. While authors state interesting observations, the claims are not supported with convincing results.\n\nI guess Figure 1 is only for mnist and for only two values of p with one particular network architecture, the dataset and empirical setup is not clear.\n\nThe convergence is demonstrated only for three dropout values in Figure 2 which may cause an unfair comparison. For instance how does the convergence compare for the best dropout rate after cross-validation (three figures each figure has three results for one method with different dropouts [bests cv result for each one])? Also how is the corresponding validation error and test iterations?  Also only mnist does not have to generalize to other benchmarks.\n\nFigure 3 gives closer results for Adam optimizer, learning rate is not selected with random search or bayesian optimization, learning decay iterations fixed and regularization coefficient is set to a small value without tuning. A slightly better tuning of parameters may close the current gap. Also Nesterov based competitor gives unreasonably worse accuracy compared to recent results which may indicate that this experiment should not be taken into account. \n\nIn Table 2, there is no significant improvement on CIFAR10. The CIFAR100 difference is not significant without including batch normalization variance re-estimation. However there is no result for 'original with BN update' therefore it is not clear whether the BN update helps in general or not. SVHN also does not have result for original with BN update.\n\nThere should be baselines with batch normalizations for Figure 1,2 3 to support the claims convincingly.  The main criticism about batch normalization is additional computational cost by giving (Mishkin et al, 2016 ) as reference however this should not be a reason to not to compare the initialization to batch-normalization.  In fact, (Mishkin et al, 2016) performs comparison to batch normalization and also with and without data augmentation with recent state of art architectures.\n\nNone of the empirical results have data augmentation. It is not clear if the initialization or  batch normalization update will help or make it worse for that case.\n\nRecent state of art methods methods like Res Net variant and Dense Net scale to many depths and report result for ImageNet. Although the authors claim that this can be extended to residual network variants, it is not clear if there is going to be any empirical gain for that architectures.  \n\nThis work requires a comprehensive and fair comparison. Otherwise the contribution is not significant."
  },
  {
    "people": [
      "Adam",
      "Nesterov",
      "Mishkin",
      "Mishkin"
    ],
    "review": "This paper proposes new initialization for particular architectures and a correction trick to batch normalization to correct variance introduced by dropout. While authors state interesting observations, the claims are not supported with convincing results.\n\nI guess Figure 1 is only for mnist and for only two values of p with one particular network architecture, the dataset and empirical setup is not clear.\n\nThe convergence is demonstrated only for three dropout values in Figure 2 which may cause an unfair comparison. For instance how does the convergence compare for the best dropout rate after cross-validation (three figures each figure has three results for one method with different dropouts [bests cv result for each one])? Also how is the corresponding validation error and test iterations?  Also only mnist does not have to generalize to other benchmarks.\n\nFigure 3 gives closer results for Adam optimizer, learning rate is not selected with random search or bayesian optimization, learning decay iterations fixed and regularization coefficient is set to a small value without tuning. A slightly better tuning of parameters may close the current gap. Also Nesterov based competitor gives unreasonably worse accuracy compared to recent results which may indicate that this experiment should not be taken into account. \n\nIn Table 2, there is no significant improvement on CIFAR10. The CIFAR100 difference is not significant without including batch normalization variance re-estimation. However there is no result for 'original with BN update' therefore it is not clear whether the BN update helps in general or not. SVHN also does not have result for original with BN update.\n\nThere should be baselines with batch normalizations for Figure 1,2 3 to support the claims convincingly.  The main criticism about batch normalization is additional computational cost by giving (Mishkin et al, 2016 ) as reference however this should not be a reason to not to compare the initialization to batch-normalization.  In fact, (Mishkin et al, 2016) performs comparison to batch normalization and also with and without data augmentation with recent state of art architectures.\n\nNone of the empirical results have data augmentation. It is not clear if the initialization or  batch normalization update will help or make it worse for that case.\n\nRecent state of art methods methods like Res Net variant and Dense Net scale to many depths and report result for ImageNet. Although the authors claim that this can be extended to residual network variants, it is not clear if there is going to be any empirical gain for that architectures.  \n\nThis work requires a comprehensive and fair comparison. Otherwise the contribution is not significant."
  },
  {
    "people": [
      "Adam",
      "Nesterov",
      "Mishkin",
      "Mishkin"
    ],
    "review": "This paper proposes new initialization for particular architectures and a correction trick to batch normalization to correct variance introduced by dropout. While authors state interesting observations, the claims are not supported with convincing results.\n\nI guess Figure 1 is only for mnist and for only two values of p with one particular network architecture, the dataset and empirical setup is not clear.\n\nThe convergence is demonstrated only for three dropout values in Figure 2 which may cause an unfair comparison. For instance how does the convergence compare for the best dropout rate after cross-validation (three figures each figure has three results for one method with different dropouts [bests cv result for each one])? Also how is the corresponding validation error and test iterations?  Also only mnist does not have to generalize to other benchmarks.\n\nFigure 3 gives closer results for Adam optimizer, learning rate is not selected with random search or bayesian optimization, learning decay iterations fixed and regularization coefficient is set to a small value without tuning. A slightly better tuning of parameters may close the current gap. Also Nesterov based competitor gives unreasonably worse accuracy compared to recent results which may indicate that this experiment should not be taken into account. \n\nIn Table 2, there is no significant improvement on CIFAR10. The CIFAR100 difference is not significant without including batch normalization variance re-estimation. However there is no result for 'original with BN update' therefore it is not clear whether the BN update helps in general or not. SVHN also does not have result for original with BN update.\n\nThere should be baselines with batch normalizations for Figure 1,2 3 to support the claims convincingly.  The main criticism about batch normalization is additional computational cost by giving (Mishkin et al, 2016 ) as reference however this should not be a reason to not to compare the initialization to batch-normalization.  In fact, (Mishkin et al, 2016) performs comparison to batch normalization and also with and without data augmentation with recent state of art architectures.\n\nNone of the empirical results have data augmentation. It is not clear if the initialization or  batch normalization update will help or make it worse for that case.\n\nRecent state of art methods methods like Res Net variant and Dense Net scale to many depths and report result for ImageNet. Although the authors claim that this can be extended to residual network variants, it is not clear if there is going to be any empirical gain for that architectures.  \n\nThis work requires a comprehensive and fair comparison. Otherwise the contribution is not significant."
  },
  {
    "people": [
      "Adam",
      "Nesterov",
      "Mishkin",
      "Mishkin"
    ],
    "review": "This paper proposes new initialization for particular architectures and a correction trick to batch normalization to correct variance introduced by dropout. While authors state interesting observations, the claims are not supported with convincing results.\n\nI guess Figure 1 is only for mnist and for only two values of p with one particular network architecture, the dataset and empirical setup is not clear.\n\nThe convergence is demonstrated only for three dropout values in Figure 2 which may cause an unfair comparison. For instance how does the convergence compare for the best dropout rate after cross-validation (three figures each figure has three results for one method with different dropouts [bests cv result for each one])? Also how is the corresponding validation error and test iterations?  Also only mnist does not have to generalize to other benchmarks.\n\nFigure 3 gives closer results for Adam optimizer, learning rate is not selected with random search or bayesian optimization, learning decay iterations fixed and regularization coefficient is set to a small value without tuning. A slightly better tuning of parameters may close the current gap. Also Nesterov based competitor gives unreasonably worse accuracy compared to recent results which may indicate that this experiment should not be taken into account. \n\nIn Table 2, there is no significant improvement on CIFAR10. The CIFAR100 difference is not significant without including batch normalization variance re-estimation. However there is no result for 'original with BN update' therefore it is not clear whether the BN update helps in general or not. SVHN also does not have result for original with BN update.\n\nThere should be baselines with batch normalizations for Figure 1,2 3 to support the claims convincingly.  The main criticism about batch normalization is additional computational cost by giving (Mishkin et al, 2016 ) as reference however this should not be a reason to not to compare the initialization to batch-normalization.  In fact, (Mishkin et al, 2016) performs comparison to batch normalization and also with and without data augmentation with recent state of art architectures.\n\nNone of the empirical results have data augmentation. It is not clear if the initialization or  batch normalization update will help or make it worse for that case.\n\nRecent state of art methods methods like Res Net variant and Dense Net scale to many depths and report result for ImageNet. Although the authors claim that this can be extended to residual network variants, it is not clear if there is going to be any empirical gain for that architectures.  \n\nThis work requires a comprehensive and fair comparison. Otherwise the contribution is not significant."
  },
  {
    "people": [
      "Jacob",
      "Weston"
    ],
    "review": "The paper proposes a model that aims at learning to label nodes of graph in a semi-supervised setting. The idea of the model is based on the use of the graph structure to regularize the representations learned at the node levels. Experimental results are provided on different tasks\n\nThe underlying idea of this paper (graph regularization) has been already explored in different papers \u2013 e.g 'Learning latent representations of nodes for classifying in heterogeneous social networks' [Jacob et al. 2014],   [Weston et al 2012] where a real graph structure is used instead of a built one. The experiments lack of strong comparisons with other graph models (e.g Iterative Classification, 'Learning from labeled and unlabeled data on a directed graph', ...). So the novelty of the paper and the experimental protocol are not strong enough to accpet the paper.\n\nPros:\n* Learning over graph is an important topic\n\nCons:\n* Many existing approaches have already exploited the same types of ideas, resulting in very close models\n* Lack of comparison w.r.t existing models"
  },
  {
    "people": [
      "Weston"
    ],
    "review": "The paper is an interesting contribution, primarily in its generalization of Weston's et al's work on semi-supervised embedding method. You have shown convincingly that it can work with multiple architectures, and with various forms of graph. And the PubMed results are good. To improve the paper in the future, I'd recommend 1) relating better to prior work, and 2) extending your exploration of its application to graphs without features."
  },
  {
    "people": [
      "Weston",
      "Weston",
      "Zhang",
      "Yang",
      "Weston",
      "Belkin",
      "Yang",
      "Sen",
      "Prithviraj",
      "Namata",
      "Galileo",
      "Bilgic",
      "Mustafa",
      "Getoor",
      "Lise",
      "Galligher",
      "Brian",
      "Eliassi-Rad",
      "Tina",
      "Yang",
      "Zhilin",
      "Cohen",
      "William",
      "Salakhutdinov",
      "Ruslan"
    ],
    "review": "\nWe thank the reviewers for all their comments. However we would like to respond, make further clarifications and show new results (see comment #3 below) that should address all reviewer concerns.\n\n1. Response to all reviewers\n\nThis work generalizes the Weston et al.\u2019s work on semi-supervised embedding and extends it to new settings (for example, when only graph inputs & no features are available). Unlike the previous works, we show that the graph augmented training method can work with multiple neural network architectures (Feed Forward NNs, CNNs, RNNs) and on multiple prediction tasks & datasets using \"natural\" as well as \"constructed\" graphs. The experiment results clearly show the effectiveness of this method in all these different settings. Besides the methodology, our study also presents an important contribution towards assessing the effectiveness of graph+neural networks as a generic training mechanism for different architectures & problems, which was not well studied in previous papers. We can add more clarifications in our paper to emphasize this point more clearly. \n\nFurthermore, the reviewers\u2019 concerns should also be addressed with details in comment #3 (please see below), where we show new experiments and direct comparison results against Weston et al., 2012; Yang et al., 2016 [2] and other graph methods. \n\n2. Regarding reviewer #2\u2019s comment about accuracy comparison against Zhang et al., 2015.\n\nWe would like to make it clear to the reviewers that the numbers reported on this task (Table 3 in our paper) for the \u201csmall CNN\u201d baseline were obtained from their paper (Zhang et al., 2015 refers to this as \u201cSmall ConvNet\u201d which uses 6 layers). They use far deeper and more complex networks (wrt frame size, hidden units) to achieve the reported accuracies on this task.\nIn comparison, NGM trained with a simple 3-layer CNN achieves much better results (86.90 vs 84.35/85.20) and also produces comparable results to their best 9-layer Large ConvNet. This shows that our graph augmented training method not only achieves better results but also helps train more efficient, compressible networks.\n\n3. Response to all reviewers \u2014 regarding comparison against Weston et al., 2012; Yang et al., 2016 [2] and other graph methods. This, along with experiments already detailed in the paper, should address all the concerns raised about how the new method performs in comparison to previous works.\n\nWe performed new experiments and compare our method on a new task with very limited supervision \u2014 the PubMed document classification task [1]. The task is to classify each document into one of 3 classes, with each document being described by a TF-IDF weighted word vector. The graph is available as a citation network: two documents are connected to each other if one cites the other. The graph has 19,717 nodes and 44,338 edges, with each class having 20 seed nodes and 1000 test nodes. In our experiments we exclude the test nodes from the graph entirely, training only on the labeled and unlabeled nodes.\n\nWe train a feed-forward network with two hidden layers with 250 and 100 neurons, using the l2 distance metric on the last hidden layer. The NGM model is trained with alpha_i = 0.2, while the baseline is trained with alpha_i = 0 (i.e., a supervised-only model). We use self-training to train the model, starting with just the 60 seed nodes as training data. The amount of\u00a0 training data is iteratively increased by assigning labels to the immediate neighbors of the labeled nodes, and retraining the model.\n\nWe compare the final NGM model against the baseline, the Planetoid models (Yang et. al, 2016 [2]), semi-supervised embedding (SemiEmb) (Weston et. al, 2012), manifold regression (ManiReg) (Belkin et. al, 2006), transductive SVM (TSVM), label propagation (LP) and other methods. The results show that the NGM model (without any \u201ctuning\u201d) outperforms the baseline, semi-supervised embedding, manifold regularization and Planetoid-G/Planetoid-T, and compares favorably with the other Planetoid system. We believe that with tuning, NGM accuracy can be improved even further.\n\n\nTable: Results for Pubmed document classification\n\nFeat*              0.698\nSemiEmb*\u00a0\u00a0\u00a0   0.711\u2028\nManiReg*\u00a0\u00a0\u00a0\u00a0   0.707\n\u2028Planetoid-I*\u00a0   0.772\nTSVM*            0.622\u2028\nLP*                 0.630\n\u2028GraphEmb*    0.653\u2028\nPlanetoid-G*\u00a0 0.664\u2028\nPlanetoid-T*   0.757\n\n\u2028------------------------------------\u2028\nFeed forward NN (similar to Feat)\u00a0\u00a0 0.709\u2028\nNGM\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0.759\n\n*numbers reported in Table 3 from Yang et al., 2016 \n\n\n[1]: Sen, Prithviraj, Namata, Galileo, Bilgic, Mustafa, Getoor, Lise, Galligher, Brian, and Eliassi-Rad, Tina. Collective classification in network data. AI magazine, 29(3):93, 2008.\n[2]: Yang, Zhilin, Cohen, William, and Salakhutdinov, Ruslan. Revisiting Semi-Supervised Learning with Graph Embeddings., 2016.\n"
  },
  {
    "people": [
      "Weston",
      "Weston",
      "Weston"
    ],
    "review": "The authors introduce a semi-supervised method for neural networks, inspired from label propagation.\n\nThe method appears to be exactly the same than the one proposed in (Weston et al, 2008) (the authors cite the 2012 paper). The optimized objective function in eq (4) is exactly the same than eq (9) in (Weston et al, 2008).\n\nAs possible novelty, the authors propose to use the adjacency matrix as input to the neural network, when there are no other features, and show success on the BlogCatalog dataset.\n\nExperiments on text classification use neighbors according to word2vec average embedding to build the adjacency matrix. Top reported accuracies are not convincing compared to (Zhang et al, 2015) reported performance. Last experiment is on semantic intent classification, which a custom dataset; neighbors are also found according to a word2vec metric.\n\nIn summary, the paper propose few applications to the original (Weston et al, 2008) paper. It rebrands the algorithm under a new name, and does not bring any scientific novelty, and the experimental section lacks existing baselines to be convincing."
  },
  {
    "people": [
      "Weston",
      "Weston"
    ],
    "review": "This paper proposes the Neural Graph Machine that adds in graph regularization on neural network hidden representations to improve network learning and take the graph structure into account.  The proposed model, however, is almost identical to that of Weston et al. 2012.\n\nAs the authors have clarified in the answers to the questions, there are a few new things that previous work did not do:\n\n1. they showed that graph augmented training for a range of different types of networks, including FF, CNN, RNNs etc. and works on a range of problems.\n2. graphs help to train better networks, e.g. 3 layer CNN with graphs does as well as than 9 layer CNNs\n3. graph augmented training works on a variety of different kinds of graphs.\n\nHowever, all these points mentioned above seems to simply be different applications of the graph augmented training idea, and observations made during the applications.  I think it is therefore not proper to call the proposed model a novel model with a new name Neural Graph Machine, but rather making it clear in the paper that this is an empirical study of the model proposed by Weston et al. 2012 to different problems would be more acceptable."
  },
  {
    "people": [
      "Jacob",
      "Weston"
    ],
    "review": "The paper proposes a model that aims at learning to label nodes of graph in a semi-supervised setting. The idea of the model is based on the use of the graph structure to regularize the representations learned at the node levels. Experimental results are provided on different tasks\n\nThe underlying idea of this paper (graph regularization) has been already explored in different papers \u2013 e.g 'Learning latent representations of nodes for classifying in heterogeneous social networks' [Jacob et al. 2014],   [Weston et al 2012] where a real graph structure is used instead of a built one. The experiments lack of strong comparisons with other graph models (e.g Iterative Classification, 'Learning from labeled and unlabeled data on a directed graph', ...). So the novelty of the paper and the experimental protocol are not strong enough to accpet the paper.\n\nPros:\n* Learning over graph is an important topic\n\nCons:\n* Many existing approaches have already exploited the same types of ideas, resulting in very close models\n* Lack of comparison w.r.t existing models\n"
  },
  {
    "people": [
      "Jacob",
      "Weston"
    ],
    "review": "The paper proposes a model that aims at learning to label nodes of graph in a semi-supervised setting. The idea of the model is based on the use of the graph structure to regularize the representations learned at the node levels. Experimental results are provided on different tasks\n\nThe underlying idea of this paper (graph regularization) has been already explored in different papers \u2013 e.g 'Learning latent representations of nodes for classifying in heterogeneous social networks' [Jacob et al. 2014],   [Weston et al 2012] where a real graph structure is used instead of a built one. The experiments lack of strong comparisons with other graph models (e.g Iterative Classification, 'Learning from labeled and unlabeled data on a directed graph', ...). So the novelty of the paper and the experimental protocol are not strong enough to accpet the paper.\n\nPros:\n* Learning over graph is an important topic\n\nCons:\n* Many existing approaches have already exploited the same types of ideas, resulting in very close models\n* Lack of comparison w.r.t existing models"
  },
  {
    "people": [
      "Weston"
    ],
    "review": "The paper is an interesting contribution, primarily in its generalization of Weston's et al's work on semi-supervised embedding method. You have shown convincingly that it can work with multiple architectures, and with various forms of graph. And the PubMed results are good. To improve the paper in the future, I'd recommend 1) relating better to prior work, and 2) extending your exploration of its application to graphs without features."
  },
  {
    "people": [
      "Weston",
      "Weston",
      "Zhang",
      "Yang",
      "Weston",
      "Belkin",
      "Yang",
      "Sen",
      "Prithviraj",
      "Namata",
      "Galileo",
      "Bilgic",
      "Mustafa",
      "Getoor",
      "Lise",
      "Galligher",
      "Brian",
      "Eliassi-Rad",
      "Tina",
      "Yang",
      "Zhilin",
      "Cohen",
      "William",
      "Salakhutdinov",
      "Ruslan"
    ],
    "review": "\nWe thank the reviewers for all their comments. However we would like to respond, make further clarifications and show new results (see comment #3 below) that should address all reviewer concerns.\n\n1. Response to all reviewers\n\nThis work generalizes the Weston et al.\u2019s work on semi-supervised embedding and extends it to new settings (for example, when only graph inputs & no features are available). Unlike the previous works, we show that the graph augmented training method can work with multiple neural network architectures (Feed Forward NNs, CNNs, RNNs) and on multiple prediction tasks & datasets using \"natural\" as well as \"constructed\" graphs. The experiment results clearly show the effectiveness of this method in all these different settings. Besides the methodology, our study also presents an important contribution towards assessing the effectiveness of graph+neural networks as a generic training mechanism for different architectures & problems, which was not well studied in previous papers. We can add more clarifications in our paper to emphasize this point more clearly. \n\nFurthermore, the reviewers\u2019 concerns should also be addressed with details in comment #3 (please see below), where we show new experiments and direct comparison results against Weston et al., 2012; Yang et al., 2016 [2] and other graph methods. \n\n2. Regarding reviewer #2\u2019s comment about accuracy comparison against Zhang et al., 2015.\n\nWe would like to make it clear to the reviewers that the numbers reported on this task (Table 3 in our paper) for the \u201csmall CNN\u201d baseline were obtained from their paper (Zhang et al., 2015 refers to this as \u201cSmall ConvNet\u201d which uses 6 layers). They use far deeper and more complex networks (wrt frame size, hidden units) to achieve the reported accuracies on this task.\nIn comparison, NGM trained with a simple 3-layer CNN achieves much better results (86.90 vs 84.35/85.20) and also produces comparable results to their best 9-layer Large ConvNet. This shows that our graph augmented training method not only achieves better results but also helps train more efficient, compressible networks.\n\n3. Response to all reviewers \u2014 regarding comparison against Weston et al., 2012; Yang et al., 2016 [2] and other graph methods. This, along with experiments already detailed in the paper, should address all the concerns raised about how the new method performs in comparison to previous works.\n\nWe performed new experiments and compare our method on a new task with very limited supervision \u2014 the PubMed document classification task [1]. The task is to classify each document into one of 3 classes, with each document being described by a TF-IDF weighted word vector. The graph is available as a citation network: two documents are connected to each other if one cites the other. The graph has 19,717 nodes and 44,338 edges, with each class having 20 seed nodes and 1000 test nodes. In our experiments we exclude the test nodes from the graph entirely, training only on the labeled and unlabeled nodes.\n\nWe train a feed-forward network with two hidden layers with 250 and 100 neurons, using the l2 distance metric on the last hidden layer. The NGM model is trained with alpha_i = 0.2, while the baseline is trained with alpha_i = 0 (i.e., a supervised-only model). We use self-training to train the model, starting with just the 60 seed nodes as training data. The amount of\u00a0 training data is iteratively increased by assigning labels to the immediate neighbors of the labeled nodes, and retraining the model.\n\nWe compare the final NGM model against the baseline, the Planetoid models (Yang et. al, 2016 [2]), semi-supervised embedding (SemiEmb) (Weston et. al, 2012), manifold regression (ManiReg) (Belkin et. al, 2006), transductive SVM (TSVM), label propagation (LP) and other methods. The results show that the NGM model (without any \u201ctuning\u201d) outperforms the baseline, semi-supervised embedding, manifold regularization and Planetoid-G/Planetoid-T, and compares favorably with the other Planetoid system. We believe that with tuning, NGM accuracy can be improved even further.\n\n\nTable: Results for Pubmed document classification\n\nFeat*              0.698\nSemiEmb*\u00a0\u00a0\u00a0   0.711\u2028\nManiReg*\u00a0\u00a0\u00a0\u00a0   0.707\n\u2028Planetoid-I*\u00a0   0.772\nTSVM*            0.622\u2028\nLP*                 0.630\n\u2028GraphEmb*    0.653\u2028\nPlanetoid-G*\u00a0 0.664\u2028\nPlanetoid-T*   0.757\n\n\u2028------------------------------------\u2028\nFeed forward NN (similar to Feat)\u00a0\u00a0 0.709\u2028\nNGM\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0.759\n\n*numbers reported in Table 3 from Yang et al., 2016 \n\n\n[1]: Sen, Prithviraj, Namata, Galileo, Bilgic, Mustafa, Getoor, Lise, Galligher, Brian, and Eliassi-Rad, Tina. Collective classification in network data. AI magazine, 29(3):93, 2008.\n[2]: Yang, Zhilin, Cohen, William, and Salakhutdinov, Ruslan. Revisiting Semi-Supervised Learning with Graph Embeddings., 2016.\n"
  },
  {
    "people": [
      "Weston",
      "Weston",
      "Weston"
    ],
    "review": "The authors introduce a semi-supervised method for neural networks, inspired from label propagation.\n\nThe method appears to be exactly the same than the one proposed in (Weston et al, 2008) (the authors cite the 2012 paper). The optimized objective function in eq (4) is exactly the same than eq (9) in (Weston et al, 2008).\n\nAs possible novelty, the authors propose to use the adjacency matrix as input to the neural network, when there are no other features, and show success on the BlogCatalog dataset.\n\nExperiments on text classification use neighbors according to word2vec average embedding to build the adjacency matrix. Top reported accuracies are not convincing compared to (Zhang et al, 2015) reported performance. Last experiment is on semantic intent classification, which a custom dataset; neighbors are also found according to a word2vec metric.\n\nIn summary, the paper propose few applications to the original (Weston et al, 2008) paper. It rebrands the algorithm under a new name, and does not bring any scientific novelty, and the experimental section lacks existing baselines to be convincing."
  },
  {
    "people": [
      "Weston",
      "Weston"
    ],
    "review": "This paper proposes the Neural Graph Machine that adds in graph regularization on neural network hidden representations to improve network learning and take the graph structure into account.  The proposed model, however, is almost identical to that of Weston et al. 2012.\n\nAs the authors have clarified in the answers to the questions, there are a few new things that previous work did not do:\n\n1. they showed that graph augmented training for a range of different types of networks, including FF, CNN, RNNs etc. and works on a range of problems.\n2. graphs help to train better networks, e.g. 3 layer CNN with graphs does as well as than 9 layer CNNs\n3. graph augmented training works on a variety of different kinds of graphs.\n\nHowever, all these points mentioned above seems to simply be different applications of the graph augmented training idea, and observations made during the applications.  I think it is therefore not proper to call the proposed model a novel model with a new name Neural Graph Machine, but rather making it clear in the paper that this is an empirical study of the model proposed by Weston et al. 2012 to different problems would be more acceptable."
  },
  {
    "people": [
      "Jacob",
      "Weston"
    ],
    "review": "The paper proposes a model that aims at learning to label nodes of graph in a semi-supervised setting. The idea of the model is based on the use of the graph structure to regularize the representations learned at the node levels. Experimental results are provided on different tasks\n\nThe underlying idea of this paper (graph regularization) has been already explored in different papers \u2013 e.g 'Learning latent representations of nodes for classifying in heterogeneous social networks' [Jacob et al. 2014],   [Weston et al 2012] where a real graph structure is used instead of a built one. The experiments lack of strong comparisons with other graph models (e.g Iterative Classification, 'Learning from labeled and unlabeled data on a directed graph', ...). So the novelty of the paper and the experimental protocol are not strong enough to accpet the paper.\n\nPros:\n* Learning over graph is an important topic\n\nCons:\n* Many existing approaches have already exploited the same types of ideas, resulting in very close models\n* Lack of comparison w.r.t existing models\n"
  },
  {
    "people": [
      "Dauphin"
    ],
    "review": "This paper proposes a novel method for accelerating optimization near saddle points. The basic idea is to repel the current parameter vector from a running average of recent parameter values. This method is shown to optimize faster than a variety of other methods in a variety of datasets and architectures.\n\nOn the surface, the proposed method seems extremely close to momentum. It would be very valuable to think of a clear diagram illustrating how it differs from momentum and why it might be better near a saddle point. The illustration of better convergence on the toy saddle example is not what I mean here\u2014optimization speed comparisons are always difficult due to the many details and hyper parameters involved, so seeing it work faster in one specific application is not as useful as a conceptual diagram which shows a critical case where CPN will behave differently from\u2014and clearly qualitatively better than\u2014momentum.\n\nAnother way of getting at the relationship to momentum would be to try to find a form for R_t(f) that yields the exact momentum update. You could then compare this with the R_t(f) used in CPN.\n\nThe overly general notation $\\phi(W,W)$ etc should be dropped\u2014Eqn 8 is enough.\n\nThe theoretical results (Eqn 1 and Thm 1) should be removed, they are irrelevant until the joint density can be specified.\n\nExperimentally, it would be valuable to standardize the results to allow comparison to other methods. For instance, recreating Figure 4 of Dauphin et al, but engaging the CPN method rather than SFN, would clearly demonstrate that CPN can escape something that momentum cannot.\n\nI think the idea here is potentially very valuable, but needs more rigorous comparison and a clear relation to momentum and other work."
  },
  {
    "people": [
      "Dauphin"
    ],
    "review": "This paper proposes a novel method for accelerating optimization near saddle points. The basic idea is to repel the current parameter vector from a running average of recent parameter values. This method is shown to optimize faster than a variety of other methods in a variety of datasets and architectures.\n\nOn the surface, the proposed method seems extremely close to momentum. It would be very valuable to think of a clear diagram illustrating how it differs from momentum and why it might be better near a saddle point. The illustration of better convergence on the toy saddle example is not what I mean here\u2014optimization speed comparisons are always difficult due to the many details and hyper parameters involved, so seeing it work faster in one specific application is not as useful as a conceptual diagram which shows a critical case where CPN will behave differently from\u2014and clearly qualitatively better than\u2014momentum.\n\nAnother way of getting at the relationship to momentum would be to try to find a form for R_t(f) that yields the exact momentum update. You could then compare this with the R_t(f) used in CPN.\n\nThe overly general notation $\\phi(W,W)$ etc should be dropped\u2014Eqn 8 is enough.\n\nThe theoretical results (Eqn 1 and Thm 1) should be removed, they are irrelevant until the joint density can be specified.\n\nExperimentally, it would be valuable to standardize the results to allow comparison to other methods. For instance, recreating Figure 4 of Dauphin et al, but engaging the CPN method rather than SFN, would clearly demonstrate that CPN can escape something that momentum cannot.\n\nI think the idea here is potentially very valuable, but needs more rigorous comparison and a clear relation to momentum and other work."
  },
  {
    "people": [
      "Gil",
      "Telgarsky",
      "Dasgupta"
    ],
    "review": "This paper shows:\n\n  1. Easy, constructive proofs to derive e-error upper-bounds on neural networks with O(log 1/e) layers and O(log 1/e) ReLU units.\n  2. Extensions of the previous results to more general function classes, such as smooth or vector-valued functions.\n  3. Lower bounds on the neural network size, as a function of its number of layers. The lower bound reveals the need of exponentially many more units to approximate functions using shallow architectures.\n\nThe paper is well written and easy to follow. The technical content, including the proofs in the Appendix, look correct. Although the proof techniques are simple (and are sometimes modifications of arguments by Gil, Telgarsky, or Dasgupta), they are brought together in a coherent manner to produce sharp results. Therefore, I am leaning toward acceptance."
  },
  {
    "people": [
      "Gil",
      "Telgarsky",
      "Dasgupta"
    ],
    "review": "This paper shows:\n\n  1. Easy, constructive proofs to derive e-error upper-bounds on neural networks with O(log 1/e) layers and O(log 1/e) ReLU units.\n  2. Extensions of the previous results to more general function classes, such as smooth or vector-valued functions.\n  3. Lower bounds on the neural network size, as a function of its number of layers. The lower bound reveals the need of exponentially many more units to approximate functions using shallow architectures.\n\nThe paper is well written and easy to follow. The technical content, including the proofs in the Appendix, look correct. Although the proof techniques are simple (and are sometimes modifications of arguments by Gil, Telgarsky, or Dasgupta), they are brought together in a coherent manner to produce sharp results. Therefore, I am leaning toward acceptance."
  },
  {
    "people": [
      "Gil",
      "Telgarsky",
      "Dasgupta"
    ],
    "review": "This paper shows:\n\n  1. Easy, constructive proofs to derive e-error upper-bounds on neural networks with O(log 1/e) layers and O(log 1/e) ReLU units.\n  2. Extensions of the previous results to more general function classes, such as smooth or vector-valued functions.\n  3. Lower bounds on the neural network size, as a function of its number of layers. The lower bound reveals the need of exponentially many more units to approximate functions using shallow architectures.\n\nThe paper is well written and easy to follow. The technical content, including the proofs in the Appendix, look correct. Although the proof techniques are simple (and are sometimes modifications of arguments by Gil, Telgarsky, or Dasgupta), they are brought together in a coherent manner to produce sharp results. Therefore, I am leaning toward acceptance."
  },
  {
    "people": [
      "Gil",
      "Telgarsky",
      "Dasgupta"
    ],
    "review": "This paper shows:\n\n  1. Easy, constructive proofs to derive e-error upper-bounds on neural networks with O(log 1/e) layers and O(log 1/e) ReLU units.\n  2. Extensions of the previous results to more general function classes, such as smooth or vector-valued functions.\n  3. Lower bounds on the neural network size, as a function of its number of layers. The lower bound reveals the need of exponentially many more units to approximate functions using shallow architectures.\n\nThe paper is well written and easy to follow. The technical content, including the proofs in the Appendix, look correct. Although the proof techniques are simple (and are sometimes modifications of arguments by Gil, Telgarsky, or Dasgupta), they are brought together in a coherent manner to produce sharp results. Therefore, I am leaning toward acceptance."
  },
  {
    "people": [
      "Pathak",
      "Bell",
      "K. Bala",
      "N. Snavely"
    ],
    "review": "The paper describes a network architecture for inverse problems in computer vision. Example inverse problems considered are image inpainting, computing intrinsic image decomposition and foreground/background separation.\nThe architecture is composed of (i) a generator that produces target (latent) output (such as foreground / background regions), \n(ii) renderer that composes that latent output back to the image that can be compared with the input to measure reconstruction error, \nand (iii) adversarial prior that ensures the target output (latent) image respects a certain image statistics.\n\nStrong  points.\n- The proposed architecture with memory database is interesting and appears to be novel. \n\nWeak points:\n- Experimental results are only proof-of-concept in toy set-ups and do not clearly demonstrate benefits of the proposed architecture.\n- It is unclear whether the memory retrieval engine that retrieves images based on L2 distance on pixel values is going generalize to other more realistic scenarios. \n- Clarity. The clarity of explanation can be also improved (see below).\n\n\nDetailed evaluation.\n\nOriginality:\n- The novelty of this work lies in the (iii) adversarial prior that places an adversarial loss between the generated latent output and a single image retrieved from a large unlabelled database of target output examples (called memory). The adversarial prior has a convolutional form matching local image statistics, rather than the entire image.  The particular form of network architecture with the memory-based fully convolutional adversarial loss appears to be novel and potentially interesting.\n\n- Motivation for the Architecture. The weakest point of the proposed architecture is the \"Memory retrieval engine\" R (section 2.4),\nwhere images are retrieved from the memory by measuring L2 distance on pixel intensities. While this maybe ok for simple problems considered in this work, it is unclear how this can generalize to other more complicated datasets and problems.  \nThis should be better discussed, better justified and ideally results in some more realistic set-up shown (see below).\n\n\nQuality:\n- Experiments. Results are shown for inpainting of MNIST digits, intrinsic image decomposition on the MIT intrinsic image database, and figure/ground layer extraction on the synthesized dataset of 3D chairs rendered onto background from real photographs.  \n The experimental validation of the model is not very strong and proof-of-concept only. All the experiments are performed in simplified toy set-ups. The MNIST digit inpainting is far from current state-of-the-art on image inpainting in real photographs (see e.g. Pathak et al., 2016). The foreground background separation is done on  only synthetically generated test data. Even for intrinsic image demposition problem there is now relatively large-scale dataset of (Bell et al., 2014), see the citation below.  \n\nWhile this is probably ok for the ICLR paper, it diminishes the significance of the work. Is this model going to be useful in a real settings? One possibility to address this would be to focus on one of the problems and show results on a challenging state-of-the-art data. It would be great to see the benefits of the memory database. \n\nS. Bell, K. Bala, and N. Snavely. Intrinsic images in the wild.\nACM Transactions on Graphics, 33(4):159, 2014.\n\nClarity:\n- The clarity of the writing can be improved. I found some of the terminology of the paper, specially the \u201cimagination\u201d and \u201cmemory\u201d confusing. From figure 2, it is not clear how the \u201cmemories\u201d for the given input image are obtained, which also took me some time to understand.\n\n- To help understand the proposed architecture, it would be useful to draw an illustration of what is happening in the \"feature space\u201d, similar in spirit e.g. to figure 2 in"
  },
  {
    "people": [
      "Pathak",
      "Bell",
      "K. Bala",
      "N. Snavely"
    ],
    "review": "The paper describes a network architecture for inverse problems in computer vision. Example inverse problems considered are image inpainting, computing intrinsic image decomposition and foreground/background separation.\nThe architecture is composed of (i) a generator that produces target (latent) output (such as foreground / background regions), \n(ii) renderer that composes that latent output back to the image that can be compared with the input to measure reconstruction error, \nand (iii) adversarial prior that ensures the target output (latent) image respects a certain image statistics.\n\nStrong  points.\n- The proposed architecture with memory database is interesting and appears to be novel. \n\nWeak points:\n- Experimental results are only proof-of-concept in toy set-ups and do not clearly demonstrate benefits of the proposed architecture.\n- It is unclear whether the memory retrieval engine that retrieves images based on L2 distance on pixel values is going generalize to other more realistic scenarios. \n- Clarity. The clarity of explanation can be also improved (see below).\n\n\nDetailed evaluation.\n\nOriginality:\n- The novelty of this work lies in the (iii) adversarial prior that places an adversarial loss between the generated latent output and a single image retrieved from a large unlabelled database of target output examples (called memory). The adversarial prior has a convolutional form matching local image statistics, rather than the entire image.  The particular form of network architecture with the memory-based fully convolutional adversarial loss appears to be novel and potentially interesting.\n\n- Motivation for the Architecture. The weakest point of the proposed architecture is the \"Memory retrieval engine\" R (section 2.4),\nwhere images are retrieved from the memory by measuring L2 distance on pixel intensities. While this maybe ok for simple problems considered in this work, it is unclear how this can generalize to other more complicated datasets and problems.  \nThis should be better discussed, better justified and ideally results in some more realistic set-up shown (see below).\n\n\nQuality:\n- Experiments. Results are shown for inpainting of MNIST digits, intrinsic image decomposition on the MIT intrinsic image database, and figure/ground layer extraction on the synthesized dataset of 3D chairs rendered onto background from real photographs.  \n The experimental validation of the model is not very strong and proof-of-concept only. All the experiments are performed in simplified toy set-ups. The MNIST digit inpainting is far from current state-of-the-art on image inpainting in real photographs (see e.g. Pathak et al., 2016). The foreground background separation is done on  only synthetically generated test data. Even for intrinsic image demposition problem there is now relatively large-scale dataset of (Bell et al., 2014), see the citation below.  \n\nWhile this is probably ok for the ICLR paper, it diminishes the significance of the work. Is this model going to be useful in a real settings? One possibility to address this would be to focus on one of the problems and show results on a challenging state-of-the-art data. It would be great to see the benefits of the memory database. \n\nS. Bell, K. Bala, and N. Snavely. Intrinsic images in the wild.\nACM Transactions on Graphics, 33(4):159, 2014.\n\nClarity:\n- The clarity of the writing can be improved. I found some of the terminology of the paper, specially the \u201cimagination\u201d and \u201cmemory\u201d confusing. From figure 2, it is not clear how the \u201cmemories\u201d for the given input image are obtained, which also took me some time to understand.\n\n- To help understand the proposed architecture, it would be useful to draw an illustration of what is happening in the \"feature space\u201d, similar in spirit e.g. to figure 2 in "
  },
  {
    "people": [
      "Pathak",
      "Bell",
      "K. Bala",
      "N. Snavely"
    ],
    "review": "The paper describes a network architecture for inverse problems in computer vision. Example inverse problems considered are image inpainting, computing intrinsic image decomposition and foreground/background separation.\nThe architecture is composed of (i) a generator that produces target (latent) output (such as foreground / background regions), \n(ii) renderer that composes that latent output back to the image that can be compared with the input to measure reconstruction error, \nand (iii) adversarial prior that ensures the target output (latent) image respects a certain image statistics.\n\nStrong  points.\n- The proposed architecture with memory database is interesting and appears to be novel. \n\nWeak points:\n- Experimental results are only proof-of-concept in toy set-ups and do not clearly demonstrate benefits of the proposed architecture.\n- It is unclear whether the memory retrieval engine that retrieves images based on L2 distance on pixel values is going generalize to other more realistic scenarios. \n- Clarity. The clarity of explanation can be also improved (see below).\n\n\nDetailed evaluation.\n\nOriginality:\n- The novelty of this work lies in the (iii) adversarial prior that places an adversarial loss between the generated latent output and a single image retrieved from a large unlabelled database of target output examples (called memory). The adversarial prior has a convolutional form matching local image statistics, rather than the entire image.  The particular form of network architecture with the memory-based fully convolutional adversarial loss appears to be novel and potentially interesting.\n\n- Motivation for the Architecture. The weakest point of the proposed architecture is the \"Memory retrieval engine\" R (section 2.4),\nwhere images are retrieved from the memory by measuring L2 distance on pixel intensities. While this maybe ok for simple problems considered in this work, it is unclear how this can generalize to other more complicated datasets and problems.  \nThis should be better discussed, better justified and ideally results in some more realistic set-up shown (see below).\n\n\nQuality:\n- Experiments. Results are shown for inpainting of MNIST digits, intrinsic image decomposition on the MIT intrinsic image database, and figure/ground layer extraction on the synthesized dataset of 3D chairs rendered onto background from real photographs.  \n The experimental validation of the model is not very strong and proof-of-concept only. All the experiments are performed in simplified toy set-ups. The MNIST digit inpainting is far from current state-of-the-art on image inpainting in real photographs (see e.g. Pathak et al., 2016). The foreground background separation is done on  only synthetically generated test data. Even for intrinsic image demposition problem there is now relatively large-scale dataset of (Bell et al., 2014), see the citation below.  \n\nWhile this is probably ok for the ICLR paper, it diminishes the significance of the work. Is this model going to be useful in a real settings? One possibility to address this would be to focus on one of the problems and show results on a challenging state-of-the-art data. It would be great to see the benefits of the memory database. \n\nS. Bell, K. Bala, and N. Snavely. Intrinsic images in the wild.\nACM Transactions on Graphics, 33(4):159, 2014.\n\nClarity:\n- The clarity of the writing can be improved. I found some of the terminology of the paper, specially the \u201cimagination\u201d and \u201cmemory\u201d confusing. From figure 2, it is not clear how the \u201cmemories\u201d for the given input image are obtained, which also took me some time to understand.\n\n- To help understand the proposed architecture, it would be useful to draw an illustration of what is happening in the \"feature space\u201d, similar in spirit e.g. to figure 2 in"
  },
  {
    "people": [
      "Pathak",
      "Bell",
      "K. Bala",
      "N. Snavely"
    ],
    "review": "The paper describes a network architecture for inverse problems in computer vision. Example inverse problems considered are image inpainting, computing intrinsic image decomposition and foreground/background separation.\nThe architecture is composed of (i) a generator that produces target (latent) output (such as foreground / background regions), \n(ii) renderer that composes that latent output back to the image that can be compared with the input to measure reconstruction error, \nand (iii) adversarial prior that ensures the target output (latent) image respects a certain image statistics.\n\nStrong  points.\n- The proposed architecture with memory database is interesting and appears to be novel. \n\nWeak points:\n- Experimental results are only proof-of-concept in toy set-ups and do not clearly demonstrate benefits of the proposed architecture.\n- It is unclear whether the memory retrieval engine that retrieves images based on L2 distance on pixel values is going generalize to other more realistic scenarios. \n- Clarity. The clarity of explanation can be also improved (see below).\n\n\nDetailed evaluation.\n\nOriginality:\n- The novelty of this work lies in the (iii) adversarial prior that places an adversarial loss between the generated latent output and a single image retrieved from a large unlabelled database of target output examples (called memory). The adversarial prior has a convolutional form matching local image statistics, rather than the entire image.  The particular form of network architecture with the memory-based fully convolutional adversarial loss appears to be novel and potentially interesting.\n\n- Motivation for the Architecture. The weakest point of the proposed architecture is the \"Memory retrieval engine\" R (section 2.4),\nwhere images are retrieved from the memory by measuring L2 distance on pixel intensities. While this maybe ok for simple problems considered in this work, it is unclear how this can generalize to other more complicated datasets and problems.  \nThis should be better discussed, better justified and ideally results in some more realistic set-up shown (see below).\n\n\nQuality:\n- Experiments. Results are shown for inpainting of MNIST digits, intrinsic image decomposition on the MIT intrinsic image database, and figure/ground layer extraction on the synthesized dataset of 3D chairs rendered onto background from real photographs.  \n The experimental validation of the model is not very strong and proof-of-concept only. All the experiments are performed in simplified toy set-ups. The MNIST digit inpainting is far from current state-of-the-art on image inpainting in real photographs (see e.g. Pathak et al., 2016). The foreground background separation is done on  only synthetically generated test data. Even for intrinsic image demposition problem there is now relatively large-scale dataset of (Bell et al., 2014), see the citation below.  \n\nWhile this is probably ok for the ICLR paper, it diminishes the significance of the work. Is this model going to be useful in a real settings? One possibility to address this would be to focus on one of the problems and show results on a challenging state-of-the-art data. It would be great to see the benefits of the memory database. \n\nS. Bell, K. Bala, and N. Snavely. Intrinsic images in the wild.\nACM Transactions on Graphics, 33(4):159, 2014.\n\nClarity:\n- The clarity of the writing can be improved. I found some of the terminology of the paper, specially the \u201cimagination\u201d and \u201cmemory\u201d confusing. From figure 2, it is not clear how the \u201cmemories\u201d for the given input image are obtained, which also took me some time to understand.\n\n- To help understand the proposed architecture, it would be useful to draw an illustration of what is happening in the \"feature space\u201d, similar in spirit e.g. to figure 2 in "
  },
  {
    "people": [
      "Xue",
      "Li",
      "Gong"
    ],
    "review": "This paper proposes to reduce model size and evaluation time of deep CNN models on mobile devices by converting multiple layers into single layer and then retraining the converted model. The paper showed that the computation time can be reduced by 3x to 5x with only 0.4% accuracy loss on a specific model.\n\nReducing model sizes and speeding up model evaluation are important in many applications. I have several concerns:\n\n1. There are many techniques that can reduce model sizes. For example, it has been shown by several groups that using the teacher-student approach, people can achieve the same and sometimes even better accuracy than the teacher (big model) using a much smaller model. However, this paper does not compare any one of them.\n2. The technique proposed in this paper is limited in its applicability since it's designed specifically for the models discussed in the paper. \n3. Replacing several layers with single layer is a relatively standard procedure. For example, the mean variance normalization layer and batch normalization layer can all be absorbed without retraining or losing accuracy.\n\nBTW, the DNN low-rank approximation technique was first proposed in speech recognition. e.g., \n\nXue, J., Li, J. and Gong, Y., 2013, August. Restructuring of deep neural network acoustic models with singular value decomposition. In INTERSPEECH (pp. 2365-2369)."
  },
  {
    "people": [
      "Xue",
      "Li",
      "Gong"
    ],
    "review": "This paper proposes to reduce model size and evaluation time of deep CNN models on mobile devices by converting multiple layers into single layer and then retraining the converted model. The paper showed that the computation time can be reduced by 3x to 5x with only 0.4% accuracy loss on a specific model.\n\nReducing model sizes and speeding up model evaluation are important in many applications. I have several concerns:\n\n1. There are many techniques that can reduce model sizes. For example, it has been shown by several groups that using the teacher-student approach, people can achieve the same and sometimes even better accuracy than the teacher (big model) using a much smaller model. However, this paper does not compare any one of them.\n2. The technique proposed in this paper is limited in its applicability since it's designed specifically for the models discussed in the paper. \n3. Replacing several layers with single layer is a relatively standard procedure. For example, the mean variance normalization layer and batch normalization layer can all be absorbed without retraining or losing accuracy.\n\nBTW, the DNN low-rank approximation technique was first proposed in speech recognition. e.g., \n\nXue, J., Li, J. and Gong, Y., 2013, August. Restructuring of deep neural network acoustic models with singular value decomposition. In INTERSPEECH (pp. 2365-2369).\n"
  },
  {
    "people": [
      "Xue",
      "Li",
      "Gong"
    ],
    "review": "This paper proposes to reduce model size and evaluation time of deep CNN models on mobile devices by converting multiple layers into single layer and then retraining the converted model. The paper showed that the computation time can be reduced by 3x to 5x with only 0.4% accuracy loss on a specific model.\n\nReducing model sizes and speeding up model evaluation are important in many applications. I have several concerns:\n\n1. There are many techniques that can reduce model sizes. For example, it has been shown by several groups that using the teacher-student approach, people can achieve the same and sometimes even better accuracy than the teacher (big model) using a much smaller model. However, this paper does not compare any one of them.\n2. The technique proposed in this paper is limited in its applicability since it's designed specifically for the models discussed in the paper. \n3. Replacing several layers with single layer is a relatively standard procedure. For example, the mean variance normalization layer and batch normalization layer can all be absorbed without retraining or losing accuracy.\n\nBTW, the DNN low-rank approximation technique was first proposed in speech recognition. e.g., \n\nXue, J., Li, J. and Gong, Y., 2013, August. Restructuring of deep neural network acoustic models with singular value decomposition. In INTERSPEECH (pp. 2365-2369)."
  },
  {
    "people": [
      "Xue",
      "Li",
      "Gong"
    ],
    "review": "This paper proposes to reduce model size and evaluation time of deep CNN models on mobile devices by converting multiple layers into single layer and then retraining the converted model. The paper showed that the computation time can be reduced by 3x to 5x with only 0.4% accuracy loss on a specific model.\n\nReducing model sizes and speeding up model evaluation are important in many applications. I have several concerns:\n\n1. There are many techniques that can reduce model sizes. For example, it has been shown by several groups that using the teacher-student approach, people can achieve the same and sometimes even better accuracy than the teacher (big model) using a much smaller model. However, this paper does not compare any one of them.\n2. The technique proposed in this paper is limited in its applicability since it's designed specifically for the models discussed in the paper. \n3. Replacing several layers with single layer is a relatively standard procedure. For example, the mean variance normalization layer and batch normalization layer can all be absorbed without retraining or losing accuracy.\n\nBTW, the DNN low-rank approximation technique was first proposed in speech recognition. e.g., \n\nXue, J., Li, J. and Gong, Y., 2013, August. Restructuring of deep neural network acoustic models with singular value decomposition. In INTERSPEECH (pp. 2365-2369).\n"
  },
  {
    "people": [
      "Mikolov",
      "Mikolov"
    ],
    "review": "This paper proposes a character-aware attention residual network for sentence embedding. Several text classification tasks are used to evaluate the effectiveness of the proposed model. On two of the three tasks, the residual network outforms a few baselines, but couldn't beat the simple TFIDF-SVM on the last one.\n\nThis work is not novel enough. Character information has been applied in many previously published work, as cited by the authors. Residual network is also not new.\n\nWhy not testing the model on a few more widely used datasets for short text classification, such as TREC? More competitive baselines can be compared to. Also, it's not clear how the \"Question\" dataset was created and which domain it is.\n\nLast, it is surprising that the format of citations throughout the paper is all wrong. \n\nFor example:\nlike Word2Vec Mikolov et al. (2013)\n->\nlike Word2Vec (Mikolov et al., 2013)\n\nThe citations can't just mix with the normal text. Please refer to other published papers."
  },
  {
    "people": [
      "Mikolov",
      "Mikolov"
    ],
    "review": "This paper proposes a character-aware attention residual network for sentence embedding. Several text classification tasks are used to evaluate the effectiveness of the proposed model. On two of the three tasks, the residual network outforms a few baselines, but couldn't beat the simple TFIDF-SVM on the last one.\n\nThis work is not novel enough. Character information has been applied in many previously published work, as cited by the authors. Residual network is also not new.\n\nWhy not testing the model on a few more widely used datasets for short text classification, such as TREC? More competitive baselines can be compared to. Also, it's not clear how the \"Question\" dataset was created and which domain it is.\n\nLast, it is surprising that the format of citations throughout the paper is all wrong. \n\nFor example:\nlike Word2Vec Mikolov et al. (2013)\n->\nlike Word2Vec (Mikolov et al., 2013)\n\nThe citations can't just mix with the normal text. Please refer to other published papers."
  },
  {
    "people": [
      "Xin Zheng",
      "Xin Zheng"
    ],
    "review": "Hi Authors,\n\nYou seem to have submitted two of the same paper? Pls advise which is the correct one\n\nCharacter-aware Attention Residual Network for Sentence Representation\nXin Zheng, Zhenzhou Wu\n5 Nov 2016\n\nCHARACTER-AWARE RESIDUAL NETWORK FOR SENTENCE REPRESENTATION\nXin Zheng, Zhenzhou Wu\n4 Nov 2016\n"
  },
  {
    "people": [
      "Mikolov",
      "Mikolov"
    ],
    "review": "This paper proposes a character-aware attention residual network for sentence embedding. Several text classification tasks are used to evaluate the effectiveness of the proposed model. On two of the three tasks, the residual network outforms a few baselines, but couldn't beat the simple TFIDF-SVM on the last one.\n\nThis work is not novel enough. Character information has been applied in many previously published work, as cited by the authors. Residual network is also not new.\n\nWhy not testing the model on a few more widely used datasets for short text classification, such as TREC? More competitive baselines can be compared to. Also, it's not clear how the \"Question\" dataset was created and which domain it is.\n\nLast, it is surprising that the format of citations throughout the paper is all wrong. \n\nFor example:\nlike Word2Vec Mikolov et al. (2013)\n->\nlike Word2Vec (Mikolov et al., 2013)\n\nThe citations can't just mix with the normal text. Please refer to other published papers."
  },
  {
    "people": [
      "Mikolov",
      "Mikolov"
    ],
    "review": "This paper proposes a character-aware attention residual network for sentence embedding. Several text classification tasks are used to evaluate the effectiveness of the proposed model. On two of the three tasks, the residual network outforms a few baselines, but couldn't beat the simple TFIDF-SVM on the last one.\n\nThis work is not novel enough. Character information has been applied in many previously published work, as cited by the authors. Residual network is also not new.\n\nWhy not testing the model on a few more widely used datasets for short text classification, such as TREC? More competitive baselines can be compared to. Also, it's not clear how the \"Question\" dataset was created and which domain it is.\n\nLast, it is surprising that the format of citations throughout the paper is all wrong. \n\nFor example:\nlike Word2Vec Mikolov et al. (2013)\n->\nlike Word2Vec (Mikolov et al., 2013)\n\nThe citations can't just mix with the normal text. Please refer to other published papers."
  },
  {
    "people": [
      "Xin Zheng",
      "Xin Zheng"
    ],
    "review": "Hi Authors,\n\nYou seem to have submitted two of the same paper? Pls advise which is the correct one\n\nCharacter-aware Attention Residual Network for Sentence Representation\nXin Zheng, Zhenzhou Wu\n5 Nov 2016\n\nCHARACTER-AWARE RESIDUAL NETWORK FOR SENTENCE REPRESENTATION\nXin Zheng, Zhenzhou Wu\n4 Nov 2016\n"
  },
  {
    "people": [
      "Stein",
      "Stein",
      "Lavengin",
      "Stein",
      "Wang",
      "Dilin",
      "Liu",
      "Qiang"
    ],
    "review": "The authors propose methods for wild variational inference, in which the\nvariational approximating distribution may not have a directly accessible\ndensity function. Their approach is based on the Stain's operator, which acts\non a given function and returns a zero mean function with respect to a given\ndensity function which may not be normalized.\n\nQuality:\n\nThe derviations seem to be technically sound. However, my impression is that\nthe authors are not very careful and honest at evaluating both the strengths\nand weaknesses of the proposed work. How does the method perform in cases in\nwhich the distribution to be approximated is high dimensional? The logistic\nregression problem considered only has 54 dimensions. How would this method\nperform in a neural network in which the number of weights is goint to be way\nmuch larger? The logistic regression model is rather simple and its posterior\nwill be likely to be close to Gaussian. How would the method perform in more\ncomplicated posteriors such as the ones of Bayesia neural networks?\n\nClarity:\n\nThe paper is not clearly written. I found it very really hard to follow and not\nfocused. The authors describe way too many methods: 1) Stein's variational\ngradient descent (SVGD), 2) Amortized SVGD, 3) Kernelized Stein discrepancy\n(KSD), 4) Lavengin inference network, not to mention the introduction to\nStein's discrepancy. I found very difficult to indentify the clear\ncontributions of the paper with so many different techniques.\n\nOriginality:\n\nIt is not clear how original the proposed contributions are. The first of the\nproposed methods is also discussed in\n\nWang, Dilin and Liu, Qiang. Learning to draw samples: With application to\namortized mle for generative adversarial learning. Submitted to ICLR 2017, 2016\n\nHow does this work differ from that one?\n\nSignificance:\n\nIt is very hard to evaluate the importance of proposed methods. The authors\nonly report results on a 1d toy problem with a mixture of Gaussians and on a\nlogistic regression model with dimension 54. In both cases the distributions to\nbe approximated are very simple and of low dimension. In the regression case\nthe posterior is also likely to be close to Gaussian and therefore not clear\nwhat advances the proposed method would provide with respect to other more\nsimple approaches. The authors do not compare with simple variational\napproaches based on Gaussian approximations."
  },
  {
    "people": [
      "Liu",
      "Ranganath",
      "Ranganath",
      "Ranganath",
      "Salimans",
      "Salimans",
      "Salimans",
      "Rezende",
      "Mohamed",
      "Tran",
      "Ranganath",
      "Tran",
      "Ranganath",
      "Salimans",
      "Ranganath"
    ],
    "review": "The authors propose two variational methods based on the theme of posterior approximations which may not have a tractable density. The first is from another ICLR submission on \"amortized SVGD\" (Wang and Liu, 2016), where here the innovation is in using SGLD as the inference network. The second is from a NIPS paper (Ranganath et al., 2016) on minimizing the Stein divergence with a parametric approximating family, where here the innovation is in defining their test functions to be an RKHS, obtaining an analytic solution to the inner optimization problem.\n\nThe methodology is incremental. Everything up to Section 3.2 is essentially motivation, background, or related work. The notion of a \"wild variational approximation\" was already defined in Ranganath et al. (2016), termed a \"variational program\". It would be useful for the authors to comment on the difference, if any.\n\nSection 3.2 is at first interesting because it analytically solves the maximum problem that is faced in Ranganath et al. (2016). However, this requires use of a kernel which will certainly not scale in high dimensions, so it is then equivalent in practice to having chosen a very simple test function family. To properly scale to high dimensions would require a deeper kernel and also learning its parameters; this is not any easier than parameterizing the test function family as a neural network to begin with, which Ranganath et al. (2016) do.\n\nSection 4 introduces a Langevin inference network, which essentially chooses the variational approximation as an evolving sequence of Markov transition operators as in Salimans et al. (2015). I had trouble understanding this for a while because I could not understand what they mean by inference network. None of it is amortized in the usual inference network sense, which is that the parameters are given by the output of a neural network. Here, the authors simple define global parameters of the SGLD chain which are used across all the latent variables (which is strictly worse?). (What then makes it an \"inference network\"?) Is this not the variational approximation used in Salimans et al. (2015), but using a different objective to train it?\n\nThe experiments are limited, on a toy mixture of Gaussians posterior and Bayesian logistic regression. None of this addresses the problems one might suspect on high-dimensional and real data, such as the lack of scalability for the kernel, the comparison to Salimans et al. (2015) for the Langevin variational approximation, and any note of runtime or difficulty of training.\n\nMinor comments\n\n+ It's not clear if the authors understood previous work on expressive variational families or inference networks. For example, they argue Rezende & Mohamed, 2015b; Tran et al., 2015; Ranganath et al., 2015 require handcrafted inference networks. However, all of them assume use of any neural network for amortized inference. None of them even require an inference network. Perhaps the authors mean handcrafted posterior approximations, which to some extent is true; however, the three mentioned are all algorithmic in nature: in Rezende & Mohamed (2015), the main decision choice is the flow length; Tran et al. (2015), the size of the variational data; Ranganath et al. (2015), the flow length on the auxiliary variable space. Each works well on different problems, but this is also true of variational objectives which admit intractable q (as the latter two consider, as does Salimans et al. (2015)). The paper's motivation could be better explained, and perhaps the authors could be clearer on what they mean by inference network.\n+ I also recommend the authors not term a variational inference method based on the class of approximating family. While black box variational inference in Ranganath et al. (2014) assumes a mean-field family, the term itself has been used in the literature to mean any variational method that imposes few constraints on the model class."
  },
  {
    "people": [
      "Stein",
      "Stein",
      "Lavengin",
      "Stein",
      "Wang",
      "Dilin",
      "Liu",
      "Qiang"
    ],
    "review": "The authors propose methods for wild variational inference, in which the\nvariational approximating distribution may not have a directly accessible\ndensity function. Their approach is based on the Stain's operator, which acts\non a given function and returns a zero mean function with respect to a given\ndensity function which may not be normalized.\n\nQuality:\n\nThe derviations seem to be technically sound. However, my impression is that\nthe authors are not very careful and honest at evaluating both the strengths\nand weaknesses of the proposed work. How does the method perform in cases in\nwhich the distribution to be approximated is high dimensional? The logistic\nregression problem considered only has 54 dimensions. How would this method\nperform in a neural network in which the number of weights is goint to be way\nmuch larger? The logistic regression model is rather simple and its posterior\nwill be likely to be close to Gaussian. How would the method perform in more\ncomplicated posteriors such as the ones of Bayesia neural networks?\n\nClarity:\n\nThe paper is not clearly written. I found it very really hard to follow and not\nfocused. The authors describe way too many methods: 1) Stein's variational\ngradient descent (SVGD), 2) Amortized SVGD, 3) Kernelized Stein discrepancy\n(KSD), 4) Lavengin inference network, not to mention the introduction to\nStein's discrepancy. I found very difficult to indentify the clear\ncontributions of the paper with so many different techniques.\n\nOriginality:\n\nIt is not clear how original the proposed contributions are. The first of the\nproposed methods is also discussed in\n\nWang, Dilin and Liu, Qiang. Learning to draw samples: With application to\namortized mle for generative adversarial learning. Submitted to ICLR 2017, 2016\n\nHow does this work differ from that one?\n\nSignificance:\n\nIt is very hard to evaluate the importance of proposed methods. The authors\nonly report results on a 1d toy problem with a mixture of Gaussians and on a\nlogistic regression model with dimension 54. In both cases the distributions to\nbe approximated are very simple and of low dimension. In the regression case\nthe posterior is also likely to be close to Gaussian and therefore not clear\nwhat advances the proposed method would provide with respect to other more\nsimple approaches. The authors do not compare with simple variational\napproaches based on Gaussian approximations."
  },
  {
    "people": [
      "Stein",
      "Stein",
      "Lavengin",
      "Stein",
      "Wang",
      "Dilin",
      "Liu",
      "Qiang"
    ],
    "review": "The authors propose methods for wild variational inference, in which the\nvariational approximating distribution may not have a directly accessible\ndensity function. Their approach is based on the Stain's operator, which acts\non a given function and returns a zero mean function with respect to a given\ndensity function which may not be normalized.\n\nQuality:\n\nThe derviations seem to be technically sound. However, my impression is that\nthe authors are not very careful and honest at evaluating both the strengths\nand weaknesses of the proposed work. How does the method perform in cases in\nwhich the distribution to be approximated is high dimensional? The logistic\nregression problem considered only has 54 dimensions. How would this method\nperform in a neural network in which the number of weights is goint to be way\nmuch larger? The logistic regression model is rather simple and its posterior\nwill be likely to be close to Gaussian. How would the method perform in more\ncomplicated posteriors such as the ones of Bayesia neural networks?\n\nClarity:\n\nThe paper is not clearly written. I found it very really hard to follow and not\nfocused. The authors describe way too many methods: 1) Stein's variational\ngradient descent (SVGD), 2) Amortized SVGD, 3) Kernelized Stein discrepancy\n(KSD), 4) Lavengin inference network, not to mention the introduction to\nStein's discrepancy. I found very difficult to indentify the clear\ncontributions of the paper with so many different techniques.\n\nOriginality:\n\nIt is not clear how original the proposed contributions are. The first of the\nproposed methods is also discussed in\n\nWang, Dilin and Liu, Qiang. Learning to draw samples: With application to\namortized mle for generative adversarial learning. Submitted to ICLR 2017, 2016\n\nHow does this work differ from that one?\n\nSignificance:\n\nIt is very hard to evaluate the importance of proposed methods. The authors\nonly report results on a 1d toy problem with a mixture of Gaussians and on a\nlogistic regression model with dimension 54. In both cases the distributions to\nbe approximated are very simple and of low dimension. In the regression case\nthe posterior is also likely to be close to Gaussian and therefore not clear\nwhat advances the proposed method would provide with respect to other more\nsimple approaches. The authors do not compare with simple variational\napproaches based on Gaussian approximations."
  },
  {
    "people": [
      "Liu",
      "Ranganath",
      "Ranganath",
      "Ranganath",
      "Salimans",
      "Salimans",
      "Salimans",
      "Rezende",
      "Mohamed",
      "Tran",
      "Ranganath",
      "Tran",
      "Ranganath",
      "Salimans",
      "Ranganath"
    ],
    "review": "The authors propose two variational methods based on the theme of posterior approximations which may not have a tractable density. The first is from another ICLR submission on \"amortized SVGD\" (Wang and Liu, 2016), where here the innovation is in using SGLD as the inference network. The second is from a NIPS paper (Ranganath et al., 2016) on minimizing the Stein divergence with a parametric approximating family, where here the innovation is in defining their test functions to be an RKHS, obtaining an analytic solution to the inner optimization problem.\n\nThe methodology is incremental. Everything up to Section 3.2 is essentially motivation, background, or related work. The notion of a \"wild variational approximation\" was already defined in Ranganath et al. (2016), termed a \"variational program\". It would be useful for the authors to comment on the difference, if any.\n\nSection 3.2 is at first interesting because it analytically solves the maximum problem that is faced in Ranganath et al. (2016). However, this requires use of a kernel which will certainly not scale in high dimensions, so it is then equivalent in practice to having chosen a very simple test function family. To properly scale to high dimensions would require a deeper kernel and also learning its parameters; this is not any easier than parameterizing the test function family as a neural network to begin with, which Ranganath et al. (2016) do.\n\nSection 4 introduces a Langevin inference network, which essentially chooses the variational approximation as an evolving sequence of Markov transition operators as in Salimans et al. (2015). I had trouble understanding this for a while because I could not understand what they mean by inference network. None of it is amortized in the usual inference network sense, which is that the parameters are given by the output of a neural network. Here, the authors simple define global parameters of the SGLD chain which are used across all the latent variables (which is strictly worse?). (What then makes it an \"inference network\"?) Is this not the variational approximation used in Salimans et al. (2015), but using a different objective to train it?\n\nThe experiments are limited, on a toy mixture of Gaussians posterior and Bayesian logistic regression. None of this addresses the problems one might suspect on high-dimensional and real data, such as the lack of scalability for the kernel, the comparison to Salimans et al. (2015) for the Langevin variational approximation, and any note of runtime or difficulty of training.\n\nMinor comments\n\n+ It's not clear if the authors understood previous work on expressive variational families or inference networks. For example, they argue Rezende & Mohamed, 2015b; Tran et al., 2015; Ranganath et al., 2015 require handcrafted inference networks. However, all of them assume use of any neural network for amortized inference. None of them even require an inference network. Perhaps the authors mean handcrafted posterior approximations, which to some extent is true; however, the three mentioned are all algorithmic in nature: in Rezende & Mohamed (2015), the main decision choice is the flow length; Tran et al. (2015), the size of the variational data; Ranganath et al. (2015), the flow length on the auxiliary variable space. Each works well on different problems, but this is also true of variational objectives which admit intractable q (as the latter two consider, as does Salimans et al. (2015)). The paper's motivation could be better explained, and perhaps the authors could be clearer on what they mean by inference network.\n+ I also recommend the authors not term a variational inference method based on the class of approximating family. While black box variational inference in Ranganath et al. (2014) assumes a mean-field family, the term itself has been used in the literature to mean any variational method that imposes few constraints on the model class."
  },
  {
    "people": [
      "Stein",
      "Stein",
      "Lavengin",
      "Stein",
      "Wang",
      "Dilin",
      "Liu",
      "Qiang"
    ],
    "review": "The authors propose methods for wild variational inference, in which the\nvariational approximating distribution may not have a directly accessible\ndensity function. Their approach is based on the Stain's operator, which acts\non a given function and returns a zero mean function with respect to a given\ndensity function which may not be normalized.\n\nQuality:\n\nThe derviations seem to be technically sound. However, my impression is that\nthe authors are not very careful and honest at evaluating both the strengths\nand weaknesses of the proposed work. How does the method perform in cases in\nwhich the distribution to be approximated is high dimensional? The logistic\nregression problem considered only has 54 dimensions. How would this method\nperform in a neural network in which the number of weights is goint to be way\nmuch larger? The logistic regression model is rather simple and its posterior\nwill be likely to be close to Gaussian. How would the method perform in more\ncomplicated posteriors such as the ones of Bayesia neural networks?\n\nClarity:\n\nThe paper is not clearly written. I found it very really hard to follow and not\nfocused. The authors describe way too many methods: 1) Stein's variational\ngradient descent (SVGD), 2) Amortized SVGD, 3) Kernelized Stein discrepancy\n(KSD), 4) Lavengin inference network, not to mention the introduction to\nStein's discrepancy. I found very difficult to indentify the clear\ncontributions of the paper with so many different techniques.\n\nOriginality:\n\nIt is not clear how original the proposed contributions are. The first of the\nproposed methods is also discussed in\n\nWang, Dilin and Liu, Qiang. Learning to draw samples: With application to\namortized mle for generative adversarial learning. Submitted to ICLR 2017, 2016\n\nHow does this work differ from that one?\n\nSignificance:\n\nIt is very hard to evaluate the importance of proposed methods. The authors\nonly report results on a 1d toy problem with a mixture of Gaussians and on a\nlogistic regression model with dimension 54. In both cases the distributions to\nbe approximated are very simple and of low dimension. In the regression case\nthe posterior is also likely to be close to Gaussian and therefore not clear\nwhat advances the proposed method would provide with respect to other more\nsimple approaches. The authors do not compare with simple variational\napproaches based on Gaussian approximations."
  },
  {
    "people": [
      "Gal"
    ],
    "review": "The work presented in this paper proposes a method to get an ensemble of neural networks at no extra training cost (i.e., at the cost of training a single network), by saving snapshots of the network during training. Network is trained using a cyclic (cosine) learning rate schedule; the snapshots are obtained when the learning rate is at the lowest points of the cycles. Using these snapshot ensembles, they show gains in performance over a single network on the image classification task on a variety of datasets.\n\n\nPositives:\n\n1. The work should be easy to adopt and re-produce, given the simple techinque and the experimental details in the paper.\n2. Well written paper, with clear description of the method and thorough experiments.\n\n\nSuggestions for improvement / other comments:\n\n1. While it is fair to compare against other techniques assuming a fixed computational budget, for a clear perspective, thorough comaprisons with \"true ensembles\" (i.e., ensembles of networks trained independently) should be provided.\nSpecificially, Table 4 should be augmented with results from \"true ensembles\".\n\n2. Comparison with true ensembles is only provided for DenseNet-40 on CIFAR100 in Figure 4. The proposed snapshot ensemble achieves approximately 66% of the improvement of \"true ensemble\" over the single baseline model. This is not reflected accurately in the authors' claim in the abstract: \"[snapshot ensembles] **almost match[es]** the results of far more expensive independently trained [true ensembles].\"\n\n3. As mentioned before: to understand the diversity of snapshot ensembles, it would help to the diversity against different ensembling technique, e.g. (1) \"true ensembles\", (2) ensembles from dropout as described by Gal et. al, 2016 (Dropout as a Bayesian Approximation)."
  },
  {
    "people": [
      "Dmytro Mishkin"
    ],
    "review": "Congratulation to the authors for getting very high review score. However I want to know the difference between Snapshot Ensembles and Horizontal Ensemble (Thank Dmytro Mishkin for reading my undergrad paper).\n\nI take a quick look of the paper, seems Snapshot Ensembles is very similar to Horizontal Ensemble, maybe with a different LR schedule? \n\n"
  },
  {
    "people": [
      "Gal"
    ],
    "review": "The work presented in this paper proposes a method to get an ensemble of neural networks at no extra training cost (i.e., at the cost of training a single network), by saving snapshots of the network during training. Network is trained using a cyclic (cosine) learning rate schedule; the snapshots are obtained when the learning rate is at the lowest points of the cycles. Using these snapshot ensembles, they show gains in performance over a single network on the image classification task on a variety of datasets.\n\n\nPositives:\n\n1. The work should be easy to adopt and re-produce, given the simple techinque and the experimental details in the paper.\n2. Well written paper, with clear description of the method and thorough experiments.\n\n\nSuggestions for improvement / other comments:\n\n1. While it is fair to compare against other techniques assuming a fixed computational budget, for a clear perspective, thorough comaprisons with \"true ensembles\" (i.e., ensembles of networks trained independently) should be provided.\nSpecificially, Table 4 should be augmented with results from \"true ensembles\".\n\n2. Comparison with true ensembles is only provided for DenseNet-40 on CIFAR100 in Figure 4. The proposed snapshot ensemble achieves approximately 66% of the improvement of \"true ensemble\" over the single baseline model. This is not reflected accurately in the authors' claim in the abstract: \"[snapshot ensembles] **almost match[es]** the results of far more expensive independently trained [true ensembles].\"\n\n3. As mentioned before: to understand the diversity of snapshot ensembles, it would help to the diversity against different ensembling technique, e.g. (1) \"true ensembles\", (2) ensembles from dropout as described by Gal et. al, 2016 (Dropout as a Bayesian Approximation)."
  },
  {
    "people": [
      "Jingjing Xie"
    ],
    "review": "Horizontal and Vertical Ensemble with Deep Representation for Classification\nJingjing Xie, Bing Xu, Zhang Chuang\n\npage 3: \"Similar to the horizontal voting method in section 3.2, it takes the output of networks within a continuous range of epoch\"\n\n"
  },
  {
    "people": [
      "Gal"
    ],
    "review": "The work presented in this paper proposes a method to get an ensemble of neural networks at no extra training cost (i.e., at the cost of training a single network), by saving snapshots of the network during training. Network is trained using a cyclic (cosine) learning rate schedule; the snapshots are obtained when the learning rate is at the lowest points of the cycles. Using these snapshot ensembles, they show gains in performance over a single network on the image classification task on a variety of datasets.\n\n\nPositives:\n\n1. The work should be easy to adopt and re-produce, given the simple techinque and the experimental details in the paper.\n2. Well written paper, with clear description of the method and thorough experiments.\n\n\nSuggestions for improvement / other comments:\n\n1. While it is fair to compare against other techniques assuming a fixed computational budget, for a clear perspective, thorough comaprisons with \"true ensembles\" (i.e., ensembles of networks trained independently) should be provided.\nSpecificially, Table 4 should be augmented with results from \"true ensembles\".\n\n2. Comparison with true ensembles is only provided for DenseNet-40 on CIFAR100 in Figure 4. The proposed snapshot ensemble achieves approximately 66% of the improvement of \"true ensemble\" over the single baseline model. This is not reflected accurately in the authors' claim in the abstract: \"[snapshot ensembles] **almost match[es]** the results of far more expensive independently trained [true ensembles].\"\n\n3. As mentioned before: to understand the diversity of snapshot ensembles, it would help to the diversity against different ensembling technique, e.g. (1) \"true ensembles\", (2) ensembles from dropout as described by Gal et. al, 2016 (Dropout as a Bayesian Approximation)."
  },
  {
    "people": [
      "Dmytro Mishkin"
    ],
    "review": "Congratulation to the authors for getting very high review score. However I want to know the difference between Snapshot Ensembles and Horizontal Ensemble (Thank Dmytro Mishkin for reading my undergrad paper).\n\nI take a quick look of the paper, seems Snapshot Ensembles is very similar to Horizontal Ensemble, maybe with a different LR schedule? \n\n"
  },
  {
    "people": [
      "Gal"
    ],
    "review": "The work presented in this paper proposes a method to get an ensemble of neural networks at no extra training cost (i.e., at the cost of training a single network), by saving snapshots of the network during training. Network is trained using a cyclic (cosine) learning rate schedule; the snapshots are obtained when the learning rate is at the lowest points of the cycles. Using these snapshot ensembles, they show gains in performance over a single network on the image classification task on a variety of datasets.\n\n\nPositives:\n\n1. The work should be easy to adopt and re-produce, given the simple techinque and the experimental details in the paper.\n2. Well written paper, with clear description of the method and thorough experiments.\n\n\nSuggestions for improvement / other comments:\n\n1. While it is fair to compare against other techniques assuming a fixed computational budget, for a clear perspective, thorough comaprisons with \"true ensembles\" (i.e., ensembles of networks trained independently) should be provided.\nSpecificially, Table 4 should be augmented with results from \"true ensembles\".\n\n2. Comparison with true ensembles is only provided for DenseNet-40 on CIFAR100 in Figure 4. The proposed snapshot ensemble achieves approximately 66% of the improvement of \"true ensemble\" over the single baseline model. This is not reflected accurately in the authors' claim in the abstract: \"[snapshot ensembles] **almost match[es]** the results of far more expensive independently trained [true ensembles].\"\n\n3. As mentioned before: to understand the diversity of snapshot ensembles, it would help to the diversity against different ensembling technique, e.g. (1) \"true ensembles\", (2) ensembles from dropout as described by Gal et. al, 2016 (Dropout as a Bayesian Approximation)."
  },
  {
    "people": [
      "Jingjing Xie"
    ],
    "review": "Horizontal and Vertical Ensemble with Deep Representation for Classification\nJingjing Xie, Bing Xu, Zhang Chuang\n\npage 3: \"Similar to the horizontal voting method in section 3.2, it takes the output of networks within a continuous range of epoch\"\n\n"
  },
  {
    "people": [
      "Fisher"
    ],
    "review": "(paper summary) The authors introduce the notion of \u201csample importance\u201d, meant to measure the influence of a particular training example on the training of a deep neural network. This quantity is closely related to the squared L2 norm of the gradient, where the summation is performed over (i) parameters of a given layer or (ii) across all parameters. Summing this quantity across time gives the \u201coverall importance\u201d, used to tease apart easy from hard examples. From this quantity, the authors illustrate the impact of [easy,hard] example during training, and their impact on layer depth.\n\n(detailed review)\nI have several objections to this paper. First and foremost, I am not convinced of the \u201csample importance\u201d as a meaningful metric. As previously mentioned, the magnitude of gradients will change significantly during learning, and I am not sure what conclusions one can draw from \\sum_t g_i^t vs \\sum_t g_j^t. For example, gradients tend to have higher norms early in training than at convergence, in which case weighting each gradient equally seems problematic. I tried illustrating the above with a small thought experiment during the question period: \u201cif\u201d the learning rate were too high, training may not even converge in which case sample importance would be ill-defined.  Having a measure which depends on the learning rate seems problematic to me, as does the use of the L2 norm. The \u201cinput Fisher\u201d norm, \\mathbb{E} \\frac{\\partial \\log p} {\\partial x} (for a given time-step) may be better suited, as it speaks directly to the sensitivity of the classifier to the input x (and is insensitive to changes in the mean gradient norm). But again summing Fisher norms across time may not be meaningful.\n\nThe experimental analysis also seems problematic. The authors claim from Fig. 2 that output layers are primarily learnt in the early stage of training. However, this is definitely not the case for CIFAR-10 and is debatable for MNIST: sample importance remains high for all layers during training, despite a small spike early on the output layer. Fig 2. (lower, middle) and Fig. 6 also seems to highlight an issue with the SI measure: the SI is dominated by the input layer which has the most parameters, and can thus more readily impact the gradient norm. Different model architectures may have yielded different conclusions. Had the authors managed to use the SI to craft a better curriculum, this would have given significant weight to the measure. Unfortunately, these results are negative.\n\nPROS:\n+ extensive experiments\n\nCONS:\n- sample importance is a heuristic, not entirely well justified\n- SI yields limited insight into training of neural nets\n- SI does not inform curriculum learning"
  },
  {
    "people": [
      "Mnist"
    ],
    "review": "The paper proposes a new criterion (sample importance) to study the impact of samples during the training of deep neural networks. This criterion is not clearly defined (the term \\phi^t_{i,j} is never defined, only \\phi^t_i is defined; Despite the unclear definition, it is understood that sample importance is the squared l2 norm of the gradient for a sample i and at time t strangely scaled by the squared learning rate (the learning rate should have nothing to do with the importance of a sample in this context).\n\nThe paper presents experiments on the well known MNIST and CIFAR datasets with correspondingly appropriate network architectures and choice of hyper-parameters and initialisations. The size of the hidden layers is a bit small for Mnist and very small for CIFAR (this could explain the very poor performance in figure 6: 50% error on CIFAR)\n\nThe study of the evolution of sample importance during training depending on layers seems to lead to trivial conclusions\n - \u201cthe overall sample importance is different under different epochs\u201d => yes the norm of the gradient is expected to vary\n - \u201cOutput layer always has the largest average sample importance per parameter, and its contribution reaches the maximum in the early training stage and then drops\u201d => 1. yes since the gradient flows backwards, the gradient is expected to be stronger for the output layer and it is expected to become more diffuse as it propagates to lower layers which are not stable. As learning progresses one would expect the output layer to have progressively smaller gradients. 2. the norm of the gradient depends on the scaling of the variables\n\nThe question of Figure 4 is absurd \u201cIs Sample Importance the same as Negative log-likelihood of a sample?\u201d. Of course not.\n\nThe results are very bad on CIFAR which discredits the applicability of those results.\n\nOn Mnist performance is not readable (figure 7): Error rate should only be presented between 0 and 10 or 20%\n\nDespite these important issues (there are others), the paper manages to raises some interesting things: the so-called easy samples and hard samples do seem to correspond (although the study is very preliminary in this regard) to what would intuitively be considered easy (the most representative/canonical samples) and hard (edge cases) samples. Also the experiments are very well presented."
  },
  {
    "people": [
      "Fisher"
    ],
    "review": "(paper summary) The authors introduce the notion of \u201csample importance\u201d, meant to measure the influence of a particular training example on the training of a deep neural network. This quantity is closely related to the squared L2 norm of the gradient, where the summation is performed over (i) parameters of a given layer or (ii) across all parameters. Summing this quantity across time gives the \u201coverall importance\u201d, used to tease apart easy from hard examples. From this quantity, the authors illustrate the impact of [easy,hard] example during training, and their impact on layer depth.\n\n(detailed review)\nI have several objections to this paper. First and foremost, I am not convinced of the \u201csample importance\u201d as a meaningful metric. As previously mentioned, the magnitude of gradients will change significantly during learning, and I am not sure what conclusions one can draw from \\sum_t g_i^t vs \\sum_t g_j^t. For example, gradients tend to have higher norms early in training than at convergence, in which case weighting each gradient equally seems problematic. I tried illustrating the above with a small thought experiment during the question period: \u201cif\u201d the learning rate were too high, training may not even converge in which case sample importance would be ill-defined.  Having a measure which depends on the learning rate seems problematic to me, as does the use of the L2 norm. The \u201cinput Fisher\u201d norm, \\mathbb{E} \\frac{\\partial \\log p} {\\partial x} (for a given time-step) may be better suited, as it speaks directly to the sensitivity of the classifier to the input x (and is insensitive to changes in the mean gradient norm). But again summing Fisher norms across time may not be meaningful.\n\nThe experimental analysis also seems problematic. The authors claim from Fig. 2 that output layers are primarily learnt in the early stage of training. However, this is definitely not the case for CIFAR-10 and is debatable for MNIST: sample importance remains high for all layers during training, despite a small spike early on the output layer. Fig 2. (lower, middle) and Fig. 6 also seems to highlight an issue with the SI measure: the SI is dominated by the input layer which has the most parameters, and can thus more readily impact the gradient norm. Different model architectures may have yielded different conclusions. Had the authors managed to use the SI to craft a better curriculum, this would have given significant weight to the measure. Unfortunately, these results are negative.\n\nPROS:\n+ extensive experiments\n\nCONS:\n- sample importance is a heuristic, not entirely well justified\n- SI yields limited insight into training of neural nets\n- SI does not inform curriculum learning\n"
  },
  {
    "people": [
      "Fisher"
    ],
    "review": "(paper summary) The authors introduce the notion of \u201csample importance\u201d, meant to measure the influence of a particular training example on the training of a deep neural network. This quantity is closely related to the squared L2 norm of the gradient, where the summation is performed over (i) parameters of a given layer or (ii) across all parameters. Summing this quantity across time gives the \u201coverall importance\u201d, used to tease apart easy from hard examples. From this quantity, the authors illustrate the impact of [easy,hard] example during training, and their impact on layer depth.\n\n(detailed review)\nI have several objections to this paper. First and foremost, I am not convinced of the \u201csample importance\u201d as a meaningful metric. As previously mentioned, the magnitude of gradients will change significantly during learning, and I am not sure what conclusions one can draw from \\sum_t g_i^t vs \\sum_t g_j^t. For example, gradients tend to have higher norms early in training than at convergence, in which case weighting each gradient equally seems problematic. I tried illustrating the above with a small thought experiment during the question period: \u201cif\u201d the learning rate were too high, training may not even converge in which case sample importance would be ill-defined.  Having a measure which depends on the learning rate seems problematic to me, as does the use of the L2 norm. The \u201cinput Fisher\u201d norm, \\mathbb{E} \\frac{\\partial \\log p} {\\partial x} (for a given time-step) may be better suited, as it speaks directly to the sensitivity of the classifier to the input x (and is insensitive to changes in the mean gradient norm). But again summing Fisher norms across time may not be meaningful.\n\nThe experimental analysis also seems problematic. The authors claim from Fig. 2 that output layers are primarily learnt in the early stage of training. However, this is definitely not the case for CIFAR-10 and is debatable for MNIST: sample importance remains high for all layers during training, despite a small spike early on the output layer. Fig 2. (lower, middle) and Fig. 6 also seems to highlight an issue with the SI measure: the SI is dominated by the input layer which has the most parameters, and can thus more readily impact the gradient norm. Different model architectures may have yielded different conclusions. Had the authors managed to use the SI to craft a better curriculum, this would have given significant weight to the measure. Unfortunately, these results are negative.\n\nPROS:\n+ extensive experiments\n\nCONS:\n- sample importance is a heuristic, not entirely well justified\n- SI yields limited insight into training of neural nets\n- SI does not inform curriculum learning"
  },
  {
    "people": [
      "Mnist"
    ],
    "review": "The paper proposes a new criterion (sample importance) to study the impact of samples during the training of deep neural networks. This criterion is not clearly defined (the term \\phi^t_{i,j} is never defined, only \\phi^t_i is defined; Despite the unclear definition, it is understood that sample importance is the squared l2 norm of the gradient for a sample i and at time t strangely scaled by the squared learning rate (the learning rate should have nothing to do with the importance of a sample in this context).\n\nThe paper presents experiments on the well known MNIST and CIFAR datasets with correspondingly appropriate network architectures and choice of hyper-parameters and initialisations. The size of the hidden layers is a bit small for Mnist and very small for CIFAR (this could explain the very poor performance in figure 6: 50% error on CIFAR)\n\nThe study of the evolution of sample importance during training depending on layers seems to lead to trivial conclusions\n - \u201cthe overall sample importance is different under different epochs\u201d => yes the norm of the gradient is expected to vary\n - \u201cOutput layer always has the largest average sample importance per parameter, and its contribution reaches the maximum in the early training stage and then drops\u201d => 1. yes since the gradient flows backwards, the gradient is expected to be stronger for the output layer and it is expected to become more diffuse as it propagates to lower layers which are not stable. As learning progresses one would expect the output layer to have progressively smaller gradients. 2. the norm of the gradient depends on the scaling of the variables\n\nThe question of Figure 4 is absurd \u201cIs Sample Importance the same as Negative log-likelihood of a sample?\u201d. Of course not.\n\nThe results are very bad on CIFAR which discredits the applicability of those results.\n\nOn Mnist performance is not readable (figure 7): Error rate should only be presented between 0 and 10 or 20%\n\nDespite these important issues (there are others), the paper manages to raises some interesting things: the so-called easy samples and hard samples do seem to correspond (although the study is very preliminary in this regard) to what would intuitively be considered easy (the most representative/canonical samples) and hard (edge cases) samples. Also the experiments are very well presented."
  },
  {
    "people": [
      "Fisher"
    ],
    "review": "(paper summary) The authors introduce the notion of \u201csample importance\u201d, meant to measure the influence of a particular training example on the training of a deep neural network. This quantity is closely related to the squared L2 norm of the gradient, where the summation is performed over (i) parameters of a given layer or (ii) across all parameters. Summing this quantity across time gives the \u201coverall importance\u201d, used to tease apart easy from hard examples. From this quantity, the authors illustrate the impact of [easy,hard] example during training, and their impact on layer depth.\n\n(detailed review)\nI have several objections to this paper. First and foremost, I am not convinced of the \u201csample importance\u201d as a meaningful metric. As previously mentioned, the magnitude of gradients will change significantly during learning, and I am not sure what conclusions one can draw from \\sum_t g_i^t vs \\sum_t g_j^t. For example, gradients tend to have higher norms early in training than at convergence, in which case weighting each gradient equally seems problematic. I tried illustrating the above with a small thought experiment during the question period: \u201cif\u201d the learning rate were too high, training may not even converge in which case sample importance would be ill-defined.  Having a measure which depends on the learning rate seems problematic to me, as does the use of the L2 norm. The \u201cinput Fisher\u201d norm, \\mathbb{E} \\frac{\\partial \\log p} {\\partial x} (for a given time-step) may be better suited, as it speaks directly to the sensitivity of the classifier to the input x (and is insensitive to changes in the mean gradient norm). But again summing Fisher norms across time may not be meaningful.\n\nThe experimental analysis also seems problematic. The authors claim from Fig. 2 that output layers are primarily learnt in the early stage of training. However, this is definitely not the case for CIFAR-10 and is debatable for MNIST: sample importance remains high for all layers during training, despite a small spike early on the output layer. Fig 2. (lower, middle) and Fig. 6 also seems to highlight an issue with the SI measure: the SI is dominated by the input layer which has the most parameters, and can thus more readily impact the gradient norm. Different model architectures may have yielded different conclusions. Had the authors managed to use the SI to craft a better curriculum, this would have given significant weight to the measure. Unfortunately, these results are negative.\n\nPROS:\n+ extensive experiments\n\nCONS:\n- sample importance is a heuristic, not entirely well justified\n- SI yields limited insight into training of neural nets\n- SI does not inform curriculum learning\n"
  },
  {
    "people": [
      "Shakespeare",
      "Shakespeare"
    ],
    "review": "The authors propose a solution for the task of synthesizing melodies. The authors claim that the \"language-model\"-type approaches with LSTMs generate melodies with certain shortcomings. They tend to lack long-range structure, to repeat notes etc. To solve this problem the authors suggest that the model could be first trained as a pure LM-style LSTM and then trained with reinforcement learning to optimize an objective which includes some non-differentiable music-theory related constraints. \n\nThe reinforcement learning methodology is appropriate but straightforward and closely resembles previous work for text modeling and dialogue generation. By itself the methodology doesn't offer a new technique. \n\nTo me, the paper's contribution then comes down to the novelty / utility / impact of the application. The authors clearly put substantial of effort into crafting the rules and user study and that is commendable. On the other hand, music itself is dealt with somewhat naively. While the user study reflects hard work, it seems premature. The semi-plausible piano melodies here are only music in the way that LSTM Shakespeare passes as poetry. So it's analogous to conducting a user study comparing LSTM Shakespeare to n-gram Shakespeare. \n\nI'd caution the author's against the uncritical motivation that a problem has previously been studied. Research contains abundant dead ends (not to say this is necessarily one) and the burden to motivate research shouldn't be forgotten. This is especially true when the application is the primary thrust of a paper.\n\nGenerally the authors should be careful about describing this model as \"composing\". By analogy to a Shakespeare-LSTM, the language model is not really composing English prose. The relationship between constructing a statistical sequence model and creating art - an activity that involves communication grounded in real-world semantics should not be overstated. \n\nI appreciate the authors' efforts to respond to some criticisms of the problem setup and encourage them to anticipate these arguments in the paper and to better motivate the work in the future. If the main contribution is the application (the methods have been used elsewhere), then the motivation is of central importance. I also appreciate their contention that the field benefits from multiple datasets and not simply relying on language modeling. Further, they are correct in asserting that MIDI can capture all the information in a score (not merely \"Gameboy music\", and that for some musics (e.g. European classical) the score is of central importance. However, the authors may overstate the role of a score in jazz music.\n\nOverall, for me, the application, while fun, doesn't add enough to the impact of the paper. And the methodology, while appropriate, doesn't stand on its own. \n\n--Update-- Thanks for your modifications and arguments. I've revised my scores to add a point."
  },
  {
    "people": [
      "Shakespeare",
      "Shakespeare"
    ],
    "review": "The authors propose a solution for the task of synthesizing melodies. The authors claim that the \"language-model\"-type approaches with LSTMs generate melodies with certain shortcomings. They tend to lack long-range structure, to repeat notes etc. To solve this problem the authors suggest that the model could be first trained as a pure LM-style LSTM and then trained with reinforcement learning to optimize an objective which includes some non-differentiable music-theory related constraints. \n\nThe reinforcement learning methodology is appropriate but straightforward and closely resembles previous work for text modeling and dialogue generation. By itself the methodology doesn't offer a new technique. \n\nTo me, the paper's contribution then comes down to the novelty / utility / impact of the application. The authors clearly put substantial of effort into crafting the rules and user study and that is commendable. On the other hand, music itself is dealt with somewhat naively. While the user study reflects hard work, it seems premature. The semi-plausible piano melodies here are only music in the way that LSTM Shakespeare passes as poetry. So it's analogous to conducting a user study comparing LSTM Shakespeare to n-gram Shakespeare. \n\nI'd caution the author's against the uncritical motivation that a problem has previously been studied. Research contains abundant dead ends (not to say this is necessarily one) and the burden to motivate research shouldn't be forgotten. This is especially true when the application is the primary thrust of a paper.\n\nGenerally the authors should be careful about describing this model as \"composing\". By analogy to a Shakespeare-LSTM, the language model is not really composing English prose. The relationship between constructing a statistical sequence model and creating art - an activity that involves communication grounded in real-world semantics should not be overstated. \n\nI appreciate the authors' efforts to respond to some criticisms of the problem setup and encourage them to anticipate these arguments in the paper and to better motivate the work in the future. If the main contribution is the application (the methods have been used elsewhere), then the motivation is of central importance. I also appreciate their contention that the field benefits from multiple datasets and not simply relying on language modeling. Further, they are correct in asserting that MIDI can capture all the information in a score (not merely \"Gameboy music\", and that for some musics (e.g. European classical) the score is of central importance. However, the authors may overstate the role of a score in jazz music.\n\nOverall, for me, the application, while fun, doesn't add enough to the impact of the paper. And the methodology, while appropriate, doesn't stand on its own. \n\n--Update-- Thanks for your modifications and arguments. I've revised my scores to add a point. "
  },
  {
    "people": [
      "Attias",
      "Toussaint",
      "Storkey",
      "Toussaint",
      "Kappen",
      "Rawlik"
    ],
    "review": "This paper uses a combination of likelihood and reward based learning to learn sequence models for music. The ability to combine likelihood and reward based learning has been long known, as a result of the unification of inference and learning first appearing in the ML literature with the EM formalism of Attias (2003) for fixed horizons, extended by Toussaint and Storkey (2006), to general horizon settings, Toussaint et al. (2011) to POMDPs and generalised further by Kappen et Al. (2012) and Rawlik et Al. (2012). These papers introduced the basic unification, and so any additional probabilistic or data driven objective can be combined with the reinforcement learning signal: it is all part of a unified reward/likelihood. Hence the optimal control target under unification is p(b=1|\\tau)E_p(A,S) \\prod_t \\pi(a_t|s_t): i.e. the probability of getting reward, and probability of the policy actions under the known data-derived distribution, thereby introducing the log p(a_t|s_t) into (9) too.\n\nThe interpretation of the secondary objective as the prior is an alternative approach under a stochastic optimal control setting, but not the most natural one given the whole principle of SOC of matching control objectives to inference objectives. The SOC off policy objective still does still contain the KL term so the approach would still differ from the approach of this paper.\n\nThough the discussion of optimal control is good, I think some further elaboration of the history and how reward augmentation can work in SOC would be valuable. This would allow SOC off-policy methods to be compared with the DQN directly, like for like.\n\nThe motivation of the objective (3) is sensible but could be made clearer via the unification argument above. Then the paper uses DCN to take a different approach from the variational SOC for achieving that objective.\n\nAnother interesting point of discussion is the choice of E_pi \\log p(a_t|s_t) \u2013 this means the policy must \u201ccover\u201d the model. But one problem in generation is that a well-trained model is often underfit, resulting in actions that, over the course of a number of iterations, move the state into data-unsupported parts of the space. As a result the model is no longer confident and quickly tends to be fairly random. This approach (as opposed to a KL(p||pi) \u2013 which is not obvious how to implement) cannot mitigate against that, without a very strong signal (to overcome the tails of a distribution). In music, with a smaller discrete alphabet, this is likely to be less of a problem than for real valued policy densities, with exponentially decaying tails. Some further discussion of what you see in light of this issue would be valuable: the use of c to balance things seems critical, and it seems clear from Figure 2 that the reward signal needed to be very high to push the log p signal into the right range.\n\nAltogether, in the music setting this paper provides a reasonable demonstration that augmentation of a sequence model with an additional reward constraint is valuable. It demonstrates that DQN is one way of learning that signal, but AFAICS it does not compare learning the same signal via other techniques. Instead for the comparator techniques it reverts to treating the p(a|s) as a \u201cprior\u201d term rather than a reward term, leaving a bit of a question as to whether DQN is particularly appropriate. \n\nAnother interesting question for the discussion is whether the music theory reward could be approximated by a differentiable model, mitigating the need for an RL approach at all."
  },
  {
    "people": [
      "Rawlik",
      "Bertsekas",
      "Ari Pakman",
      "Roy Fox"
    ],
    "review": "Very nice work!\nInteresting to see how well it works to combine a reward signal with cross-entropy from an informative prior, even without the extra entropy regularization of the KL cost.\n\nWe would like to note a few important things, though:\n\n1. Psi-learning is not the method given in (11), (12), and Appendix 8.1.1.\nUsing the current policy as the prior for each small policy update is central to Psi-learning and for the properties of the Psi function.\nThis is best seen by comparing your (23) with Rawlik et al.'s (14), where they have Psi-\\bar{Psi} substituted for log(p).\nBy having p fixed and not dependent of Psi, you are qualitatively changing the fixed point of the equation.\nFor example, their Psi function is gap increasing, in that it only recovers the value function in the optimal action, and is negative infinity for suboptimal actions \u2014 your \"Psi\" function does not have this property, and the two should not be confused.\n\n2. Psi-learning is distinct from G-learning, in that the former uses small divergences from a changing prior, while the latter uses large divergences from a fixed prior.\nBy fixing the prior p in both of your derivations, both are using G-learning with a fixed temperature, albeit with different parameterization of the function G.\nThe two functions defined in (16) and (24) differ by the constant log(p), and this is then reflected in (23) and (30).\nIntriguingly, parameterizing G+log(p) yields better results than parameterizing G.\nThis is somewhat reminiscent of advantage learning, where one parameterizes Q-V rather than Q, and certainly merits further study.\n\n3. In DQN, the Q-network is a feed-forward network that maps (s,a) inputs into real outputs.\nIf this is the architecture used here, it is unclear how it is initialized from the differently shaped Note RNN.\nOn other hand, if the Q-network here is a RNN, it is unclear what state it keeps and how it computes Q(s,a;theta).\n\n4. As Figure 2 shows, \"G-learning\" achieves a different tradeoff with worse music-theory reward but better log(p).\nIt is therefore inaccurate to characterize it in Section 6 and Table 1 as doing worse.\nFor a fair comparison, one needs to set c differently for each algorithm, so that one reward is the same, and compare the other reward.\nIt would also be useful to repeat this for various values of c, and plot the two rewards on a plane, to allow comparing the reward-achievability regions of the algorithms.\n\n5. Stochastic optimal control is a much broader field (cf. Bertsekas 1995). In Section 3.1 you are referring specifically to the field known as KL control.\n\n\nAri Pakman and Roy Fox"
  },
  {
    "people": [
      "Shakespeare",
      "Shakespeare"
    ],
    "review": "The authors propose a solution for the task of synthesizing melodies. The authors claim that the \"language-model\"-type approaches with LSTMs generate melodies with certain shortcomings. They tend to lack long-range structure, to repeat notes etc. To solve this problem the authors suggest that the model could be first trained as a pure LM-style LSTM and then trained with reinforcement learning to optimize an objective which includes some non-differentiable music-theory related constraints. \n\nThe reinforcement learning methodology is appropriate but straightforward and closely resembles previous work for text modeling and dialogue generation. By itself the methodology doesn't offer a new technique. \n\nTo me, the paper's contribution then comes down to the novelty / utility / impact of the application. The authors clearly put substantial of effort into crafting the rules and user study and that is commendable. On the other hand, music itself is dealt with somewhat naively. While the user study reflects hard work, it seems premature. The semi-plausible piano melodies here are only music in the way that LSTM Shakespeare passes as poetry. So it's analogous to conducting a user study comparing LSTM Shakespeare to n-gram Shakespeare. \n\nI'd caution the author's against the uncritical motivation that a problem has previously been studied. Research contains abundant dead ends (not to say this is necessarily one) and the burden to motivate research shouldn't be forgotten. This is especially true when the application is the primary thrust of a paper.\n\nGenerally the authors should be careful about describing this model as \"composing\". By analogy to a Shakespeare-LSTM, the language model is not really composing English prose. The relationship between constructing a statistical sequence model and creating art - an activity that involves communication grounded in real-world semantics should not be overstated. \n\nI appreciate the authors' efforts to respond to some criticisms of the problem setup and encourage them to anticipate these arguments in the paper and to better motivate the work in the future. If the main contribution is the application (the methods have been used elsewhere), then the motivation is of central importance. I also appreciate their contention that the field benefits from multiple datasets and not simply relying on language modeling. Further, they are correct in asserting that MIDI can capture all the information in a score (not merely \"Gameboy music\", and that for some musics (e.g. European classical) the score is of central importance. However, the authors may overstate the role of a score in jazz music.\n\nOverall, for me, the application, while fun, doesn't add enough to the impact of the paper. And the methodology, while appropriate, doesn't stand on its own. \n\n--Update-- Thanks for your modifications and arguments. I've revised my scores to add a point."
  },
  {
    "people": [
      "Shakespeare",
      "Shakespeare"
    ],
    "review": "The authors propose a solution for the task of synthesizing melodies. The authors claim that the \"language-model\"-type approaches with LSTMs generate melodies with certain shortcomings. They tend to lack long-range structure, to repeat notes etc. To solve this problem the authors suggest that the model could be first trained as a pure LM-style LSTM and then trained with reinforcement learning to optimize an objective which includes some non-differentiable music-theory related constraints. \n\nThe reinforcement learning methodology is appropriate but straightforward and closely resembles previous work for text modeling and dialogue generation. By itself the methodology doesn't offer a new technique. \n\nTo me, the paper's contribution then comes down to the novelty / utility / impact of the application. The authors clearly put substantial of effort into crafting the rules and user study and that is commendable. On the other hand, music itself is dealt with somewhat naively. While the user study reflects hard work, it seems premature. The semi-plausible piano melodies here are only music in the way that LSTM Shakespeare passes as poetry. So it's analogous to conducting a user study comparing LSTM Shakespeare to n-gram Shakespeare. \n\nI'd caution the author's against the uncritical motivation that a problem has previously been studied. Research contains abundant dead ends (not to say this is necessarily one) and the burden to motivate research shouldn't be forgotten. This is especially true when the application is the primary thrust of a paper.\n\nGenerally the authors should be careful about describing this model as \"composing\". By analogy to a Shakespeare-LSTM, the language model is not really composing English prose. The relationship between constructing a statistical sequence model and creating art - an activity that involves communication grounded in real-world semantics should not be overstated. \n\nI appreciate the authors' efforts to respond to some criticisms of the problem setup and encourage them to anticipate these arguments in the paper and to better motivate the work in the future. If the main contribution is the application (the methods have been used elsewhere), then the motivation is of central importance. I also appreciate their contention that the field benefits from multiple datasets and not simply relying on language modeling. Further, they are correct in asserting that MIDI can capture all the information in a score (not merely \"Gameboy music\", and that for some musics (e.g. European classical) the score is of central importance. However, the authors may overstate the role of a score in jazz music.\n\nOverall, for me, the application, while fun, doesn't add enough to the impact of the paper. And the methodology, while appropriate, doesn't stand on its own. \n\n--Update-- Thanks for your modifications and arguments. I've revised my scores to add a point. "
  },
  {
    "people": [
      "Attias",
      "Toussaint",
      "Storkey",
      "Toussaint",
      "Kappen",
      "Rawlik"
    ],
    "review": "This paper uses a combination of likelihood and reward based learning to learn sequence models for music. The ability to combine likelihood and reward based learning has been long known, as a result of the unification of inference and learning first appearing in the ML literature with the EM formalism of Attias (2003) for fixed horizons, extended by Toussaint and Storkey (2006), to general horizon settings, Toussaint et al. (2011) to POMDPs and generalised further by Kappen et Al. (2012) and Rawlik et Al. (2012). These papers introduced the basic unification, and so any additional probabilistic or data driven objective can be combined with the reinforcement learning signal: it is all part of a unified reward/likelihood. Hence the optimal control target under unification is p(b=1|\\tau)E_p(A,S) \\prod_t \\pi(a_t|s_t): i.e. the probability of getting reward, and probability of the policy actions under the known data-derived distribution, thereby introducing the log p(a_t|s_t) into (9) too.\n\nThe interpretation of the secondary objective as the prior is an alternative approach under a stochastic optimal control setting, but not the most natural one given the whole principle of SOC of matching control objectives to inference objectives. The SOC off policy objective still does still contain the KL term so the approach would still differ from the approach of this paper.\n\nThough the discussion of optimal control is good, I think some further elaboration of the history and how reward augmentation can work in SOC would be valuable. This would allow SOC off-policy methods to be compared with the DQN directly, like for like.\n\nThe motivation of the objective (3) is sensible but could be made clearer via the unification argument above. Then the paper uses DCN to take a different approach from the variational SOC for achieving that objective.\n\nAnother interesting point of discussion is the choice of E_pi \\log p(a_t|s_t) \u2013 this means the policy must \u201ccover\u201d the model. But one problem in generation is that a well-trained model is often underfit, resulting in actions that, over the course of a number of iterations, move the state into data-unsupported parts of the space. As a result the model is no longer confident and quickly tends to be fairly random. This approach (as opposed to a KL(p||pi) \u2013 which is not obvious how to implement) cannot mitigate against that, without a very strong signal (to overcome the tails of a distribution). In music, with a smaller discrete alphabet, this is likely to be less of a problem than for real valued policy densities, with exponentially decaying tails. Some further discussion of what you see in light of this issue would be valuable: the use of c to balance things seems critical, and it seems clear from Figure 2 that the reward signal needed to be very high to push the log p signal into the right range.\n\nAltogether, in the music setting this paper provides a reasonable demonstration that augmentation of a sequence model with an additional reward constraint is valuable. It demonstrates that DQN is one way of learning that signal, but AFAICS it does not compare learning the same signal via other techniques. Instead for the comparator techniques it reverts to treating the p(a|s) as a \u201cprior\u201d term rather than a reward term, leaving a bit of a question as to whether DQN is particularly appropriate. \n\nAnother interesting question for the discussion is whether the music theory reward could be approximated by a differentiable model, mitigating the need for an RL approach at all."
  },
  {
    "people": [
      "Rawlik",
      "Bertsekas",
      "Ari Pakman",
      "Roy Fox"
    ],
    "review": "Very nice work!\nInteresting to see how well it works to combine a reward signal with cross-entropy from an informative prior, even without the extra entropy regularization of the KL cost.\n\nWe would like to note a few important things, though:\n\n1. Psi-learning is not the method given in (11), (12), and Appendix 8.1.1.\nUsing the current policy as the prior for each small policy update is central to Psi-learning and for the properties of the Psi function.\nThis is best seen by comparing your (23) with Rawlik et al.'s (14), where they have Psi-\\bar{Psi} substituted for log(p).\nBy having p fixed and not dependent of Psi, you are qualitatively changing the fixed point of the equation.\nFor example, their Psi function is gap increasing, in that it only recovers the value function in the optimal action, and is negative infinity for suboptimal actions \u2014 your \"Psi\" function does not have this property, and the two should not be confused.\n\n2. Psi-learning is distinct from G-learning, in that the former uses small divergences from a changing prior, while the latter uses large divergences from a fixed prior.\nBy fixing the prior p in both of your derivations, both are using G-learning with a fixed temperature, albeit with different parameterization of the function G.\nThe two functions defined in (16) and (24) differ by the constant log(p), and this is then reflected in (23) and (30).\nIntriguingly, parameterizing G+log(p) yields better results than parameterizing G.\nThis is somewhat reminiscent of advantage learning, where one parameterizes Q-V rather than Q, and certainly merits further study.\n\n3. In DQN, the Q-network is a feed-forward network that maps (s,a) inputs into real outputs.\nIf this is the architecture used here, it is unclear how it is initialized from the differently shaped Note RNN.\nOn other hand, if the Q-network here is a RNN, it is unclear what state it keeps and how it computes Q(s,a;theta).\n\n4. As Figure 2 shows, \"G-learning\" achieves a different tradeoff with worse music-theory reward but better log(p).\nIt is therefore inaccurate to characterize it in Section 6 and Table 1 as doing worse.\nFor a fair comparison, one needs to set c differently for each algorithm, so that one reward is the same, and compare the other reward.\nIt would also be useful to repeat this for various values of c, and plot the two rewards on a plane, to allow comparing the reward-achievability regions of the algorithms.\n\n5. Stochastic optimal control is a much broader field (cf. Bertsekas 1995). In Section 3.1 you are referring specifically to the field known as KL control.\n\n\nAri Pakman and Roy Fox"
  },
  {
    "people": [
      "K\u00fcmmerer"
    ],
    "review": "The authors formulate a recurrent deep neural network to predict human fixation locations in videos as a mixture of Gaussians. They train the model using maximum likelihood with actual fixation data. Apart from evaluating how good the model performs at predicting fixations, they combine the saliency predictions with the C3D features for action recognition.\n\nquality: I am missing a more thorough evaluation of the fixation prediction performance. The center bias performance in Table 1 differs significantly from the on in Table 2. All the state-of-the-art models reported in Table 2 have a performance worse than the center bias performance reported in Table 1. Is there really no other model better than the center bias? Additionally I am missing details on how central bias and human performance are modelled. Is human performance cross-validated?\n\nYou claim that your \"results are very close to human performance (the difference is only 3.2%). This difference is actually larger than the difference between Central Bias and your model reported in Table 1. Apart from this, it is dangerous to compare AUC performance differences due to e.g. saturation issues.\n\nclarity: the explanation for Table 3 is a bit confusing, also it is not clear why the CONV5 and the FC6 models differ in how the saliency map is used. At least one should also evaluate the CONV5 model when multiplying the input with the saliency map to see how much of the difference comes from the different ways to use the saliency map and how much from the different features.\n\nOther issues:\n\nYou cite K\u00fcmmerer et. al 2015 as a model which \"learns ... indirectly rather than from explicit information of where humans look\", however the their model has been trained on fixation data using maximum-likelihood.\n\nApart from these issues, I think the paper make a very interesting contribution to spatio-temporal fixation prediction. If the evaluation issues given above are sorted out, I will happily improve my rating."
  },
  {
    "people": [
      "K\u00fcmmerer"
    ],
    "review": "The authors formulate a recurrent deep neural network to predict human fixation locations in videos as a mixture of Gaussians. They train the model using maximum likelihood with actual fixation data. Apart from evaluating how good the model performs at predicting fixations, they combine the saliency predictions with the C3D features for action recognition.\n\nquality: I am missing a more thorough evaluation of the fixation prediction performance. The center bias performance in Table 1 differs significantly from the on in Table 2. All the state-of-the-art models reported in Table 2 have a performance worse than the center bias performance reported in Table 1. Is there really no other model better than the center bias? Additionally I am missing details on how central bias and human performance are modelled. Is human performance cross-validated?\n\nYou claim that your \"results are very close to human performance (the difference is only 3.2%). This difference is actually larger than the difference between Central Bias and your model reported in Table 1. Apart from this, it is dangerous to compare AUC performance differences due to e.g. saturation issues.\n\nclarity: the explanation for Table 3 is a bit confusing, also it is not clear why the CONV5 and the FC6 models differ in how the saliency map is used. At least one should also evaluate the CONV5 model when multiplying the input with the saliency map to see how much of the difference comes from the different ways to use the saliency map and how much from the different features.\n\nOther issues:\n\nYou cite K\u00fcmmerer et. al 2015 as a model which \"learns ... indirectly rather than from explicit information of where humans look\", however the their model has been trained on fixation data using maximum-likelihood.\n\nApart from these issues, I think the paper make a very interesting contribution to spatio-temporal fixation prediction. If the evaluation issues given above are sorted out, I will happily improve my rating."
  },
  {
    "people": [
      "Ilya Sutskever",
      "Geoffrey Hinton",
      "Maximillian Nickel"
    ],
    "review": "This paper discusses recurrent networks with an update rule of the form h_{t+1} = R_x R h_{t}, where R_x is an embedding of the input x into the space of orthogonal or unitary matrices, and R is a shared orthogonal or unitary matrix.    While this is an interesting model, it is by no means a *new* model:  the idea of using matrices to represent input objects (and multiplication to update state) is often used in the embedding-knowledge-bases or embedding-logic literature (e.g. Using matrices to model symbolic relationships by Ilya Sutskever and Geoffrey Hinton, or Holographic Embeddings of Knowledge Graphs by Maximillian Nickel et al.).  I don't think the experiments or analysis in this work add much to our understanding of it.    In particular, the experiments are especially weak, consisting only of a very simplified version of the copy task (which is already very much a toy).  I know several people who have played with this model in the setting of language modeling, and as the other reviewer notes, the inability of the model to forget is an actual annoyance.   \n\nI think it is incumbent on the authors to show how this model can be really useful on a nontrivial task; as it is we should not accept this paper.\n\nSome questions:  is there any reason to use the shared R instead of absorbing it into all the R_x?  Can you find any nice ways of using the fact that the model is linear in h or linear in R_x ?"
  },
  {
    "people": [
      "Ilya Sutskever",
      "Geoffrey Hinton",
      "Maximillian Nickel"
    ],
    "review": "This paper discusses recurrent networks with an update rule of the form h_{t+1} = R_x R h_{t}, where R_x is an embedding of the input x into the space of orthogonal or unitary matrices, and R is a shared orthogonal or unitary matrix.    While this is an interesting model, it is by no means a *new* model:  the idea of using matrices to represent input objects (and multiplication to update state) is often used in the embedding-knowledge-bases or embedding-logic literature (e.g. Using matrices to model symbolic relationships by Ilya Sutskever and Geoffrey Hinton, or Holographic Embeddings of Knowledge Graphs by Maximillian Nickel et al.).  I don't think the experiments or analysis in this work add much to our understanding of it.    In particular, the experiments are especially weak, consisting only of a very simplified version of the copy task (which is already very much a toy).  I know several people who have played with this model in the setting of language modeling, and as the other reviewer notes, the inability of the model to forget is an actual annoyance.   \n\nI think it is incumbent on the authors to show how this model can be really useful on a nontrivial task; as it is we should not accept this paper.\n\nSome questions:  is there any reason to use the shared R instead of absorbing it into all the R_x?  Can you find any nice ways of using the fact that the model is linear in h or linear in R_x ?"
  },
  {
    "people": [
      "Easton",
      "McGurk",
      "Neti"
    ],
    "review": "UPDATE:  I have read the authors' responses.  I did not read the social media comments about this paper prior to reviewing it.  \n\nI appreciate the authors' updates in response to the reviewer comments.  Overall, however, my review stands.  The authors have taken a task that had not yet been addressed with a straightforward modern deep learning approach, and addressed it with such an approach.  I assume that if we pick up any task that hasn't been worked on for a while, and give it a solid deep learning treatment, we will do well.  I do not see such papers as a contribution to ICLR, unless they also provide new insights, analysis, or surprising results (which, to my mind, this paper does not).  This is a general point and the program chairs may disagree with it, of course.\n\nI have removed my recommendation that this be accepted as a workshop paper, as I have since noticed that the workshop track this year has a different focus.  \n\n************************\n\nORIGINAL REVIEW:\n\nThe authors show that an appropriately engineered LSTM+CNN+CTC network does an excellent job of lipreading on the GRID corpus.  This is a nice result to know about--yet another example of a really nice result that one can get the first time one applies such methods to an old task--and all of the work that went into getting it looks solid (and likely involved some significant engineering effort).  However, this in itself is not sufficiently novel for publication at ICLR.  The paper also needs to be revised to better represent prior work, and ideally remove some of the vague motivational language.  Some specifics on what I think needs to be revised:\n\n- First, the claim of being the first to do sentence-level lipreading.  As mentioned in a pre-review comment, this is not true.  The paper should be revised to discuss the prior work on this task (even though much of it used data that is not public).  Ideally the title should also be changed in light of this.\n\n- The comparison with human lipreaders needs to be qualified a bit.  This task is presumably very unnatural for humans because of the unusual grammar, so perhaps what you are showing is that a machine can better take into account the strong contraints.  This is great, but not a general statement about LipNet vs. humans.\n\n- The paper contains some unnecessary motivational platitudes.  We do not need to invoke Easton and Basala 1982 to motivate modeling context in a linguistic sequence prediction task, and prior work using older sequence models (e.g. HMMs) for lipreading has modeled context as well.  The McGurk effect does not show that lipreading plays a crucial role in human communication.\n\n- It is worth noting that even without the spatial convolution, your Baseline-2D already does extremely well.  So I am not sure about the \"importance of spatiotemporal feature extraction\" as stated in the conclusion.\n\nSome more minor comments, typos, etc.:\n\n- citations for LSTMs, CTC, etc. should be provided the first time they are mentioned.\n- I did not quite follow the justification for upsampling.\n- what is meant by \"lip-rounding vowels\"?  They seem to include almost all English vowels.\n- Did you consider keeping the vowel visemes V1-V4 separate rather than collapsing them into one?  Since you list Neti et al.'s full viseme set, it is worth mentioning why you modified it.\n- \"Given that the speakers are British, the confusion between /aa/ and /ay/...\" -- I am not sure what this has to do with British speakers, as the relationship between these vowels exists in other English dialects as well (e.g. American).\n- The discussion about confusions within bilabial stops and within alveolar stops is a bit mismatched with the actual confusion data in Fig. 3(b,c).  For example, there does not seem to be any confusion between /m/ and /b/ or between /m/ and /p/.\n- \"lipreading actuations\":  I am not sure what \"actuations\" means in this context\n- \"palato-alvealoar\" --> \"palato-alveolar\"\n- \"Articulatorily alveolar\" --> \"Alveolar\"?"
  },
  {
    "people": [
      "McGurk"
    ],
    "review": "Let me start by saying that your area chair does not read Twitter, Reddit/ML, etc. The metareview below is, therefore, based purely on the manuscript and the reviews and rebuttal on OpenReview.\n \n The goal of the ICLR review process is to establish a constructive discussion between the authors of a paper on one side and reviewers and the broader machine-learning community on the other side. The goal of this discussion is to help the authors leverage the community for improving their manuscript.\n \n Whilst one may argue that some of the initial reviews could have provided a more detailed motivation for their rating, there is no evidence that the reviewers were influenced (or even aware of) discussions about this paper on social or other media --- in fact, none of the reviews refers to claims made in those media. Suggestions by the authors that the reviewers are biased by (social) media are, therefore, unfounded: there can be many valid reasons for the differences in opinion between reviewers and authors on the novelty, originality, or importance of this work. The authors are free to debate the opinion of the reviewers, but referring to the reviews as \"absolute nonsense\", \"unreasonable\", \"condescending\", and \"disrespectful\" is not helping the constructive scientific discussion that ICLR envisions and, frankly, is very offensive to reviewers who voluntarily spend their time in order to improve the quality of scientific research in our field.\n \n Two area chairs have read the paper. They independently reached the conclusion that (1) the reviewers raise valid concerns with respect to the novelty and importance of this work and (2) that the paper is, indeed, borderline for ICLR. The paper is an application paper, in which the authors propose the first\u00caend-to-end sentence level lip reading using deep learning. Positive aspects of the paper include:\n \n - A comprehensive and organized review about previous work.\n - Clear description of the model and experimental methods.\n - Careful reporting of the results, with attention to detail.\n - Proposed method appears to perform better than the prior state-of-the-art, and generalizes across speakers.\n \n However, the paper has several prominent negative aspects as well:\n \n - The GRID corpus that is used for experimentation has very substantial (known) limitations. In particular, it is constructed in a way that leads to a very limited (non-natural) set of sentences.\u00ca(For every word, there is an average of just 8.5 possible options the model has to choose from.)\n - The paper overstates some of its claims. In particular, the claim that the model is \"outperforming experienced human lipreaders\" is questionable: it is not unlikely that model achieves its performance by exploiting unrealistic statistical biases in the corpus that humans cannot / do not exploit. Similarly, the claims about the \"sentence-level\" nature of the model are not substantiated: it remains unclear what aspects of the model make this a sentence-level model, nor is there much empirical evidence that the sentence-level treatment of video data is helping much (the NoLM baseline is almost as good as LipNet, despite the strong biases in the GRID corpus).\n - The paper makes several other statements that are not well-founded. As one of the reviewers correctly remarks, the McGurk effect does not show that lipreading plays a crucial role in human communication (it merely shows that vision can influence speech recognition). Similarly, the claim that \"Bi-GRUs are crucial for efficient further aggregation\" is not supported by empirical evidence.\n \n A high-level downside of this paper is that, while studying a relevant application of deep learning, it presents no technical contributions or novel insights that have impact beyond the application studied in the paper."
  },
  {
    "people": [
      "Easton",
      "McGurk",
      "Neti"
    ],
    "review": "UPDATE:  I have read the authors' responses.  I did not read the social media comments about this paper prior to reviewing it.  \n\nI appreciate the authors' updates in response to the reviewer comments.  Overall, however, my review stands.  The authors have taken a task that had not yet been addressed with a straightforward modern deep learning approach, and addressed it with such an approach.  I assume that if we pick up any task that hasn't been worked on for a while, and give it a solid deep learning treatment, we will do well.  I do not see such papers as a contribution to ICLR, unless they also provide new insights, analysis, or surprising results (which, to my mind, this paper does not).  This is a general point and the program chairs may disagree with it, of course.\n\nI have removed my recommendation that this be accepted as a workshop paper, as I have since noticed that the workshop track this year has a different focus.  \n\n************************\n\nORIGINAL REVIEW:\n\nThe authors show that an appropriately engineered LSTM+CNN+CTC network does an excellent job of lipreading on the GRID corpus.  This is a nice result to know about--yet another example of a really nice result that one can get the first time one applies such methods to an old task--and all of the work that went into getting it looks solid (and likely involved some significant engineering effort).  However, this in itself is not sufficiently novel for publication at ICLR.  The paper also needs to be revised to better represent prior work, and ideally remove some of the vague motivational language.  Some specifics on what I think needs to be revised:\n\n- First, the claim of being the first to do sentence-level lipreading.  As mentioned in a pre-review comment, this is not true.  The paper should be revised to discuss the prior work on this task (even though much of it used data that is not public).  Ideally the title should also be changed in light of this.\n\n- The comparison with human lipreaders needs to be qualified a bit.  This task is presumably very unnatural for humans because of the unusual grammar, so perhaps what you are showing is that a machine can better take into account the strong contraints.  This is great, but not a general statement about LipNet vs. humans.\n\n- The paper contains some unnecessary motivational platitudes.  We do not need to invoke Easton and Basala 1982 to motivate modeling context in a linguistic sequence prediction task, and prior work using older sequence models (e.g. HMMs) for lipreading has modeled context as well.  The McGurk effect does not show that lipreading plays a crucial role in human communication.\n\n- It is worth noting that even without the spatial convolution, your Baseline-2D already does extremely well.  So I am not sure about the \"importance of spatiotemporal feature extraction\" as stated in the conclusion.\n\nSome more minor comments, typos, etc.:\n\n- citations for LSTMs, CTC, etc. should be provided the first time they are mentioned.\n- I did not quite follow the justification for upsampling.\n- what is meant by \"lip-rounding vowels\"?  They seem to include almost all English vowels.\n- Did you consider keeping the vowel visemes V1-V4 separate rather than collapsing them into one?  Since you list Neti et al.'s full viseme set, it is worth mentioning why you modified it.\n- \"Given that the speakers are British, the confusion between /aa/ and /ay/...\" -- I am not sure what this has to do with British speakers, as the relationship between these vowels exists in other English dialects as well (e.g. American).\n- The discussion about confusions within bilabial stops and within alveolar stops is a bit mismatched with the actual confusion data in Fig. 3(b,c).  For example, there does not seem to be any confusion between /m/ and /b/ or between /m/ and /p/.\n- \"lipreading actuations\":  I am not sure what \"actuations\" means in this context\n- \"palato-alvealoar\" --> \"palato-alveolar\"\n- \"Articulatorily alveolar\" --> \"Alveolar\"?"
  },
  {
    "people": [
      "Ngiam",
      "Hilder"
    ],
    "review": "\n\nThis is an interesting paper. Here are some comments:\n\n\n1) Testing on 4 subjects only is rather limited given that there are 33 subjects. For example Ngiam et al. used 18 subjects for training and 18 for testing on CUAVE which contains a similar number of subjects. So a similar setup would be 17 subjects for training and 16 for testing. This would make the results much more convincing.\n\n2) The authors state that the performance of human lipreaders is around 20% and cite Hilder et al. In that paper, the human performance is around 70%, please correct this claim. In addition, in that paper machine lip-reading outperforms humans as well so please make this point clear. \n\n3) The state of the art performance on GRID is not 79.6% as mentioned in the text. It is the one mentioned in the other comments. Actually, as of last week there is a new state-of-the-art "
  },
  {
    "people": [
      "S. Gergen",
      "S. Zeiler",
      "A. Hussen Abdelaziz",
      "R. Nickel",
      "D. Kolossa"
    ],
    "review": "Dear all,\n\nmany thanks for sharing the paper, which I did find hugely interesting - while the GRiD corpus is small, I do agree that it is currently a good task for taking first steps towards video-based and audio-visual speech recognition, and I believe the work you did here will ultimately be very useful for lipreading (and for lipreading-based speech enhancement) in more general scenarios.\n\nI do have one comment regarding the prior-state-of-the-art system that you cite: As far as I know, our group's Interspeech 2016 holds the record for best lipreading performance on the GRiD corpus, with 86.4% word accuracy (this does include a grammar, but it might still be interesting).\nPlease cite: S. Gergen, S. Zeiler, A. Hussen Abdelaziz, R. Nickel and D. Kolossa: \" Dynamic Stream Weighting for Turbo-Decoding-Based Audiovisual ASR,\" in Proc. Interspeech 2016, San Francisco, Sept. 2016.\n"
  },
  {
    "people": [
      "M. Wand",
      "J. Koutnik",
      "J. Schmidhuber",
      "S.  Zhang",
      "Shen",
      "Xingjian Shi",
      "Hao Wang",
      "Dit-Yan Yeung",
      "Wai-Kin Wong",
      "Wang-chun Woo"
    ],
    "review": "- It is not fair to use a character 5-gram language model and compare to [1] which doesn't use any language modeling, the authors should also report their results without using any language modeling (IMHO using a language model with a limited vocabulary corpus like GRID which has only 51 words and 64000 possible sentence makes the results misleading, which, I think, is the reason why no language modeling was used in [1])\n\n- It is not fair to augment the training data to 15x, then compare to [1] which doesn't use any data augmentation\n\n- It should be mentioned and cited that a CNN+RNN+CTC architecture is not novel and has been widely used in literature for sequence recognition tasks (e.g. [2],[3],[4])\n\n- I encourage the authors to also try a ConvLSTM [5], which have recently shown very promising performance in a number of video-related tasks\n\n---------------------------------------------------\n[1] M. Wand, J. Koutnik, and J. Schmidhuber. Lipreading with long short-term memory.\n[2] B.  Shi,  X.  Bai,  and  C.  Yao. An  end-to-end  trainable  neural  network for  image-based  sequence  recognition  and  its  application  to  scene  text recognition.\n[3]  Z.  Xie,  Z.  Sun,  L.  Jin,  Z.  Feng,  and  S.  Zhang. Fully  convolutional recurrent network for handwritten chinese text recognition.\n[4] Li, H., Shen, C.: Reading car license plates using deep convolutional neural networks and lstms\n[5] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. Convolutional LSTM network: A machine learning approach for precipitation nowcasting."
  },
  {
    "people": [
      "Martin Cooke",
      "Jon Barker",
      "Stuart Cunningham",
      "Xu Shao",
      "Martin"
    ],
    "review": "This corpus is a small data set created 10 years ago by colleagues and friends (Martin Cooke, Jon Barker, Stuart Cunningham and Xu Shao) at the Department of Computer Science. I recall that Martin gave me a bottle of Spanish wine for my trouble.\nAs far as I remember the corpus, it was designed to remove higher order language structure. That structure that (I believe) is used by humans to cue on when reading lips.\n\nThe corpus has a limited vocabulary and a single syntax grammar. So while it's promising to perform well on this data, it's not really ground breaking, particularly if you are interested in sentence models: the corpus sentence structure is super simple.\nSo while the model may be able to read my lips better than a human, it can only do so when I say a meaningless list of words from a highly constrained vocabulary in a specific order. That may be an advance, but it's not one worthy of disturbing me on a Sunday (serves me right for reading Twitter on a Sunday).\n\nI'm not making a comment about whether the paper should be accepted or not, but merely reacting to the large number of claims for the paper we are seeing on social media. The particular result for this data set may well be state of the art.\n"
  },
  {
    "people": [
      "Easton",
      "McGurk",
      "Neti"
    ],
    "review": "UPDATE:  I have read the authors' responses.  I did not read the social media comments about this paper prior to reviewing it.  \n\nI appreciate the authors' updates in response to the reviewer comments.  Overall, however, my review stands.  The authors have taken a task that had not yet been addressed with a straightforward modern deep learning approach, and addressed it with such an approach.  I assume that if we pick up any task that hasn't been worked on for a while, and give it a solid deep learning treatment, we will do well.  I do not see such papers as a contribution to ICLR, unless they also provide new insights, analysis, or surprising results (which, to my mind, this paper does not).  This is a general point and the program chairs may disagree with it, of course.\n\nI have removed my recommendation that this be accepted as a workshop paper, as I have since noticed that the workshop track this year has a different focus.  \n\n************************\n\nORIGINAL REVIEW:\n\nThe authors show that an appropriately engineered LSTM+CNN+CTC network does an excellent job of lipreading on the GRID corpus.  This is a nice result to know about--yet another example of a really nice result that one can get the first time one applies such methods to an old task--and all of the work that went into getting it looks solid (and likely involved some significant engineering effort).  However, this in itself is not sufficiently novel for publication at ICLR.  The paper also needs to be revised to better represent prior work, and ideally remove some of the vague motivational language.  Some specifics on what I think needs to be revised:\n\n- First, the claim of being the first to do sentence-level lipreading.  As mentioned in a pre-review comment, this is not true.  The paper should be revised to discuss the prior work on this task (even though much of it used data that is not public).  Ideally the title should also be changed in light of this.\n\n- The comparison with human lipreaders needs to be qualified a bit.  This task is presumably very unnatural for humans because of the unusual grammar, so perhaps what you are showing is that a machine can better take into account the strong contraints.  This is great, but not a general statement about LipNet vs. humans.\n\n- The paper contains some unnecessary motivational platitudes.  We do not need to invoke Easton and Basala 1982 to motivate modeling context in a linguistic sequence prediction task, and prior work using older sequence models (e.g. HMMs) for lipreading has modeled context as well.  The McGurk effect does not show that lipreading plays a crucial role in human communication.\n\n- It is worth noting that even without the spatial convolution, your Baseline-2D already does extremely well.  So I am not sure about the \"importance of spatiotemporal feature extraction\" as stated in the conclusion.\n\nSome more minor comments, typos, etc.:\n\n- citations for LSTMs, CTC, etc. should be provided the first time they are mentioned.\n- I did not quite follow the justification for upsampling.\n- what is meant by \"lip-rounding vowels\"?  They seem to include almost all English vowels.\n- Did you consider keeping the vowel visemes V1-V4 separate rather than collapsing them into one?  Since you list Neti et al.'s full viseme set, it is worth mentioning why you modified it.\n- \"Given that the speakers are British, the confusion between /aa/ and /ay/...\" -- I am not sure what this has to do with British speakers, as the relationship between these vowels exists in other English dialects as well (e.g. American).\n- The discussion about confusions within bilabial stops and within alveolar stops is a bit mismatched with the actual confusion data in Fig. 3(b,c).  For example, there does not seem to be any confusion between /m/ and /b/ or between /m/ and /p/.\n- \"lipreading actuations\":  I am not sure what \"actuations\" means in this context\n- \"palato-alvealoar\" --> \"palato-alveolar\"\n- \"Articulatorily alveolar\" --> \"Alveolar\"?"
  },
  {
    "people": [
      "McGurk"
    ],
    "review": "Let me start by saying that your area chair does not read Twitter, Reddit/ML, etc. The metareview below is, therefore, based purely on the manuscript and the reviews and rebuttal on OpenReview.\n \n The goal of the ICLR review process is to establish a constructive discussion between the authors of a paper on one side and reviewers and the broader machine-learning community on the other side. The goal of this discussion is to help the authors leverage the community for improving their manuscript.\n \n Whilst one may argue that some of the initial reviews could have provided a more detailed motivation for their rating, there is no evidence that the reviewers were influenced (or even aware of) discussions about this paper on social or other media --- in fact, none of the reviews refers to claims made in those media. Suggestions by the authors that the reviewers are biased by (social) media are, therefore, unfounded: there can be many valid reasons for the differences in opinion between reviewers and authors on the novelty, originality, or importance of this work. The authors are free to debate the opinion of the reviewers, but referring to the reviews as \"absolute nonsense\", \"unreasonable\", \"condescending\", and \"disrespectful\" is not helping the constructive scientific discussion that ICLR envisions and, frankly, is very offensive to reviewers who voluntarily spend their time in order to improve the quality of scientific research in our field.\n \n Two area chairs have read the paper. They independently reached the conclusion that (1) the reviewers raise valid concerns with respect to the novelty and importance of this work and (2) that the paper is, indeed, borderline for ICLR. The paper is an application paper, in which the authors propose the first\u00caend-to-end sentence level lip reading using deep learning. Positive aspects of the paper include:\n \n - A comprehensive and organized review about previous work.\n - Clear description of the model and experimental methods.\n - Careful reporting of the results, with attention to detail.\n - Proposed method appears to perform better than the prior state-of-the-art, and generalizes across speakers.\n \n However, the paper has several prominent negative aspects as well:\n \n - The GRID corpus that is used for experimentation has very substantial (known) limitations. In particular, it is constructed in a way that leads to a very limited (non-natural) set of sentences.\u00ca(For every word, there is an average of just 8.5 possible options the model has to choose from.)\n - The paper overstates some of its claims. In particular, the claim that the model is \"outperforming experienced human lipreaders\" is questionable: it is not unlikely that model achieves its performance by exploiting unrealistic statistical biases in the corpus that humans cannot / do not exploit. Similarly, the claims about the \"sentence-level\" nature of the model are not substantiated: it remains unclear what aspects of the model make this a sentence-level model, nor is there much empirical evidence that the sentence-level treatment of video data is helping much (the NoLM baseline is almost as good as LipNet, despite the strong biases in the GRID corpus).\n - The paper makes several other statements that are not well-founded. As one of the reviewers correctly remarks, the McGurk effect does not show that lipreading plays a crucial role in human communication (it merely shows that vision can influence speech recognition). Similarly, the claim that \"Bi-GRUs are crucial for efficient further aggregation\" is not supported by empirical evidence.\n \n A high-level downside of this paper is that, while studying a relevant application of deep learning, it presents no technical contributions or novel insights that have impact beyond the application studied in the paper."
  },
  {
    "people": [
      "Easton",
      "McGurk",
      "Neti"
    ],
    "review": "UPDATE:  I have read the authors' responses.  I did not read the social media comments about this paper prior to reviewing it.  \n\nI appreciate the authors' updates in response to the reviewer comments.  Overall, however, my review stands.  The authors have taken a task that had not yet been addressed with a straightforward modern deep learning approach, and addressed it with such an approach.  I assume that if we pick up any task that hasn't been worked on for a while, and give it a solid deep learning treatment, we will do well.  I do not see such papers as a contribution to ICLR, unless they also provide new insights, analysis, or surprising results (which, to my mind, this paper does not).  This is a general point and the program chairs may disagree with it, of course.\n\nI have removed my recommendation that this be accepted as a workshop paper, as I have since noticed that the workshop track this year has a different focus.  \n\n************************\n\nORIGINAL REVIEW:\n\nThe authors show that an appropriately engineered LSTM+CNN+CTC network does an excellent job of lipreading on the GRID corpus.  This is a nice result to know about--yet another example of a really nice result that one can get the first time one applies such methods to an old task--and all of the work that went into getting it looks solid (and likely involved some significant engineering effort).  However, this in itself is not sufficiently novel for publication at ICLR.  The paper also needs to be revised to better represent prior work, and ideally remove some of the vague motivational language.  Some specifics on what I think needs to be revised:\n\n- First, the claim of being the first to do sentence-level lipreading.  As mentioned in a pre-review comment, this is not true.  The paper should be revised to discuss the prior work on this task (even though much of it used data that is not public).  Ideally the title should also be changed in light of this.\n\n- The comparison with human lipreaders needs to be qualified a bit.  This task is presumably very unnatural for humans because of the unusual grammar, so perhaps what you are showing is that a machine can better take into account the strong contraints.  This is great, but not a general statement about LipNet vs. humans.\n\n- The paper contains some unnecessary motivational platitudes.  We do not need to invoke Easton and Basala 1982 to motivate modeling context in a linguistic sequence prediction task, and prior work using older sequence models (e.g. HMMs) for lipreading has modeled context as well.  The McGurk effect does not show that lipreading plays a crucial role in human communication.\n\n- It is worth noting that even without the spatial convolution, your Baseline-2D already does extremely well.  So I am not sure about the \"importance of spatiotemporal feature extraction\" as stated in the conclusion.\n\nSome more minor comments, typos, etc.:\n\n- citations for LSTMs, CTC, etc. should be provided the first time they are mentioned.\n- I did not quite follow the justification for upsampling.\n- what is meant by \"lip-rounding vowels\"?  They seem to include almost all English vowels.\n- Did you consider keeping the vowel visemes V1-V4 separate rather than collapsing them into one?  Since you list Neti et al.'s full viseme set, it is worth mentioning why you modified it.\n- \"Given that the speakers are British, the confusion between /aa/ and /ay/...\" -- I am not sure what this has to do with British speakers, as the relationship between these vowels exists in other English dialects as well (e.g. American).\n- The discussion about confusions within bilabial stops and within alveolar stops is a bit mismatched with the actual confusion data in Fig. 3(b,c).  For example, there does not seem to be any confusion between /m/ and /b/ or between /m/ and /p/.\n- \"lipreading actuations\":  I am not sure what \"actuations\" means in this context\n- \"palato-alvealoar\" --> \"palato-alveolar\"\n- \"Articulatorily alveolar\" --> \"Alveolar\"?"
  },
  {
    "people": [
      "Ngiam",
      "Hilder"
    ],
    "review": "\n\nThis is an interesting paper. Here are some comments:\n\n\n1) Testing on 4 subjects only is rather limited given that there are 33 subjects. For example Ngiam et al. used 18 subjects for training and 18 for testing on CUAVE which contains a similar number of subjects. So a similar setup would be 17 subjects for training and 16 for testing. This would make the results much more convincing.\n\n2) The authors state that the performance of human lipreaders is around 20% and cite Hilder et al. In that paper, the human performance is around 70%, please correct this claim. In addition, in that paper machine lip-reading outperforms humans as well so please make this point clear. \n\n3) The state of the art performance on GRID is not 79.6% as mentioned in the text. It is the one mentioned in the other comments. Actually, as of last week there is a new state-of-the-art "
  },
  {
    "people": [
      "S. Gergen",
      "S. Zeiler",
      "A. Hussen Abdelaziz",
      "R. Nickel",
      "D. Kolossa"
    ],
    "review": "Dear all,\n\nmany thanks for sharing the paper, which I did find hugely interesting - while the GRiD corpus is small, I do agree that it is currently a good task for taking first steps towards video-based and audio-visual speech recognition, and I believe the work you did here will ultimately be very useful for lipreading (and for lipreading-based speech enhancement) in more general scenarios.\n\nI do have one comment regarding the prior-state-of-the-art system that you cite: As far as I know, our group's Interspeech 2016 holds the record for best lipreading performance on the GRiD corpus, with 86.4% word accuracy (this does include a grammar, but it might still be interesting).\nPlease cite: S. Gergen, S. Zeiler, A. Hussen Abdelaziz, R. Nickel and D. Kolossa: \" Dynamic Stream Weighting for Turbo-Decoding-Based Audiovisual ASR,\" in Proc. Interspeech 2016, San Francisco, Sept. 2016.\n"
  },
  {
    "people": [
      "M. Wand",
      "J. Koutnik",
      "J. Schmidhuber",
      "S.  Zhang",
      "Shen",
      "Xingjian Shi",
      "Hao Wang",
      "Dit-Yan Yeung",
      "Wai-Kin Wong",
      "Wang-chun Woo"
    ],
    "review": "- It is not fair to use a character 5-gram language model and compare to [1] which doesn't use any language modeling, the authors should also report their results without using any language modeling (IMHO using a language model with a limited vocabulary corpus like GRID which has only 51 words and 64000 possible sentence makes the results misleading, which, I think, is the reason why no language modeling was used in [1])\n\n- It is not fair to augment the training data to 15x, then compare to [1] which doesn't use any data augmentation\n\n- It should be mentioned and cited that a CNN+RNN+CTC architecture is not novel and has been widely used in literature for sequence recognition tasks (e.g. [2],[3],[4])\n\n- I encourage the authors to also try a ConvLSTM [5], which have recently shown very promising performance in a number of video-related tasks\n\n---------------------------------------------------\n[1] M. Wand, J. Koutnik, and J. Schmidhuber. Lipreading with long short-term memory.\n[2] B.  Shi,  X.  Bai,  and  C.  Yao. An  end-to-end  trainable  neural  network for  image-based  sequence  recognition  and  its  application  to  scene  text recognition.\n[3]  Z.  Xie,  Z.  Sun,  L.  Jin,  Z.  Feng,  and  S.  Zhang. Fully  convolutional recurrent network for handwritten chinese text recognition.\n[4] Li, H., Shen, C.: Reading car license plates using deep convolutional neural networks and lstms\n[5] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. Convolutional LSTM network: A machine learning approach for precipitation nowcasting."
  },
  {
    "people": [
      "Martin Cooke",
      "Jon Barker",
      "Stuart Cunningham",
      "Xu Shao",
      "Martin"
    ],
    "review": "This corpus is a small data set created 10 years ago by colleagues and friends (Martin Cooke, Jon Barker, Stuart Cunningham and Xu Shao) at the Department of Computer Science. I recall that Martin gave me a bottle of Spanish wine for my trouble.\nAs far as I remember the corpus, it was designed to remove higher order language structure. That structure that (I believe) is used by humans to cue on when reading lips.\n\nThe corpus has a limited vocabulary and a single syntax grammar. So while it's promising to perform well on this data, it's not really ground breaking, particularly if you are interested in sentence models: the corpus sentence structure is super simple.\nSo while the model may be able to read my lips better than a human, it can only do so when I say a meaningless list of words from a highly constrained vocabulary in a specific order. That may be an advance, but it's not one worthy of disturbing me on a Sunday (serves me right for reading Twitter on a Sunday).\n\nI'm not making a comment about whether the paper should be accepted or not, but merely reacting to the large number of claims for the paper we are seeing on social media. The particular result for this data set may well be state of the art.\n"
  },
  {
    "people": [
      "Taylor"
    ],
    "review": "The paper presents a method for pruning filters from convolutional neural networks based on the first order Taylor expansion of the loss change. The method is novel and well justified with extensive empirical evaluation."
  },
  {
    "people": [
      "Wen",
      "Wei",
      "Lebedev",
      "Vadim",
      "Victor Lempitsky",
      "Alvarez",
      "Jose M.",
      "Mathieu Salzmann"
    ],
    "review": "1) Wen, Wei, et al. \"Learning structured sparsity in deep neural networks.\" Advances in Neural Information Processing Systems. 2016.\n2) Lebedev, Vadim, and Victor Lempitsky. \"Fast convnets using group-wise brain damage.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.\n3) Alvarez, Jose M., and Mathieu Salzmann. \"Learning the Number of Neurons in Deep Networks.\" Advances in Neural Information Processing Systems. 2016."
  },
  {
    "people": [
      "Hu",
      "Taylor",
      "Taylor"
    ],
    "review": "We would like to thank reviewers for their comments and suggestions. We added a new revision which addresses main points requested for clarification.\nList of changes:\n- Added comparison with Optimal Brain Damage. See section 2.2, paragraph \"Relation to Optimal Brain Damage\" for discussion, appendix A.6 for implementation details, Tables 1 and 3, Figures 4 and 5 for results and comparisons\n- Comparison with average percentage of zeros criterion (APoZ) proposed by Hu et al. (2016)\n- Comparison with pruning by regularization in appendix A.4\n- Wall-clock time measurements for inference of pruned networks in section 3.6\n- Results of combining Taylor and activation criteria in appendix A.5\n- (Jan 16) Correlation of Taylor criterion with gradient and activation, appendix A.7"
  },
  {
    "people": [
      "Taylor"
    ],
    "review": "This paper presents a novel way of pruning filters from convolutional neural networks with a strong theoretical justification. The proposed methods is derived from the first order Taylor expansion of the loss change while pruning a particular unit. This leads to simple weighting of the unit activation with its gradient w.r.t. loss function and performs better than simply using the activation magnitude as the heuristic for pruning. This intuitively makes sense, as we would like to remove not only the filters with low activation, but also filters where the incorrect activation value would not have small influence on the target loss.\n\nAuthors thoroughly investigate multiple baselines, including an oracle which sets an upper bound on the target performance even though it is computationally expensive. The devised method seems to be quite elegant and authors show that it generalizes well on multiple tasks and is computationally more than feasible as it is easy to combine with traditional fine tuning procedure. Also, the work clearly shows the trade-offs of increased speed and decreased performance, which is useful for practical applications.\n\nIt would be also useful to compare against different baselines, e.g. [1]. However this method seems to be more useful as it does not involve training of a new network (and thus is probably much faster).\n\nSuggestion - maybe it can be extended in the future towards also removing only parts of the filters(e.g. for the 3D convolution)? This may be more complicated as it would need to change the implementation of convolution operator, but can lead to further speedup.\n\n[1] "
  },
  {
    "people": [
      "Taylor"
    ],
    "review": "The paper presents a method for pruning filters from convolutional neural networks based on the first order Taylor expansion of the loss change. The method is novel and well justified with extensive empirical evaluation."
  },
  {
    "people": [
      "Wen",
      "Wei",
      "Lebedev",
      "Vadim",
      "Victor Lempitsky",
      "Alvarez",
      "Jose M.",
      "Mathieu Salzmann"
    ],
    "review": "1) Wen, Wei, et al. \"Learning structured sparsity in deep neural networks.\" Advances in Neural Information Processing Systems. 2016.\n2) Lebedev, Vadim, and Victor Lempitsky. \"Fast convnets using group-wise brain damage.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.\n3) Alvarez, Jose M., and Mathieu Salzmann. \"Learning the Number of Neurons in Deep Networks.\" Advances in Neural Information Processing Systems. 2016."
  },
  {
    "people": [
      "Hu",
      "Taylor",
      "Taylor"
    ],
    "review": "We would like to thank reviewers for their comments and suggestions. We added a new revision which addresses main points requested for clarification.\nList of changes:\n- Added comparison with Optimal Brain Damage. See section 2.2, paragraph \"Relation to Optimal Brain Damage\" for discussion, appendix A.6 for implementation details, Tables 1 and 3, Figures 4 and 5 for results and comparisons\n- Comparison with average percentage of zeros criterion (APoZ) proposed by Hu et al. (2016)\n- Comparison with pruning by regularization in appendix A.4\n- Wall-clock time measurements for inference of pruned networks in section 3.6\n- Results of combining Taylor and activation criteria in appendix A.5\n- (Jan 16) Correlation of Taylor criterion with gradient and activation, appendix A.7"
  },
  {
    "people": [
      "Taylor"
    ],
    "review": "This paper presents a novel way of pruning filters from convolutional neural networks with a strong theoretical justification. The proposed methods is derived from the first order Taylor expansion of the loss change while pruning a particular unit. This leads to simple weighting of the unit activation with its gradient w.r.t. loss function and performs better than simply using the activation magnitude as the heuristic for pruning. This intuitively makes sense, as we would like to remove not only the filters with low activation, but also filters where the incorrect activation value would not have small influence on the target loss.\n\nAuthors thoroughly investigate multiple baselines, including an oracle which sets an upper bound on the target performance even though it is computationally expensive. The devised method seems to be quite elegant and authors show that it generalizes well on multiple tasks and is computationally more than feasible as it is easy to combine with traditional fine tuning procedure. Also, the work clearly shows the trade-offs of increased speed and decreased performance, which is useful for practical applications.\n\nIt would be also useful to compare against different baselines, e.g. [1]. However this method seems to be more useful as it does not involve training of a new network (and thus is probably much faster).\n\nSuggestion - maybe it can be extended in the future towards also removing only parts of the filters(e.g. for the 3D convolution)? This may be more complicated as it would need to change the implementation of convolution operator, but can lead to further speedup.\n\n[1] "
  },
  {
    "people": [
      "Hernandez-Lobato\net al."
    ],
    "review": "This paper considers the problem of model-based policy search. The authors \nconsider the use of Bayesian Neural Networks to learn a model of the environment\nand advocate for the $\\alpha$-divergence minimization rather than the more usual \nvariational Bayes. \n\nThe ability of alpha-divergence to capture bi-modality however \ncomes at a price and most of the paper is devoted to finding tractable approximations. \nThe authors therefore use the approach of Hernandez-Lobato\net al. (2016) as proxy to the alpha-divergence . \n\nThe environment/system dynamics is clearly defined as a well as the policy parametrization \n(section 3) and would constitute a useful reference point for other researchers. \nSimulated roll-outs, using the learned model, then provide samples of the expected \nreturn. Since a model of the environment is available, stochastic gradient descent \ncan be performed in the usual way, without policy gradient estimators, via automatic \ndifferentiation tools. \n\nThe experiments demonstrate that alpha-divergence is capable of capturing multi-model \nstructure which competing methods (variational Bayes and GP) would otherwise\nstruggle with. The proposed approach also compares favorably in a real-world\nbatch setting.\n\nThe paper is well-written, technically rich and combines many recent tools \ninto a coherent algorithm. However, the repeated use of approximations to original \nquantities seems to somehow defeat the benefits of the original problem formulation. \nThe scalability and computational effectiveness of this approach is also questionable \nand I am uncertain if many problem would warrant such complexity in their solution. \nAs with other Bayesian methods, the proposed approach would probably shine in low-samples \nregime and in this case might be preferable to other methods in the same class (VB, GP)."
  },
  {
    "people": [
      "Girard",
      "Girard",
      "Rasmussen",
      "Qui\u00f1onero Candela",
      "Murray Smith,",
      "R."
    ],
    "review": "This paper introduces an approach for model-based control of stochastic dynamical systems with policy search, based on (1) learning the stochastic dynamics of the underlying system with a Bayesian deep neural network (BNN) that allows some of its inputs to be stochastic, and (2) a policy optimization method based on simulated rollouts from the learned dynamics. BNN training is carried out using \\alpha-divergence minimization, the specific form of which was introduced in previous work by the authors. Validation and comparison of the approach is undertaken on a simulated domain, as well as real-world scenarios. \n\nThe paper is tightly written, and easy to follow. Its approach to fitting Bayesian neural networks with \\alpha divergence is interesting and appears novel in this context. The resulting application to model-based control appears to have significant practical impact, particularly in light of the explainability that a system model can bring to specific decisions made by the policy. As such, I think that the paper brings a valuable contribution to the literature.\n\nThat said, I have a few questions and suggestions:\n\n1) In section 2.2, it should be explained how the random z_n input is used by the neural network: is it just concatenated to the other inputs and used as-is, or is there a special treatment?\n\n2) Moreover, much case is made for the need to have stochastic inputs, but only a scalar input seems to be provided throughout. Is this enough? How computationally difficult would providing stochastic inputs of higher dimensionality be?\n\n3) How important is the normality assumption in z_n? How is the variance \\gamma established?\n\n4) It is mentioned that the hidden layers of the neural network are made of rectifiers, but no further utilization of this fact is made in the paper. Is this assumption somehow important in the optimization of the alpha-divergence (beyond what we know about rectifiers to mitigate the vanishing gradient problem) ?\n\n5) Equation (3), denominator \\mathbf{y} should be \\mathbf{Y} ?\n\n6) Section 2.3: it would be helpful to have an overview or discussion of the computational complexity of training BNNs, to understand whether and when they can practicably be used.\n\n7) Between eq (12) and (13), a citation to the statement of the time embedding theorem would be helpful, as well as an indication of how the embedding dimension should be chosen.\n\n8) Figure 1: the subplots should have the letters by which they are referenced in the text on p. 7.\n\n9) In section 4.2.1, it is not clear if the gas turbine data is publicly available, and if so where. In addition more details should be provided, such as the dimensionality of the variables E_t, N_t and A_t.\n\n10) Perhaps the comparisons with Gaussian processes should include variants that support stochastic inputs, such as Girard et al. (2003), to provide some of the same modelling capabilities as what\u2019s made use of here. At least, this strand of work should be mentioned in Section 5.\n\n\nReferences:\n\nGirard, A., Rasmussen, C. E., Qui\u00f1onero Candela, J., & Murray Smith, R. (2003). Gaussian process priors with uncertain inputs-application to multiple-step ahead time series forecasting. Advances in Neural Information Processing Systems, 545-552.\n"
  },
  {
    "people": [
      "Hernandez-Lobato\net al."
    ],
    "review": "This paper considers the problem of model-based policy search. The authors \nconsider the use of Bayesian Neural Networks to learn a model of the environment\nand advocate for the $\\alpha$-divergence minimization rather than the more usual \nvariational Bayes. \n\nThe ability of alpha-divergence to capture bi-modality however \ncomes at a price and most of the paper is devoted to finding tractable approximations. \nThe authors therefore use the approach of Hernandez-Lobato\net al. (2016) as proxy to the alpha-divergence . \n\nThe environment/system dynamics is clearly defined as a well as the policy parametrization \n(section 3) and would constitute a useful reference point for other researchers. \nSimulated roll-outs, using the learned model, then provide samples of the expected \nreturn. Since a model of the environment is available, stochastic gradient descent \ncan be performed in the usual way, without policy gradient estimators, via automatic \ndifferentiation tools. \n\nThe experiments demonstrate that alpha-divergence is capable of capturing multi-model \nstructure which competing methods (variational Bayes and GP) would otherwise\nstruggle with. The proposed approach also compares favorably in a real-world\nbatch setting.\n\nThe paper is well-written, technically rich and combines many recent tools \ninto a coherent algorithm. However, the repeated use of approximations to original \nquantities seems to somehow defeat the benefits of the original problem formulation. \nThe scalability and computational effectiveness of this approach is also questionable \nand I am uncertain if many problem would warrant such complexity in their solution. \nAs with other Bayesian methods, the proposed approach would probably shine in low-samples \nregime and in this case might be preferable to other methods in the same class (VB, GP).\n\n"
  },
  {
    "people": [
      "Hernandez-Lobato\net al."
    ],
    "review": "This paper considers the problem of model-based policy search. The authors \nconsider the use of Bayesian Neural Networks to learn a model of the environment\nand advocate for the $\\alpha$-divergence minimization rather than the more usual \nvariational Bayes. \n\nThe ability of alpha-divergence to capture bi-modality however \ncomes at a price and most of the paper is devoted to finding tractable approximations. \nThe authors therefore use the approach of Hernandez-Lobato\net al. (2016) as proxy to the alpha-divergence . \n\nThe environment/system dynamics is clearly defined as a well as the policy parametrization \n(section 3) and would constitute a useful reference point for other researchers. \nSimulated roll-outs, using the learned model, then provide samples of the expected \nreturn. Since a model of the environment is available, stochastic gradient descent \ncan be performed in the usual way, without policy gradient estimators, via automatic \ndifferentiation tools. \n\nThe experiments demonstrate that alpha-divergence is capable of capturing multi-model \nstructure which competing methods (variational Bayes and GP) would otherwise\nstruggle with. The proposed approach also compares favorably in a real-world\nbatch setting.\n\nThe paper is well-written, technically rich and combines many recent tools \ninto a coherent algorithm. However, the repeated use of approximations to original \nquantities seems to somehow defeat the benefits of the original problem formulation. \nThe scalability and computational effectiveness of this approach is also questionable \nand I am uncertain if many problem would warrant such complexity in their solution. \nAs with other Bayesian methods, the proposed approach would probably shine in low-samples \nregime and in this case might be preferable to other methods in the same class (VB, GP)."
  },
  {
    "people": [
      "Girard",
      "Girard",
      "Rasmussen",
      "Qui\u00f1onero Candela",
      "Murray Smith,",
      "R."
    ],
    "review": "This paper introduces an approach for model-based control of stochastic dynamical systems with policy search, based on (1) learning the stochastic dynamics of the underlying system with a Bayesian deep neural network (BNN) that allows some of its inputs to be stochastic, and (2) a policy optimization method based on simulated rollouts from the learned dynamics. BNN training is carried out using \\alpha-divergence minimization, the specific form of which was introduced in previous work by the authors. Validation and comparison of the approach is undertaken on a simulated domain, as well as real-world scenarios. \n\nThe paper is tightly written, and easy to follow. Its approach to fitting Bayesian neural networks with \\alpha divergence is interesting and appears novel in this context. The resulting application to model-based control appears to have significant practical impact, particularly in light of the explainability that a system model can bring to specific decisions made by the policy. As such, I think that the paper brings a valuable contribution to the literature.\n\nThat said, I have a few questions and suggestions:\n\n1) In section 2.2, it should be explained how the random z_n input is used by the neural network: is it just concatenated to the other inputs and used as-is, or is there a special treatment?\n\n2) Moreover, much case is made for the need to have stochastic inputs, but only a scalar input seems to be provided throughout. Is this enough? How computationally difficult would providing stochastic inputs of higher dimensionality be?\n\n3) How important is the normality assumption in z_n? How is the variance \\gamma established?\n\n4) It is mentioned that the hidden layers of the neural network are made of rectifiers, but no further utilization of this fact is made in the paper. Is this assumption somehow important in the optimization of the alpha-divergence (beyond what we know about rectifiers to mitigate the vanishing gradient problem) ?\n\n5) Equation (3), denominator \\mathbf{y} should be \\mathbf{Y} ?\n\n6) Section 2.3: it would be helpful to have an overview or discussion of the computational complexity of training BNNs, to understand whether and when they can practicably be used.\n\n7) Between eq (12) and (13), a citation to the statement of the time embedding theorem would be helpful, as well as an indication of how the embedding dimension should be chosen.\n\n8) Figure 1: the subplots should have the letters by which they are referenced in the text on p. 7.\n\n9) In section 4.2.1, it is not clear if the gas turbine data is publicly available, and if so where. In addition more details should be provided, such as the dimensionality of the variables E_t, N_t and A_t.\n\n10) Perhaps the comparisons with Gaussian processes should include variants that support stochastic inputs, such as Girard et al. (2003), to provide some of the same modelling capabilities as what\u2019s made use of here. At least, this strand of work should be mentioned in Section 5.\n\n\nReferences:\n\nGirard, A., Rasmussen, C. E., Qui\u00f1onero Candela, J., & Murray Smith, R. (2003). Gaussian process priors with uncertain inputs-application to multiple-step ahead time series forecasting. Advances in Neural Information Processing Systems, 545-552.\n"
  },
  {
    "people": [
      "Hernandez-Lobato\net al."
    ],
    "review": "This paper considers the problem of model-based policy search. The authors \nconsider the use of Bayesian Neural Networks to learn a model of the environment\nand advocate for the $\\alpha$-divergence minimization rather than the more usual \nvariational Bayes. \n\nThe ability of alpha-divergence to capture bi-modality however \ncomes at a price and most of the paper is devoted to finding tractable approximations. \nThe authors therefore use the approach of Hernandez-Lobato\net al. (2016) as proxy to the alpha-divergence . \n\nThe environment/system dynamics is clearly defined as a well as the policy parametrization \n(section 3) and would constitute a useful reference point for other researchers. \nSimulated roll-outs, using the learned model, then provide samples of the expected \nreturn. Since a model of the environment is available, stochastic gradient descent \ncan be performed in the usual way, without policy gradient estimators, via automatic \ndifferentiation tools. \n\nThe experiments demonstrate that alpha-divergence is capable of capturing multi-model \nstructure which competing methods (variational Bayes and GP) would otherwise\nstruggle with. The proposed approach also compares favorably in a real-world\nbatch setting.\n\nThe paper is well-written, technically rich and combines many recent tools \ninto a coherent algorithm. However, the repeated use of approximations to original \nquantities seems to somehow defeat the benefits of the original problem formulation. \nThe scalability and computational effectiveness of this approach is also questionable \nand I am uncertain if many problem would warrant such complexity in their solution. \nAs with other Bayesian methods, the proposed approach would probably shine in low-samples \nregime and in this case might be preferable to other methods in the same class (VB, GP).\n\n"
  },
  {
    "people": [
      "Merity",
      "Merity",
      "Merity"
    ],
    "review": "Reviewers agree that this paper is based on a \"trick\" to build memory without requiring long-distance backprop. This method allows the model to utilize a cache-like mechanism, simply by storing previous states. Everyone agrees that this roughly works (although there could be stronger experimental evidence), and provides long-term memory to simple models. Reviewers/authors also agree that it might not work as well as other pointer-network like method, but there is controversy over whether that is necessary. \n \n - Further discussion indicated a sense by some reviewers that this method could be quite impactful, even if it was not a huge technical contribution, due to its speed and computational benefits over pointer methods. \n \n - The clarity of the writing and good use of references was appreciated \n \n - This paper is a nice complement/rebuttal to \"Frustratingly Short Attention Spans in Neural Language Modeling\".\n \n Including the discussion about this paper as it might be helpful as it was controversial: \n \n \"\"\"\n The technical contribution may appear \"limited\" but I feel part of that is necessary to ensure the method can scale to both large datasets and long term dependencies. For me, this is similar to simpler machine learning methods being able to scale to more data (though replacing \"data\" with \"timesteps\"). More complex methods may do better with a small number of data/timesteps but they won't be able to scale, where other specific advantages may come in to play.\n \n (Timesteps)\n \n Looking back 2000 timesteps is something I've not seen done and speaks to a broader aspect of language modeling - properly capturing recent article level context. Most language models limit BPTT to around 35 timesteps, with some even arguing we don't need that much (i.e. \"Frustratingly Short Attention Spans in Neural Language Modeling\" that's under review for ICLR). From a general perspective, this is vaguely mad given many sentences are longer than 35 timesteps, yet we know both intuitively and from the evidence they present that the rest of an article is very likely to help modeling the following words, especially for PTB or WikiText.\n \n This paper introduces a technique that not only allows for utilizing dependencies far further back than 35 timesteps but shows it consistently helps, even when thrown against a larger number of timesteps, a larger dataset, or a larger vocabulary. Given it is also a post-processing step that can be applied to any vaguely RNN type model, it's widely applicable and trivial to train in comparison to any more complicated models.\n \n (Data)\n \n Speaking to AnonReviewer1's comment, \"A side-by-side comparison of models with pointer networks vs. models with cache with roughly the same number of parameters is needed to convincingly argue that the proposed method is a better alternative (either because it achieves lower perplexity, faster to train but similar test perplexity, faster at test time, etc.)\"\n \n Existing pointer network approaches for language modeling are very slow to train - or at least more optimal methods are yet to be discovered - and has such limited the BPTT length they tackle. Merity et al. use 100 at most and that's the only pointer method for language modeling attending to article style text that I am aware of. Merity et al. also have a section of their paper specifically discussing the training speed complications that come from integrating the pointer network. There is a comparison to Merity et al. in Table 1 and Table 2.\n \n The scaling becomes more obvious on the WikiText datasets which have a more realistic long tail vocabulary than PTB's 10k. For WikiText-2, at a cache size of 100, Merity et al. get 80.8 with their pointer network method while the neural cache model get 81.6. Increasing the neural model cache size to 2000 however gives quite a substantial drop to 68.9. They're also able to apply their method to WikiText-103, a far larger dataset than PTB or WikiText-2, and show that it still provides improvements even when there is more data and a larger vocabulary. Scaling to this dataset is only sanely possible as the neural cache model doesn't add to the training time of the base neural model at all - that it's equivalent to training a standard LSTM.\n \"\"\""
  },
  {
    "people": [
      "Merity",
      "Merity",
      "Merity"
    ],
    "review": "Reviewers agree that this paper is based on a \"trick\" to build memory without requiring long-distance backprop. This method allows the model to utilize a cache-like mechanism, simply by storing previous states. Everyone agrees that this roughly works (although there could be stronger experimental evidence), and provides long-term memory to simple models. Reviewers/authors also agree that it might not work as well as other pointer-network like method, but there is controversy over whether that is necessary. \n \n - Further discussion indicated a sense by some reviewers that this method could be quite impactful, even if it was not a huge technical contribution, due to its speed and computational benefits over pointer methods. \n \n - The clarity of the writing and good use of references was appreciated \n \n - This paper is a nice complement/rebuttal to \"Frustratingly Short Attention Spans in Neural Language Modeling\".\n \n Including the discussion about this paper as it might be helpful as it was controversial: \n \n \"\"\"\n The technical contribution may appear \"limited\" but I feel part of that is necessary to ensure the method can scale to both large datasets and long term dependencies. For me, this is similar to simpler machine learning methods being able to scale to more data (though replacing \"data\" with \"timesteps\"). More complex methods may do better with a small number of data/timesteps but they won't be able to scale, where other specific advantages may come in to play.\n \n (Timesteps)\n \n Looking back 2000 timesteps is something I've not seen done and speaks to a broader aspect of language modeling - properly capturing recent article level context. Most language models limit BPTT to around 35 timesteps, with some even arguing we don't need that much (i.e. \"Frustratingly Short Attention Spans in Neural Language Modeling\" that's under review for ICLR). From a general perspective, this is vaguely mad given many sentences are longer than 35 timesteps, yet we know both intuitively and from the evidence they present that the rest of an article is very likely to help modeling the following words, especially for PTB or WikiText.\n \n This paper introduces a technique that not only allows for utilizing dependencies far further back than 35 timesteps but shows it consistently helps, even when thrown against a larger number of timesteps, a larger dataset, or a larger vocabulary. Given it is also a post-processing step that can be applied to any vaguely RNN type model, it's widely applicable and trivial to train in comparison to any more complicated models.\n \n (Data)\n \n Speaking to AnonReviewer1's comment, \"A side-by-side comparison of models with pointer networks vs. models with cache with roughly the same number of parameters is needed to convincingly argue that the proposed method is a better alternative (either because it achieves lower perplexity, faster to train but similar test perplexity, faster at test time, etc.)\"\n \n Existing pointer network approaches for language modeling are very slow to train - or at least more optimal methods are yet to be discovered - and has such limited the BPTT length they tackle. Merity et al. use 100 at most and that's the only pointer method for language modeling attending to article style text that I am aware of. Merity et al. also have a section of their paper specifically discussing the training speed complications that come from integrating the pointer network. There is a comparison to Merity et al. in Table 1 and Table 2.\n \n The scaling becomes more obvious on the WikiText datasets which have a more realistic long tail vocabulary than PTB's 10k. For WikiText-2, at a cache size of 100, Merity et al. get 80.8 with their pointer network method while the neural cache model get 81.6. Increasing the neural model cache size to 2000 however gives quite a substantial drop to 68.9. They're also able to apply their method to WikiText-103, a far larger dataset than PTB or WikiText-2, and show that it still provides improvements even when there is more data and a larger vocabulary. Scaling to this dataset is only sanely possible as the neural cache model doesn't add to the training time of the base neural model at all - that it's equivalent to training a standard LSTM.\n \"\"\""
  },
  {
    "people": [
      "Fathi",
      "Mabahi",
      "Srivastava",
      "Wang",
      "Gupta",
      "Vondrick",
      "Dai",
      "Lerrer",
      "Fathi",
      "Alireza",
      "Mori",
      "Mobahi",
      "Hossein",
      "Ronan Collobert",
      "Jason Weston",
      "Srivastava",
      "Nitish",
      "Elman Mansimov",
      "Ruslan Salakhutdinov",
      "A. Dai",
      "Q.V. Le",
      "X Wang",
      "A Gupta",
      "C Vondrick",
      "H Pirsiavash",
      "A Torralba"
    ],
    "review": "*** Paper Summary ***\n\nThe paper proposes to learn a predictive model (aka predict the next video frames given an input image) and uses the prediction from this model to improve a supervised classifier. The effectiveness of the approach is illustrated on a tower stability dataset.\n\n*** Review Summary ***\n\nThis work seems rather preliminary in terms of experimentation and using forward modeling as pretraining has already been proposed and applied to video and text classification tasks. Discussion on related work is insufficient. The end task choice (will there be motion?) might not be the best to advocate for unsupervised training.\n\n*** Detailed Review ***\n\nThis work seems rather preliminary. There is no comparison with alternative semi-supervised strategies. Any approach that consider the next frames as latent variables (or privileged information) can be considered. Also I am not sure if the supervised stability prediction model is actually needed once the next frame is predicted. Basically the task can be reduced to predict whether there will be motion in the video following the current frame or not (for instance comparing the first frame and last prediction or the density of gray in the top part of the video might work just as well). Also training a model to predict the presence of motion from the unsupervised data only would probably do very well. I would suggest to stir away from task where the label can be inferred trivially from the unsupervised data, meaning that unlabeled videos can be considered labeled frames in that case.\n\nThe related work section misses a discussion on previous work on learning unsupervised features from video (through predictive models, dimensionality reduction...) for helping classification of still images or videos [Fathi et al 2008; Mabahi et al 2009; Srivastava et al 2015]. More recently, Wang and Gupta (2015) have obtained excellent ImageNet results from features pre trained on unlabeled videos. Vondrick et al (2016) have shown that generative models of video can help initialize models for video classification tasks. Also in the field of text classification, pre training of classifier with a language model is a form predictive modeling, e.g. Dai & Le 2015.\n\nI would also suggest to report test results on the dataset from Lerrer et al 2016 (I understand that you need your own videos to pre train the predictive model) but stability prediction only require still images.\n\nOverall, I feel the experimental section is too preliminary. It would be better to focus on a task where solving the unsupervised task does not necessarily imply that the supervised task is trivially solved (or conversely that a simple rule can turn the unlabeled data into label data).\n\n*** Reference ***\n\nFathi, Alireza, and Greg Mori. \"Action recognition by learning mid-level motion features.\" Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on. IEEE, 2008.\nMobahi, Hossein, Ronan Collobert, and Jason Weston. \"Deep learning from temporal coherence in video.\" Proceedings of the 26th Annual International Conference on Machine Learning. ACM, 2009.\nSrivastava, Nitish, Elman Mansimov, and Ruslan Salakhutdinov. \"Unsupervised learning of video representations using lstms.\" CoRR, abs/1502.04681 2 (2015).\nA. Dai, Q.V. Le, Semi-supervised Sequence Learning, NIPS, 2015\nUnsupervised learning of visual representations using videos, X Wang, A Gupta, ICCV 2015 \nGenerating videos with scene dynamics, C Vondrick, H Pirsiavash, A Torralba, NIPS 16"
  },
  {
    "people": [
      "Zhang",
      "Lerer",
      "Zhang"
    ],
    "review": "Summary\n===\nThis paper trains models to predict whether block towers will fall down\nor not. It shows that an additional model of how blocks fall down\n(predicting a sequence of frames via unsupervised learning) helps the original\nsupervised task to generalize better.\n\nThis work constructs a synthetic dataset of block towers containing\n3 to 5 blocks places in more or less precarious positions. It includes both\nlabels (the tower falls or not) and video frame sequences of the tower's\nevolution according to a physics engine.\n\nThree kinds of models are trained. The first (S) simply takes an image of a\ntower's starting state and predicts whether it will fall or not. The\nother two types (CD and CLD) take both the start state and the final state of the\ntower (after it has or has not fallen) and predict whether it has fallen or not,\nthey only differ in how the final state is provided. One model (ConvDeconv, CD)\npredicts the final frame using only the start frame and the other\n(ConvLSTMDeconv) predicts a series of intermediate frames before coming\nto the final frame. Both CD and CLD are unsupervised.\n\nEach model is trained on towers of a particular heigh and tested on\ntowers with an unseen height. When the height of the train towers\nis the same as the test tower height, all models perform roughly the same\n(with in a few percentage points). However, when the test height is\ngreater than the train height it is extremely helpful to explicitly\nmodel the final state of the block tower before deciding whether it has\nfallen or not (via CD and CLD models).\n\n\nPros\n===\n\n* There are very clear (large) gains in accuracy from adding an unsupervised\nfinal frame predictor. Because the generalization problem is also particularly\nclear (train and test with different numbers of blocks), this makes for\na very nice toy example where unsupervised learning provides a clear benefit.\n\n* The writing is clear.\n\n\nCons\n===\n\nMy one major concern is a lack of more detailed analysis. The paper\nestablishes a base result, but does not explore the idea to the extent\nto which I think an ICLR paper should. Two general directions for potential\nanalysis follow:\n\n* Is this a limitation of the particular way the block towers are rendered?\n\nThe LSTM model could be limited by the sub-sampling strategy. It looks\nlike the sampling may be too coarse from the provided examples. For the\ntwo towers in figure 2 that fall, they have fallen after only 1 or 2\ntime steps. How quickly do most towers fall? What happens if the LSTM\nis trained at a higher frame rate? What is the frame-by-frame video\nprediction accuracy of the LSTM? (Is that quantity meaningful?)\nHow much does performance improve if the LSTM is provided ground truth\nfor only the first k frames?\n\n* Why is generalization to different block heights limited?\n\nIs it limited by model capacity or architecture design?\nWhat would happen if the S-type models were made wider/deeper with the CD/CLD\nfall predictor capacity fixed?\n\nIs it limited by the precise task specification?\nWhat would happen if networks were trained with towers of multiple heights\n(apparently this experiment is in the works)?\nI appreciate that one experiment in this direction was provided.\n\nIs it limited by training procedure? What if the CD/CLD models were trained\nin an end-to-end manner? What if the double frame fall predictor were trained\nwith ground truth final frames instead of generated final frames?\n\n\nMinor concerns:\n\n* It may be asking too much to re-implement Zhang et. al. 2016 and PhysNet\nfor the newly proposed dataset, but it would help the paper to have baselines\nwhich are directly comparable to the proposed results. I do not think this\nis a major concern because the point of the paper is about the role of\nunsupervised learning rather than creating the best fall prediction network.\n\n* The auxiliary experiment provided is motivated as follows: \n\"One solution could be to train these models to predict how many blocks have\nfallen instead of a binary stability label.\"\nIs there a clear intuition for why this might make the task easier?\n\n* Will the dataset, or code to generate it, be released?\n\n\n\nOverall Evaluation\n===\nThe writing, presentation, and experiments are clear and of high enough\nquality for ICLR. However the experiments provide limited analysis past\nthe main result (see comments above). The idea is a clear extension of ideas behind unsupervised\nlearning (video prediction) and recent results in intuitive physics from\nLerer et. al. 2016 and Zhang et. al. 2016, so there is only moderate novelty.\nHowever, these results would provide a valuable addition to the literation,\nespecially if more analysis was provided.\n"
  },
  {
    "people": [
      "Fathi",
      "Mabahi",
      "Srivastava",
      "Wang",
      "Gupta",
      "Vondrick",
      "Dai",
      "Lerrer",
      "Fathi",
      "Alireza",
      "Mori",
      "Mobahi",
      "Hossein",
      "Ronan Collobert",
      "Jason Weston",
      "Srivastava",
      "Nitish",
      "Elman Mansimov",
      "Ruslan Salakhutdinov",
      "A. Dai",
      "Q.V. Le",
      "X Wang",
      "A Gupta",
      "C Vondrick",
      "H Pirsiavash",
      "A Torralba"
    ],
    "review": "*** Paper Summary ***\n\nThe paper proposes to learn a predictive model (aka predict the next video frames given an input image) and uses the prediction from this model to improve a supervised classifier. The effectiveness of the approach is illustrated on a tower stability dataset.\n\n*** Review Summary ***\n\nThis work seems rather preliminary in terms of experimentation and using forward modeling as pretraining has already been proposed and applied to video and text classification tasks. Discussion on related work is insufficient. The end task choice (will there be motion?) might not be the best to advocate for unsupervised training.\n\n*** Detailed Review ***\n\nThis work seems rather preliminary. There is no comparison with alternative semi-supervised strategies. Any approach that consider the next frames as latent variables (or privileged information) can be considered. Also I am not sure if the supervised stability prediction model is actually needed once the next frame is predicted. Basically the task can be reduced to predict whether there will be motion in the video following the current frame or not (for instance comparing the first frame and last prediction or the density of gray in the top part of the video might work just as well). Also training a model to predict the presence of motion from the unsupervised data only would probably do very well. I would suggest to stir away from task where the label can be inferred trivially from the unsupervised data, meaning that unlabeled videos can be considered labeled frames in that case.\n\nThe related work section misses a discussion on previous work on learning unsupervised features from video (through predictive models, dimensionality reduction...) for helping classification of still images or videos [Fathi et al 2008; Mabahi et al 2009; Srivastava et al 2015]. More recently, Wang and Gupta (2015) have obtained excellent ImageNet results from features pre trained on unlabeled videos. Vondrick et al (2016) have shown that generative models of video can help initialize models for video classification tasks. Also in the field of text classification, pre training of classifier with a language model is a form predictive modeling, e.g. Dai & Le 2015.\n\nI would also suggest to report test results on the dataset from Lerrer et al 2016 (I understand that you need your own videos to pre train the predictive model) but stability prediction only require still images.\n\nOverall, I feel the experimental section is too preliminary. It would be better to focus on a task where solving the unsupervised task does not necessarily imply that the supervised task is trivially solved (or conversely that a simple rule can turn the unlabeled data into label data).\n\n*** Reference ***\n\nFathi, Alireza, and Greg Mori. \"Action recognition by learning mid-level motion features.\" Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on. IEEE, 2008.\nMobahi, Hossein, Ronan Collobert, and Jason Weston. \"Deep learning from temporal coherence in video.\" Proceedings of the 26th Annual International Conference on Machine Learning. ACM, 2009.\nSrivastava, Nitish, Elman Mansimov, and Ruslan Salakhutdinov. \"Unsupervised learning of video representations using lstms.\" CoRR, abs/1502.04681 2 (2015).\nA. Dai, Q.V. Le, Semi-supervised Sequence Learning, NIPS, 2015\nUnsupervised learning of visual representations using videos, X Wang, A Gupta, ICCV 2015 \nGenerating videos with scene dynamics, C Vondrick, H Pirsiavash, A Torralba, NIPS 16\n"
  },
  {
    "people": [
      "Fathi",
      "Mabahi",
      "Srivastava",
      "Wang",
      "Gupta",
      "Vondrick",
      "Dai",
      "Lerrer",
      "Fathi",
      "Alireza",
      "Mori",
      "Mobahi",
      "Hossein",
      "Ronan Collobert",
      "Jason Weston",
      "Srivastava",
      "Nitish",
      "Elman Mansimov",
      "Ruslan Salakhutdinov",
      "A. Dai",
      "Q.V. Le",
      "X Wang",
      "A Gupta",
      "C Vondrick",
      "H Pirsiavash",
      "A Torralba"
    ],
    "review": "*** Paper Summary ***\n\nThe paper proposes to learn a predictive model (aka predict the next video frames given an input image) and uses the prediction from this model to improve a supervised classifier. The effectiveness of the approach is illustrated on a tower stability dataset.\n\n*** Review Summary ***\n\nThis work seems rather preliminary in terms of experimentation and using forward modeling as pretraining has already been proposed and applied to video and text classification tasks. Discussion on related work is insufficient. The end task choice (will there be motion?) might not be the best to advocate for unsupervised training.\n\n*** Detailed Review ***\n\nThis work seems rather preliminary. There is no comparison with alternative semi-supervised strategies. Any approach that consider the next frames as latent variables (or privileged information) can be considered. Also I am not sure if the supervised stability prediction model is actually needed once the next frame is predicted. Basically the task can be reduced to predict whether there will be motion in the video following the current frame or not (for instance comparing the first frame and last prediction or the density of gray in the top part of the video might work just as well). Also training a model to predict the presence of motion from the unsupervised data only would probably do very well. I would suggest to stir away from task where the label can be inferred trivially from the unsupervised data, meaning that unlabeled videos can be considered labeled frames in that case.\n\nThe related work section misses a discussion on previous work on learning unsupervised features from video (through predictive models, dimensionality reduction...) for helping classification of still images or videos [Fathi et al 2008; Mabahi et al 2009; Srivastava et al 2015]. More recently, Wang and Gupta (2015) have obtained excellent ImageNet results from features pre trained on unlabeled videos. Vondrick et al (2016) have shown that generative models of video can help initialize models for video classification tasks. Also in the field of text classification, pre training of classifier with a language model is a form predictive modeling, e.g. Dai & Le 2015.\n\nI would also suggest to report test results on the dataset from Lerrer et al 2016 (I understand that you need your own videos to pre train the predictive model) but stability prediction only require still images.\n\nOverall, I feel the experimental section is too preliminary. It would be better to focus on a task where solving the unsupervised task does not necessarily imply that the supervised task is trivially solved (or conversely that a simple rule can turn the unlabeled data into label data).\n\n*** Reference ***\n\nFathi, Alireza, and Greg Mori. \"Action recognition by learning mid-level motion features.\" Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on. IEEE, 2008.\nMobahi, Hossein, Ronan Collobert, and Jason Weston. \"Deep learning from temporal coherence in video.\" Proceedings of the 26th Annual International Conference on Machine Learning. ACM, 2009.\nSrivastava, Nitish, Elman Mansimov, and Ruslan Salakhutdinov. \"Unsupervised learning of video representations using lstms.\" CoRR, abs/1502.04681 2 (2015).\nA. Dai, Q.V. Le, Semi-supervised Sequence Learning, NIPS, 2015\nUnsupervised learning of visual representations using videos, X Wang, A Gupta, ICCV 2015 \nGenerating videos with scene dynamics, C Vondrick, H Pirsiavash, A Torralba, NIPS 16"
  },
  {
    "people": [
      "Zhang",
      "Lerer",
      "Zhang"
    ],
    "review": "Summary\n===\nThis paper trains models to predict whether block towers will fall down\nor not. It shows that an additional model of how blocks fall down\n(predicting a sequence of frames via unsupervised learning) helps the original\nsupervised task to generalize better.\n\nThis work constructs a synthetic dataset of block towers containing\n3 to 5 blocks places in more or less precarious positions. It includes both\nlabels (the tower falls or not) and video frame sequences of the tower's\nevolution according to a physics engine.\n\nThree kinds of models are trained. The first (S) simply takes an image of a\ntower's starting state and predicts whether it will fall or not. The\nother two types (CD and CLD) take both the start state and the final state of the\ntower (after it has or has not fallen) and predict whether it has fallen or not,\nthey only differ in how the final state is provided. One model (ConvDeconv, CD)\npredicts the final frame using only the start frame and the other\n(ConvLSTMDeconv) predicts a series of intermediate frames before coming\nto the final frame. Both CD and CLD are unsupervised.\n\nEach model is trained on towers of a particular heigh and tested on\ntowers with an unseen height. When the height of the train towers\nis the same as the test tower height, all models perform roughly the same\n(with in a few percentage points). However, when the test height is\ngreater than the train height it is extremely helpful to explicitly\nmodel the final state of the block tower before deciding whether it has\nfallen or not (via CD and CLD models).\n\n\nPros\n===\n\n* There are very clear (large) gains in accuracy from adding an unsupervised\nfinal frame predictor. Because the generalization problem is also particularly\nclear (train and test with different numbers of blocks), this makes for\na very nice toy example where unsupervised learning provides a clear benefit.\n\n* The writing is clear.\n\n\nCons\n===\n\nMy one major concern is a lack of more detailed analysis. The paper\nestablishes a base result, but does not explore the idea to the extent\nto which I think an ICLR paper should. Two general directions for potential\nanalysis follow:\n\n* Is this a limitation of the particular way the block towers are rendered?\n\nThe LSTM model could be limited by the sub-sampling strategy. It looks\nlike the sampling may be too coarse from the provided examples. For the\ntwo towers in figure 2 that fall, they have fallen after only 1 or 2\ntime steps. How quickly do most towers fall? What happens if the LSTM\nis trained at a higher frame rate? What is the frame-by-frame video\nprediction accuracy of the LSTM? (Is that quantity meaningful?)\nHow much does performance improve if the LSTM is provided ground truth\nfor only the first k frames?\n\n* Why is generalization to different block heights limited?\n\nIs it limited by model capacity or architecture design?\nWhat would happen if the S-type models were made wider/deeper with the CD/CLD\nfall predictor capacity fixed?\n\nIs it limited by the precise task specification?\nWhat would happen if networks were trained with towers of multiple heights\n(apparently this experiment is in the works)?\nI appreciate that one experiment in this direction was provided.\n\nIs it limited by training procedure? What if the CD/CLD models were trained\nin an end-to-end manner? What if the double frame fall predictor were trained\nwith ground truth final frames instead of generated final frames?\n\n\nMinor concerns:\n\n* It may be asking too much to re-implement Zhang et. al. 2016 and PhysNet\nfor the newly proposed dataset, but it would help the paper to have baselines\nwhich are directly comparable to the proposed results. I do not think this\nis a major concern because the point of the paper is about the role of\nunsupervised learning rather than creating the best fall prediction network.\n\n* The auxiliary experiment provided is motivated as follows: \n\"One solution could be to train these models to predict how many blocks have\nfallen instead of a binary stability label.\"\nIs there a clear intuition for why this might make the task easier?\n\n* Will the dataset, or code to generate it, be released?\n\n\n\nOverall Evaluation\n===\nThe writing, presentation, and experiments are clear and of high enough\nquality for ICLR. However the experiments provide limited analysis past\nthe main result (see comments above). The idea is a clear extension of ideas behind unsupervised\nlearning (video prediction) and recent results in intuitive physics from\nLerer et. al. 2016 and Zhang et. al. 2016, so there is only moderate novelty.\nHowever, these results would provide a valuable addition to the literation,\nespecially if more analysis was provided.\n"
  },
  {
    "people": [
      "Fathi",
      "Mabahi",
      "Srivastava",
      "Wang",
      "Gupta",
      "Vondrick",
      "Dai",
      "Lerrer",
      "Fathi",
      "Alireza",
      "Mori",
      "Mobahi",
      "Hossein",
      "Ronan Collobert",
      "Jason Weston",
      "Srivastava",
      "Nitish",
      "Elman Mansimov",
      "Ruslan Salakhutdinov",
      "A. Dai",
      "Q.V. Le",
      "X Wang",
      "A Gupta",
      "C Vondrick",
      "H Pirsiavash",
      "A Torralba"
    ],
    "review": "*** Paper Summary ***\n\nThe paper proposes to learn a predictive model (aka predict the next video frames given an input image) and uses the prediction from this model to improve a supervised classifier. The effectiveness of the approach is illustrated on a tower stability dataset.\n\n*** Review Summary ***\n\nThis work seems rather preliminary in terms of experimentation and using forward modeling as pretraining has already been proposed and applied to video and text classification tasks. Discussion on related work is insufficient. The end task choice (will there be motion?) might not be the best to advocate for unsupervised training.\n\n*** Detailed Review ***\n\nThis work seems rather preliminary. There is no comparison with alternative semi-supervised strategies. Any approach that consider the next frames as latent variables (or privileged information) can be considered. Also I am not sure if the supervised stability prediction model is actually needed once the next frame is predicted. Basically the task can be reduced to predict whether there will be motion in the video following the current frame or not (for instance comparing the first frame and last prediction or the density of gray in the top part of the video might work just as well). Also training a model to predict the presence of motion from the unsupervised data only would probably do very well. I would suggest to stir away from task where the label can be inferred trivially from the unsupervised data, meaning that unlabeled videos can be considered labeled frames in that case.\n\nThe related work section misses a discussion on previous work on learning unsupervised features from video (through predictive models, dimensionality reduction...) for helping classification of still images or videos [Fathi et al 2008; Mabahi et al 2009; Srivastava et al 2015]. More recently, Wang and Gupta (2015) have obtained excellent ImageNet results from features pre trained on unlabeled videos. Vondrick et al (2016) have shown that generative models of video can help initialize models for video classification tasks. Also in the field of text classification, pre training of classifier with a language model is a form predictive modeling, e.g. Dai & Le 2015.\n\nI would also suggest to report test results on the dataset from Lerrer et al 2016 (I understand that you need your own videos to pre train the predictive model) but stability prediction only require still images.\n\nOverall, I feel the experimental section is too preliminary. It would be better to focus on a task where solving the unsupervised task does not necessarily imply that the supervised task is trivially solved (or conversely that a simple rule can turn the unlabeled data into label data).\n\n*** Reference ***\n\nFathi, Alireza, and Greg Mori. \"Action recognition by learning mid-level motion features.\" Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on. IEEE, 2008.\nMobahi, Hossein, Ronan Collobert, and Jason Weston. \"Deep learning from temporal coherence in video.\" Proceedings of the 26th Annual International Conference on Machine Learning. ACM, 2009.\nSrivastava, Nitish, Elman Mansimov, and Ruslan Salakhutdinov. \"Unsupervised learning of video representations using lstms.\" CoRR, abs/1502.04681 2 (2015).\nA. Dai, Q.V. Le, Semi-supervised Sequence Learning, NIPS, 2015\nUnsupervised learning of visual representations using videos, X Wang, A Gupta, ICCV 2015 \nGenerating videos with scene dynamics, C Vondrick, H Pirsiavash, A Torralba, NIPS 16\n"
  },
  {
    "people": [
      "Zhao",
      "Meng",
      "Gretton",
      "Sutherland"
    ],
    "review": "Variational auto-encoders, adversarial networks, and kernel scoring rules like MMD have recently gained popularity as methods for learning directed generative models and for other applications like domain adaptation. This paper gives an additional method along the scoring rules direction that uses the matching of central moments to match two probability distributions. The technique is simple, and in the case of domain adaptation, highly effective.\n\nCMD seems like a very nice and straightforward solution to the domain adaptation problem. The method is computationally straightforward to implement, and seems quite stable with respect to the tuning parameters when compared to MMD. I was skeptical reading through this, especially given the fact that you only use K=5 in your experiments, but the results seem quite good. The natural question that I have now is: how will this method do in training generative models? This is beyond the scope of this paper, but it\u2019s the lowest hanging fruit.\n\nBelow I give more detailed feedback.\n\nOne way to speed up MMD is to use a random Fourier basis as was done in \u201cFastmmd: Ensemble of circular discrepancy for efficient two-sample test\u201d by Zhao and Meng, 2015. There are also linear time estimators, e.g., in \u201cA Kernel Two-Sample Test\u201c by Gretton et al., 2012. I don\u2019t think you need to compare against these approaches since you compare to the full MMD, but they should be cited.\n\nThe paper \u201cGenerative Models and Model Criticism via Optimized Maximum Mean Discrepancy\u201d by Sutherland et al. submitted to ICLR 2017 as well, discusses techniques for optimizing the kernel used in MMD and is worth citing in section 3.\n\nHow limiting is the assumption that the distribution has independent marginals?\n\nThe sample complexity of MMD depends heavily on the dimensionality of the input space - do you have any intuitions about the sample complexity of CMD? It seems like it's relatively insensitive based on the results in Figure 4, but I would be surprised if this were the case with 10,000 hidden units. I mainly ask this because with generative models, the output space can be quite high-dimensional.\n\nI\u2019m concerned that the central moments won\u2019t be numerically stable at higher orders when backpropagating. This doesn\u2019t seem to be a problem in the experimental results, but perhaps the authors could comment a bit on this? I\u2019m referring to the fact that ck(X) can be very large for k >= 3. Proposition 1 alleviates my concerns that the overall objective is unstable, I\u2019m referring specifically to the individual terms within.\n\nFigure 3 is rather cluttered, and aside from the mouse class it\u2019s not clear to me from the visualization that the CMD regularizer is actually helping. It would be useful to remove some of the classes for the purpose of visualization.\n\nI would like some clarification about the natural geometric interpretations of K=5. Do you mean that the moments up to K=5 have been well-studied? Do you have any references for this? Why does K >= 6 not have a natural geometric interpretation?\n\nFigure 4 should have a legend"
  },
  {
    "people": [
      "Sriperumbudur",
      "Gretton",
      "Li"
    ],
    "review": "This paper proposed a new metric central moment discrepancy (CMD) for matching two distributions, with applications to domain adaptation.  Compared to a more well-known variant, MMD, CMD has the benefit of not over penalizing the mean, and therefore can focus more on the shape of distribution around the center.\n\nIn terms of discriminative power (the ability to tell two distributions apart), MMD and CMD should be equivalent, but in practice I can understand that CMD may be better as MMD tries to match the raw moments which may over penalize data that are not zero centered.\n\nIn the paper CMD is used only up to Kth order, and not all the central moments are used, but rather only the diagonal entries are considered in the CMD objective, I think this is mostly motivated for computation efficiency.  A natural comparison with MMD therefore can be made, by also explicitly include raw moments up to Kth order.  Another thing to compare against is to include all moments, not just the diagonal terms, in the objective.  This is computationally expensive, but can be done for e.g. 1st and 2nd orders.\n\nSince the experiments only compare CMD in the above form with kernelized MMD, the claim that explicit moment matching is helpful is not very well supported.  To make this a solid claim CMD should be compared against MMD with explicit raw moments.\n\nThe claim that the kernel parameter in MMD is hard to tune and CMD does not have such parameters only applies to kernel MMD, not explicit MMD.  For kernel MMD, there are also studies on how to set these parameters, for example:\n\nSriperumbudur et al.  Kernel choice and classifiability for rkhs embeddings of probability distributions.\nGretton et al.  A kernel two-sample test.\n\nand also using multiple kernels (Li et al. 2015) which removes the need to tune them.  Tuning the beta directly like done in this paper is usually not the way MMD is tuned.  At least simple heuristics like dividing |x-y|^2 by dimensionality or mean pairwise distance first should be applied first before trying beta in the way done in this paper.\n\nOverall I think CMD could be better than MMD, and could have applications in many domains.  But it also has the problem of not easily kernelizable (you can argue this both ways though).  The experiments demonstrating that CMD is better could be done more convincinly.\n\n"
  },
  {
    "people": [
      "Zhao",
      "Meng",
      "Gretton",
      "Sutherland"
    ],
    "review": "Variational auto-encoders, adversarial networks, and kernel scoring rules like MMD have recently gained popularity as methods for learning directed generative models and for other applications like domain adaptation. This paper gives an additional method along the scoring rules direction that uses the matching of central moments to match two probability distributions. The technique is simple, and in the case of domain adaptation, highly effective.\n\nCMD seems like a very nice and straightforward solution to the domain adaptation problem. The method is computationally straightforward to implement, and seems quite stable with respect to the tuning parameters when compared to MMD. I was skeptical reading through this, especially given the fact that you only use K=5 in your experiments, but the results seem quite good. The natural question that I have now is: how will this method do in training generative models? This is beyond the scope of this paper, but it\u2019s the lowest hanging fruit.\n\nBelow I give more detailed feedback.\n\nOne way to speed up MMD is to use a random Fourier basis as was done in \u201cFastmmd: Ensemble of circular discrepancy for efficient two-sample test\u201d by Zhao and Meng, 2015. There are also linear time estimators, e.g., in \u201cA Kernel Two-Sample Test\u201c by Gretton et al., 2012. I don\u2019t think you need to compare against these approaches since you compare to the full MMD, but they should be cited.\n\nThe paper \u201cGenerative Models and Model Criticism via Optimized Maximum Mean Discrepancy\u201d by Sutherland et al. submitted to ICLR 2017 as well, discusses techniques for optimizing the kernel used in MMD and is worth citing in section 3.\n\nHow limiting is the assumption that the distribution has independent marginals?\n\nThe sample complexity of MMD depends heavily on the dimensionality of the input space - do you have any intuitions about the sample complexity of CMD? It seems like it's relatively insensitive based on the results in Figure 4, but I would be surprised if this were the case with 10,000 hidden units. I mainly ask this because with generative models, the output space can be quite high-dimensional.\n\nI\u2019m concerned that the central moments won\u2019t be numerically stable at higher orders when backpropagating. This doesn\u2019t seem to be a problem in the experimental results, but perhaps the authors could comment a bit on this? I\u2019m referring to the fact that ck(X) can be very large for k >= 3. Proposition 1 alleviates my concerns that the overall objective is unstable, I\u2019m referring specifically to the individual terms within.\n\nFigure 3 is rather cluttered, and aside from the mouse class it\u2019s not clear to me from the visualization that the CMD regularizer is actually helping. It would be useful to remove some of the classes for the purpose of visualization.\n\nI would like some clarification about the natural geometric interpretations of K=5. Do you mean that the moments up to K=5 have been well-studied? Do you have any references for this? Why does K >= 6 not have a natural geometric interpretation?\n\nFigure 4 should have a legend"
  },
  {
    "people": [
      "Zhao",
      "Meng",
      "Gretton",
      "Sutherland"
    ],
    "review": "Variational auto-encoders, adversarial networks, and kernel scoring rules like MMD have recently gained popularity as methods for learning directed generative models and for other applications like domain adaptation. This paper gives an additional method along the scoring rules direction that uses the matching of central moments to match two probability distributions. The technique is simple, and in the case of domain adaptation, highly effective.\n\nCMD seems like a very nice and straightforward solution to the domain adaptation problem. The method is computationally straightforward to implement, and seems quite stable with respect to the tuning parameters when compared to MMD. I was skeptical reading through this, especially given the fact that you only use K=5 in your experiments, but the results seem quite good. The natural question that I have now is: how will this method do in training generative models? This is beyond the scope of this paper, but it\u2019s the lowest hanging fruit.\n\nBelow I give more detailed feedback.\n\nOne way to speed up MMD is to use a random Fourier basis as was done in \u201cFastmmd: Ensemble of circular discrepancy for efficient two-sample test\u201d by Zhao and Meng, 2015. There are also linear time estimators, e.g., in \u201cA Kernel Two-Sample Test\u201c by Gretton et al., 2012. I don\u2019t think you need to compare against these approaches since you compare to the full MMD, but they should be cited.\n\nThe paper \u201cGenerative Models and Model Criticism via Optimized Maximum Mean Discrepancy\u201d by Sutherland et al. submitted to ICLR 2017 as well, discusses techniques for optimizing the kernel used in MMD and is worth citing in section 3.\n\nHow limiting is the assumption that the distribution has independent marginals?\n\nThe sample complexity of MMD depends heavily on the dimensionality of the input space - do you have any intuitions about the sample complexity of CMD? It seems like it's relatively insensitive based on the results in Figure 4, but I would be surprised if this were the case with 10,000 hidden units. I mainly ask this because with generative models, the output space can be quite high-dimensional.\n\nI\u2019m concerned that the central moments won\u2019t be numerically stable at higher orders when backpropagating. This doesn\u2019t seem to be a problem in the experimental results, but perhaps the authors could comment a bit on this? I\u2019m referring to the fact that ck(X) can be very large for k >= 3. Proposition 1 alleviates my concerns that the overall objective is unstable, I\u2019m referring specifically to the individual terms within.\n\nFigure 3 is rather cluttered, and aside from the mouse class it\u2019s not clear to me from the visualization that the CMD regularizer is actually helping. It would be useful to remove some of the classes for the purpose of visualization.\n\nI would like some clarification about the natural geometric interpretations of K=5. Do you mean that the moments up to K=5 have been well-studied? Do you have any references for this? Why does K >= 6 not have a natural geometric interpretation?\n\nFigure 4 should have a legend"
  },
  {
    "people": [
      "Sriperumbudur",
      "Gretton",
      "Li"
    ],
    "review": "This paper proposed a new metric central moment discrepancy (CMD) for matching two distributions, with applications to domain adaptation.  Compared to a more well-known variant, MMD, CMD has the benefit of not over penalizing the mean, and therefore can focus more on the shape of distribution around the center.\n\nIn terms of discriminative power (the ability to tell two distributions apart), MMD and CMD should be equivalent, but in practice I can understand that CMD may be better as MMD tries to match the raw moments which may over penalize data that are not zero centered.\n\nIn the paper CMD is used only up to Kth order, and not all the central moments are used, but rather only the diagonal entries are considered in the CMD objective, I think this is mostly motivated for computation efficiency.  A natural comparison with MMD therefore can be made, by also explicitly include raw moments up to Kth order.  Another thing to compare against is to include all moments, not just the diagonal terms, in the objective.  This is computationally expensive, but can be done for e.g. 1st and 2nd orders.\n\nSince the experiments only compare CMD in the above form with kernelized MMD, the claim that explicit moment matching is helpful is not very well supported.  To make this a solid claim CMD should be compared against MMD with explicit raw moments.\n\nThe claim that the kernel parameter in MMD is hard to tune and CMD does not have such parameters only applies to kernel MMD, not explicit MMD.  For kernel MMD, there are also studies on how to set these parameters, for example:\n\nSriperumbudur et al.  Kernel choice and classifiability for rkhs embeddings of probability distributions.\nGretton et al.  A kernel two-sample test.\n\nand also using multiple kernels (Li et al. 2015) which removes the need to tune them.  Tuning the beta directly like done in this paper is usually not the way MMD is tuned.  At least simple heuristics like dividing |x-y|^2 by dimensionality or mean pairwise distance first should be applied first before trying beta in the way done in this paper.\n\nOverall I think CMD could be better than MMD, and could have applications in many domains.  But it also has the problem of not easily kernelizable (you can argue this both ways though).  The experiments demonstrating that CMD is better could be done more convincinly.\n\n"
  },
  {
    "people": [
      "Zhao",
      "Meng",
      "Gretton",
      "Sutherland"
    ],
    "review": "Variational auto-encoders, adversarial networks, and kernel scoring rules like MMD have recently gained popularity as methods for learning directed generative models and for other applications like domain adaptation. This paper gives an additional method along the scoring rules direction that uses the matching of central moments to match two probability distributions. The technique is simple, and in the case of domain adaptation, highly effective.\n\nCMD seems like a very nice and straightforward solution to the domain adaptation problem. The method is computationally straightforward to implement, and seems quite stable with respect to the tuning parameters when compared to MMD. I was skeptical reading through this, especially given the fact that you only use K=5 in your experiments, but the results seem quite good. The natural question that I have now is: how will this method do in training generative models? This is beyond the scope of this paper, but it\u2019s the lowest hanging fruit.\n\nBelow I give more detailed feedback.\n\nOne way to speed up MMD is to use a random Fourier basis as was done in \u201cFastmmd: Ensemble of circular discrepancy for efficient two-sample test\u201d by Zhao and Meng, 2015. There are also linear time estimators, e.g., in \u201cA Kernel Two-Sample Test\u201c by Gretton et al., 2012. I don\u2019t think you need to compare against these approaches since you compare to the full MMD, but they should be cited.\n\nThe paper \u201cGenerative Models and Model Criticism via Optimized Maximum Mean Discrepancy\u201d by Sutherland et al. submitted to ICLR 2017 as well, discusses techniques for optimizing the kernel used in MMD and is worth citing in section 3.\n\nHow limiting is the assumption that the distribution has independent marginals?\n\nThe sample complexity of MMD depends heavily on the dimensionality of the input space - do you have any intuitions about the sample complexity of CMD? It seems like it's relatively insensitive based on the results in Figure 4, but I would be surprised if this were the case with 10,000 hidden units. I mainly ask this because with generative models, the output space can be quite high-dimensional.\n\nI\u2019m concerned that the central moments won\u2019t be numerically stable at higher orders when backpropagating. This doesn\u2019t seem to be a problem in the experimental results, but perhaps the authors could comment a bit on this? I\u2019m referring to the fact that ck(X) can be very large for k >= 3. Proposition 1 alleviates my concerns that the overall objective is unstable, I\u2019m referring specifically to the individual terms within.\n\nFigure 3 is rather cluttered, and aside from the mouse class it\u2019s not clear to me from the visualization that the CMD regularizer is actually helping. It would be useful to remove some of the classes for the purpose of visualization.\n\nI would like some clarification about the natural geometric interpretations of K=5. Do you mean that the moments up to K=5 have been well-studied? Do you have any references for this? Why does K >= 6 not have a natural geometric interpretation?\n\nFigure 4 should have a legend"
  },
  {
    "people": [
      "Bob",
      "Alice",
      "Bob",
      "Eve"
    ],
    "review": "The submission proposes to modify the typical GAN architecture slightly to include \"encrypt\" (Alice) and \"decrypt\" (Bob) modules as well as a module trying to decrypt the signal without a key (Eve).  Through repeated transmission of signals, the adversarial game is intended to converge to a system in which Alice and Bob can communicate securely (or at least a designated part of the signal should be secure), while a sophisticated Eve cannot break their code.  Examples are given on toy data:\n\"As a proof-of-concept, we implemented Alice, Bob, and Eve networks that take N-bit random plain-text and key values, and produce N-entry floating-point ciphertexts, for N = 16, 32, and 64.  Both plaintext and key values are uniformly distributed.\"\n\nThe idea considered here is cute.  If some, but not necessarily all of the signal is meant to be secure, the modules can learn to encrypt and decrypt a signal, while an adversary is simultaneously learned that tries to break the encryption.  In this way, some of the data can remain unencrypted, while the portion that is e.g. correlated with the encrypted signal will have to be encrypted in order for Eve to not be able to predict the encrypted part.\n\nWhile this is a nice thought experiment, there are significant barriers to this submission having a practical impact:\n1) GANs, and from the convergence figures also the objective considered here, are quite unstable to optimize.  The only guarantees of privacy are for an Eve that is converged to a very strong adversary (stronger than a dedicated attack over time).  I do not see how one can have any sort of reliable guarantee of the safety of the data transmission from the proposed approach, at least the paper does not outline such a guarantee.\n2) Public key encryption systems are readily available, computationally feasible, and successfully applied almost anywhere.  The toy examples given in the paper do not at all convince me that this is solving a real-world problem at this point.  Perhaps a good example will come up in the near future, and this work will be shown to be justified, but until such an example is shown, the approach is more of an interesting thought experiment."
  },
  {
    "people": [
      "Alice",
      "Eve",
      "Bob",
      "Eve",
      "Eve",
      "Bob",
      "Bob",
      "Eve",
      "Eve",
      "Alice",
      "Bob",
      "Eve"
    ],
    "review": "The paper deals with an interesting application of adversarial training to encryption. It considers the standard scenario of Alice, Eve and Bob, where A and B aim to exchange messages conditioned on a shared key, while Eve should be unable to encrypt the message. Experiments are performed in a simple symmetric 16 bit encryption task, and an application on privacy. The concepts, ideas and previous literature are quite nicely and carefully presented.\n\nThe only major concern I have - and I apologize to the authors for not raising this earlier - are the experiments in section 3. In particular, I don't quite get the scenario. The reasoning here seems to be as follows: given information < A, B, C, D >, I want to give the public the value of D (e.g. movies watched) without releasing information about C (e.g. gender). In this scenario, Eve would need to be able to reconstruct D as good as possible without gaining information about C. What is described in section 3, however, is that D and D-public are both reconstructed by Bob, but why would Bob reconstruct the latter (he is not public, in particular because he is allowed to reconstruct C, which is not tested here)? Also, Eve only tries to estimate C, thus rendering the scenario not different in any way to the scenario considered in section 2.\n\nI have two more minor concerns:\n\n1) As raised in the pre-review, Eve should actually be stronger then Alice and Bob in order to be able to compensate for the missing key. The authors noted they have been doing these experiments and are going to add the results.\n\n2) In any natural encryption case I would expect the length of the key to be much shorter then the length of the message. This, however, could potentially make the scenario much easier for Eve (although I doubt any of the results will change if the key is long enough).\n\nI like the creative application of adversarial training to a completely different domain, and I believe it could be the starting point of a very interesting direction in cryptographic systems or in privacy applications (although it is unclear whether the weak guarantees of neural network based approaches can ever be overcome). At the same time the application in the privacy setting leaves me quite confused, and the symmetric encryption example is not particularly strong either. I'd appreciate if the authors could address the major concern I raised above, and I will be quite happy to raise the score in case this confusion can be resolved."
  },
  {
    "people": [
      "Bob",
      "Alice",
      "Bob",
      "Eve"
    ],
    "review": "The submission proposes to modify the typical GAN architecture slightly to include \"encrypt\" (Alice) and \"decrypt\" (Bob) modules as well as a module trying to decrypt the signal without a key (Eve).  Through repeated transmission of signals, the adversarial game is intended to converge to a system in which Alice and Bob can communicate securely (or at least a designated part of the signal should be secure), while a sophisticated Eve cannot break their code.  Examples are given on toy data:\n\"As a proof-of-concept, we implemented Alice, Bob, and Eve networks that take N-bit random plain-text and key values, and produce N-entry floating-point ciphertexts, for N = 16, 32, and 64.  Both plaintext and key values are uniformly distributed.\"\n\nThe idea considered here is cute.  If some, but not necessarily all of the signal is meant to be secure, the modules can learn to encrypt and decrypt a signal, while an adversary is simultaneously learned that tries to break the encryption.  In this way, some of the data can remain unencrypted, while the portion that is e.g. correlated with the encrypted signal will have to be encrypted in order for Eve to not be able to predict the encrypted part.\n\nWhile this is a nice thought experiment, there are significant barriers to this submission having a practical impact:\n1) GANs, and from the convergence figures also the objective considered here, are quite unstable to optimize.  The only guarantees of privacy are for an Eve that is converged to a very strong adversary (stronger than a dedicated attack over time).  I do not see how one can have any sort of reliable guarantee of the safety of the data transmission from the proposed approach, at least the paper does not outline such a guarantee.\n2) Public key encryption systems are readily available, computationally feasible, and successfully applied almost anywhere.  The toy examples given in the paper do not at all convince me that this is solving a real-world problem at this point.  Perhaps a good example will come up in the near future, and this work will be shown to be justified, but until such an example is shown, the approach is more of an interesting thought experiment."
  },
  {
    "people": [
      "Alice",
      "Eve",
      "Bob",
      "Eve",
      "Eve",
      "Bob",
      "Bob",
      "Eve",
      "Eve",
      "Alice",
      "Bob",
      "Eve"
    ],
    "review": "The paper deals with an interesting application of adversarial training to encryption. It considers the standard scenario of Alice, Eve and Bob, where A and B aim to exchange messages conditioned on a shared key, while Eve should be unable to encrypt the message. Experiments are performed in a simple symmetric 16 bit encryption task, and an application on privacy. The concepts, ideas and previous literature are quite nicely and carefully presented.\n\nThe only major concern I have - and I apologize to the authors for not raising this earlier - are the experiments in section 3. In particular, I don't quite get the scenario. The reasoning here seems to be as follows: given information < A, B, C, D >, I want to give the public the value of D (e.g. movies watched) without releasing information about C (e.g. gender). In this scenario, Eve would need to be able to reconstruct D as good as possible without gaining information about C. What is described in section 3, however, is that D and D-public are both reconstructed by Bob, but why would Bob reconstruct the latter (he is not public, in particular because he is allowed to reconstruct C, which is not tested here)? Also, Eve only tries to estimate C, thus rendering the scenario not different in any way to the scenario considered in section 2.\n\nI have two more minor concerns:\n\n1) As raised in the pre-review, Eve should actually be stronger then Alice and Bob in order to be able to compensate for the missing key. The authors noted they have been doing these experiments and are going to add the results.\n\n2) In any natural encryption case I would expect the length of the key to be much shorter then the length of the message. This, however, could potentially make the scenario much easier for Eve (although I doubt any of the results will change if the key is long enough).\n\nI like the creative application of adversarial training to a completely different domain, and I believe it could be the starting point of a very interesting direction in cryptographic systems or in privacy applications (although it is unclear whether the weak guarantees of neural network based approaches can ever be overcome). At the same time the application in the privacy setting leaves me quite confused, and the symmetric encryption example is not particularly strong either. I'd appreciate if the authors could address the major concern I raised above, and I will be quite happy to raise the score in case this confusion can be resolved."
  },
  {
    "people": [
      "Todorov"
    ],
    "review": "A differentiable physics engine is indeed a wonderful thing to have. \n\nThe key selling point of the proposed software is its speed, however there is no comparison to other physics engines. Besides describing the engine's speed in rather creative units (e.g. \"model seconds per day\"), the reader has no idea if this is fast or slow. Todorov's engine (my simulator of choice) computes a dynamics step and its derivatives wrt both states and controls (using finite-differences) in less than 1ms for a *full humanoid* model (his code is available here mujoco.org/book/programming.html#saDerivative). I think this actually faster than the engine described in this paper, but I can't be sure.\n\nBecause this engine is so limited in what it can collide (sphere/sphere and sphere/plane), it would be trivial to build the example models in several other popular engines (e.g. ODE, Bullet and MuJoCo) and simply compare the performance. Until this comparison is done I consider the paper to be incomplete."
  },
  {
    "people": [
      "Todorov"
    ],
    "review": "A differentiable physics engine is indeed a wonderful thing to have. \n\nThe key selling point of the proposed software is its speed, however there is no comparison to other physics engines. Besides describing the engine's speed in rather creative units (e.g. \"model seconds per day\"), the reader has no idea if this is fast or slow. Todorov's engine (my simulator of choice) computes a dynamics step and its derivatives wrt both states and controls (using finite-differences) in less than 1ms for a *full humanoid* model (his code is available here mujoco.org/book/programming.html#saDerivative). I think this actually faster than the engine described in this paper, but I can't be sure.\n\nBecause this engine is so limited in what it can collide (sphere/sphere and sphere/plane), it would be trivial to build the example models in several other popular engines (e.g. ODE, Bullet and MuJoCo) and simply compare the performance. Until this comparison is done I consider the paper to be incomplete."
  },
  {
    "people": [
      "Adam"
    ],
    "review": "In this paper, the authors use a separate introspection neural network to predict the future value of the weights directly from their past history. The introspection network is trained on the parameter progressions collected from training separate set of meta learning models using a typical optimizer, e.g. SGD.  \n\nPros:\n+ The organization is generally very clear\n+ Novel meta-learning approach that is different than the previous learning to learn approach\n\nCons: \n- The paper will benefit from more thorough experiments on other neural network architectures where the geometry of the parameter space are sufficiently different than CNNs such as fully connected and recurrent neural networks.  \n- Neither MNIST nor CIFAR experimental section explained the architectural details\n- Mini-batch size for the experiments were not included in the paper\n- Comparison with different baseline optimizer such as Adam would be a strong addition or at least explain how the hyper-parameters, such as learning rate and momentum, are chosen for the baseline SGD method. \n\nOverall, due to the omission of the experimental details in the current revision, it is hard to draw any conclusive insight about the proposed method. "
  },
  {
    "people": [
      "Adam"
    ],
    "review": "In this paper, the authors use a separate introspection neural network to predict the future value of the weights directly from their past history. The introspection network is trained on the parameter progressions collected from training separate set of meta learning models using a typical optimizer, e.g. SGD.  \n\nPros:\n+ The organization is generally very clear\n+ Novel meta-learning approach that is different than the previous learning to learn approach\n\nCons: \n- The paper will benefit from more thorough experiments on other neural network architectures where the geometry of the parameter space are sufficiently different than CNNs such as fully connected and recurrent neural networks.  \n- Neither MNIST nor CIFAR experimental section explained the architectural details\n- Mini-batch size for the experiments were not included in the paper\n- Comparison with different baseline optimizer such as Adam would be a strong addition or at least explain how the hyper-parameters, such as learning rate and momentum, are chosen for the baseline SGD method. \n\nOverall, due to the omission of the experimental details in the current revision, it is hard to draw any conclusive insight about the proposed method. "
  },
  {
    "people": [
      "van den Oord",
      "Aaron",
      "Nal Kalchbrenner",
      "Koray Kavukcuoglu"
    ],
    "review": "Dear Reviewers & AC:\n\nWe'd like to thank all the reviewers & commenters for their useful suggestions. By taking into account discussion so far, we have significantly updated the manuscript to address questions/concerns.\n\nMinor clarification questions have been answered in discussion below. Here we summarize the main concerns of reviewers and subsequently explain how our latest revision address these concerns:\n\n*** Larger scale experiments (Reviewer1, Reviewer3)\nThis is the main focus of the latest revision. In Section 4.3 of the latest revision, we have shown:\n1. VLAE has the current best density estimation performance on CIFAR10 among latent-code models. It also outperforms PixelCNN/PixelRNN [1] and is only second to PixelCNN++ [2].\n2. We show several different ways to encode different kinds of information into latent code to demonstrate that one can similarly control information placement on CIFAR10.\n\n*** \"the necessity of \"crippling\" the decoder ... has already been pointed out\" (Reviewer1)\nThanks for making the distinction clear. We have revised relevant text (last paragraph of introduction for example) to highlight that the analysis of \"why crippling is necessary\" is our main contribution as opposed to \"pointing out it's necessary\" [3].\n\n*** \"However, they never actually showed how a VAE without AF prior but that has a PixelCNN decoder performs. What would be the impact on the latent code is no AF prior is used?\" (Reviewer2)\nThanks for your suggestion and we have added related experiments in Appendix. C (edit: should be Appendix. D) of the latest revision of the paper. These experiments show removing AF prior will result in worse performance and make the latent code carry less information.\n\nNext we briefly summarize additional contributions in the latest revision.\n\nIn the initial revision, we hypothesized that by improving VAE training procedure, VLAE's modelling performance can be improved. In initial experiments, we used the \u201cfree-bits\u201d technique [5] and this turned out to create optimization challenges. In the latest revision, we proposed a new technique \"soft free-bits\" (described in Appendix.C) which has resulted in more robust optimization. \n\nWe also investigated replacing ResNet VAEs [4,5] with DenseNet [6] VAEs and this change reduces overfitting. \u201cSoft free-bits\u201d and DenseNet VAEs are novel changes and enable VLAE models to outperform previous latent-code models like ResNet VAE w/ IAF [5] as well as competitive autoregressive density estimators like PixelRNN [1] on CIFAR10. These techniques are not unique to VLAE and we suspect they will also be useful to other types of VAEs.\n\nWe also have added one more way to separate information -- by restricting the PixelCNN decoder to condition only on grayscale image, a lossy latent code can be forced to learn color information.\n\nWe believe we have addressed reviewers' concerns and overall strengthened this submission. We hope reviewers can take a look at the latest revision and possibly adjust rating in light of new evidence.\n\nWe are happy to answer any additional questions!\n\n---------------------------------------------------------\n\nPS: We also plan to include experiments with 64x64 datasets but due to time constraints we haven't been able to finish them in time for inclusion in the current revision. We will include them in a later version and release all code when experiments are finished.\n\n[1]: van den Oord, Aaron, Nal Kalchbrenner, and Koray Kavukcuoglu. \"Pixel Recurrent Neural Networks.\" arXiv preprint arXiv:1601.06759 (2016).\n[2]: "
  },
  {
    "people": [
      "Tim Salimans"
    ],
    "review": "The AR prior and its equivalent - the inverse AR posterior - is one of the more elegant ways to improve the unfortunately poor generative qualities of VAE-s. It is only an incremental but important step. Incremental, because, judging by the lack of, say, CIFAR10 pictures of the VLAE in its \"creative\" regime ( i.e., when sampling from prior), it will not answer many of the questions hanging over. We hope to see the paper accepted: in relative terms, the paper shines in the landscape of the other papers which are rich on engineering hacks but lacking on theoretical insights.\n\nSome disagreements with the theoretical suppositions in the paper:\n\ni) The VAE-s posterior converges to the prior faster than we would like because the gradients of the \"generative\" error (the KL divergence of prior and posterior) w.r.t. mu & sigma are simple, inf differentiable functions and their magnitude far exceeds the magnitude of the resp. gradients of the reconstruction error. Especially when more \"hairy\" decoders like pixelCNN are used. We always considered this obvious and certainly not worthy of one page of CS mumbo-jumbo to explain. Dumbing-down the decoder via variations of dropout or \"complexifying\" the sampler as in here, or slapping the generative error with a \"DeepMind\" constant (beta_VAE), are the natural band-aids, but seem to fail in the \"creative\" regime, for real-life sets like CIFAR10 or more complex ones. Other conceptual solutions are needed, some are discussed in [2].\n\nii) The claim near the end of section 2.2 that \"the extra coding cost a.k.a. variational error will exist and will not be negligible\" is a speculation, which, in our empirical experience at least, is incorrect. The variational error is quantifiable for the Gibbs/exponential family of priors/posteriors, as described in [1], section 3.8, and as Tim Salimans knows from his own previous work. In the case of CIFAR10 for example, the variational error is negligible, even for simple sampling families like Gaussian, Laplacian, etc. Moreover, in hindsight, using the closed-form for generative error (the KL divergence of prior and posterior) in the pioneering VAE papers, was likely a mistake inherited by the unnecessary Bayseanism which inspired them (beautiful but a mistake nonetheless): The combo of generative and variational error should together be approximated by the same naive Monte Carlo used for the reconstruction error (easily follows from equation (3.13) in [1]) i.e. arithmetic average over observations.\n\nOn the lighter side, guys, please do not recycle ridiculous terms like \"optimizationally challenged\", as in section 2.2! The English language has recently acquired \"mentally-challenged\", \"emotionally-challenged\", etc, and now political correctness has sadly found its way to machines?\n\n[1] "
  },
  {
    "people": [
      "Bowman"
    ],
    "review": "Hi, I'm one of the authors of Bowman et al. 2016, and I wanted to point out that while our description of the difficulties in training RNN type models with global latent variables occurs in a section called \"Optimization Challenges,\" we make similar observations as you do in your paper, namely that having a very powerful decoder model like an RNN can result in the model ignoring the global latent variable even at the optimum (perhaps the section should have more generally been called \"Learning Challenges\"). \n\nSpecifically, we found that without using \"word dropout\" in the decoder, which effectively weakens the decoder model and forces the global variable to encode more information, we would not learn a useful latent variable. In our Figure 3, we show the results of using different amounts of word dropout and how the proportional split between the global latent code KL and the p(x|z) likelihood changes, even though the total lower bound always gets worse. This indicates that the word dropout is not simply optimizing the same model better, but is giving us a tradeoff between global and local coding.\n\nWe also note that the factorized p(x|z) used in the independent pixel decoding models of previous work forces the model to use the latent variable to achieve good likelihoods, as you mention in Section 2.2. In fact, because a word-dropout decoder models some things as (randomly) conditionally independent, this is sort of a semi-factored decoding distribution."
  },
  {
    "people": [
      "Kundan"
    ],
    "review": "Dear authors,\n\nI liked the paper very much and particularly enjoyed the section involving Bits-Back interpretation of VAE.\n\nI just wanted to point out a very minor stuff with your paper. It appears as if you have placed results for dynamically binarized MNIST for \"Discrete VAE\" model in the table for statically binarized MNIST e.g. table 1 in your paper.\n\nThanks,\nKundan"
  },
  {
    "people": [
      "van den Oord",
      "Aaron",
      "Nal Kalchbrenner",
      "Koray Kavukcuoglu"
    ],
    "review": "Dear Reviewers & AC:\n\nWe'd like to thank all the reviewers & commenters for their useful suggestions. By taking into account discussion so far, we have significantly updated the manuscript to address questions/concerns.\n\nMinor clarification questions have been answered in discussion below. Here we summarize the main concerns of reviewers and subsequently explain how our latest revision address these concerns:\n\n*** Larger scale experiments (Reviewer1, Reviewer3)\nThis is the main focus of the latest revision. In Section 4.3 of the latest revision, we have shown:\n1. VLAE has the current best density estimation performance on CIFAR10 among latent-code models. It also outperforms PixelCNN/PixelRNN [1] and is only second to PixelCNN++ [2].\n2. We show several different ways to encode different kinds of information into latent code to demonstrate that one can similarly control information placement on CIFAR10.\n\n*** \"the necessity of \"crippling\" the decoder ... has already been pointed out\" (Reviewer1)\nThanks for making the distinction clear. We have revised relevant text (last paragraph of introduction for example) to highlight that the analysis of \"why crippling is necessary\" is our main contribution as opposed to \"pointing out it's necessary\" [3].\n\n*** \"However, they never actually showed how a VAE without AF prior but that has a PixelCNN decoder performs. What would be the impact on the latent code is no AF prior is used?\" (Reviewer2)\nThanks for your suggestion and we have added related experiments in Appendix. C (edit: should be Appendix. D) of the latest revision of the paper. These experiments show removing AF prior will result in worse performance and make the latent code carry less information.\n\nNext we briefly summarize additional contributions in the latest revision.\n\nIn the initial revision, we hypothesized that by improving VAE training procedure, VLAE's modelling performance can be improved. In initial experiments, we used the \u201cfree-bits\u201d technique [5] and this turned out to create optimization challenges. In the latest revision, we proposed a new technique \"soft free-bits\" (described in Appendix.C) which has resulted in more robust optimization. \n\nWe also investigated replacing ResNet VAEs [4,5] with DenseNet [6] VAEs and this change reduces overfitting. \u201cSoft free-bits\u201d and DenseNet VAEs are novel changes and enable VLAE models to outperform previous latent-code models like ResNet VAE w/ IAF [5] as well as competitive autoregressive density estimators like PixelRNN [1] on CIFAR10. These techniques are not unique to VLAE and we suspect they will also be useful to other types of VAEs.\n\nWe also have added one more way to separate information -- by restricting the PixelCNN decoder to condition only on grayscale image, a lossy latent code can be forced to learn color information.\n\nWe believe we have addressed reviewers' concerns and overall strengthened this submission. We hope reviewers can take a look at the latest revision and possibly adjust rating in light of new evidence.\n\nWe are happy to answer any additional questions!\n\n---------------------------------------------------------\n\nPS: We also plan to include experiments with 64x64 datasets but due to time constraints we haven't been able to finish them in time for inclusion in the current revision. We will include them in a later version and release all code when experiments are finished.\n\n[1]: van den Oord, Aaron, Nal Kalchbrenner, and Koray Kavukcuoglu. \"Pixel Recurrent Neural Networks.\" arXiv preprint arXiv:1601.06759 (2016).\n[2]: "
  },
  {
    "people": [
      "Tim Salimans"
    ],
    "review": "The AR prior and its equivalent - the inverse AR posterior - is one of the more elegant ways to improve the unfortunately poor generative qualities of VAE-s. It is only an incremental but important step. Incremental, because, judging by the lack of, say, CIFAR10 pictures of the VLAE in its \"creative\" regime ( i.e., when sampling from prior), it will not answer many of the questions hanging over. We hope to see the paper accepted: in relative terms, the paper shines in the landscape of the other papers which are rich on engineering hacks but lacking on theoretical insights.\n\nSome disagreements with the theoretical suppositions in the paper:\n\ni) The VAE-s posterior converges to the prior faster than we would like because the gradients of the \"generative\" error (the KL divergence of prior and posterior) w.r.t. mu & sigma are simple, inf differentiable functions and their magnitude far exceeds the magnitude of the resp. gradients of the reconstruction error. Especially when more \"hairy\" decoders like pixelCNN are used. We always considered this obvious and certainly not worthy of one page of CS mumbo-jumbo to explain. Dumbing-down the decoder via variations of dropout or \"complexifying\" the sampler as in here, or slapping the generative error with a \"DeepMind\" constant (beta_VAE), are the natural band-aids, but seem to fail in the \"creative\" regime, for real-life sets like CIFAR10 or more complex ones. Other conceptual solutions are needed, some are discussed in [2].\n\nii) The claim near the end of section 2.2 that \"the extra coding cost a.k.a. variational error will exist and will not be negligible\" is a speculation, which, in our empirical experience at least, is incorrect. The variational error is quantifiable for the Gibbs/exponential family of priors/posteriors, as described in [1], section 3.8, and as Tim Salimans knows from his own previous work. In the case of CIFAR10 for example, the variational error is negligible, even for simple sampling families like Gaussian, Laplacian, etc. Moreover, in hindsight, using the closed-form for generative error (the KL divergence of prior and posterior) in the pioneering VAE papers, was likely a mistake inherited by the unnecessary Bayseanism which inspired them (beautiful but a mistake nonetheless): The combo of generative and variational error should together be approximated by the same naive Monte Carlo used for the reconstruction error (easily follows from equation (3.13) in [1]) i.e. arithmetic average over observations.\n\nOn the lighter side, guys, please do not recycle ridiculous terms like \"optimizationally challenged\", as in section 2.2! The English language has recently acquired \"mentally-challenged\", \"emotionally-challenged\", etc, and now political correctness has sadly found its way to machines?\n\n[1] "
  },
  {
    "people": [
      "Bowman"
    ],
    "review": "Hi, I'm one of the authors of Bowman et al. 2016, and I wanted to point out that while our description of the difficulties in training RNN type models with global latent variables occurs in a section called \"Optimization Challenges,\" we make similar observations as you do in your paper, namely that having a very powerful decoder model like an RNN can result in the model ignoring the global latent variable even at the optimum (perhaps the section should have more generally been called \"Learning Challenges\"). \n\nSpecifically, we found that without using \"word dropout\" in the decoder, which effectively weakens the decoder model and forces the global variable to encode more information, we would not learn a useful latent variable. In our Figure 3, we show the results of using different amounts of word dropout and how the proportional split between the global latent code KL and the p(x|z) likelihood changes, even though the total lower bound always gets worse. This indicates that the word dropout is not simply optimizing the same model better, but is giving us a tradeoff between global and local coding.\n\nWe also note that the factorized p(x|z) used in the independent pixel decoding models of previous work forces the model to use the latent variable to achieve good likelihoods, as you mention in Section 2.2. In fact, because a word-dropout decoder models some things as (randomly) conditionally independent, this is sort of a semi-factored decoding distribution."
  },
  {
    "people": [
      "Kundan"
    ],
    "review": "Dear authors,\n\nI liked the paper very much and particularly enjoyed the section involving Bits-Back interpretation of VAE.\n\nI just wanted to point out a very minor stuff with your paper. It appears as if you have placed results for dynamically binarized MNIST for \"Discrete VAE\" model in the table for statically binarized MNIST e.g. table 1 in your paper.\n\nThanks,\nKundan"
  },
  {
    "people": [
      "Touzet",
      "Dehaene",
      "Krizhevsky",
      "Viterbi",
      "Viterbi"
    ],
    "review": "This paper uses an LSTM model to predict what it calls \"open bigrams\" (bigrams of characters that may or may not have letters inbetween) from handwriting data. These open bigrams are subsequently used to predict the written word in a decoding step. The experiments indicate that the system does slightly better than a baseline model that uses Viterbi decoding. I have some major concerns about this paper:\n\n- I find the \"cortical inspired\" claim troublesome. If anything, it is psychology/cognitive science inspired, in the sense that open bigrams appear to help for word recognition (Touzet et al. 2014). But the implied cortical characteristics, implicitly referred to e.g. by pointing to analogies between deep neural nets for object recognition and in that case the visual cortex, is unfounded. Is there any direct evidence from neuroscience that open-bigrams constitute a wholly separate layer in the cortex for a handwriting recognition task? Dehaene's work is a proposal, so you'll need to describe more \"findings in cognitive neurosciences [sic] research on reading\" (p. 8) to substantiate those claims. I am further worried by the fact that the authors seem to think that \"deep neural networks are based on a series of about five pairs of neurons [sic] layers\". Unless I misunderstand something, you are specifically referring to Krizhevsky's AlexNet here (which you should probably have cited there)? I hope you don't mean to imply that all deep neural nets need five layers. It is also not true that ten is \"quite close to the number of layers of an efficient deep NN\" -- what network? what task? etc.\n\n- The model is not clearly explained. There is a short paragraph in Appendix A.3. that roughly describes the setup, but this does not include e.g. the objective function, or answer why the network output is only considered each two consecutive time steps, rather than at each time step (or so it seems?). This is probably because the paper argues that it \"is focused on the decoder\" (p. 6), rather than on the whole problem. I find this problematic, because in that case we're effectively measuring how easy it is to reconstruct a word from its open bigrams, which has very little to do with handwriting recognition (it could have been evaluated on any text corpus). In fact, as the example on page 4 shows, handwriting is not necessary to illustrate the open bigram hypothesis. Which leads me to wonder why these particular tasks were chosen, if we are only interested in the decoding mechanism?\n\n- The comparison is not really fair. The Viterbi decoder only has access to unigrams, as far as I can tell. The only model that does better than that baseline has access to a lot more information, and does not do that much better. Did the Viterbi model have access to the word boundary information (at one point rather confusingly called \"extremities\") that pushed the open bigram model over the edge in terms of performance? Why is there no comparison to e.g. rnn_0,1' (unigram+bigram+boundary markers)? The dataset also appears to be biased in favor of the proposed approach (longer words, only ). I am not convinced that this paper really shows that open bigrams help.\n\nI very much like the idea of the paper, but I am simply not convinced by its claims.\n\nMinor points:\n- There are quite a few typos. Just a sample: \"independant\" (Fig.1), \"we evaluate an handwritten\", \", hand written words [..], an the results\", \"their approach include\", \"the letter bigrams of a word w is\", \"for the two considered database\"\n- Wouldn't it be easy to add how many times a bigram occurs, which would improve the decoding process? You can just normalize over the full counts instead of the binary occurrence counts.\n- The results in Table 5 are the same (but different precision) as the results in Table 2, except that edit distance and SER are added, this is confusing."
  },
  {
    "people": [
      "Thodore Bluche",
      "Theodore Bluche"
    ],
    "review": "This submission investigates the usability of cortical-inspired distant bigram representations for handwritten word recognition. Instead of generating neural network based posterior features for character (optionally in local context), sets posterior for character bigrams of different length are used to represent words.  The aim here is to investigate the viability of this approach and to compare to the standard approach.\n\nOverall, the submission is well written, although information is missing w.r.t. to the comparison between the proposed approach and the standard approach, see below.\n\nIt would be desirable to see the model complexity of all the different models used here, i.e. the number of parameters used.\n\nLanguage models are not used here. Since the different models utilize different levels of context, language models can be expected to have a different effect on the different approaches. Therefore I suggest to include the use of language models into the evaluation.\n\nFor your comparative experiments you use only 70% of the data by choosing longer words only. On the other hand, it is well known that the shorter words are more prone to result in misrecognitions. The question remains, if this choice is advantageous for one of the tasks, or not - corresponding quantitative results should be provided to be able to better evaluate the effect of using this constrained corpus. Without clarification of this I would not readily agree that the error rates are competitive or better than the standard approach, as stated at the end of Sec. 5.\n\nI do see the motivation for introducing open-bigrams in an unordered way due to the corresponding evidence from cognitive research. However, decision theoretically I wonder, why the order should be given up, if the underlying sequential classification problem clearly is of a monotonous nature. It would be interesting to see an experiment, where only the use of the order is varied, to differentiate the effect of the order from the effect of other aspects of the approach.\n\nEnd of page 1: \"whole language method\" - please explain what is meant by this.\n\nPage 6: define your notation for rnn_d(x,t).\n\nThe number of target for the RNNs modeling order 0 (unigrams effectively) and the RNNs modeling order 1 and larger are very much different.  Therefore the precision and recall numbers in Table 2 do not seem to be readily comparable between order 0 and orders >=1. At least, the column for order 0 should be visually separated to highlight this.\n\n\nMinor comments: a spell check is recommended\np. 2: state-of-art -> state-of-the-art\np. 2: predict character sequence -> predict a character sequence\np. 3, top: Their approach include -> Their approach includes\np. 3, top: an handwritten -> a handwritten\np. 3, bottom: consituent -> constituent\np. 4, top: in classical approach -> in the classical approach\np. 4, top: transformed in a vector -> transformed into a vector\np. 5: were build -> were built\nReferences: first authors name written wrongly: Thodore Bluche -> Theodore Bluche\n"
  },
  {
    "people": [
      "Touzet",
      "Dehaene",
      "Krizhevsky",
      "Viterbi",
      "Viterbi"
    ],
    "review": "This paper uses an LSTM model to predict what it calls \"open bigrams\" (bigrams of characters that may or may not have letters inbetween) from handwriting data. These open bigrams are subsequently used to predict the written word in a decoding step. The experiments indicate that the system does slightly better than a baseline model that uses Viterbi decoding. I have some major concerns about this paper:\n\n- I find the \"cortical inspired\" claim troublesome. If anything, it is psychology/cognitive science inspired, in the sense that open bigrams appear to help for word recognition (Touzet et al. 2014). But the implied cortical characteristics, implicitly referred to e.g. by pointing to analogies between deep neural nets for object recognition and in that case the visual cortex, is unfounded. Is there any direct evidence from neuroscience that open-bigrams constitute a wholly separate layer in the cortex for a handwriting recognition task? Dehaene's work is a proposal, so you'll need to describe more \"findings in cognitive neurosciences [sic] research on reading\" (p. 8) to substantiate those claims. I am further worried by the fact that the authors seem to think that \"deep neural networks are based on a series of about five pairs of neurons [sic] layers\". Unless I misunderstand something, you are specifically referring to Krizhevsky's AlexNet here (which you should probably have cited there)? I hope you don't mean to imply that all deep neural nets need five layers. It is also not true that ten is \"quite close to the number of layers of an efficient deep NN\" -- what network? what task? etc.\n\n- The model is not clearly explained. There is a short paragraph in Appendix A.3. that roughly describes the setup, but this does not include e.g. the objective function, or answer why the network output is only considered each two consecutive time steps, rather than at each time step (or so it seems?). This is probably because the paper argues that it \"is focused on the decoder\" (p. 6), rather than on the whole problem. I find this problematic, because in that case we're effectively measuring how easy it is to reconstruct a word from its open bigrams, which has very little to do with handwriting recognition (it could have been evaluated on any text corpus). In fact, as the example on page 4 shows, handwriting is not necessary to illustrate the open bigram hypothesis. Which leads me to wonder why these particular tasks were chosen, if we are only interested in the decoding mechanism?\n\n- The comparison is not really fair. The Viterbi decoder only has access to unigrams, as far as I can tell. The only model that does better than that baseline has access to a lot more information, and does not do that much better. Did the Viterbi model have access to the word boundary information (at one point rather confusingly called \"extremities\") that pushed the open bigram model over the edge in terms of performance? Why is there no comparison to e.g. rnn_0,1' (unigram+bigram+boundary markers)? The dataset also appears to be biased in favor of the proposed approach (longer words, only ). I am not convinced that this paper really shows that open bigrams help.\n\nI very much like the idea of the paper, but I am simply not convinced by its claims.\n\nMinor points:\n- There are quite a few typos. Just a sample: \"independant\" (Fig.1), \"we evaluate an handwritten\", \", hand written words [..], an the results\", \"their approach include\", \"the letter bigrams of a word w is\", \"for the two considered database\"\n- Wouldn't it be easy to add how many times a bigram occurs, which would improve the decoding process? You can just normalize over the full counts instead of the binary occurrence counts.\n- The results in Table 5 are the same (but different precision) as the results in Table 2, except that edit distance and SER are added, this is confusing."
  },
  {
    "people": [
      "Thodore Bluche",
      "Theodore Bluche"
    ],
    "review": "This submission investigates the usability of cortical-inspired distant bigram representations for handwritten word recognition. Instead of generating neural network based posterior features for character (optionally in local context), sets posterior for character bigrams of different length are used to represent words.  The aim here is to investigate the viability of this approach and to compare to the standard approach.\n\nOverall, the submission is well written, although information is missing w.r.t. to the comparison between the proposed approach and the standard approach, see below.\n\nIt would be desirable to see the model complexity of all the different models used here, i.e. the number of parameters used.\n\nLanguage models are not used here. Since the different models utilize different levels of context, language models can be expected to have a different effect on the different approaches. Therefore I suggest to include the use of language models into the evaluation.\n\nFor your comparative experiments you use only 70% of the data by choosing longer words only. On the other hand, it is well known that the shorter words are more prone to result in misrecognitions. The question remains, if this choice is advantageous for one of the tasks, or not - corresponding quantitative results should be provided to be able to better evaluate the effect of using this constrained corpus. Without clarification of this I would not readily agree that the error rates are competitive or better than the standard approach, as stated at the end of Sec. 5.\n\nI do see the motivation for introducing open-bigrams in an unordered way due to the corresponding evidence from cognitive research. However, decision theoretically I wonder, why the order should be given up, if the underlying sequential classification problem clearly is of a monotonous nature. It would be interesting to see an experiment, where only the use of the order is varied, to differentiate the effect of the order from the effect of other aspects of the approach.\n\nEnd of page 1: \"whole language method\" - please explain what is meant by this.\n\nPage 6: define your notation for rnn_d(x,t).\n\nThe number of target for the RNNs modeling order 0 (unigrams effectively) and the RNNs modeling order 1 and larger are very much different.  Therefore the precision and recall numbers in Table 2 do not seem to be readily comparable between order 0 and orders >=1. At least, the column for order 0 should be visually separated to highlight this.\n\n\nMinor comments: a spell check is recommended\np. 2: state-of-art -> state-of-the-art\np. 2: predict character sequence -> predict a character sequence\np. 3, top: Their approach include -> Their approach includes\np. 3, top: an handwritten -> a handwritten\np. 3, bottom: consituent -> constituent\np. 4, top: in classical approach -> in the classical approach\np. 4, top: transformed in a vector -> transformed into a vector\np. 5: were build -> were built\nReferences: first authors name written wrongly: Thodore Bluche -> Theodore Bluche\n"
  },
  {
    "people": [
      "Alex",
      "Judy"
    ],
    "review": "This paper proposes a k-shot learning framework that can be used on existing pre-trained networks by grouping filters that produce similar activations. The grouped filters are learned together to address overfitting when only few training samples are available. \n\nThe idea of the paper is interesting there are some encouraging results, but the current version doesn't seem ready for publication:\n\nPerformance:\nThe method should be compared with other state-of-the-art k-shot learning methods (e.g., Matching Networks by Vinyals et al., 2016). It's not clear how this method compares against them.\n\nMissing explanation:\nExperimental setting for k-shot learning should be more detailed.\n\nMeasure:\nAccuracy difference does not look like a good idea for comparing the baseline method and the proposed one. Just raw accuracies would be fine. \n\nMany grammatical errors and inappropriate formatting of citations, such as:\nM. et al. (2011)\nImageNet (Alex et al. (2012))\nJudy et al. (2013): this reference appears three times in the reference section.\n"
  },
  {
    "people": [
      "Alex",
      "Judy"
    ],
    "review": "This paper proposes a k-shot learning framework that can be used on existing pre-trained networks by grouping filters that produce similar activations. The grouped filters are learned together to address overfitting when only few training samples are available. \n\nThe idea of the paper is interesting there are some encouraging results, but the current version doesn't seem ready for publication:\n\nPerformance:\nThe method should be compared with other state-of-the-art k-shot learning methods (e.g., Matching Networks by Vinyals et al., 2016). It's not clear how this method compares against them.\n\nMissing explanation:\nExperimental setting for k-shot learning should be more detailed.\n\nMeasure:\nAccuracy difference does not look like a good idea for comparing the baseline method and the proposed one. Just raw accuracies would be fine. \n\nMany grammatical errors and inappropriate formatting of citations, such as:\nM. et al. (2011)\nImageNet (Alex et al. (2012))\nJudy et al. (2013): this reference appears three times in the reference section.\n"
  },
  {
    "people": [
      "Hao"
    ],
    "review": "We would like to thank all reviewers for their comments and suggestions. We have added new experiments based on reviewer suggestions (and will continue to add more as necessary). The new experiments from Hao in Appendix A report fprop times for the pruned networks and random pruning results that address Reviewer 4's question about random pruning and Reviewer 2's concerns."
  },
  {
    "people": [
      "Hao"
    ],
    "review": "We would like to thank all reviewers for their comments and suggestions. We have added new experiments based on reviewer suggestions (and will continue to add more as necessary). The new experiments from Hao in Appendix A report fprop times for the pruned networks and random pruning results that address Reviewer 4's question about random pruning and Reviewer 2's concerns."
  },
  {
    "people": [
      "Ba",
      "Bell",
      "Bell",
      "Bell",
      "Visin",
      "Bell",
      "Visin",
      "Ba",
      "Sergey Zagoruyko",
      "Nikos Komodakis",
      "Huang"
    ],
    "review": "Thank you very much for all the comments and suggestions.\nSome of the comments were already addressed in version 2 of the paper.\nVersion 3 addresses most of the remaining points. \n\nWe summarize the main changes here:\n \nVersion 3:\n1. Added additional architectures for CIFAR-10 classification, including\n-- Network E, F, where CNN modules and LRNN modules are interleaved at multiple levels. Network F outperforms the ResNet-164.\n2. Reformulated Figure 4 (Architectures for CIFAR experiments) as Table 1,\n3. Extended discussions on the experimental results.\n\nVersion 2: \n1. L-RNN modules with vanilla RNNs trained with Layer Normalization (Ba et al. 2016)\n2. Baseline-CNN added containing only convolutional layers.\n\nThe following are the response to all reviewers on the common points:\n\n---- All the reviewers comment on the very good paper by Bell et al. We address those points here.\n\nReviewer 1:\n* Idea somewhat incremental (e.g. can be seen as derivative from Bell 2016).\n \nReviewer 2:\n* Regarding novelty, the idea of combining a recurrent layer with a CNN, something practically very similar was proposed in Bell et al (2016). There are a few technical differences (e.g. cascading versus applying in parallel the recurrent layers), but in my understanding these are minor changes. The idea of initializing the recurrent network with the CNN is reasonable but is at the level of improving one wrong choice in the original work of Bell, rather than really proposing something novel.\n \nReviewer 3:\n* Contributions relative to, e.g. Bell et al., are minor.\n\nResponse:\nIn our paper, the proposed L-RNN module aims to be general; it can be inserted at any stage in the architectures for capturing contextual information at multiple levels.\n\nTo be clear on the differences:\n\n1. Interleaving CNN with L-RNN modules:\nIn Bell et al., the spatial RNNs are applied on top of CNN features (VGG-net) to learn contextual information at the final stage. A similar idea was also proposed in ReSeg (Visin et al. 2016).\nOur paper goes further than Bell et al. or Visin et al. by interleaving the CNN and LRNN modules at multiple levels of the network.\nIn consequence, the network is capable of learning representations from both local and larger context at every layer, alleviating the limitations of a fixed kernel size.\n \n2. How L-RNNs are inserted and initialized:\nUnder the scenario when the amount of data is limited, e.g. detection in Bell et al, semantic segmentation in our case.\nIn Bell et al., the authors take pre-trained CNN networks (VGG-16 up to conv5), and then train spatial RNNs on top of the pre-computed features.\nIn contrast, we show that a L-RNN can be directly inserted and trained within existing convolutional layers. This means that we increase the representational power of the pre-trained model directly. All layers in the pre-trained networks can therefore be adapted efficiently.\n\nIn our case, we re-purpose several layers in the pre-trained network as L-RNN modules at multiple levels, e.g. after pool3, pool4, and the final fully connected layers. As a result, we get a performance boost of 5% (mean IOU).\n \n3. Choice of RNNs.\nIn Bell et al., to avoid computational cost, the authors choose to use ReLU-based vanilla RNN (rather than GRU or LSTM).\nIn our paper, we validate this choice by showing that vanilla RNN with Layer Normalization (Ba et al. 2016) achieves similar performance to GRU.\n \n4. Separable Convolutions:\nIn Bell et al., 4 bidirectional spatial RNNs are applied to learn the global context. While in our paper, we propose to use 2, each bidirectional spatial RNNs is learning to approximate 1D convolutions, this idea comes from separable convolution.\n\n---- Comments on CIFAR classification experiments.\n \nReviewer 2:\n* Furthermore, as noted in my early question, Wide Residual Networks (Sergey Zagoruyko, Nikos Komodakis, BMVC16) report  better results on CIFAR-10 (4% error), while not using any recurrent layers (rather using instead a wide, VGG-type, ResNet variant). \nSo, the authors answer: \"Wide Residual Networks use the depth of the network to spread the receptive field across the entire image (DenseNet (Huang et al., 2016) similarly uses depth). Thus there is no need for recurrence within layers to capture contextual information. In contrast, we show that a shallow CNN, where the receptive field would be limited, can capture contextual information within the whole image if a L-RNN is used.\"\n \n* So, we agree that WRN do not need recurrence - and can still do better.\nThe point of my question has practically been whether using a recurrent layer is really necessary; I can understand the answer as being \"yes, if you want to keep your network shallow\".  I do not necessarily see why one would want to keep one's network shallow.\n \nReviewer 3:\n* One additional issue with the CIFAR experiments is that I expect to see a direct comparison of models A-F with and without L-RNN. It is hard to understand from the presented results if L-RNN actually adds much. In sum, I have a hard time taking away any valuable information from the CIFAR experiments.\n \nResponse:\nHere is a short summary of the discussion part of the CIFAR experiments (now included in  the updated paper).\n\nThe L-RNN module is a general computational module; it is not our intention to replace the deep networks (e.g. residual variants). \n\n1. Why only use shallow networks?\nIt is well known that deep networks can generally achieve better results than shallow ones. Thus, in our experiments, we use relatively shallow networks to avoid the possibility that the performance gain is due to network depth. Our experiments confirm that increasing the network depth by only adding low-level CNN modules below a L-RNN improves the results (Table 2, Network A to D)\n\n2. Interleaving L-RNN with CNN modules.\nWe show a comparison between a Baseline-CNN and Network-E in Table 2. Baseline-CNN is composed of 7 convolutional layers with 1.56M parameters, it achieves 8.48% top1 error. While the Network-E interleaved with CNN and L-RNN modules contains 0.97M parameters, achieving 5.96% top1 error. The difference between them is the added LRNN modules.\nMoreover, by adding more layers, Network F(5.39% top1 error) achieves comparable performance to ResNet-164 (5.46%). \n\n---- Questions related to ImageNet.\n\nReviewer 2:\n* Regarding the evaluation, experiments on CIFAR are interesting, but only as proof of concept.\n* Probably an evaluation on imageNet would bring some more insight about the merit of this layer.\n \nReviewer 3:\n* Classification experiments are not convincing.\n* I am generally skeptical of the utility of classification experiments on CIFAR-10 when presented in isolation (e.g., no results on ImageNet too). The issue is that CIFAR-10 is not interesting as a task unto itself *and* methods that work well on CIFAR-10 do not necessarily generalize to other tasks. ImageNet has been useful because, thus far, it produces features that generalize well to other tasks. Showing good results on ImageNet is much more likely to demonstrate a model that learns generalizable features. However, that is not even necessarily true, and ideally I would like to see that that a model that does well on ImageNet in fact transfers its benefit to at least one other tasks (e.g., detection).\n \nResponse:\nFurther experiments on ImageNet are definitely on the top list of our future work.\n\n---- Comments on Training Details.\n \nReviewer 1:\n* Section 5.2.1 (and appendix A), how is the learning rate increased and decreased? Manually ? This is an important detail that should be made explicit. Is the learning rate schedule the same in all experiments of each table? If there is a human in the loop, what is the variance in results between \u201ctwo human schedulers\u201d?\n \nReviewer 2:\n* Appendix A: this is very mysterious. Did you try other learning rate schedules? (e.g. polynomial)\n \nResponse:\nThe important message here is that restarting the learning rate several times can help the networks to escape saddle points or local minima.\nWe only experiment with the stepwise decay (no polynomial), and change the learning rate every 40, 60 or 80 epochs, we did not find much difference on this detail.\nWe only provide the intuitive explanation because it is not the main focus of this paper, it is just a training trick. If the readers are interested in this, we found two other related papers in this ICLR submission.\n\n"
  },
  {
    "people": [
      "Bell",
      "Bell",
      "Bell",
      "Sergey Zagoruyko",
      "Nikos Komodakis"
    ],
    "review": "The paper proposes a method of integrating recurrent layers within larger, potentially pre-trained, convolutional networks. The objective is to combine the feature extraction abilities of CNNs with the ability of RNNs to gather global context information.\nThe authors validate their idea on two tasks, image classification (on CIFAR-10) and semantic segmentation (on PASCAL VOC12).\n\nOn the positive side, the paper is clear and well-written (apart from some occasional typos), the proposed idea is simple and could be adopted by other works, and can be deployed as a beneficial perturbation of existing systems, which is practically important if one wants to increase the performance of a system without retraining it from scratch. The evaluation is also systematic, providing a clear ablation study. \n\nOn the negative side, the novelty of the work is relatively limited, while the validation is lacking a bit. \nRegarding novelty, the idea of combining a recurrent layer with a CNN, something practically very similar was proposed in Bell et al (2016). There are a few technical differences (e.g. cascading versus applying in parallel the recurrent layers), but in my understanding these are minor changes. The idea of initializing the recurrent network with the CNN is reasonable but is at the level of improving one wrong choice in the original work of Bell, rather than really proposing something novel. \nThis contribution (\" we use RNNs within layers\") is repeatedly mentioned in the paper (including intro &  conclusion), but in my understanding was part of Bell et al, modulo minor changes. \n\nRegarding the evaluation, experiments on CIFAR are interesting, but only as proof of concept. \n\nFurthermore, as noted in my early question, Wide Residual Networks (Sergey Zagoruyko, Nikos Komodakis, BMVC16)\nreport  better results on CIFAR-10 (4% error), while not using any recurrent layers (rather using instead a wide, VGG-type, ResNet variant). So. \nThe authors answer: \"Wide Residual Networks use the depth of the network to spread the receptive field across the entire image (DenseNet (Huang et al., 2016) similarly uses depth). Thus there is no need for recurrence within layers to capture contextual information. In contrast, we show that a shallow CNN, where the receptive field would be limited, can capture contextual information within the whole image if a L-RNN is used.\"\n\nSo, we agree that WRN do not need recurrence - and can still do better. \nThe point of my question has practically been whether using a recurrent layer is really necessary; I can understand the answer as being \"yes, if you want to keep your network shallow\".  I do not necessarily see why one would want to keep one's network shallow.\n\nProbably an evaluation on imagenet would bring some more insight about the merit of this layer. \n\n\nRegarding semantic segmentation, one of my questions has been:\n\"Is the boost you are obtaining due to something special to the recurrent layer, or is simply because one is adding extra parameters on top of a pre-trained network? (I admit I may have missed some details of your experimental evaluation)\"\nThe answer was:\n\"...For PASCAL segmentation, we add the L-RNN into a pre-trained network (this adds recurrence parameters), and again show that this boosts performance - more so than adding the same number of parameters as extra CNN layers - as it is able to model long-range dependences\"\nI could not find one such experiment in the paper ('more so than adding the same number of parameters as extra CNN layers'); I understand that you have 2048 x 2048 connections for the recurrence, it would be interesting to see what you get by spreading them over (non-recurrent) residual layers.\nClearly, this is not going to be my criterion for rejection/acceptance, since one can easily make it fail - but I was mostly asking for some sanity check \n\nFurthermore, it is a bit misleading to put in Table 3 FCN-8s and FCN8s-LRNN, since this gives the impression that the LRNN gives a  boost by 10%. In practice the \"FCN8s\" prefix of \"FCN8s-LRNN\" is that of the authors, and not of Long et al (as indicated in Table 2, 8s original is quite worse than 8s here). \n\nAnother thing that is not clear to me is where the boost comes from in Table 2; the authors mention that \"when inserting the L-RNN after pool 3 and pool4 in FCN-8s, the L-RNN is able to learn contextual information over a much larger range than the receptive field of pure local convolutions. \"\nThis is potentially true, but I do not see why this was not also the case for FCN-32s (this is more a property of the recurrence rather than the 8/32 factor, right?)\n\nA few additional points: \nIt seems like Fig 2b and Fig2c never made it into the pdf. \n\nFigure 4 is unstructured and throws some 30 boxes to the reader - I would be surprised if anyone is able to get some information out of this (why not have a table?) \n\nAppendix A: this is very mysterious. Did you try other learning rate schedules? (e.g. polynomial)\nWhat is the performance if you apply a standard training schedule? (e.g. step). \nAppendix C: \"maps .. is\" -> \"maps ... are\"\n\n\n"
  },
  {
    "people": [
      "Bell",
      "Bell",
      "Bell"
    ],
    "review": "This paper proposes a cascade of paired (left/right, up/down) 1D RNNs as a module in CNNs in order to quickly add global context information to features without the need for stacking many convolutional layers. Experimental results are presented on image classification and semantic segmentation tasks.\n\nPros:\n- The paper is very clear and easy to read.\n- Enough details are given that the paper can likely be reproduced with or without source code.\n- Using 1D RNNs inside CNNs is a topic that deserves more experimental exploration than what exists in the literature.\n\nCons (elaborated on below):\n(1) Contributions relative to, e.g. Bell et al., are minor.\n(2) Disappointed in the actual use of the proposed L-RNN module versus how it's sold in the intro.\n(3) Classification experiments are not convincing.\n\n(1,2): The introduction states w.r.t. Bell et al. \"more substantial differences are two fold: first, we treat the L-RNN module as a general block, that can be inserted into any layer of a modern architecture, such as into a residual module. Second, we show (section 4) that the\nL-RNN can be formulated to be inserted into a pre-trained FCN (by initializing with zero recurrence\nmatrices), and that the entire network can then be fine-tuned end-to-end.\"\n\nI felt positive about these contributions after reading the intro, but then much less so after reading the experimental sections. Based on the first contribution (\"general block that can be inserted into any layer\"), I strongly expected to see the L-RNN block integrated throughout the CNN starting from near the input. However, the architectures for classification and segmentation only place the module towards the very end of the network. While not exactly the same as Bell et al. (there are many technical details that differ), it is close. The paper does not compare to the design from Bell et al. Is there any advantage to the proposed design? Or is it a variation that performs similarly? What happens if L-RNN is integrated earlier in the network, as suggested by the introduction?\n\nThe second difference is a bit more solid, but still does not rise to a 'substantive difference' in my view. Note that Bell et al. also integrate 1D RNNs into an ImageNet pretrained VGG-16 model. I do, however, think that the method of integration proposed in this paper (zero initialization) may be more elegant and does not require two-stage training by first freezing the lower layers and then later unfreezing them.\n\n(3) I am generally skeptical of the utility of classification experiments on CIFAR-10 when presented in isolation (e.g., no results on ImageNet too). The issue is that CIFAR-10 is not interesting as a task unto itself *and* methods that work well on CIFAR-10 do not necessarily generalize to other tasks. ImageNet has been useful because, thus far, it produces features that generalize well to other tasks. Showing good results on ImageNet is much more likely to demonstrate a model that learns generalizable features. However, that is not even necessarily true, and ideally I would like to see that that a model that does well on ImageNet in fact transfers its benefit to at least one other ask (e.g., detection).\n\nOne additional issue with the CIFAR experiments is that I expect to see a direct comparison of models A-F with and without L-RNN. It is hard to understand from the presented results if L-RNN actually adds much. In sum, I have a hard time taking away any valuable information from the CIFAR experiments.\n\nMinor suggestion:\n- Figure 4 is hard to read. The pixelated rounded corners on the yellow boxes are distracting."
  },
  {
    "people": [
      "He",
      "Bell"
    ],
    "review": "The authors propose the use of a vertical and horizontal one-dimensional RNN (denoted as L-RNN module) to capture long-range dependencies and summarize convolutional feature maps. L-RNN modules are an alternative to deeper or wider networks, 2D RNNs, dilated (Atrous) convolutional layers, and a simple flatten or global pooling layer when applied to the last convolutional layer for classification. L-RNN modules are faster than 2D RNNs, since rows and columns can be processed in parallel, are easy to implemented, and can be inserted in existing convolutional networks. The authors demonstrate improvements for classification and semantic segmentation.\n\nHowever, further evaluations are required that show for which use cases L-RNNs are superior to alternatives for summarizing convolutional feature maps:\n\n1. I suggest to use a fixed CNN with as certain number of layers, and summarize the last feature map by a) a flatten layer, b) global average pooling, c) a 2D RNN, d) and dilated convolutional layers for segmentation. The authors should report both the run-time and number of parameters for these variants in addition to prediction performances. For segmentation, the number of dilated convolutional layers should be chosen such that the number of parameters is similar to a single L-RNN module.\n\n2. The authors compare classification performances only on 32x32 CIFAR-10 images. For higher resolution images, the benefit of L-RNN modules to capture long-range dependencies might be more pronounced. I therefore suggest evaluating classification performances on one additional dataset with higher resolution images, e.g. ImageNet or the CUB bird dataset.\n\nAdditionally, I have the following minor comments:\n\n3. The authors use vanilla RNNs. It might be worth investigating LSTMs or GRUs instead.\n\n4. For classification, the authors summarize hidden states of the final vertical recurrent layer by global max pooling. Is this different from more common global average pooling or concatenating the final forward and backward recurrent states?\n\n5. Table 3 is hard to understand since it mingles datasets (Pascal P and COCO C) and methods (CRF post-processing). I suggest, e.g., using an additional column with CRF \u2018yes\u2019 or \u2018no\u2019. I further suggest listing the number of parameters and runtime if possible.\n\n6. Section 3 does not clearly describe in which order batch-normalization is applied in residual blocks. Figure 2 suggest that the newer BN-ReLU-Conv order described in He et al. (2016) is used. This should be mentioned in the text.\n\nFinally, the text needs to be revised to reach publication level quality. Specifically, I have the following comments:\n\n7. Equation (1) is the update of a vanilla RNN, which should be stated more clearly. I suggest to first describe (bidirectional) RNNs, to reference GRUs and LSTMs, and then describe how they are applied here to images. Figure 1 should also be referenced in the text.\n\n8. In section 2.2, I suggest to describe Bell at al. more clearly. Why are they using eight instead of four RNNs? \n\n9. Section 4 starts with a verbose description about transfer learning, which can be compressed into a single reference or skipped entirely.\n\n10. Equation (6) seems to be missing an index i.\n\n11.In particular section 5 and 6 contain a lot of clutter and slang, which should be avoided:\n11.1 page 8: \u2018As can be seen\u2019, \u2018we turn to that case next\u2019\n11.2 page 9: \u2018to the very high value\u2019, \u2018as noted earlier\u2019,  \u2018less context to contribute here\u2019\n11.3 page 10: \u2018In fact\u2019, \u2018far deeper\u2019, \u2018a simple matter of\u2019, \u2018there is much left to investigate.\n\n\n\n\n"
  },
  {
    "people": [
      "Ba",
      "Bell",
      "Bell",
      "Bell",
      "Visin",
      "Bell",
      "Visin",
      "Ba",
      "Sergey Zagoruyko",
      "Nikos Komodakis",
      "Huang"
    ],
    "review": "Thank you very much for all the comments and suggestions.\nSome of the comments were already addressed in version 2 of the paper.\nVersion 3 addresses most of the remaining points. \n\nWe summarize the main changes here:\n \nVersion 3:\n1. Added additional architectures for CIFAR-10 classification, including\n-- Network E, F, where CNN modules and LRNN modules are interleaved at multiple levels. Network F outperforms the ResNet-164.\n2. Reformulated Figure 4 (Architectures for CIFAR experiments) as Table 1,\n3. Extended discussions on the experimental results.\n\nVersion 2: \n1. L-RNN modules with vanilla RNNs trained with Layer Normalization (Ba et al. 2016)\n2. Baseline-CNN added containing only convolutional layers.\n\nThe following are the response to all reviewers on the common points:\n\n---- All the reviewers comment on the very good paper by Bell et al. We address those points here.\n\nReviewer 1:\n* Idea somewhat incremental (e.g. can be seen as derivative from Bell 2016).\n \nReviewer 2:\n* Regarding novelty, the idea of combining a recurrent layer with a CNN, something practically very similar was proposed in Bell et al (2016). There are a few technical differences (e.g. cascading versus applying in parallel the recurrent layers), but in my understanding these are minor changes. The idea of initializing the recurrent network with the CNN is reasonable but is at the level of improving one wrong choice in the original work of Bell, rather than really proposing something novel.\n \nReviewer 3:\n* Contributions relative to, e.g. Bell et al., are minor.\n\nResponse:\nIn our paper, the proposed L-RNN module aims to be general; it can be inserted at any stage in the architectures for capturing contextual information at multiple levels.\n\nTo be clear on the differences:\n\n1. Interleaving CNN with L-RNN modules:\nIn Bell et al., the spatial RNNs are applied on top of CNN features (VGG-net) to learn contextual information at the final stage. A similar idea was also proposed in ReSeg (Visin et al. 2016).\nOur paper goes further than Bell et al. or Visin et al. by interleaving the CNN and LRNN modules at multiple levels of the network.\nIn consequence, the network is capable of learning representations from both local and larger context at every layer, alleviating the limitations of a fixed kernel size.\n \n2. How L-RNNs are inserted and initialized:\nUnder the scenario when the amount of data is limited, e.g. detection in Bell et al, semantic segmentation in our case.\nIn Bell et al., the authors take pre-trained CNN networks (VGG-16 up to conv5), and then train spatial RNNs on top of the pre-computed features.\nIn contrast, we show that a L-RNN can be directly inserted and trained within existing convolutional layers. This means that we increase the representational power of the pre-trained model directly. All layers in the pre-trained networks can therefore be adapted efficiently.\n\nIn our case, we re-purpose several layers in the pre-trained network as L-RNN modules at multiple levels, e.g. after pool3, pool4, and the final fully connected layers. As a result, we get a performance boost of 5% (mean IOU).\n \n3. Choice of RNNs.\nIn Bell et al., to avoid computational cost, the authors choose to use ReLU-based vanilla RNN (rather than GRU or LSTM).\nIn our paper, we validate this choice by showing that vanilla RNN with Layer Normalization (Ba et al. 2016) achieves similar performance to GRU.\n \n4. Separable Convolutions:\nIn Bell et al., 4 bidirectional spatial RNNs are applied to learn the global context. While in our paper, we propose to use 2, each bidirectional spatial RNNs is learning to approximate 1D convolutions, this idea comes from separable convolution.\n\n---- Comments on CIFAR classification experiments.\n \nReviewer 2:\n* Furthermore, as noted in my early question, Wide Residual Networks (Sergey Zagoruyko, Nikos Komodakis, BMVC16) report  better results on CIFAR-10 (4% error), while not using any recurrent layers (rather using instead a wide, VGG-type, ResNet variant). \nSo, the authors answer: \"Wide Residual Networks use the depth of the network to spread the receptive field across the entire image (DenseNet (Huang et al., 2016) similarly uses depth). Thus there is no need for recurrence within layers to capture contextual information. In contrast, we show that a shallow CNN, where the receptive field would be limited, can capture contextual information within the whole image if a L-RNN is used.\"\n \n* So, we agree that WRN do not need recurrence - and can still do better.\nThe point of my question has practically been whether using a recurrent layer is really necessary; I can understand the answer as being \"yes, if you want to keep your network shallow\".  I do not necessarily see why one would want to keep one's network shallow.\n \nReviewer 3:\n* One additional issue with the CIFAR experiments is that I expect to see a direct comparison of models A-F with and without L-RNN. It is hard to understand from the presented results if L-RNN actually adds much. In sum, I have a hard time taking away any valuable information from the CIFAR experiments.\n \nResponse:\nHere is a short summary of the discussion part of the CIFAR experiments (now included in  the updated paper).\n\nThe L-RNN module is a general computational module; it is not our intention to replace the deep networks (e.g. residual variants). \n\n1. Why only use shallow networks?\nIt is well known that deep networks can generally achieve better results than shallow ones. Thus, in our experiments, we use relatively shallow networks to avoid the possibility that the performance gain is due to network depth. Our experiments confirm that increasing the network depth by only adding low-level CNN modules below a L-RNN improves the results (Table 2, Network A to D)\n\n2. Interleaving L-RNN with CNN modules.\nWe show a comparison between a Baseline-CNN and Network-E in Table 2. Baseline-CNN is composed of 7 convolutional layers with 1.56M parameters, it achieves 8.48% top1 error. While the Network-E interleaved with CNN and L-RNN modules contains 0.97M parameters, achieving 5.96% top1 error. The difference between them is the added LRNN modules.\nMoreover, by adding more layers, Network F(5.39% top1 error) achieves comparable performance to ResNet-164 (5.46%). \n\n---- Questions related to ImageNet.\n\nReviewer 2:\n* Regarding the evaluation, experiments on CIFAR are interesting, but only as proof of concept.\n* Probably an evaluation on imageNet would bring some more insight about the merit of this layer.\n \nReviewer 3:\n* Classification experiments are not convincing.\n* I am generally skeptical of the utility of classification experiments on CIFAR-10 when presented in isolation (e.g., no results on ImageNet too). The issue is that CIFAR-10 is not interesting as a task unto itself *and* methods that work well on CIFAR-10 do not necessarily generalize to other tasks. ImageNet has been useful because, thus far, it produces features that generalize well to other tasks. Showing good results on ImageNet is much more likely to demonstrate a model that learns generalizable features. However, that is not even necessarily true, and ideally I would like to see that that a model that does well on ImageNet in fact transfers its benefit to at least one other tasks (e.g., detection).\n \nResponse:\nFurther experiments on ImageNet are definitely on the top list of our future work.\n\n---- Comments on Training Details.\n \nReviewer 1:\n* Section 5.2.1 (and appendix A), how is the learning rate increased and decreased? Manually ? This is an important detail that should be made explicit. Is the learning rate schedule the same in all experiments of each table? If there is a human in the loop, what is the variance in results between \u201ctwo human schedulers\u201d?\n \nReviewer 2:\n* Appendix A: this is very mysterious. Did you try other learning rate schedules? (e.g. polynomial)\n \nResponse:\nThe important message here is that restarting the learning rate several times can help the networks to escape saddle points or local minima.\nWe only experiment with the stepwise decay (no polynomial), and change the learning rate every 40, 60 or 80 epochs, we did not find much difference on this detail.\nWe only provide the intuitive explanation because it is not the main focus of this paper, it is just a training trick. If the readers are interested in this, we found two other related papers in this ICLR submission.\n\n"
  },
  {
    "people": [
      "Bell",
      "Bell",
      "Bell",
      "Sergey Zagoruyko",
      "Nikos Komodakis"
    ],
    "review": "The paper proposes a method of integrating recurrent layers within larger, potentially pre-trained, convolutional networks. The objective is to combine the feature extraction abilities of CNNs with the ability of RNNs to gather global context information.\nThe authors validate their idea on two tasks, image classification (on CIFAR-10) and semantic segmentation (on PASCAL VOC12).\n\nOn the positive side, the paper is clear and well-written (apart from some occasional typos), the proposed idea is simple and could be adopted by other works, and can be deployed as a beneficial perturbation of existing systems, which is practically important if one wants to increase the performance of a system without retraining it from scratch. The evaluation is also systematic, providing a clear ablation study. \n\nOn the negative side, the novelty of the work is relatively limited, while the validation is lacking a bit. \nRegarding novelty, the idea of combining a recurrent layer with a CNN, something practically very similar was proposed in Bell et al (2016). There are a few technical differences (e.g. cascading versus applying in parallel the recurrent layers), but in my understanding these are minor changes. The idea of initializing the recurrent network with the CNN is reasonable but is at the level of improving one wrong choice in the original work of Bell, rather than really proposing something novel. \nThis contribution (\" we use RNNs within layers\") is repeatedly mentioned in the paper (including intro &  conclusion), but in my understanding was part of Bell et al, modulo minor changes. \n\nRegarding the evaluation, experiments on CIFAR are interesting, but only as proof of concept. \n\nFurthermore, as noted in my early question, Wide Residual Networks (Sergey Zagoruyko, Nikos Komodakis, BMVC16)\nreport  better results on CIFAR-10 (4% error), while not using any recurrent layers (rather using instead a wide, VGG-type, ResNet variant). So. \nThe authors answer: \"Wide Residual Networks use the depth of the network to spread the receptive field across the entire image (DenseNet (Huang et al., 2016) similarly uses depth). Thus there is no need for recurrence within layers to capture contextual information. In contrast, we show that a shallow CNN, where the receptive field would be limited, can capture contextual information within the whole image if a L-RNN is used.\"\n\nSo, we agree that WRN do not need recurrence - and can still do better. \nThe point of my question has practically been whether using a recurrent layer is really necessary; I can understand the answer as being \"yes, if you want to keep your network shallow\".  I do not necessarily see why one would want to keep one's network shallow.\n\nProbably an evaluation on imagenet would bring some more insight about the merit of this layer. \n\n\nRegarding semantic segmentation, one of my questions has been:\n\"Is the boost you are obtaining due to something special to the recurrent layer, or is simply because one is adding extra parameters on top of a pre-trained network? (I admit I may have missed some details of your experimental evaluation)\"\nThe answer was:\n\"...For PASCAL segmentation, we add the L-RNN into a pre-trained network (this adds recurrence parameters), and again show that this boosts performance - more so than adding the same number of parameters as extra CNN layers - as it is able to model long-range dependences\"\nI could not find one such experiment in the paper ('more so than adding the same number of parameters as extra CNN layers'); I understand that you have 2048 x 2048 connections for the recurrence, it would be interesting to see what you get by spreading them over (non-recurrent) residual layers.\nClearly, this is not going to be my criterion for rejection/acceptance, since one can easily make it fail - but I was mostly asking for some sanity check \n\nFurthermore, it is a bit misleading to put in Table 3 FCN-8s and FCN8s-LRNN, since this gives the impression that the LRNN gives a  boost by 10%. In practice the \"FCN8s\" prefix of \"FCN8s-LRNN\" is that of the authors, and not of Long et al (as indicated in Table 2, 8s original is quite worse than 8s here). \n\nAnother thing that is not clear to me is where the boost comes from in Table 2; the authors mention that \"when inserting the L-RNN after pool 3 and pool4 in FCN-8s, the L-RNN is able to learn contextual information over a much larger range than the receptive field of pure local convolutions. \"\nThis is potentially true, but I do not see why this was not also the case for FCN-32s (this is more a property of the recurrence rather than the 8/32 factor, right?)\n\nA few additional points: \nIt seems like Fig 2b and Fig2c never made it into the pdf. \n\nFigure 4 is unstructured and throws some 30 boxes to the reader - I would be surprised if anyone is able to get some information out of this (why not have a table?) \n\nAppendix A: this is very mysterious. Did you try other learning rate schedules? (e.g. polynomial)\nWhat is the performance if you apply a standard training schedule? (e.g. step). \nAppendix C: \"maps .. is\" -> \"maps ... are\"\n\n\n"
  },
  {
    "people": [
      "Bell",
      "Bell",
      "Bell"
    ],
    "review": "This paper proposes a cascade of paired (left/right, up/down) 1D RNNs as a module in CNNs in order to quickly add global context information to features without the need for stacking many convolutional layers. Experimental results are presented on image classification and semantic segmentation tasks.\n\nPros:\n- The paper is very clear and easy to read.\n- Enough details are given that the paper can likely be reproduced with or without source code.\n- Using 1D RNNs inside CNNs is a topic that deserves more experimental exploration than what exists in the literature.\n\nCons (elaborated on below):\n(1) Contributions relative to, e.g. Bell et al., are minor.\n(2) Disappointed in the actual use of the proposed L-RNN module versus how it's sold in the intro.\n(3) Classification experiments are not convincing.\n\n(1,2): The introduction states w.r.t. Bell et al. \"more substantial differences are two fold: first, we treat the L-RNN module as a general block, that can be inserted into any layer of a modern architecture, such as into a residual module. Second, we show (section 4) that the\nL-RNN can be formulated to be inserted into a pre-trained FCN (by initializing with zero recurrence\nmatrices), and that the entire network can then be fine-tuned end-to-end.\"\n\nI felt positive about these contributions after reading the intro, but then much less so after reading the experimental sections. Based on the first contribution (\"general block that can be inserted into any layer\"), I strongly expected to see the L-RNN block integrated throughout the CNN starting from near the input. However, the architectures for classification and segmentation only place the module towards the very end of the network. While not exactly the same as Bell et al. (there are many technical details that differ), it is close. The paper does not compare to the design from Bell et al. Is there any advantage to the proposed design? Or is it a variation that performs similarly? What happens if L-RNN is integrated earlier in the network, as suggested by the introduction?\n\nThe second difference is a bit more solid, but still does not rise to a 'substantive difference' in my view. Note that Bell et al. also integrate 1D RNNs into an ImageNet pretrained VGG-16 model. I do, however, think that the method of integration proposed in this paper (zero initialization) may be more elegant and does not require two-stage training by first freezing the lower layers and then later unfreezing them.\n\n(3) I am generally skeptical of the utility of classification experiments on CIFAR-10 when presented in isolation (e.g., no results on ImageNet too). The issue is that CIFAR-10 is not interesting as a task unto itself *and* methods that work well on CIFAR-10 do not necessarily generalize to other tasks. ImageNet has been useful because, thus far, it produces features that generalize well to other tasks. Showing good results on ImageNet is much more likely to demonstrate a model that learns generalizable features. However, that is not even necessarily true, and ideally I would like to see that that a model that does well on ImageNet in fact transfers its benefit to at least one other ask (e.g., detection).\n\nOne additional issue with the CIFAR experiments is that I expect to see a direct comparison of models A-F with and without L-RNN. It is hard to understand from the presented results if L-RNN actually adds much. In sum, I have a hard time taking away any valuable information from the CIFAR experiments.\n\nMinor suggestion:\n- Figure 4 is hard to read. The pixelated rounded corners on the yellow boxes are distracting."
  },
  {
    "people": [
      "He",
      "Bell"
    ],
    "review": "The authors propose the use of a vertical and horizontal one-dimensional RNN (denoted as L-RNN module) to capture long-range dependencies and summarize convolutional feature maps. L-RNN modules are an alternative to deeper or wider networks, 2D RNNs, dilated (Atrous) convolutional layers, and a simple flatten or global pooling layer when applied to the last convolutional layer for classification. L-RNN modules are faster than 2D RNNs, since rows and columns can be processed in parallel, are easy to implemented, and can be inserted in existing convolutional networks. The authors demonstrate improvements for classification and semantic segmentation.\n\nHowever, further evaluations are required that show for which use cases L-RNNs are superior to alternatives for summarizing convolutional feature maps:\n\n1. I suggest to use a fixed CNN with as certain number of layers, and summarize the last feature map by a) a flatten layer, b) global average pooling, c) a 2D RNN, d) and dilated convolutional layers for segmentation. The authors should report both the run-time and number of parameters for these variants in addition to prediction performances. For segmentation, the number of dilated convolutional layers should be chosen such that the number of parameters is similar to a single L-RNN module.\n\n2. The authors compare classification performances only on 32x32 CIFAR-10 images. For higher resolution images, the benefit of L-RNN modules to capture long-range dependencies might be more pronounced. I therefore suggest evaluating classification performances on one additional dataset with higher resolution images, e.g. ImageNet or the CUB bird dataset.\n\nAdditionally, I have the following minor comments:\n\n3. The authors use vanilla RNNs. It might be worth investigating LSTMs or GRUs instead.\n\n4. For classification, the authors summarize hidden states of the final vertical recurrent layer by global max pooling. Is this different from more common global average pooling or concatenating the final forward and backward recurrent states?\n\n5. Table 3 is hard to understand since it mingles datasets (Pascal P and COCO C) and methods (CRF post-processing). I suggest, e.g., using an additional column with CRF \u2018yes\u2019 or \u2018no\u2019. I further suggest listing the number of parameters and runtime if possible.\n\n6. Section 3 does not clearly describe in which order batch-normalization is applied in residual blocks. Figure 2 suggest that the newer BN-ReLU-Conv order described in He et al. (2016) is used. This should be mentioned in the text.\n\nFinally, the text needs to be revised to reach publication level quality. Specifically, I have the following comments:\n\n7. Equation (1) is the update of a vanilla RNN, which should be stated more clearly. I suggest to first describe (bidirectional) RNNs, to reference GRUs and LSTMs, and then describe how they are applied here to images. Figure 1 should also be referenced in the text.\n\n8. In section 2.2, I suggest to describe Bell at al. more clearly. Why are they using eight instead of four RNNs? \n\n9. Section 4 starts with a verbose description about transfer learning, which can be compressed into a single reference or skipped entirely.\n\n10. Equation (6) seems to be missing an index i.\n\n11.In particular section 5 and 6 contain a lot of clutter and slang, which should be avoided:\n11.1 page 8: \u2018As can be seen\u2019, \u2018we turn to that case next\u2019\n11.2 page 9: \u2018to the very high value\u2019, \u2018as noted earlier\u2019,  \u2018less context to contribute here\u2019\n11.3 page 10: \u2018In fact\u2019, \u2018far deeper\u2019, \u2018a simple matter of\u2019, \u2018there is much left to investigate.\n\n\n\n\n"
  },
  {
    "people": [
      "Tenenbaum",
      "Hao Su",
      "Yangyan Li",
      "Charles Qi",
      "Noa Fish",
      "Daniel Cohen-Or",
      "Leonidas Guibas"
    ],
    "review": "This paper proposes a model to learn across different views of objects.  The key insight is to use a triplet loss that encourages two different views of the same object to be closer than an image of a different object.  The approach is evaluated on object instance and category retrieval and compared against baseline CNNs (untrained AlexNet and AlexNet fine-tuned for category classification) using fc7 features with cosine distance.  Furthermore, a comparison against human perception on the \"Tenenbaum objects\u201d is shown.\n\nPositives: Leveraging a triplet loss for this problem may have some novelty (although it may be somewhat limited given some concurrent work; see below).  The paper is reasonably written.\n\nNegatives: The paper is missing relevant references of related work in this space and should compare against an existing approach.\n\nMore details:\n\nThe \u201cimage purification\u201d paper is very related to this work:\n\n[A] Joint Embeddings of Shapes and Images via CNN Image Purification. Hao Su*, Yangyan Li*, Charles Qi, Noa Fish, Daniel Cohen-Or, Leonidas Guibas. SIGGRAPH Asia 2015.\n\nThere they learn to map CNN features to (hand-designed) light field descriptors of 3D shapes for view-invariant object retrieval.  If possible, it would be good to compare directly against this approach (e.g., the cross-view retrieval experiment in Table 1 of [A]).  It appears that code and data is available online ("
  },
  {
    "people": [
      "Tenenbaum",
      "Hao Su",
      "Yangyan Li",
      "Charles Qi",
      "Noa Fish",
      "Daniel Cohen-Or",
      "Leonidas Guibas"
    ],
    "review": "This paper proposes a model to learn across different views of objects.  The key insight is to use a triplet loss that encourages two different views of the same object to be closer than an image of a different object.  The approach is evaluated on object instance and category retrieval and compared against baseline CNNs (untrained AlexNet and AlexNet fine-tuned for category classification) using fc7 features with cosine distance.  Furthermore, a comparison against human perception on the \"Tenenbaum objects\u201d is shown.\n\nPositives: Leveraging a triplet loss for this problem may have some novelty (although it may be somewhat limited given some concurrent work; see below).  The paper is reasonably written.\n\nNegatives: The paper is missing relevant references of related work in this space and should compare against an existing approach.\n\nMore details:\n\nThe \u201cimage purification\u201d paper is very related to this work:\n\n[A] Joint Embeddings of Shapes and Images via CNN Image Purification. Hao Su*, Yangyan Li*, Charles Qi, Noa Fish, Daniel Cohen-Or, Leonidas Guibas. SIGGRAPH Asia 2015.\n\nThere they learn to map CNN features to (hand-designed) light field descriptors of 3D shapes for view-invariant object retrieval.  If possible, it would be good to compare directly against this approach (e.g., the cross-view retrieval experiment in Table 1 of [A]).  It appears that code and data is available online ("
  },
  {
    "people": [
      "Mnih"
    ],
    "review": "This paper is motivated by the ability that human's visual system can recognize contents of environment by from critical features, and tried to investigate whether neural networks can also have this kind of ability.  Specifically, the paper proposed to use Auto-Encoder (AE) as the network to reconstruct the low fidelity of visual input. Moreover, similar to Mnih et al. (2014),  the paper also proposed to use a recurrent fashion to mimic the sequential behavior the  human visual system. \n\nI think the paper is well motivated. However, there are several concerns:\n1. The baselines of the paper are too weak. Nearest neighbor, bilinear, bicubic and cubic interpolations without any learning procedure are of course performed worse than AE based models. The author should compare with the STOA methods such as"
  },
  {
    "people": [
      "Mnih"
    ],
    "review": "This paper is motivated by the ability that human's visual system can recognize contents of environment by from critical features, and tried to investigate whether neural networks can also have this kind of ability.  Specifically, the paper proposed to use Auto-Encoder (AE) as the network to reconstruct the low fidelity of visual input. Moreover, similar to Mnih et al. (2014),  the paper also proposed to use a recurrent fashion to mimic the sequential behavior the  human visual system. \n\nI think the paper is well motivated. However, there are several concerns:\n1. The baselines of the paper are too weak. Nearest neighbor, bilinear, bicubic and cubic interpolations without any learning procedure are of course performed worse than AE based models. The author should compare with the STOA methods such as "
  },
  {
    "people": [
      "Mnih"
    ],
    "review": "This paper is motivated by the ability that human's visual system can recognize contents of environment by from critical features, and tried to investigate whether neural networks can also have this kind of ability.  Specifically, the paper proposed to use Auto-Encoder (AE) as the network to reconstruct the low fidelity of visual input. Moreover, similar to Mnih et al. (2014),  the paper also proposed to use a recurrent fashion to mimic the sequential behavior the  human visual system. \n\nI think the paper is well motivated. However, there are several concerns:\n1. The baselines of the paper are too weak. Nearest neighbor, bilinear, bicubic and cubic interpolations without any learning procedure are of course performed worse than AE based models. The author should compare with the STOA methods such as"
  },
  {
    "people": [
      "Mnih"
    ],
    "review": "This paper is motivated by the ability that human's visual system can recognize contents of environment by from critical features, and tried to investigate whether neural networks can also have this kind of ability.  Specifically, the paper proposed to use Auto-Encoder (AE) as the network to reconstruct the low fidelity of visual input. Moreover, similar to Mnih et al. (2014),  the paper also proposed to use a recurrent fashion to mimic the sequential behavior the  human visual system. \n\nI think the paper is well motivated. However, there are several concerns:\n1. The baselines of the paper are too weak. Nearest neighbor, bilinear, bicubic and cubic interpolations without any learning procedure are of course performed worse than AE based models. The author should compare with the STOA methods such as "
  },
  {
    "people": [
      "Mensink",
      "Weinberger",
      "Min",
      "Chris Burges",
      "Akata",
      "Weinberger",
      "Chris J.C. Burges",
      "Min"
    ],
    "review": "*** Paper Summary ***\n\nThis paper simplify matching network by considering only a single prototype per class which is obtained as the average of the embedding of the training class samples. Empirical comparisons with matching networks are reported.\n\n*** Review ***\n\nThe paper reads well and clearly motivate the work. This work of learning metric learning propose to simplify an earlier work (matching network) which is a great objective. However, I am not sure it achieve better results than matching networks. The space of learning embeddings to optimize nearest neighbor classification has been explored before, but the idea of averaging the propotypes is interesting (as a non-linear extension of Mensink et al 2013). I would suggest to improve the discussion of related work and to consolidate the results section to help distinguish between the methods you outperform and the one you do not. \n\nThe related work section can be extended to include work on learning distance metric to optimize a nearest neighbor classification, see Weinberger et al, 2005 and subsequent work. Extensions to perform the same task with neural networks can be found in Min et al, 09 that purse a goal very close to yours. Regarding approaches pursuing similar goals with a different learning objective, you cite siamese network with pairwise supervision. The learning to rank (for websearch) litterature with triplet supervision or global ranking losses is also highly relevant, ie. one example \"the query\" defines the class and the embedding space need to be such that positive/relevant document are closer to the query than the others. I would suggest to start with Chris Burges 2010 tutorial. One learning class \n\nI am not sure the reported results correctly reflect the state of the art for all tasks. The results are positive on Omniglot but I feel that you should also report the better results of matching networks on miniImageNet with fine tuning and full contextual embeddings. It can be considered misleading not to report it. On Cub 200, I thought that the state-of-the-art was 50.1%, when using features from GoogLeNet (Akata et al 2015), could you comment on this?\n\nOverall, paper could greatly be improved, both in the discussion of related work and with a less partial reporting of prior empirical results.\n\n*** References ***\n\nLarge Margin Nearest Neighbors. Weinberger et al, 2005\nFrom RankNet to LambdaRank to LambdaMART: An Overview, Chris J.C. Burges, June 23, 2010\nA Deep Non-linear Feature Mapping for Large-Margin kNN Classification, Min et al, 09"
  },
  {
    "people": [
      "Mensink",
      "Weinberger",
      "Min",
      "Chris Burges",
      "Akata",
      "Weinberger",
      "Chris J.C. Burges",
      "Min"
    ],
    "review": "*** Paper Summary ***\n\nThis paper simplify matching network by considering only a single prototype per class which is obtained as the average of the embedding of the training class samples. Empirical comparisons with matching networks are reported.\n\n*** Review ***\n\nThe paper reads well and clearly motivate the work. This work of learning metric learning propose to simplify an earlier work (matching network) which is a great objective. However, I am not sure it achieve better results than matching networks. The space of learning embeddings to optimize nearest neighbor classification has been explored before, but the idea of averaging the propotypes is interesting (as a non-linear extension of Mensink et al 2013). I would suggest to improve the discussion of related work and to consolidate the results section to help distinguish between the methods you outperform and the one you do not. \n\nThe related work section can be extended to include work on learning distance metric to optimize a nearest neighbor classification, see Weinberger et al, 2005 and subsequent work. Extensions to perform the same task with neural networks can be found in Min et al, 09 that purse a goal very close to yours. Regarding approaches pursuing similar goals with a different learning objective, you cite siamese network with pairwise supervision. The learning to rank (for websearch) litterature with triplet supervision or global ranking losses is also highly relevant, ie. one example \"the query\" defines the class and the embedding space need to be such that positive/relevant document are closer to the query than the others. I would suggest to start with Chris Burges 2010 tutorial. One learning class \n\nI am not sure the reported results correctly reflect the state of the art for all tasks. The results are positive on Omniglot but I feel that you should also report the better results of matching networks on miniImageNet with fine tuning and full contextual embeddings. It can be considered misleading not to report it. On Cub 200, I thought that the state-of-the-art was 50.1%, when using features from GoogLeNet (Akata et al 2015), could you comment on this?\n\nOverall, paper could greatly be improved, both in the discussion of related work and with a less partial reporting of prior empirical results.\n\n*** References ***\n\nLarge Margin Nearest Neighbors. Weinberger et al, 2005\nFrom RankNet to LambdaRank to LambdaMART: An Overview, Chris J.C. Burges, June 23, 2010\nA Deep Non-linear Feature Mapping for Large-Margin kNN Classification, Min et al, 09"
  },
  {
    "people": [
      "Mensink",
      "Weinberger",
      "Min",
      "Chris Burges",
      "Akata",
      "Weinberger",
      "Chris J.C. Burges",
      "Min"
    ],
    "review": "*** Paper Summary ***\n\nThis paper simplify matching network by considering only a single prototype per class which is obtained as the average of the embedding of the training class samples. Empirical comparisons with matching networks are reported.\n\n*** Review ***\n\nThe paper reads well and clearly motivate the work. This work of learning metric learning propose to simplify an earlier work (matching network) which is a great objective. However, I am not sure it achieve better results than matching networks. The space of learning embeddings to optimize nearest neighbor classification has been explored before, but the idea of averaging the propotypes is interesting (as a non-linear extension of Mensink et al 2013). I would suggest to improve the discussion of related work and to consolidate the results section to help distinguish between the methods you outperform and the one you do not. \n\nThe related work section can be extended to include work on learning distance metric to optimize a nearest neighbor classification, see Weinberger et al, 2005 and subsequent work. Extensions to perform the same task with neural networks can be found in Min et al, 09 that purse a goal very close to yours. Regarding approaches pursuing similar goals with a different learning objective, you cite siamese network with pairwise supervision. The learning to rank (for websearch) litterature with triplet supervision or global ranking losses is also highly relevant, ie. one example \"the query\" defines the class and the embedding space need to be such that positive/relevant document are closer to the query than the others. I would suggest to start with Chris Burges 2010 tutorial. One learning class \n\nI am not sure the reported results correctly reflect the state of the art for all tasks. The results are positive on Omniglot but I feel that you should also report the better results of matching networks on miniImageNet with fine tuning and full contextual embeddings. It can be considered misleading not to report it. On Cub 200, I thought that the state-of-the-art was 50.1%, when using features from GoogLeNet (Akata et al 2015), could you comment on this?\n\nOverall, paper could greatly be improved, both in the discussion of related work and with a less partial reporting of prior empirical results.\n\n*** References ***\n\nLarge Margin Nearest Neighbors. Weinberger et al, 2005\nFrom RankNet to LambdaRank to LambdaMART: An Overview, Chris J.C. Burges, June 23, 2010\nA Deep Non-linear Feature Mapping for Large-Margin kNN Classification, Min et al, 09"
  },
  {
    "people": [
      "Mensink",
      "Weinberger",
      "Min",
      "Chris Burges",
      "Akata",
      "Weinberger",
      "Chris J.C. Burges",
      "Min"
    ],
    "review": "*** Paper Summary ***\n\nThis paper simplify matching network by considering only a single prototype per class which is obtained as the average of the embedding of the training class samples. Empirical comparisons with matching networks are reported.\n\n*** Review ***\n\nThe paper reads well and clearly motivate the work. This work of learning metric learning propose to simplify an earlier work (matching network) which is a great objective. However, I am not sure it achieve better results than matching networks. The space of learning embeddings to optimize nearest neighbor classification has been explored before, but the idea of averaging the propotypes is interesting (as a non-linear extension of Mensink et al 2013). I would suggest to improve the discussion of related work and to consolidate the results section to help distinguish between the methods you outperform and the one you do not. \n\nThe related work section can be extended to include work on learning distance metric to optimize a nearest neighbor classification, see Weinberger et al, 2005 and subsequent work. Extensions to perform the same task with neural networks can be found in Min et al, 09 that purse a goal very close to yours. Regarding approaches pursuing similar goals with a different learning objective, you cite siamese network with pairwise supervision. The learning to rank (for websearch) litterature with triplet supervision or global ranking losses is also highly relevant, ie. one example \"the query\" defines the class and the embedding space need to be such that positive/relevant document are closer to the query than the others. I would suggest to start with Chris Burges 2010 tutorial. One learning class \n\nI am not sure the reported results correctly reflect the state of the art for all tasks. The results are positive on Omniglot but I feel that you should also report the better results of matching networks on miniImageNet with fine tuning and full contextual embeddings. It can be considered misleading not to report it. On Cub 200, I thought that the state-of-the-art was 50.1%, when using features from GoogLeNet (Akata et al 2015), could you comment on this?\n\nOverall, paper could greatly be improved, both in the discussion of related work and with a less partial reporting of prior empirical results.\n\n*** References ***\n\nLarge Margin Nearest Neighbors. Weinberger et al, 2005\nFrom RankNet to LambdaRank to LambdaMART: An Overview, Chris J.C. Burges, June 23, 2010\nA Deep Non-linear Feature Mapping for Large-Margin kNN Classification, Min et al, 09"
  },
  {
    "people": [
      "Geiger"
    ],
    "review": "This paper presents an approach to non-linear kernel dimensionality reduction with a trace norm regularizer in the feature space. The authors proposed an iterative minimization approach in order to obtain a local optimum of a relaxed problem. \nThe paper contains errors and the experimental evaluation is not convincing. Only old techniques are compared against in very toy datasets. \n\nThe authors claim state-of-the-art, however, the oil dataset is not a real benchmark, and the comparisons are to very old approaches. \nThe experimental evaluation should demonstrate robustness to more complex noise and outliers, as this was one of the motivations in the introduction.\n\nThe authors do not address the out-of-sample problem. This is a problem of kernel-based methods vs LVMs, and thus should be address here.\n\n\nThe paper contains errors:\n\n- The last paragraph of section 1 says that this paper proposes a closed form solution to robust KPCA. This is simply wrong, as the proposed approach consists of iteratively solving iterativey a set of closed form updates  and Levenberg-Marquard optimizationd. This is not any more closed form!\n\n- In the same paragraph (and later in the text) the authors claim that the proposed approach can be trivially generalized to incorporate other cost functions. This is not true, as in general there will be no more inner loop closed form updates and the authors will need to solve a much more complex optimization problem. \n\n- The third paragraph of section 2 claims that this paper presents a novel energy minimization framework to solve problems of the general form of eq. (2). However, this is not what the authors solve at the end. They solve a different problem that has been subject to at least two relaxations. It is not clear how solving for a local optima of this double relaxed problem is related to the original problem they want to solve. \n\n- The paper says that Geiger et al defined non linearities on a latent space of pre-defined dimensionality. This is wrong. This paper discovers the dimensionality of the latent space by means of a regularizer that encourages the singular values to be sparse. Thus, it does not have a fixed dimensionality, the latent space is just bounded to be smaller or equal than the dimensionality of the original space. \n\n\nIt is not clear to me why the author say for LVMs such as GPLVM that \"the latent space is learned a priority with clean training data\". One can use different noise models within the GP framework. Furthermore, the proposed approach assumes Gaussian noise (see eq. 6), which is also the trivial case for GP-based LVMs.  \n\n\nIt is not clear what the authors mean in the paper by \"pre-training\" or saying that techniques do not have a training phase. KPCA is trained via a closed-form update, but there is still training."
  },
  {
    "people": [
      "Geiger"
    ],
    "review": "This paper presents an approach to non-linear kernel dimensionality reduction with a trace norm regularizer in the feature space. The authors proposed an iterative minimization approach in order to obtain a local optimum of a relaxed problem. \nThe paper contains errors and the experimental evaluation is not convincing. Only old techniques are compared against in very toy datasets. \n\nThe authors claim state-of-the-art, however, the oil dataset is not a real benchmark, and the comparisons are to very old approaches. \nThe experimental evaluation should demonstrate robustness to more complex noise and outliers, as this was one of the motivations in the introduction.\n\nThe authors do not address the out-of-sample problem. This is a problem of kernel-based methods vs LVMs, and thus should be address here.\n\n\nThe paper contains errors:\n\n- The last paragraph of section 1 says that this paper proposes a closed form solution to robust KPCA. This is simply wrong, as the proposed approach consists of iteratively solving iterativey a set of closed form updates  and Levenberg-Marquard optimizationd. This is not any more closed form!\n\n- In the same paragraph (and later in the text) the authors claim that the proposed approach can be trivially generalized to incorporate other cost functions. This is not true, as in general there will be no more inner loop closed form updates and the authors will need to solve a much more complex optimization problem. \n\n- The third paragraph of section 2 claims that this paper presents a novel energy minimization framework to solve problems of the general form of eq. (2). However, this is not what the authors solve at the end. They solve a different problem that has been subject to at least two relaxations. It is not clear how solving for a local optima of this double relaxed problem is related to the original problem they want to solve. \n\n- The paper says that Geiger et al defined non linearities on a latent space of pre-defined dimensionality. This is wrong. This paper discovers the dimensionality of the latent space by means of a regularizer that encourages the singular values to be sparse. Thus, it does not have a fixed dimensionality, the latent space is just bounded to be smaller or equal than the dimensionality of the original space. \n\n\nIt is not clear to me why the author say for LVMs such as GPLVM that \"the latent space is learned a priority with clean training data\". One can use different noise models within the GP framework. Furthermore, the proposed approach assumes Gaussian noise (see eq. 6), which is also the trivial case for GP-based LVMs.  \n\n\nIt is not clear what the authors mean in the paper by \"pre-training\" or saying that techniques do not have a training phase. KPCA is trained via a closed-form update, but there is still training.  \n"
  },
  {
    "people": [
      "Geiger"
    ],
    "review": "This paper presents an approach to non-linear kernel dimensionality reduction with a trace norm regularizer in the feature space. The authors proposed an iterative minimization approach in order to obtain a local optimum of a relaxed problem. \nThe paper contains errors and the experimental evaluation is not convincing. Only old techniques are compared against in very toy datasets. \n\nThe authors claim state-of-the-art, however, the oil dataset is not a real benchmark, and the comparisons are to very old approaches. \nThe experimental evaluation should demonstrate robustness to more complex noise and outliers, as this was one of the motivations in the introduction.\n\nThe authors do not address the out-of-sample problem. This is a problem of kernel-based methods vs LVMs, and thus should be address here.\n\n\nThe paper contains errors:\n\n- The last paragraph of section 1 says that this paper proposes a closed form solution to robust KPCA. This is simply wrong, as the proposed approach consists of iteratively solving iterativey a set of closed form updates  and Levenberg-Marquard optimizationd. This is not any more closed form!\n\n- In the same paragraph (and later in the text) the authors claim that the proposed approach can be trivially generalized to incorporate other cost functions. This is not true, as in general there will be no more inner loop closed form updates and the authors will need to solve a much more complex optimization problem. \n\n- The third paragraph of section 2 claims that this paper presents a novel energy minimization framework to solve problems of the general form of eq. (2). However, this is not what the authors solve at the end. They solve a different problem that has been subject to at least two relaxations. It is not clear how solving for a local optima of this double relaxed problem is related to the original problem they want to solve. \n\n- The paper says that Geiger et al defined non linearities on a latent space of pre-defined dimensionality. This is wrong. This paper discovers the dimensionality of the latent space by means of a regularizer that encourages the singular values to be sparse. Thus, it does not have a fixed dimensionality, the latent space is just bounded to be smaller or equal than the dimensionality of the original space. \n\n\nIt is not clear to me why the author say for LVMs such as GPLVM that \"the latent space is learned a priority with clean training data\". One can use different noise models within the GP framework. Furthermore, the proposed approach assumes Gaussian noise (see eq. 6), which is also the trivial case for GP-based LVMs.  \n\n\nIt is not clear what the authors mean in the paper by \"pre-training\" or saying that techniques do not have a training phase. KPCA is trained via a closed-form update, but there is still training."
  },
  {
    "people": [
      "Geiger"
    ],
    "review": "This paper presents an approach to non-linear kernel dimensionality reduction with a trace norm regularizer in the feature space. The authors proposed an iterative minimization approach in order to obtain a local optimum of a relaxed problem. \nThe paper contains errors and the experimental evaluation is not convincing. Only old techniques are compared against in very toy datasets. \n\nThe authors claim state-of-the-art, however, the oil dataset is not a real benchmark, and the comparisons are to very old approaches. \nThe experimental evaluation should demonstrate robustness to more complex noise and outliers, as this was one of the motivations in the introduction.\n\nThe authors do not address the out-of-sample problem. This is a problem of kernel-based methods vs LVMs, and thus should be address here.\n\n\nThe paper contains errors:\n\n- The last paragraph of section 1 says that this paper proposes a closed form solution to robust KPCA. This is simply wrong, as the proposed approach consists of iteratively solving iterativey a set of closed form updates  and Levenberg-Marquard optimizationd. This is not any more closed form!\n\n- In the same paragraph (and later in the text) the authors claim that the proposed approach can be trivially generalized to incorporate other cost functions. This is not true, as in general there will be no more inner loop closed form updates and the authors will need to solve a much more complex optimization problem. \n\n- The third paragraph of section 2 claims that this paper presents a novel energy minimization framework to solve problems of the general form of eq. (2). However, this is not what the authors solve at the end. They solve a different problem that has been subject to at least two relaxations. It is not clear how solving for a local optima of this double relaxed problem is related to the original problem they want to solve. \n\n- The paper says that Geiger et al defined non linearities on a latent space of pre-defined dimensionality. This is wrong. This paper discovers the dimensionality of the latent space by means of a regularizer that encourages the singular values to be sparse. Thus, it does not have a fixed dimensionality, the latent space is just bounded to be smaller or equal than the dimensionality of the original space. \n\n\nIt is not clear to me why the author say for LVMs such as GPLVM that \"the latent space is learned a priority with clean training data\". One can use different noise models within the GP framework. Furthermore, the proposed approach assumes Gaussian noise (see eq. 6), which is also the trivial case for GP-based LVMs.  \n\n\nIt is not clear what the authors mean in the paper by \"pre-training\" or saying that techniques do not have a training phase. KPCA is trained via a closed-form update, but there is still training.  \n"
  },
  {
    "people": [
      "Alex Graves",
      "Jan Chorowski's"
    ],
    "review": "I didn't read the paper in detail, but for the TIMIT results the authors reported 23.x PER, this is actually quite poor and quite off from SOTA. SOTA is around 16.5 PER and even end-to-end methods (w/o HMMs) can achieve ~17.6% (see Alex Graves CTC/RNN transducer paper and Jan Chorowski's Attention paper)."
  },
  {
    "people": [
      "Alex Graves",
      "Jan Chorowski's"
    ],
    "review": "I didn't read the paper in detail, but for the TIMIT results the authors reported 23.x PER, this is actually quite poor and quite off from SOTA. SOTA is around 16.5 PER and even end-to-end methods (w/o HMMs) can achieve ~17.6% (see Alex Graves CTC/RNN transducer paper and Jan Chorowski's Attention paper)."
  },
  {
    "people": [
      "Mehdi"
    ],
    "review": "Note: the latest version includes significantly improved ImageNet classification results in Table 2 (up ~2% across the board compared to the previous version).  This is due to the fact that in previous versions, we evaluated using the predictions for a single (center) crop at test time, rather than averaging over 10 crops as we learned by correspondence was how the previous results from Noroozi & Favaro (2016) were obtained.  (Thanks to Mehdi for bringing this up!) There are also very slightly improved VOC classification results (Table 3) due to an unrelated minor bug."
  },
  {
    "people": [
      "Doersch",
      "Agrawal",
      "Gupta"
    ],
    "review": "We thank all reviewers for their thoughtful comments and suggestions. We\u2019ve uploaded a revision with an expanded introduction which clarifies the motivation for BiGAN and the theory in Section 3 in the context of feature learning.\n\nNote that our theoretical arguments go beyond simply showing that BiGAN maintains the properties of the original GAN framework.  In particular, we show that the BiGAN objective is equivalent to that of an \u201cL0 autoencoder\u201d, encouraging the encoder and generator to invert one another (Theorem 3). If fully optimized, the encoder and generator are inverses of one another almost everywhere (Theorem 2).  We believe that these insights are quite important to understanding BiGAN\u2019s behavior and its use as a model for feature learning.  We hope that the revised introduction explains this better.\n\nWith respect to the ImageNet visual feature learning evaluation, our results show BiGAN is state-of-the-art among purely unsupervised learning methods.  The methods that outperform BiGAN (e.g., Doersch et al.) are all based on \u201cself-supervision,\u201d requiring the design of a domain-specific supervised prediction task, and some (Agrawal et al. and Wang & Gupta) are \u201cweakly supervised,\u201d requiring auxiliary information (video, egomotion, or tracking) unavailable in images alone.  We\u2019ve reorganized Table 3 to emphasize this point, and added discussion to Section 4.4 on the benefits of purely unsupervised approaches vs. self-supervised approaches.  (Following your suggestion, we\u2019ve also bolded the best results of each group in Tables 2 and 3.)  We do agree, however, that our results are somewhat preliminary and likely to improve significantly with further model architecture search and improvements to the optimization.\n\nMost optimization details (optimization algorithm, learning rate / step size, etc.) were included in Appendix C.  In the updated version, network initialization details are now included in Appendix C as well.  We\u2019ll also be releasing the training code."
  },
  {
    "people": [
      "Mehdi"
    ],
    "review": "Note: the latest version includes significantly improved ImageNet classification results in Table 2 (up ~2% across the board compared to the previous version).  This is due to the fact that in previous versions, we evaluated using the predictions for a single (center) crop at test time, rather than averaging over 10 crops as we learned by correspondence was how the previous results from Noroozi & Favaro (2016) were obtained.  (Thanks to Mehdi for bringing this up!) There are also very slightly improved VOC classification results (Table 3) due to an unrelated minor bug."
  },
  {
    "people": [
      "Doersch",
      "Agrawal",
      "Gupta"
    ],
    "review": "We thank all reviewers for their thoughtful comments and suggestions. We\u2019ve uploaded a revision with an expanded introduction which clarifies the motivation for BiGAN and the theory in Section 3 in the context of feature learning.\n\nNote that our theoretical arguments go beyond simply showing that BiGAN maintains the properties of the original GAN framework.  In particular, we show that the BiGAN objective is equivalent to that of an \u201cL0 autoencoder\u201d, encouraging the encoder and generator to invert one another (Theorem 3). If fully optimized, the encoder and generator are inverses of one another almost everywhere (Theorem 2).  We believe that these insights are quite important to understanding BiGAN\u2019s behavior and its use as a model for feature learning.  We hope that the revised introduction explains this better.\n\nWith respect to the ImageNet visual feature learning evaluation, our results show BiGAN is state-of-the-art among purely unsupervised learning methods.  The methods that outperform BiGAN (e.g., Doersch et al.) are all based on \u201cself-supervision,\u201d requiring the design of a domain-specific supervised prediction task, and some (Agrawal et al. and Wang & Gupta) are \u201cweakly supervised,\u201d requiring auxiliary information (video, egomotion, or tracking) unavailable in images alone.  We\u2019ve reorganized Table 3 to emphasize this point, and added discussion to Section 4.4 on the benefits of purely unsupervised approaches vs. self-supervised approaches.  (Following your suggestion, we\u2019ve also bolded the best results of each group in Tables 2 and 3.)  We do agree, however, that our results are somewhat preliminary and likely to improve significantly with further model architecture search and improvements to the optimization.\n\nMost optimization details (optimization algorithm, learning rate / step size, etc.) were included in Appendix C.  In the updated version, network initialization details are now included in Appendix C as well.  We\u2019ll also be releasing the training code."
  },
  {
    "people": [
      "S Wang",
      "C Manning",
      "Bengio",
      "Rifai",
      "Minmin Chen",
      "K Weinberger",
      "Pascal Vincent",
      "Salah Rifai",
      "Xavier Muller",
      "Xavier Glorot",
      "Gregoire Mesnil",
      "Yoshua Bengio",
      "Pascal Vincent",
      "Yoshua Bengio",
      "Ronan Collobert",
      "O Chapelle",
      "V Sindhwani"
    ],
    "review": "*** Paper Summary ***\n\nThis paper applies adversarial and virtual adversarial training to LSTM for text classification. Since text inputs are discrete adversarial perturbation are applied to the (normalized) word embeddings. Extensive experiments are reported and demonstrate the advantage of these methods.\n\n*** Review Summary ***\n\nThe paper reads well and has sufficent references. The application of adversarial training to text data is a simple but not trivial extension. The experimental section presents extensive experiments with comparison to alternative strategies. The proposed method is simple and effective and can be easily be applied after reading the paper.\n\n*** Detailed Review ***\n\nThe paper reads well. I have only a few comments regarding experiments and link to prior resarch:\n\nExperiments:\n\n- In Table 2 (and for other datasets as well), could you include an SVM baseline? e.g. S Wang and C Manning 2012?\n- As another baseline, did you consider dropping words, i.e. masking noise? It is generally better than dropout/gaussian noise for text application (e.g. denoising autoencoders)?\n- I am not sure I understand why virtual adversarial is worse than the baseline in Table 5. If you tune epsilon, in the worse case you would get the same performance as the baseline? Was it that validation was unreliable?\n\nRelated Work:\n\nI think it would be interesting to point at SVM, transductive SVM who achieve something similar to adversarial training. When maximizing the margin in a (transductive) SVM, it is equivalent to move the example toward the decision boundary, i.e. moving them in the direction of increase of the loss gradient.\n\nAlso it would be interesting to draw a parallel between adversarial training and contrastive divergence. The adversarial samples are very close in nature to the one step Markov Chain samples from CD. See Bengio 2009. Related to this technique are also approaches that try to explicitely cancel the Jacobian at data points, e.g. Rifai et al 2011.\n\n*** References ***\n\nMarginalized Denoising Autoencoders for Domain Adaptation. Minmin Chen, K Weinberger.\nStacked Denoising Autoencoders. Pascal Vincent. JMLR 2011.\nLearning invariant features through local space contraction, Salah Rifai, Xavier Muller, Xavier Glorot, Gregoire Mesnil, Yoshua Bengio and Pascal Vincent, 2011.\nLearning Deep Architectures for AI, Yoshua Bengio 2009\nLarge Scale Transductive SVMs. Ronan Collobert et al 2006\nOptimization for Transductive SVM.  O Chapelle, V Sindhwani, SS Keerthi JMLR 2008"
  },
  {
    "people": [
      "S Wang",
      "C Manning",
      "Bengio",
      "Rifai",
      "Minmin Chen",
      "K Weinberger",
      "Pascal Vincent",
      "Salah Rifai",
      "Xavier Muller",
      "Xavier Glorot",
      "Gregoire Mesnil",
      "Yoshua Bengio",
      "Pascal Vincent",
      "Yoshua Bengio",
      "Ronan Collobert",
      "O Chapelle",
      "V Sindhwani"
    ],
    "review": "*** Paper Summary ***\n\nThis paper applies adversarial and virtual adversarial training to LSTM for text classification. Since text inputs are discrete adversarial perturbation are applied to the (normalized) word embeddings. Extensive experiments are reported and demonstrate the advantage of these methods.\n\n*** Review Summary ***\n\nThe paper reads well and has sufficent references. The application of adversarial training to text data is a simple but not trivial extension. The experimental section presents extensive experiments with comparison to alternative strategies. The proposed method is simple and effective and can be easily be applied after reading the paper.\n\n*** Detailed Review ***\n\nThe paper reads well. I have only a few comments regarding experiments and link to prior resarch:\n\nExperiments:\n\n- In Table 2 (and for other datasets as well), could you include an SVM baseline? e.g. S Wang and C Manning 2012?\n- As another baseline, did you consider dropping words, i.e. masking noise? It is generally better than dropout/gaussian noise for text application (e.g. denoising autoencoders)?\n- I am not sure I understand why virtual adversarial is worse than the baseline in Table 5. If you tune epsilon, in the worse case you would get the same performance as the baseline? Was it that validation was unreliable?\n\nRelated Work:\n\nI think it would be interesting to point at SVM, transductive SVM who achieve something similar to adversarial training. When maximizing the margin in a (transductive) SVM, it is equivalent to move the example toward the decision boundary, i.e. moving them in the direction of increase of the loss gradient.\n\nAlso it would be interesting to draw a parallel between adversarial training and contrastive divergence. The adversarial samples are very close in nature to the one step Markov Chain samples from CD. See Bengio 2009. Related to this technique are also approaches that try to explicitely cancel the Jacobian at data points, e.g. Rifai et al 2011.\n\n*** References ***\n\nMarginalized Denoising Autoencoders for Domain Adaptation. Minmin Chen, K Weinberger.\nStacked Denoising Autoencoders. Pascal Vincent. JMLR 2011.\nLearning invariant features through local space contraction, Salah Rifai, Xavier Muller, Xavier Glorot, Gregoire Mesnil, Yoshua Bengio and Pascal Vincent, 2011.\nLearning Deep Architectures for AI, Yoshua Bengio 2009\nLarge Scale Transductive SVMs. Ronan Collobert et al 2006\nOptimization for Transductive SVM.  O Chapelle, V Sindhwani, SS Keerthi JMLR 2008"
  },
  {
    "people": [
      "S Wang",
      "C Manning",
      "Bengio",
      "Rifai",
      "Minmin Chen",
      "K Weinberger",
      "Pascal Vincent",
      "Salah Rifai",
      "Xavier Muller",
      "Xavier Glorot",
      "Gregoire Mesnil",
      "Yoshua Bengio",
      "Pascal Vincent",
      "Yoshua Bengio",
      "Ronan Collobert",
      "O Chapelle",
      "V Sindhwani"
    ],
    "review": "*** Paper Summary ***\n\nThis paper applies adversarial and virtual adversarial training to LSTM for text classification. Since text inputs are discrete adversarial perturbation are applied to the (normalized) word embeddings. Extensive experiments are reported and demonstrate the advantage of these methods.\n\n*** Review Summary ***\n\nThe paper reads well and has sufficent references. The application of adversarial training to text data is a simple but not trivial extension. The experimental section presents extensive experiments with comparison to alternative strategies. The proposed method is simple and effective and can be easily be applied after reading the paper.\n\n*** Detailed Review ***\n\nThe paper reads well. I have only a few comments regarding experiments and link to prior resarch:\n\nExperiments:\n\n- In Table 2 (and for other datasets as well), could you include an SVM baseline? e.g. S Wang and C Manning 2012?\n- As another baseline, did you consider dropping words, i.e. masking noise? It is generally better than dropout/gaussian noise for text application (e.g. denoising autoencoders)?\n- I am not sure I understand why virtual adversarial is worse than the baseline in Table 5. If you tune epsilon, in the worse case you would get the same performance as the baseline? Was it that validation was unreliable?\n\nRelated Work:\n\nI think it would be interesting to point at SVM, transductive SVM who achieve something similar to adversarial training. When maximizing the margin in a (transductive) SVM, it is equivalent to move the example toward the decision boundary, i.e. moving them in the direction of increase of the loss gradient.\n\nAlso it would be interesting to draw a parallel between adversarial training and contrastive divergence. The adversarial samples are very close in nature to the one step Markov Chain samples from CD. See Bengio 2009. Related to this technique are also approaches that try to explicitely cancel the Jacobian at data points, e.g. Rifai et al 2011.\n\n*** References ***\n\nMarginalized Denoising Autoencoders for Domain Adaptation. Minmin Chen, K Weinberger.\nStacked Denoising Autoencoders. Pascal Vincent. JMLR 2011.\nLearning invariant features through local space contraction, Salah Rifai, Xavier Muller, Xavier Glorot, Gregoire Mesnil, Yoshua Bengio and Pascal Vincent, 2011.\nLearning Deep Architectures for AI, Yoshua Bengio 2009\nLarge Scale Transductive SVMs. Ronan Collobert et al 2006\nOptimization for Transductive SVM.  O Chapelle, V Sindhwani, SS Keerthi JMLR 2008"
  },
  {
    "people": [
      "S Wang",
      "C Manning",
      "Bengio",
      "Rifai",
      "Minmin Chen",
      "K Weinberger",
      "Pascal Vincent",
      "Salah Rifai",
      "Xavier Muller",
      "Xavier Glorot",
      "Gregoire Mesnil",
      "Yoshua Bengio",
      "Pascal Vincent",
      "Yoshua Bengio",
      "Ronan Collobert",
      "O Chapelle",
      "V Sindhwani"
    ],
    "review": "*** Paper Summary ***\n\nThis paper applies adversarial and virtual adversarial training to LSTM for text classification. Since text inputs are discrete adversarial perturbation are applied to the (normalized) word embeddings. Extensive experiments are reported and demonstrate the advantage of these methods.\n\n*** Review Summary ***\n\nThe paper reads well and has sufficent references. The application of adversarial training to text data is a simple but not trivial extension. The experimental section presents extensive experiments with comparison to alternative strategies. The proposed method is simple and effective and can be easily be applied after reading the paper.\n\n*** Detailed Review ***\n\nThe paper reads well. I have only a few comments regarding experiments and link to prior resarch:\n\nExperiments:\n\n- In Table 2 (and for other datasets as well), could you include an SVM baseline? e.g. S Wang and C Manning 2012?\n- As another baseline, did you consider dropping words, i.e. masking noise? It is generally better than dropout/gaussian noise for text application (e.g. denoising autoencoders)?\n- I am not sure I understand why virtual adversarial is worse than the baseline in Table 5. If you tune epsilon, in the worse case you would get the same performance as the baseline? Was it that validation was unreliable?\n\nRelated Work:\n\nI think it would be interesting to point at SVM, transductive SVM who achieve something similar to adversarial training. When maximizing the margin in a (transductive) SVM, it is equivalent to move the example toward the decision boundary, i.e. moving them in the direction of increase of the loss gradient.\n\nAlso it would be interesting to draw a parallel between adversarial training and contrastive divergence. The adversarial samples are very close in nature to the one step Markov Chain samples from CD. See Bengio 2009. Related to this technique are also approaches that try to explicitely cancel the Jacobian at data points, e.g. Rifai et al 2011.\n\n*** References ***\n\nMarginalized Denoising Autoencoders for Domain Adaptation. Minmin Chen, K Weinberger.\nStacked Denoising Autoencoders. Pascal Vincent. JMLR 2011.\nLearning invariant features through local space contraction, Salah Rifai, Xavier Muller, Xavier Glorot, Gregoire Mesnil, Yoshua Bengio and Pascal Vincent, 2011.\nLearning Deep Architectures for AI, Yoshua Bengio 2009\nLarge Scale Transductive SVMs. Ronan Collobert et al 2006\nOptimization for Transductive SVM.  O Chapelle, V Sindhwani, SS Keerthi JMLR 2008"
  },
  {
    "people": [
      "Kawaguchi"
    ],
    "review": "In this paper, the author analyzes the convergence dynamics of a single layer non-linear network under Gaussian iid input assumptions. The first half of the paper, dealing with a single hidden node, was somewhat clear, although I have some specific questions below. The second half, dealing with multiple hidden nodes, was very difficult for me to understand, and the final \"punchline\" is quite unclear. I think the author should focus on intuition and hide detailed derivations and symbols in an appendix. \n\nIn terms of significance, it is very hard for me to be sure how generalizable these results are: the Gaussian assumption is a very strong one, and so is the assumption of iid inputs. Real-world feature inputs are highly correlated and are probably not Gaussian. Such assumptions are not made (as far as I can tell) in recent papers analyzing the convergence of deep networks e.g. Kawaguchi, NIPS 2016. Although the author says the no assumption is made on the independence of activations, this assumption is shifted to the input instead. I think this means that the activations are combinations of iid random variables, and are probably Gaussian like, right? So I'm not sure where this leaves us.\n\nSpecific comments:\n\n1. Please use D_w instead of D to show that D is a function of w, and not a constant. This gets particularly confusing when switching to D(w) and D(e) in Section 3. In general, notation in the paper is hard to follow and should be clearly introduced.\n\n2. Section 3, statement that says \"when the neuron is cut off at sample l, then (D^(t))_u\" what is the relationship between l and u? Also, this is another example of notational inconsistency that causes problems to the reader.\n\n3. Section 3.1, what is F(e, w) and why is D(e) introduced? This was unclear to me.\n\n4. Theorem 3.3 suggests that (if \\epsilon is > 0), then to have the maximal probability of convergence, \\epsilon should be very close to 0, which means that the ball B_r has radius r -> 0? This seems contradictory from Figure 2. \n\n5. Section 4 was really unclear and I still do not understand what the symmetry group really represents. Is there an intuitive explanation why this is important?\n\n6. Figure 5: what is a_j ?\n\nI encourage the author to rewrite this paper for clarity. In it's present form, it would be very difficult to understand the takeaways from the paper."
  },
  {
    "people": [
      "Kawaguchi"
    ],
    "review": "In this paper, the author analyzes the convergence dynamics of a single layer non-linear network under Gaussian iid input assumptions. The first half of the paper, dealing with a single hidden node, was somewhat clear, although I have some specific questions below. The second half, dealing with multiple hidden nodes, was very difficult for me to understand, and the final \"punchline\" is quite unclear. I think the author should focus on intuition and hide detailed derivations and symbols in an appendix. \n\nIn terms of significance, it is very hard for me to be sure how generalizable these results are: the Gaussian assumption is a very strong one, and so is the assumption of iid inputs. Real-world feature inputs are highly correlated and are probably not Gaussian. Such assumptions are not made (as far as I can tell) in recent papers analyzing the convergence of deep networks e.g. Kawaguchi, NIPS 2016. Although the author says the no assumption is made on the independence of activations, this assumption is shifted to the input instead. I think this means that the activations are combinations of iid random variables, and are probably Gaussian like, right? So I'm not sure where this leaves us.\n\nSpecific comments:\n\n1. Please use D_w instead of D to show that D is a function of w, and not a constant. This gets particularly confusing when switching to D(w) and D(e) in Section 3. In general, notation in the paper is hard to follow and should be clearly introduced.\n\n2. Section 3, statement that says \"when the neuron is cut off at sample l, then (D^(t))_u\" what is the relationship between l and u? Also, this is another example of notational inconsistency that causes problems to the reader.\n\n3. Section 3.1, what is F(e, w) and why is D(e) introduced? This was unclear to me.\n\n4. Theorem 3.3 suggests that (if \\epsilon is > 0), then to have the maximal probability of convergence, \\epsilon should be very close to 0, which means that the ball B_r has radius r -> 0? This seems contradictory from Figure 2. \n\n5. Section 4 was really unclear and I still do not understand what the symmetry group really represents. Is there an intuitive explanation why this is important?\n\n6. Figure 5: what is a_j ?\n\nI encourage the author to rewrite this paper for clarity. In it's present form, it would be very difficult to understand the takeaways from the paper."
  },
  {
    "people": [
      "Kawaguchi"
    ],
    "review": "In this paper, the author analyzes the convergence dynamics of a single layer non-linear network under Gaussian iid input assumptions. The first half of the paper, dealing with a single hidden node, was somewhat clear, although I have some specific questions below. The second half, dealing with multiple hidden nodes, was very difficult for me to understand, and the final \"punchline\" is quite unclear. I think the author should focus on intuition and hide detailed derivations and symbols in an appendix. \n\nIn terms of significance, it is very hard for me to be sure how generalizable these results are: the Gaussian assumption is a very strong one, and so is the assumption of iid inputs. Real-world feature inputs are highly correlated and are probably not Gaussian. Such assumptions are not made (as far as I can tell) in recent papers analyzing the convergence of deep networks e.g. Kawaguchi, NIPS 2016. Although the author says the no assumption is made on the independence of activations, this assumption is shifted to the input instead. I think this means that the activations are combinations of iid random variables, and are probably Gaussian like, right? So I'm not sure where this leaves us.\n\nSpecific comments:\n\n1. Please use D_w instead of D to show that D is a function of w, and not a constant. This gets particularly confusing when switching to D(w) and D(e) in Section 3. In general, notation in the paper is hard to follow and should be clearly introduced.\n\n2. Section 3, statement that says \"when the neuron is cut off at sample l, then (D^(t))_u\" what is the relationship between l and u? Also, this is another example of notational inconsistency that causes problems to the reader.\n\n3. Section 3.1, what is F(e, w) and why is D(e) introduced? This was unclear to me.\n\n4. Theorem 3.3 suggests that (if \\epsilon is > 0), then to have the maximal probability of convergence, \\epsilon should be very close to 0, which means that the ball B_r has radius r -> 0? This seems contradictory from Figure 2. \n\n5. Section 4 was really unclear and I still do not understand what the symmetry group really represents. Is there an intuitive explanation why this is important?\n\n6. Figure 5: what is a_j ?\n\nI encourage the author to rewrite this paper for clarity. In it's present form, it would be very difficult to understand the takeaways from the paper."
  },
  {
    "people": [
      "Kawaguchi"
    ],
    "review": "In this paper, the author analyzes the convergence dynamics of a single layer non-linear network under Gaussian iid input assumptions. The first half of the paper, dealing with a single hidden node, was somewhat clear, although I have some specific questions below. The second half, dealing with multiple hidden nodes, was very difficult for me to understand, and the final \"punchline\" is quite unclear. I think the author should focus on intuition and hide detailed derivations and symbols in an appendix. \n\nIn terms of significance, it is very hard for me to be sure how generalizable these results are: the Gaussian assumption is a very strong one, and so is the assumption of iid inputs. Real-world feature inputs are highly correlated and are probably not Gaussian. Such assumptions are not made (as far as I can tell) in recent papers analyzing the convergence of deep networks e.g. Kawaguchi, NIPS 2016. Although the author says the no assumption is made on the independence of activations, this assumption is shifted to the input instead. I think this means that the activations are combinations of iid random variables, and are probably Gaussian like, right? So I'm not sure where this leaves us.\n\nSpecific comments:\n\n1. Please use D_w instead of D to show that D is a function of w, and not a constant. This gets particularly confusing when switching to D(w) and D(e) in Section 3. In general, notation in the paper is hard to follow and should be clearly introduced.\n\n2. Section 3, statement that says \"when the neuron is cut off at sample l, then (D^(t))_u\" what is the relationship between l and u? Also, this is another example of notational inconsistency that causes problems to the reader.\n\n3. Section 3.1, what is F(e, w) and why is D(e) introduced? This was unclear to me.\n\n4. Theorem 3.3 suggests that (if \\epsilon is > 0), then to have the maximal probability of convergence, \\epsilon should be very close to 0, which means that the ball B_r has radius r -> 0? This seems contradictory from Figure 2. \n\n5. Section 4 was really unclear and I still do not understand what the symmetry group really represents. Is there an intuitive explanation why this is important?\n\n6. Figure 5: what is a_j ?\n\nI encourage the author to rewrite this paper for clarity. In it's present form, it would be very difficult to understand the takeaways from the paper."
  },
  {
    "people": [
      "I. Badr",
      "J. Glass",
      "L. Lu",
      "A. Ghoshal",
      "R. Singh",
      "B. Raj",
      "R. Stern",
      "Bahdanau"
    ],
    "review": "This submission proposes to learn the word decomposition, or word to sub-word sequence mapping jointly with the attention based sequence-to-sequence model. A particular feature of this approach is that the decomposition is not static, instead, it also conditions on the acoustic input, and the mapping is probabilistic, i.e., one word may map to multiple sub-word sequences. The authors argue that the dynamic decomposition approach can more naturally reflect the acoustic pattern. Interestingly, the motivation behind this approach is analogous to learning the pronunciation mixture model for HMM based speech recognition, where the probabilistic mapping from a word to its pronunciations also conditions on the acoustic input, e.g.,\n\nI. McGraw, I. Badr, and J. Glass, \"Learning lexicons form speech using a pronunciation mixture model,\" in IEEE Transactions on Audio, Speech, and Language Processing, 2013\n\nL. Lu, A. Ghoshal, S. Renals, \"Acoustic data-driven pronunciation lexicon for large vocabulary speech recognition\", in Proc. ASRU \n\nR. Singh, B. Raj, and R. Stern, \"Automatic generation of subword units for speech recognition systems,\"  in IEEE Transactions on Speech and Audio Processing, 2002\n\nIt would be interesting to put this work in the context by linking it to some previous works in the HMM framework.\n\nOverall, the paper is well written, and it is theoretically convincing. The experimental study could be more solid, e.g., it is reasonable to have a word-level baseline, as the proposed approach lies in between the character-level and word-level systems. the vocabulary size of the WSJ si284 dataset is 20K at maximum, which is not very large for the softmax layer, and it is a closed vocabulary task. I guess the word-level system may be also competitive to the numbers reported in this paper. Furthermore, can you explain what is the computational bottleneck of the proposed approach? You downsampled the data by the factor of 4 using an RNN, and it still took around 5 days to converge. To me, it is a bit expensive, especially given that you only take one sample when computing the gradient. Table 2 is a little bit misleading, as CTC with language model and seq2seq with a language model model from Bahdanau et al. is much closer to the best number reported in this Table 2, while you may only get a very small improvement using a language model. Finally, \"O(5) days to converge\" sounds a bit odd to me."
  },
  {
    "people": [
      "I. Badr",
      "J. Glass",
      "L. Lu",
      "A. Ghoshal",
      "R. Singh",
      "B. Raj",
      "R. Stern",
      "Bahdanau"
    ],
    "review": "This submission proposes to learn the word decomposition, or word to sub-word sequence mapping jointly with the attention based sequence-to-sequence model. A particular feature of this approach is that the decomposition is not static, instead, it also conditions on the acoustic input, and the mapping is probabilistic, i.e., one word may map to multiple sub-word sequences. The authors argue that the dynamic decomposition approach can more naturally reflect the acoustic pattern. Interestingly, the motivation behind this approach is analogous to learning the pronunciation mixture model for HMM based speech recognition, where the probabilistic mapping from a word to its pronunciations also conditions on the acoustic input, e.g.,\n\nI. McGraw, I. Badr, and J. Glass, \"Learning lexicons form speech using a pronunciation mixture model,\" in IEEE Transactions on Audio, Speech, and Language Processing, 2013\n\nL. Lu, A. Ghoshal, S. Renals, \"Acoustic data-driven pronunciation lexicon for large vocabulary speech recognition\", in Proc. ASRU \n\nR. Singh, B. Raj, and R. Stern, \"Automatic generation of subword units for speech recognition systems,\"  in IEEE Transactions on Speech and Audio Processing, 2002\n\nIt would be interesting to put this work in the context by linking it to some previous works in the HMM framework.\n\nOverall, the paper is well written, and it is theoretically convincing. The experimental study could be more solid, e.g., it is reasonable to have a word-level baseline, as the proposed approach lies in between the character-level and word-level systems. the vocabulary size of the WSJ si284 dataset is 20K at maximum, which is not very large for the softmax layer, and it is a closed vocabulary task. I guess the word-level system may be also competitive to the numbers reported in this paper. Furthermore, can you explain what is the computational bottleneck of the proposed approach? You downsampled the data by the factor of 4 using an RNN, and it still took around 5 days to converge. To me, it is a bit expensive, especially given that you only take one sample when computing the gradient. Table 2 is a little bit misleading, as CTC with language model and seq2seq with a language model model from Bahdanau et al. is much closer to the best number reported in this Table 2, while you may only get a very small improvement using a language model. Finally, \"O(5) days to converge\" sounds a bit odd to me. "
  },
  {
    "people": [
      "I. Badr",
      "J. Glass",
      "L. Lu",
      "A. Ghoshal",
      "R. Singh",
      "B. Raj",
      "R. Stern",
      "Bahdanau"
    ],
    "review": "This submission proposes to learn the word decomposition, or word to sub-word sequence mapping jointly with the attention based sequence-to-sequence model. A particular feature of this approach is that the decomposition is not static, instead, it also conditions on the acoustic input, and the mapping is probabilistic, i.e., one word may map to multiple sub-word sequences. The authors argue that the dynamic decomposition approach can more naturally reflect the acoustic pattern. Interestingly, the motivation behind this approach is analogous to learning the pronunciation mixture model for HMM based speech recognition, where the probabilistic mapping from a word to its pronunciations also conditions on the acoustic input, e.g.,\n\nI. McGraw, I. Badr, and J. Glass, \"Learning lexicons form speech using a pronunciation mixture model,\" in IEEE Transactions on Audio, Speech, and Language Processing, 2013\n\nL. Lu, A. Ghoshal, S. Renals, \"Acoustic data-driven pronunciation lexicon for large vocabulary speech recognition\", in Proc. ASRU \n\nR. Singh, B. Raj, and R. Stern, \"Automatic generation of subword units for speech recognition systems,\"  in IEEE Transactions on Speech and Audio Processing, 2002\n\nIt would be interesting to put this work in the context by linking it to some previous works in the HMM framework.\n\nOverall, the paper is well written, and it is theoretically convincing. The experimental study could be more solid, e.g., it is reasonable to have a word-level baseline, as the proposed approach lies in between the character-level and word-level systems. the vocabulary size of the WSJ si284 dataset is 20K at maximum, which is not very large for the softmax layer, and it is a closed vocabulary task. I guess the word-level system may be also competitive to the numbers reported in this paper. Furthermore, can you explain what is the computational bottleneck of the proposed approach? You downsampled the data by the factor of 4 using an RNN, and it still took around 5 days to converge. To me, it is a bit expensive, especially given that you only take one sample when computing the gradient. Table 2 is a little bit misleading, as CTC with language model and seq2seq with a language model model from Bahdanau et al. is much closer to the best number reported in this Table 2, while you may only get a very small improvement using a language model. Finally, \"O(5) days to converge\" sounds a bit odd to me."
  },
  {
    "people": [
      "I. Badr",
      "J. Glass",
      "L. Lu",
      "A. Ghoshal",
      "R. Singh",
      "B. Raj",
      "R. Stern",
      "Bahdanau"
    ],
    "review": "This submission proposes to learn the word decomposition, or word to sub-word sequence mapping jointly with the attention based sequence-to-sequence model. A particular feature of this approach is that the decomposition is not static, instead, it also conditions on the acoustic input, and the mapping is probabilistic, i.e., one word may map to multiple sub-word sequences. The authors argue that the dynamic decomposition approach can more naturally reflect the acoustic pattern. Interestingly, the motivation behind this approach is analogous to learning the pronunciation mixture model for HMM based speech recognition, where the probabilistic mapping from a word to its pronunciations also conditions on the acoustic input, e.g.,\n\nI. McGraw, I. Badr, and J. Glass, \"Learning lexicons form speech using a pronunciation mixture model,\" in IEEE Transactions on Audio, Speech, and Language Processing, 2013\n\nL. Lu, A. Ghoshal, S. Renals, \"Acoustic data-driven pronunciation lexicon for large vocabulary speech recognition\", in Proc. ASRU \n\nR. Singh, B. Raj, and R. Stern, \"Automatic generation of subword units for speech recognition systems,\"  in IEEE Transactions on Speech and Audio Processing, 2002\n\nIt would be interesting to put this work in the context by linking it to some previous works in the HMM framework.\n\nOverall, the paper is well written, and it is theoretically convincing. The experimental study could be more solid, e.g., it is reasonable to have a word-level baseline, as the proposed approach lies in between the character-level and word-level systems. the vocabulary size of the WSJ si284 dataset is 20K at maximum, which is not very large for the softmax layer, and it is a closed vocabulary task. I guess the word-level system may be also competitive to the numbers reported in this paper. Furthermore, can you explain what is the computational bottleneck of the proposed approach? You downsampled the data by the factor of 4 using an RNN, and it still took around 5 days to converge. To me, it is a bit expensive, especially given that you only take one sample when computing the gradient. Table 2 is a little bit misleading, as CTC with language model and seq2seq with a language model model from Bahdanau et al. is much closer to the best number reported in this Table 2, while you may only get a very small improvement using a language model. Finally, \"O(5) days to converge\" sounds a bit odd to me. "
  },
  {
    "people": [
      "Andrew"
    ],
    "review": "It is not clear to me at all what this paper is contributing. Deep CCA (Andrew et al, 2013) already gives the gradient derivation of the correlation objective with respect to the network outputs which are then back-propagated to update the network weights. Again, the paper gives the gradient of the correlation (i.e. the CCA objective) w.r.t. the network outputs, so it is confusing to me when authors say that their differentiable version enables them to back-propagate directly through the computation of CCA. \n"
  },
  {
    "people": [
      "Andrew"
    ],
    "review": "It is not clear to me at all what this paper is contributing. Deep CCA (Andrew et al, 2013) already gives the gradient derivation of the correlation objective with respect to the network outputs which are then back-propagated to update the network weights. Again, the paper gives the gradient of the correlation (i.e. the CCA objective) w.r.t. the network outputs, so it is confusing to me when authors say that their differentiable version enables them to back-propagate directly through the computation of CCA. \n"
  },
  {
    "people": [
      "Dina",
      "Faruqui",
      "Mikolov",
      "Faruqui"
    ],
    "review": "This paper extends preceding works to create a mapping between the word embedding space of two languages. The word embeddings had been independently trained on monolingual data only, and various forms of bilingual information is used to learn the mapping. This mapping is then used to measure the precision of translations.\n\nIn this paper, the authors propose two changes: \"CCA\" and \"inverted softmax\".  Looking at Table 1, CCA is only better than Dina et al in 1 out of 6 cases (It/En @1).  Most of the improvements are in fact obtained by the introduction of the inverted softmax normalization.\n\nOverall, I wonder which aspect of this paper is really new. You mention:\n - Faruqui & Dyer 2014 already used CCA and dimensionality reduction\n - Xing et al 2015 argued already that Mikolov's linear matrix should be orthogonal\n\nCould you make clear in what aspect your work is different from Faruqui & Dyer 2014 (other the fact that you applied the method to measure translation precision) ?\n\nUsing cognates instead of a bilingual directory is a nice trick. Please explain how you obtained this list of cognates ? Obviously, this only works for languages with the same alphabet (for instance Greek and Russian are excluded)\n\nAlso, it seems to me that in linguistics the term \"cognate\" refers to words which have a common etymological origin - they don't necessarily have the same written form (e.g. night, nuit, noche, Nacht). Maybe, you should use a different term ? Those words are probably proper names in news texts."
  },
  {
    "people": [
      "Sam"
    ],
    "review": "We have uploaded the final version. The text is unchanged, but we have modified the title to emphasise the aspects of the paper which have been of most interest to readers (particularly the inverted softmax).\n\nWe'd like to thank the PC for accepting our manuscript,\nSam"
  },
  {
    "people": [
      "Sam",
      "Agirre"
    ],
    "review": "Dear reviewers and readers,\n\nWe\u2019d like to thank you all for your positive comments about our manuscript. We were particularly pleased that all three reviewers recommended our work be accepted, and by the interest reviewers expressed in the \u201cinverted softmax\u201d. We have uploaded an updated version. There are three main changes we would like to draw readers\u2019 attention to:\n\nOur use of the term \u201ccognates\u201d was misleading and we have removed it from the new version. To be completely clear, we extract the pseudo-dictionary by finding the identical character strings like \u201cDNA\u201d and \u201cCiao\u201d which appear in both the English and Italian vocabularies. These identical strings can be found trivially without any expert knowledge. \n\nWe also realised that our procedure, while very similar to CCA, is not identical. We apologise for this mistake, which we have corrected in the new version. We believe this realisation strengthens the manuscript. We provide additional experiments, and a discussion of the very close relationship between the methods. The two methods have very similar performance, but our approach is numerically cheaper.\n\nShortly after our manuscript was submitted to ICLR, another paper was published [1], which presents a similar theoretical analysis of offline bilingual word vectors. We would like to thank the anonymous reader for bringing this work to our attention, now properly cited. This paper also discusses the need for an orthogonal transformation, and proposes the same novel SVD procedure we propose here to obtain this transformation. However, our work contains a number of contributions not present in their work, including:\n\n1.\tThe use of dimensionality reduction after the SVD\n2.\tThe inverted softmax\n3.\tThe identical strings pseudo-dictionary\n4.\tOffline vector alignment using a phrase dictionary\n5.\tSentence translation retrieval using bilingual vectors\n\nWe will respond to the specific comments of each reviewer underneath their reviews.\nBest wishes,\nSam\n\n[1] Artetxe, M., Labaka, G., & Agirre, E. (2016). Learning principled bilingual mappings of word embeddings while preserving monolingual invariance. Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP-16), 2289\u20132294.\n"
  },
  {
    "people": [
      "Liu",
      "Wang",
      "Lin",
      "Labaka",
      "Agirre",
      "Faruqui",
      "Dyer"
    ],
    "review": "Thank you for the interesting paper.\n1. Could you elaborate on how your method provides additional theoretical insight into the importance of orthogonality beyond existing work [1] and [2]? We would additionally encourage you to cite [2].\n2. In accordance with the review of AnonReviewer1, could you elaborate how your method is different from the existing use of CCA in [3]?\n3. As AnonReviewer1 pointed out, cognates are words with the same etymological origin but are usually spelled differently (see [4] for more examples). You should replace this term to make your manuscript more accurate.\n4. Given the above points, the main contributions of your paper are a) the inverted softmax and b) the \"cognate\" dictionary. Is that correct?\n\n[1] Xing, C., Liu, C., Wang, D., & Lin, Y. (2015). Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation. NAACL-2015, 1005\u20131010. \n[2] Artetxe, M., Labaka, G., & Agirre, E. (2016). Learning principled bilingual mappings of word embeddings while preserving monolingual invariance. Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP-16), 2289\u20132294. \n[3] Faruqui, M., & Dyer, C. (2014). Improving Vector Space Word Representations Using Multilingual Correlation. Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, 462 \u2013 471.\n[4] "
  },
  {
    "people": [
      "Haghighi",
      "Mikolov"
    ],
    "review": "This paper discusses aligning word vectors across language when those embeddings have been learned independently in monolingual settings. There are reasonable scenarios in which such a strategy could come in helpful, so I feel this paper addresses an interesting problem. The paper is mostly well executed but somewhat lacks in evaluation. It would have been nice if a stronger downstream task had been attempted.\n\nThe inverted Softmax idea is very nice.\n\nA few minor issues that ought to be addressed in a published version of this paper:\n\n1) There is no mention of Haghighi et al (2008) \"Learning Bilingual Lexicons from Monolingual Corpora.\", which strikes me as a key piece of prior work regarding the use of CCA in learning bilingual alignment. This paper and links to the work here ought to be discussed.\n2) Likewise, Hermann & Blunsom (2013) \"Multilingual distributed representations without word alignment.\" is probably the correct paper to cite for learning multilingual word embeddings from multilingual aligned data.\n3) It would have been nicer if experiments had been performed with more divergent language pairs rather than just European/Romance languages\n4) A lot of the argumentation around the orthogonality requirements feels related to the idea of using a Mahalanobis distance / covar matrix to learn such mappings. This might be worth including in the discussion\n5) I don't have a better suggestion, but is there an alternative to using the term \"translation (performance/etc.)\" when discussing word alignment across languages? Translation implies something more complex than this in my mind.\n6) The Mikolov citation in the abstract is messed up"
  },
  {
    "people": [
      "Dina",
      "Faruqui",
      "Mikolov",
      "Faruqui"
    ],
    "review": "This paper extends preceding works to create a mapping between the word embedding space of two languages. The word embeddings had been independently trained on monolingual data only, and various forms of bilingual information is used to learn the mapping. This mapping is then used to measure the precision of translations.\n\nIn this paper, the authors propose two changes: \"CCA\" and \"inverted softmax\".  Looking at Table 1, CCA is only better than Dina et al in 1 out of 6 cases (It/En @1).  Most of the improvements are in fact obtained by the introduction of the inverted softmax normalization.\n\nOverall, I wonder which aspect of this paper is really new. You mention:\n - Faruqui & Dyer 2014 already used CCA and dimensionality reduction\n - Xing et al 2015 argued already that Mikolov's linear matrix should be orthogonal\n\nCould you make clear in what aspect your work is different from Faruqui & Dyer 2014 (other the fact that you applied the method to measure translation precision) ?\n\nUsing cognates instead of a bilingual directory is a nice trick. Please explain how you obtained this list of cognates ? Obviously, this only works for languages with the same alphabet (for instance Greek and Russian are excluded)\n\nAlso, it seems to me that in linguistics the term \"cognate\" refers to words which have a common etymological origin - they don't necessarily have the same written form (e.g. night, nuit, noche, Nacht). Maybe, you should use a different term ? Those words are probably proper names in news texts.\n"
  },
  {
    "people": [
      "Dina",
      "Faruqui",
      "Mikolov",
      "Faruqui"
    ],
    "review": "This paper extends preceding works to create a mapping between the word embedding space of two languages. The word embeddings had been independently trained on monolingual data only, and various forms of bilingual information is used to learn the mapping. This mapping is then used to measure the precision of translations.\n\nIn this paper, the authors propose two changes: \"CCA\" and \"inverted softmax\".  Looking at Table 1, CCA is only better than Dina et al in 1 out of 6 cases (It/En @1).  Most of the improvements are in fact obtained by the introduction of the inverted softmax normalization.\n\nOverall, I wonder which aspect of this paper is really new. You mention:\n - Faruqui & Dyer 2014 already used CCA and dimensionality reduction\n - Xing et al 2015 argued already that Mikolov's linear matrix should be orthogonal\n\nCould you make clear in what aspect your work is different from Faruqui & Dyer 2014 (other the fact that you applied the method to measure translation precision) ?\n\nUsing cognates instead of a bilingual directory is a nice trick. Please explain how you obtained this list of cognates ? Obviously, this only works for languages with the same alphabet (for instance Greek and Russian are excluded)\n\nAlso, it seems to me that in linguistics the term \"cognate\" refers to words which have a common etymological origin - they don't necessarily have the same written form (e.g. night, nuit, noche, Nacht). Maybe, you should use a different term ? Those words are probably proper names in news texts."
  },
  {
    "people": [
      "Sam"
    ],
    "review": "We have uploaded the final version. The text is unchanged, but we have modified the title to emphasise the aspects of the paper which have been of most interest to readers (particularly the inverted softmax).\n\nWe'd like to thank the PC for accepting our manuscript,\nSam"
  },
  {
    "people": [
      "Sam",
      "Agirre"
    ],
    "review": "Dear reviewers and readers,\n\nWe\u2019d like to thank you all for your positive comments about our manuscript. We were particularly pleased that all three reviewers recommended our work be accepted, and by the interest reviewers expressed in the \u201cinverted softmax\u201d. We have uploaded an updated version. There are three main changes we would like to draw readers\u2019 attention to:\n\nOur use of the term \u201ccognates\u201d was misleading and we have removed it from the new version. To be completely clear, we extract the pseudo-dictionary by finding the identical character strings like \u201cDNA\u201d and \u201cCiao\u201d which appear in both the English and Italian vocabularies. These identical strings can be found trivially without any expert knowledge. \n\nWe also realised that our procedure, while very similar to CCA, is not identical. We apologise for this mistake, which we have corrected in the new version. We believe this realisation strengthens the manuscript. We provide additional experiments, and a discussion of the very close relationship between the methods. The two methods have very similar performance, but our approach is numerically cheaper.\n\nShortly after our manuscript was submitted to ICLR, another paper was published [1], which presents a similar theoretical analysis of offline bilingual word vectors. We would like to thank the anonymous reader for bringing this work to our attention, now properly cited. This paper also discusses the need for an orthogonal transformation, and proposes the same novel SVD procedure we propose here to obtain this transformation. However, our work contains a number of contributions not present in their work, including:\n\n1.\tThe use of dimensionality reduction after the SVD\n2.\tThe inverted softmax\n3.\tThe identical strings pseudo-dictionary\n4.\tOffline vector alignment using a phrase dictionary\n5.\tSentence translation retrieval using bilingual vectors\n\nWe will respond to the specific comments of each reviewer underneath their reviews.\nBest wishes,\nSam\n\n[1] Artetxe, M., Labaka, G., & Agirre, E. (2016). Learning principled bilingual mappings of word embeddings while preserving monolingual invariance. Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP-16), 2289\u20132294.\n"
  },
  {
    "people": [
      "Liu",
      "Wang",
      "Lin",
      "Labaka",
      "Agirre",
      "Faruqui",
      "Dyer"
    ],
    "review": "Thank you for the interesting paper.\n1. Could you elaborate on how your method provides additional theoretical insight into the importance of orthogonality beyond existing work [1] and [2]? We would additionally encourage you to cite [2].\n2. In accordance with the review of AnonReviewer1, could you elaborate how your method is different from the existing use of CCA in [3]?\n3. As AnonReviewer1 pointed out, cognates are words with the same etymological origin but are usually spelled differently (see [4] for more examples). You should replace this term to make your manuscript more accurate.\n4. Given the above points, the main contributions of your paper are a) the inverted softmax and b) the \"cognate\" dictionary. Is that correct?\n\n[1] Xing, C., Liu, C., Wang, D., & Lin, Y. (2015). Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation. NAACL-2015, 1005\u20131010. \n[2] Artetxe, M., Labaka, G., & Agirre, E. (2016). Learning principled bilingual mappings of word embeddings while preserving monolingual invariance. Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP-16), 2289\u20132294. \n[3] Faruqui, M., & Dyer, C. (2014). Improving Vector Space Word Representations Using Multilingual Correlation. Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, 462 \u2013 471.\n[4] "
  },
  {
    "people": [
      "Haghighi",
      "Mikolov"
    ],
    "review": "This paper discusses aligning word vectors across language when those embeddings have been learned independently in monolingual settings. There are reasonable scenarios in which such a strategy could come in helpful, so I feel this paper addresses an interesting problem. The paper is mostly well executed but somewhat lacks in evaluation. It would have been nice if a stronger downstream task had been attempted.\n\nThe inverted Softmax idea is very nice.\n\nA few minor issues that ought to be addressed in a published version of this paper:\n\n1) There is no mention of Haghighi et al (2008) \"Learning Bilingual Lexicons from Monolingual Corpora.\", which strikes me as a key piece of prior work regarding the use of CCA in learning bilingual alignment. This paper and links to the work here ought to be discussed.\n2) Likewise, Hermann & Blunsom (2013) \"Multilingual distributed representations without word alignment.\" is probably the correct paper to cite for learning multilingual word embeddings from multilingual aligned data.\n3) It would have been nicer if experiments had been performed with more divergent language pairs rather than just European/Romance languages\n4) A lot of the argumentation around the orthogonality requirements feels related to the idea of using a Mahalanobis distance / covar matrix to learn such mappings. This might be worth including in the discussion\n5) I don't have a better suggestion, but is there an alternative to using the term \"translation (performance/etc.)\" when discussing word alignment across languages? Translation implies something more complex than this in my mind.\n6) The Mikolov citation in the abstract is messed up"
  },
  {
    "people": [
      "Dina",
      "Faruqui",
      "Mikolov",
      "Faruqui"
    ],
    "review": "This paper extends preceding works to create a mapping between the word embedding space of two languages. The word embeddings had been independently trained on monolingual data only, and various forms of bilingual information is used to learn the mapping. This mapping is then used to measure the precision of translations.\n\nIn this paper, the authors propose two changes: \"CCA\" and \"inverted softmax\".  Looking at Table 1, CCA is only better than Dina et al in 1 out of 6 cases (It/En @1).  Most of the improvements are in fact obtained by the introduction of the inverted softmax normalization.\n\nOverall, I wonder which aspect of this paper is really new. You mention:\n - Faruqui & Dyer 2014 already used CCA and dimensionality reduction\n - Xing et al 2015 argued already that Mikolov's linear matrix should be orthogonal\n\nCould you make clear in what aspect your work is different from Faruqui & Dyer 2014 (other the fact that you applied the method to measure translation precision) ?\n\nUsing cognates instead of a bilingual directory is a nice trick. Please explain how you obtained this list of cognates ? Obviously, this only works for languages with the same alphabet (for instance Greek and Russian are excluded)\n\nAlso, it seems to me that in linguistics the term \"cognate\" refers to words which have a common etymological origin - they don't necessarily have the same written form (e.g. night, nuit, noche, Nacht). Maybe, you should use a different term ? Those words are probably proper names in news texts.\n"
  },
  {
    "people": [
      "Sugiyama"
    ],
    "review": "Thank you for an interesting read.\n\nGiven the huge interest in generative modelling nowadays, this paper is very timely and does provide very clear connections between methods that don't use maximum likelihood for training. It made a very useful observation that the generative and the discriminative loss do **not** need to be coupled with each other. I think this paper in summary provides some very useful insights to the practitioners on how to select the objective function to train the implicit generative model.\n\nThe only reason that I decided to hold back my strong acceptance recommendation is that I don't understand the acceptance criteria of ICLR. First this paper has the style very similar to the Sugiyama et al. papers that are cited (e.g. presenting in different perspectives that were all covered in those papers but in a different context), making me unsure about how to evaluate the novelty. Second this paper has no experiment nor mathematical theorem, and I'm not exactly sure what kinds of contributions the ICLR committee is looking for."
  },
  {
    "people": [
      "AnonReviewer1",
      "Lopez",
      "Oquab",
      "Uehara",
      "AnonReviewer4",
      "Kareletsos"
    ],
    "review": "The reviewers have two common concerns (1) relevance of this paper to ICLR, and (2) its novelty. We address the common concerns here and address other questions individually.\n\nThe aim of our paper is to review different approaches for learning in implicit generative models; GANs are a special case of implicit generative models and our work helps understand connections between GAN variants as well as understand how GANs are related to the wider statistical literature. We are not aware of any other work in the GAN literature which discusses the connections to approximate Bayesian computation, moment-matching and two sample-testing, unsupervised-as-supervised learning, optimal transport, likelihood free inference, or non-maximum likelihood estimation, even though these methods are clearly related. We believe these connections would be interesting to ICLR community for at least two reasons: (1) we can borrow tools from the related literature to improve optimisation and analyse convergence and (2) we can use GAN-like techniques to train differentiable simulators in other application domains (e.g high energy physics, economics, ecology), thereby opening up exciting research directions. AnonReviewer4 writes \u201cI believe this work is significant - it provides a bridge for language and methods used in multiple parts of statistics and machine learning. This has the potential to accelerate progress.\u201d And AnonReviewer1 writes \u201cIt is well written and a good read, and one I would recommend to people who would like to get involved in GANs.\u201d This was our aim, and why this paper is of relevance to ICLR.\n\nNovelty: The view of hypothesis testing gives an insight about the learning principle used in likelihood free / implicit models. This view also helps understand connections between variants of GANs; for instance, methods that use the density ratio (e.g. the original GAN paper, f-GAN and b-GAN) or the density difference (e.g. generative moment matching networks). To the best of our knowledge, this unifying view through hypothesis testing is novel (see also the concurrent submissions by Lopez-paz and Oquab (2016) and Uehara et al. (2016)). To quote AnonReviewer4, \u201cthe individual pieces are not novel, and yet the exposition of all of them in the same space with clear outline of the connections between them is novel.\u201d \nThe unifying view opens up lots of interesting research directions, e.g., since our initial arxiv submission, Kareletsos (2016) has published \u201cAdversarial Message Passing For Graphical Models\u201d "
  },
  {
    "people": [
      "Shugiyama",
      "Csiszar",
      "Kanamori",
      "Sugiyama"
    ],
    "review": "I just noticed I submitted my review as a pre-review question - sorry about this. Here it is again, with a few more thoughts added...\n\nThe authors present a great and - as far as I can tell - accurate and honest overview of the emerging theory about GANs from a likelihood ratio estimation/divergence minimisation perspective. It is well written and a good read, and one I would recommend to people who would like to get involved in GANs.\n\nMy main problem with this submission is that it is hard as a reviewer to pin down what precisely the novelty is - beyond perhaps articulating these views better than other papers have done in the past. A sentence from the paper \"But it has left us unsatisfied since we have not gained the insight needed to choose between them.\u201d summarises my feeling about this paper: this is a nice 'unifying review\u2019 type paper that - for me - lacks a novel insight.\n\nIn summary, my assessment is mixed: I think this is a great paper, I enjoyed reading it. I was left a bit disappointed by the lack of novel insight, or a singular key new idea which you often expect in conference presentations, and this is why I\u2019m not highly confident about this as a conference submission (and hence my low score) I am open to be convinced either way.\n\nDetailed comments:\n\nI think the authors should probably discuss the connection of Eq. (13) to KLIEP: Kullback-Leibler Importance Estimation by Shugiyama and colleagues.\n\nI don\u2019t quite see how the part with equation (13) and (14) fit into the flow of the paper. By this point the authors have established the view that GANs are about estimating likelihood ratios - and then using these likelihood ratios to improve the generator. These paragraphs read like: we also tried to derive another particular formulation for doing this but we failed to do it in a practical way.\n\nThere is a typo in spelling Csiszar divergence\n\nEquation (15) is known (to me) as Least Squares Importance Estimation by Kanamori et al (2009). A variant of least-squares likelihood estimation uses the kernel trick, and finds a function from an RKHS that best represents the likelihood ratio between the two distributions in a least squares sense. I think it would be interesting to think about how this function is related to the witness function commonly used in MMD and what the properties of this function are compared to the witness function - perhaps showing the two things for simple distributions.\n\nI have stumbled upon the work of Sugiyama and collaborators on direct density ratio estimation before, and I found that work very insightful. Generally, while some of this work is cited in this paper, I felt that the authors could do more to highlight the great work of this group, who have made highly significant contributions to density ratio estimation, albeit with a different goal in mind.\n\nOn likelihood ratio estimation: some methods approximate the likelihood ratio directly (such as least-squares importance estimation), some can be thought of more as approximating the log of this quantity (logistic regression, denoising autoencoders). An unbiased estimate of the ratio will provide a biased estimate of the logarithm and vice versa. To me it feels like estimating the log of the ratio directly is more useful, and in more generality estimating the convex function of the ratio which is used to define the f-divergence seems like a good approach. Could the authors comment on this?\n\nI think the hypothesis testing angle is oversold in the paper.  I\u2019m not sure what additional insight is gained by mixing in some hypothesis testing terminology. Other than using quantities that appear in hypothesis testing as tests statistics, his work does not really talk about hypothesis testing, nor does it use any tools from the hypothesis testing literature. In this sense, this paper is in contrast with Sutherland et al (in review for ICLR) who do borrow concepts from two-sample testing to optimise hyperparameters of the divergence used."
  },
  {
    "people": [
      "Sugiyama"
    ],
    "review": "Thank you for an interesting read.\n\nGiven the huge interest in generative modelling nowadays, this paper is very timely and does provide very clear connections between methods that don't use maximum likelihood for training. It made a very useful observation that the generative and the discriminative loss do **not** need to be coupled with each other. I think this paper in summary provides some very useful insights to the practitioners on how to select the objective function to train the implicit generative model.\n\nThe only reason that I decided to hold back my strong acceptance recommendation is that I don't understand the acceptance criteria of ICLR. First this paper has the style very similar to the Sugiyama et al. papers that are cited (e.g. presenting in different perspectives that were all covered in those papers but in a different context), making me unsure about how to evaluate the novelty. Second this paper has no experiment nor mathematical theorem, and I'm not exactly sure what kinds of contributions the ICLR committee is looking for."
  },
  {
    "people": [
      "Sugiyama"
    ],
    "review": "Thank you for an interesting read.\n\nGiven the huge interest in generative modelling nowadays, this paper is very timely and does provide very clear connections between methods that don't use maximum likelihood for training. It made a very useful observation that the generative and the discriminative loss do **not** need to be coupled with each other. I think this paper in summary provides some very useful insights to the practitioners on how to select the objective function to train the implicit generative model.\n\nThe only reason that I decided to hold back my strong acceptance recommendation is that I don't understand the acceptance criteria of ICLR. First this paper has the style very similar to the Sugiyama et al. papers that are cited (e.g. presenting in different perspectives that were all covered in those papers but in a different context), making me unsure about how to evaluate the novelty. Second this paper has no experiment nor mathematical theorem, and I'm not exactly sure what kinds of contributions the ICLR committee is looking for."
  },
  {
    "people": [
      "AnonReviewer1",
      "Lopez",
      "Oquab",
      "Uehara",
      "AnonReviewer4",
      "Kareletsos"
    ],
    "review": "The reviewers have two common concerns (1) relevance of this paper to ICLR, and (2) its novelty. We address the common concerns here and address other questions individually.\n\nThe aim of our paper is to review different approaches for learning in implicit generative models; GANs are a special case of implicit generative models and our work helps understand connections between GAN variants as well as understand how GANs are related to the wider statistical literature. We are not aware of any other work in the GAN literature which discusses the connections to approximate Bayesian computation, moment-matching and two sample-testing, unsupervised-as-supervised learning, optimal transport, likelihood free inference, or non-maximum likelihood estimation, even though these methods are clearly related. We believe these connections would be interesting to ICLR community for at least two reasons: (1) we can borrow tools from the related literature to improve optimisation and analyse convergence and (2) we can use GAN-like techniques to train differentiable simulators in other application domains (e.g high energy physics, economics, ecology), thereby opening up exciting research directions. AnonReviewer4 writes \u201cI believe this work is significant - it provides a bridge for language and methods used in multiple parts of statistics and machine learning. This has the potential to accelerate progress.\u201d And AnonReviewer1 writes \u201cIt is well written and a good read, and one I would recommend to people who would like to get involved in GANs.\u201d This was our aim, and why this paper is of relevance to ICLR.\n\nNovelty: The view of hypothesis testing gives an insight about the learning principle used in likelihood free / implicit models. This view also helps understand connections between variants of GANs; for instance, methods that use the density ratio (e.g. the original GAN paper, f-GAN and b-GAN) or the density difference (e.g. generative moment matching networks). To the best of our knowledge, this unifying view through hypothesis testing is novel (see also the concurrent submissions by Lopez-paz and Oquab (2016) and Uehara et al. (2016)). To quote AnonReviewer4, \u201cthe individual pieces are not novel, and yet the exposition of all of them in the same space with clear outline of the connections between them is novel.\u201d \nThe unifying view opens up lots of interesting research directions, e.g., since our initial arxiv submission, Kareletsos (2016) has published \u201cAdversarial Message Passing For Graphical Models\u201d "
  },
  {
    "people": [
      "Shugiyama",
      "Csiszar",
      "Kanamori",
      "Sugiyama"
    ],
    "review": "I just noticed I submitted my review as a pre-review question - sorry about this. Here it is again, with a few more thoughts added...\n\nThe authors present a great and - as far as I can tell - accurate and honest overview of the emerging theory about GANs from a likelihood ratio estimation/divergence minimisation perspective. It is well written and a good read, and one I would recommend to people who would like to get involved in GANs.\n\nMy main problem with this submission is that it is hard as a reviewer to pin down what precisely the novelty is - beyond perhaps articulating these views better than other papers have done in the past. A sentence from the paper \"But it has left us unsatisfied since we have not gained the insight needed to choose between them.\u201d summarises my feeling about this paper: this is a nice 'unifying review\u2019 type paper that - for me - lacks a novel insight.\n\nIn summary, my assessment is mixed: I think this is a great paper, I enjoyed reading it. I was left a bit disappointed by the lack of novel insight, or a singular key new idea which you often expect in conference presentations, and this is why I\u2019m not highly confident about this as a conference submission (and hence my low score) I am open to be convinced either way.\n\nDetailed comments:\n\nI think the authors should probably discuss the connection of Eq. (13) to KLIEP: Kullback-Leibler Importance Estimation by Shugiyama and colleagues.\n\nI don\u2019t quite see how the part with equation (13) and (14) fit into the flow of the paper. By this point the authors have established the view that GANs are about estimating likelihood ratios - and then using these likelihood ratios to improve the generator. These paragraphs read like: we also tried to derive another particular formulation for doing this but we failed to do it in a practical way.\n\nThere is a typo in spelling Csiszar divergence\n\nEquation (15) is known (to me) as Least Squares Importance Estimation by Kanamori et al (2009). A variant of least-squares likelihood estimation uses the kernel trick, and finds a function from an RKHS that best represents the likelihood ratio between the two distributions in a least squares sense. I think it would be interesting to think about how this function is related to the witness function commonly used in MMD and what the properties of this function are compared to the witness function - perhaps showing the two things for simple distributions.\n\nI have stumbled upon the work of Sugiyama and collaborators on direct density ratio estimation before, and I found that work very insightful. Generally, while some of this work is cited in this paper, I felt that the authors could do more to highlight the great work of this group, who have made highly significant contributions to density ratio estimation, albeit with a different goal in mind.\n\nOn likelihood ratio estimation: some methods approximate the likelihood ratio directly (such as least-squares importance estimation), some can be thought of more as approximating the log of this quantity (logistic regression, denoising autoencoders). An unbiased estimate of the ratio will provide a biased estimate of the logarithm and vice versa. To me it feels like estimating the log of the ratio directly is more useful, and in more generality estimating the convex function of the ratio which is used to define the f-divergence seems like a good approach. Could the authors comment on this?\n\nI think the hypothesis testing angle is oversold in the paper.  I\u2019m not sure what additional insight is gained by mixing in some hypothesis testing terminology. Other than using quantities that appear in hypothesis testing as tests statistics, his work does not really talk about hypothesis testing, nor does it use any tools from the hypothesis testing literature. In this sense, this paper is in contrast with Sutherland et al (in review for ICLR) who do borrow concepts from two-sample testing to optimise hyperparameters of the divergence used."
  },
  {
    "people": [
      "Sugiyama"
    ],
    "review": "Thank you for an interesting read.\n\nGiven the huge interest in generative modelling nowadays, this paper is very timely and does provide very clear connections between methods that don't use maximum likelihood for training. It made a very useful observation that the generative and the discriminative loss do **not** need to be coupled with each other. I think this paper in summary provides some very useful insights to the practitioners on how to select the objective function to train the implicit generative model.\n\nThe only reason that I decided to hold back my strong acceptance recommendation is that I don't understand the acceptance criteria of ICLR. First this paper has the style very similar to the Sugiyama et al. papers that are cited (e.g. presenting in different perspectives that were all covered in those papers but in a different context), making me unsure about how to evaluate the novelty. Second this paper has no experiment nor mathematical theorem, and I'm not exactly sure what kinds of contributions the ICLR committee is looking for."
  },
  {
    "people": [
      "Arnaud Sors"
    ],
    "review": "Hi!\nI think this is an outstanding paper. \nIt has greatly helped me towards understanding where failure modes of GANs come from, and I think it will also have great practical implications on how to train GANs. \n\nFollowing your ideas, I have played with added noise on GANs. I am able to get the generator to yield 'good looking' samples in much less G iterations than in the standard setting. Also, it appears that 'successful' training is a LOT less sensitive to the setting of hyperparameters. For example I am able to use 10 times higher learning rates, forget about gradient clipping, etc... so this is great! I think this paper paves the way to further research on the following points:\n- how to choose noise variance and schedule its decrease over training ?\n- what does 'training D to convergence' mean ? In practice it seems that training D 'more than G' but only a few steps is already sufficient. Can we get a theoretical understanding of this...\n\nAlso, in my (quick and dirty) first experiments, using instance noise helped training in less G iterations, but the mode dropping problem remained. In my understanding, your theoretical analysis demonstrates that the addition of noise helps D provide gradients to G so that at anytime, G is able to escape its current modes. However, and due to the fact that D outputs are calculated from single examples there is no 'coverage guarantee', for example G could keep switching between different modes. Is this right ? Minibatch discrimination appears to be a good first way to answer this, although not 100% satisfactory because the metric is combinatorial... What is your view on these practical considerations ?\n\nMany thanks !\n\nArnaud Sors"
  },
  {
    "people": [
      "Martin"
    ],
    "review": "Hi! We would like to comment that we added a revision with the following changes at its core:\n\n- We extended the proofs and definitions in section 2 to work with manifolds with boundary.\n\n- We included a one page Appendix B with further clarifications for the things that were suggested in the comments, such as a small comment on continuity vs absolute continuity of random variables, and how it is relevant to our paper.\n\n- We did some minor rewriting to take into account the suggestions from the reviewers and commenters.\n\n- We fixed all the typos :)\n\nBest!\nMartin"
  },
  {
    "people": [
      "Martin"
    ],
    "review": "Hi! We would first like to thank the reviewers for your comments. We will aim to make all the suggestions fit into a revision. We will update the paper shortly and provide individual responses to the reviews :)\n\nBest!\nMartin"
  },
  {
    "people": [
      "Tim",
      "Christopher Olah",
      "Jonathon Shlens"
    ],
    "review": "This paper makes many valuable contributions and explains many of the issues related to training GANs; I feel it will be an essential platform for future work. However I have the following questions and found the following inconsistencies.\n\n1) \n\t\u201cIf n <= d, we are done since the image of \u03c0 is contained in all Rn, a manifold with at \n\tmost dimension d. We now turn to the case where d > n.\u201d\n\nIs the case  d>n  not contained in the first,  n<=d ? Should this be n>d?\n\n2) The perfect discriminator theorems: \n\n\t\u201cWe say that a discriminator D : X \u2192 [0, 1] has accuracy 1 if it takes the value 1 on a set that \n\tcontains the support of P_r  and value 0 on a set that contains the support of P_g .\u201d\n\nIf a set S_g or S_r contains the support of P_g or P_r, this suggests that it may contain the support but may also contain regions outside of the support.  Are you suggesting that for x in S_r that are outside the support of P_r, D(x)=1? However, surely regions outside the support of P_r should correspond to generated images, so D(x) should be 0.\n\nb) Following from a), if a set S_g (or S_r) contained regions outside the support (as well as the support) then S_g (or S_r)  would contain the support of P_g (or P_r) but may also contain regions in the support of P_r (or P_g).\n\n3) \n\t\tP_r[D(x) = 1] = 1 and P_g[D(x) = 0] = 1 \n\na) This notation is not clear, do you mean:\n\tP_r(x)=1 for all x in {x : D(x)=1} and P_g(x)=1 for all x in {x : D(x)=0}\n\n4) Theorem 2.1\n\na) It is not clear what distance measure you are using between sets\nb) Where does the delta/3 come from? Is this arbitrary?\n\n5) Theorem 2.2\na) What is P^c?\nb) Is the ball B(x,e_x) defined for all x in M\\L? This is not explicit.\n\nIf defined for all x in M\\L:\nc) For x on the boundary of M\\L and P\\L is e_x=0? Otherwise, is it possible that for x on the boundary of M\\L or P\\L , M_hat and P_hat intersect with P\\L or M\\L respectively? \n\nFinally:\nd) M_hat is a superset of M\\L, how can you say D*(x)=1 for all x in M_hat, when M_hat may include regions outside the support of M?\n\n9) Corollary 2.1; \n\t\"Under the same assumptions of Theorem 1.3\u2028\"\na) What is Theorem 1.3?\n\n11) Theorem 2.5 \na) Could this be explicitly linked to the collapsing generator problem talked about by [1] and [2].  Citations might make the link clearer. \n\n12) Theorem 2.6 \na) Why do you assume that the distribution of difference between D and D* is white noise? This suggests that most of the time D is optimal, from the rest of the paper this is believable, however if this is what you are assuming, this could be made more explicit. \nb) Similar question for difference in gradient, though since grad D* is zero most of the time, that would suggest that grad D is zero most of the time. Is this true?\nc) extra bracket in final line of proof\n\n16) \n       \u201cThis is in drastic contrast to the noiseless variants P_g  and P_g,\u201d\na) repeated P_g\n\n17) \n         \u201cagain that JSD(P_x,P_{x+e} is maxed out,\u201d\na) missing bracket and inconsistent notation, JSD(P_x||P_{x+e})\nb) \u201cmaxed out\u201c is a colloquialism\n\n[1] Salimans, Tim, et al. \"Improved techniques for training gans.\" arXiv preprint arXiv:1606.03498 (2016).\n[2]Odena, Augustus, Christopher Olah, and Jonathon Shlens. \"Conditional Image Synthesis With Auxiliary Classifier GANs.\" arXiv preprint arXiv:1610.09585 (2016)."
  },
  {
    "people": [
      "Arnaud Sors"
    ],
    "review": "Hi!\nI think this is an outstanding paper. \nIt has greatly helped me towards understanding where failure modes of GANs come from, and I think it will also have great practical implications on how to train GANs. \n\nFollowing your ideas, I have played with added noise on GANs. I am able to get the generator to yield 'good looking' samples in much less G iterations than in the standard setting. Also, it appears that 'successful' training is a LOT less sensitive to the setting of hyperparameters. For example I am able to use 10 times higher learning rates, forget about gradient clipping, etc... so this is great! I think this paper paves the way to further research on the following points:\n- how to choose noise variance and schedule its decrease over training ?\n- what does 'training D to convergence' mean ? In practice it seems that training D 'more than G' but only a few steps is already sufficient. Can we get a theoretical understanding of this...\n\nAlso, in my (quick and dirty) first experiments, using instance noise helped training in less G iterations, but the mode dropping problem remained. In my understanding, your theoretical analysis demonstrates that the addition of noise helps D provide gradients to G so that at anytime, G is able to escape its current modes. However, and due to the fact that D outputs are calculated from single examples there is no 'coverage guarantee', for example G could keep switching between different modes. Is this right ? Minibatch discrimination appears to be a good first way to answer this, although not 100% satisfactory because the metric is combinatorial... What is your view on these practical considerations ?\n\nMany thanks !\n\nArnaud Sors"
  },
  {
    "people": [
      "Martin"
    ],
    "review": "Hi! We would like to comment that we added a revision with the following changes at its core:\n\n- We extended the proofs and definitions in section 2 to work with manifolds with boundary.\n\n- We included a one page Appendix B with further clarifications for the things that were suggested in the comments, such as a small comment on continuity vs absolute continuity of random variables, and how it is relevant to our paper.\n\n- We did some minor rewriting to take into account the suggestions from the reviewers and commenters.\n\n- We fixed all the typos :)\n\nBest!\nMartin"
  },
  {
    "people": [
      "Martin"
    ],
    "review": "Hi! We would first like to thank the reviewers for your comments. We will aim to make all the suggestions fit into a revision. We will update the paper shortly and provide individual responses to the reviews :)\n\nBest!\nMartin"
  },
  {
    "people": [
      "Tim",
      "Christopher Olah",
      "Jonathon Shlens"
    ],
    "review": "This paper makes many valuable contributions and explains many of the issues related to training GANs; I feel it will be an essential platform for future work. However I have the following questions and found the following inconsistencies.\n\n1) \n\t\u201cIf n <= d, we are done since the image of \u03c0 is contained in all Rn, a manifold with at \n\tmost dimension d. We now turn to the case where d > n.\u201d\n\nIs the case  d>n  not contained in the first,  n<=d ? Should this be n>d?\n\n2) The perfect discriminator theorems: \n\n\t\u201cWe say that a discriminator D : X \u2192 [0, 1] has accuracy 1 if it takes the value 1 on a set that \n\tcontains the support of P_r  and value 0 on a set that contains the support of P_g .\u201d\n\nIf a set S_g or S_r contains the support of P_g or P_r, this suggests that it may contain the support but may also contain regions outside of the support.  Are you suggesting that for x in S_r that are outside the support of P_r, D(x)=1? However, surely regions outside the support of P_r should correspond to generated images, so D(x) should be 0.\n\nb) Following from a), if a set S_g (or S_r) contained regions outside the support (as well as the support) then S_g (or S_r)  would contain the support of P_g (or P_r) but may also contain regions in the support of P_r (or P_g).\n\n3) \n\t\tP_r[D(x) = 1] = 1 and P_g[D(x) = 0] = 1 \n\na) This notation is not clear, do you mean:\n\tP_r(x)=1 for all x in {x : D(x)=1} and P_g(x)=1 for all x in {x : D(x)=0}\n\n4) Theorem 2.1\n\na) It is not clear what distance measure you are using between sets\nb) Where does the delta/3 come from? Is this arbitrary?\n\n5) Theorem 2.2\na) What is P^c?\nb) Is the ball B(x,e_x) defined for all x in M\\L? This is not explicit.\n\nIf defined for all x in M\\L:\nc) For x on the boundary of M\\L and P\\L is e_x=0? Otherwise, is it possible that for x on the boundary of M\\L or P\\L , M_hat and P_hat intersect with P\\L or M\\L respectively? \n\nFinally:\nd) M_hat is a superset of M\\L, how can you say D*(x)=1 for all x in M_hat, when M_hat may include regions outside the support of M?\n\n9) Corollary 2.1; \n\t\"Under the same assumptions of Theorem 1.3\u2028\"\na) What is Theorem 1.3?\n\n11) Theorem 2.5 \na) Could this be explicitly linked to the collapsing generator problem talked about by [1] and [2].  Citations might make the link clearer. \n\n12) Theorem 2.6 \na) Why do you assume that the distribution of difference between D and D* is white noise? This suggests that most of the time D is optimal, from the rest of the paper this is believable, however if this is what you are assuming, this could be made more explicit. \nb) Similar question for difference in gradient, though since grad D* is zero most of the time, that would suggest that grad D is zero most of the time. Is this true?\nc) extra bracket in final line of proof\n\n16) \n       \u201cThis is in drastic contrast to the noiseless variants P_g  and P_g,\u201d\na) repeated P_g\n\n17) \n         \u201cagain that JSD(P_x,P_{x+e} is maxed out,\u201d\na) missing bracket and inconsistent notation, JSD(P_x||P_{x+e})\nb) \u201cmaxed out\u201c is a colloquialism\n\n[1] Salimans, Tim, et al. \"Improved techniques for training gans.\" arXiv preprint arXiv:1606.03498 (2016).\n[2]Odena, Augustus, Christopher Olah, and Jonathon Shlens. \"Conditional Image Synthesis With Auxiliary Classifier GANs.\" arXiv preprint arXiv:1610.09585 (2016)."
  },
  {
    "people": [
      "Kim",
      "Jozefowicz",
      "Kim",
      "Jozefowicz",
      "Jozefowicz",
      "Kim",
      "Shaik"
    ],
    "review": "In this submission, an interesting approach to character-based language modeling is pursued that retains word-level representations both in the context, and optionally also in the output. However, the approach is not new, cf. (Kim et al. 2015) as cited in the submission, as well as (Jozefowicz et al. 2016). Both Kim and Jozefowicz already go beyond this submission by applying the approach using RNNs/LSTMs. Also, Jozefowicz et al. provide a comparative discussion of different approaches to character-level modeling, which I am missing here, at least by discussing this existing work. THe remaining novelty of the approach then would be its application to machine translation, although it remains somewhat unclear, inhowfar reranking of N-best lists can handle the OOV problem - the translation-related part of the OVV problem should be elaborated here. That said, some of the claims of this submission seems somewhat exaggerated, like the statement in Sec. 2.3: \"making the notion of vocabulary obsolete\", whereas the authors e.g. express doubts concerning the interpretation of perplexity w/o an explicit output vocabulary. For example modeling of especially frequent word forms still can be expected to contribute, as shown in e.g. arXiv:1609.08144\n\nSec. 2.3: You claim that the objective requires a finite vocabulary. This statement only is correct if the units considered are limited to full word forms. However, using subwords and even individual characters, implicitly larger and even infinite vocabularies can be covered with the log-likelihood criterion. Even though this require a model different from the one proposed here, the corresponding statement should qualified in this respect.\n\nThe way character embeddings are used for the output should be clarified. The description in Sec. 2.4 is not explicit enough in my view.\n\nConcerning the configuration of NCE, it would be desirable to get a better idea of how you arrived at your specific configuration and parameterization described in Sec. 3.4.\n\nSec. 4.1: you might want to mention that (Kim et al. 2015) came to similar conclusions w.r.t. the performance of using character embeddings at the output, and discuss the suggestions for possible improvements given therein.\n\nSec. 4.2: there are ways to calculate and interpret perplexity for unknown words, cf. (Shaik et al. IWSLT 2013).\n\nSec. 4.4 and Table 4: the size of the full training vocabulary should be provided here.\n\nMinor comments:\np. 2, bottom: three different input layer -> three different input layers (plural)\nFig. 1: fonts within the figure are way too small\np. 3, first item below Fig. 1: that we will note WE -> that we will denote WE\nSec. 2.3: the parameters estimation -> the parameter estimation (or: the parameters' estimation)\np. 5, first paragraph: in factored way -> in a factored way\np. 5, second paragraph: a n-best list, a nk-best list -> an n-best list, an nk-best list\nSec. 4.2, last sentence: Despite adaptive gradient, -> verb and article missing"
  },
  {
    "people": [
      "Kim",
      "Jozefowicz",
      "Kim",
      "Jozefowicz",
      "Jozefowicz",
      "Kim",
      "Shaik"
    ],
    "review": "In this submission, an interesting approach to character-based language modeling is pursued that retains word-level representations both in the context, and optionally also in the output. However, the approach is not new, cf. (Kim et al. 2015) as cited in the submission, as well as (Jozefowicz et al. 2016). Both Kim and Jozefowicz already go beyond this submission by applying the approach using RNNs/LSTMs. Also, Jozefowicz et al. provide a comparative discussion of different approaches to character-level modeling, which I am missing here, at least by discussing this existing work. THe remaining novelty of the approach then would be its application to machine translation, although it remains somewhat unclear, inhowfar reranking of N-best lists can handle the OOV problem - the translation-related part of the OVV problem should be elaborated here. That said, some of the claims of this submission seems somewhat exaggerated, like the statement in Sec. 2.3: \"making the notion of vocabulary obsolete\", whereas the authors e.g. express doubts concerning the interpretation of perplexity w/o an explicit output vocabulary. For example modeling of especially frequent word forms still can be expected to contribute, as shown in e.g. arXiv:1609.08144\n\nSec. 2.3: You claim that the objective requires a finite vocabulary. This statement only is correct if the units considered are limited to full word forms. However, using subwords and even individual characters, implicitly larger and even infinite vocabularies can be covered with the log-likelihood criterion. Even though this require a model different from the one proposed here, the corresponding statement should qualified in this respect.\n\nThe way character embeddings are used for the output should be clarified. The description in Sec. 2.4 is not explicit enough in my view.\n\nConcerning the configuration of NCE, it would be desirable to get a better idea of how you arrived at your specific configuration and parameterization described in Sec. 3.4.\n\nSec. 4.1: you might want to mention that (Kim et al. 2015) came to similar conclusions w.r.t. the performance of using character embeddings at the output, and discuss the suggestions for possible improvements given therein.\n\nSec. 4.2: there are ways to calculate and interpret perplexity for unknown words, cf. (Shaik et al. IWSLT 2013).\n\nSec. 4.4 and Table 4: the size of the full training vocabulary should be provided here.\n\nMinor comments:\np. 2, bottom: three different input layer -> three different input layers (plural)\nFig. 1: fonts within the figure are way too small\np. 3, first item below Fig. 1: that we will note WE -> that we will denote WE\nSec. 2.3: the parameters estimation -> the parameter estimation (or: the parameters' estimation)\np. 5, first paragraph: in factored way -> in a factored way\np. 5, second paragraph: a n-best list, a nk-best list -> an n-best list, an nk-best list\nSec. 4.2, last sentence: Despite adaptive gradient, -> verb and article missing\n"
  },
  {
    "people": [
      "Shaik"
    ],
    "review": "In Sec. 4.2 you mention that perplexity is hard to interpret for models not using an explicit output vocabulary. When analysing open vocabulary approaches, perplexity can also be renormalized to character level, cf. e.g. Shaik et al. IWLST 2013. Did you consider this?"
  },
  {
    "people": [
      "Kim"
    ],
    "review": "Also Kim et al. AAAI 2015 got the similar conclusions w.r.t. the performance of character-level embeddings and also provided a discussion with suggestions for improvements. Did you consider these?"
  },
  {
    "people": [
      "Kim"
    ],
    "review": "Can you confirm that the character-level word embedding used here is the same as in the google paper by Kim et al. AAAI 2015? It is not cited in Sec. 2.1.\n\n"
  },
  {
    "people": [
      "Kim",
      "Jozefowicz",
      "Kim",
      "Jozefowicz",
      "Jozefowicz",
      "Kim",
      "Shaik"
    ],
    "review": "In this submission, an interesting approach to character-based language modeling is pursued that retains word-level representations both in the context, and optionally also in the output. However, the approach is not new, cf. (Kim et al. 2015) as cited in the submission, as well as (Jozefowicz et al. 2016). Both Kim and Jozefowicz already go beyond this submission by applying the approach using RNNs/LSTMs. Also, Jozefowicz et al. provide a comparative discussion of different approaches to character-level modeling, which I am missing here, at least by discussing this existing work. THe remaining novelty of the approach then would be its application to machine translation, although it remains somewhat unclear, inhowfar reranking of N-best lists can handle the OOV problem - the translation-related part of the OVV problem should be elaborated here. That said, some of the claims of this submission seems somewhat exaggerated, like the statement in Sec. 2.3: \"making the notion of vocabulary obsolete\", whereas the authors e.g. express doubts concerning the interpretation of perplexity w/o an explicit output vocabulary. For example modeling of especially frequent word forms still can be expected to contribute, as shown in e.g. arXiv:1609.08144\n\nSec. 2.3: You claim that the objective requires a finite vocabulary. This statement only is correct if the units considered are limited to full word forms. However, using subwords and even individual characters, implicitly larger and even infinite vocabularies can be covered with the log-likelihood criterion. Even though this require a model different from the one proposed here, the corresponding statement should qualified in this respect.\n\nThe way character embeddings are used for the output should be clarified. The description in Sec. 2.4 is not explicit enough in my view.\n\nConcerning the configuration of NCE, it would be desirable to get a better idea of how you arrived at your specific configuration and parameterization described in Sec. 3.4.\n\nSec. 4.1: you might want to mention that (Kim et al. 2015) came to similar conclusions w.r.t. the performance of using character embeddings at the output, and discuss the suggestions for possible improvements given therein.\n\nSec. 4.2: there are ways to calculate and interpret perplexity for unknown words, cf. (Shaik et al. IWSLT 2013).\n\nSec. 4.4 and Table 4: the size of the full training vocabulary should be provided here.\n\nMinor comments:\np. 2, bottom: three different input layer -> three different input layers (plural)\nFig. 1: fonts within the figure are way too small\np. 3, first item below Fig. 1: that we will note WE -> that we will denote WE\nSec. 2.3: the parameters estimation -> the parameter estimation (or: the parameters' estimation)\np. 5, first paragraph: in factored way -> in a factored way\np. 5, second paragraph: a n-best list, a nk-best list -> an n-best list, an nk-best list\nSec. 4.2, last sentence: Despite adaptive gradient, -> verb and article missing"
  },
  {
    "people": [
      "Kim",
      "Jozefowicz",
      "Kim",
      "Jozefowicz",
      "Jozefowicz",
      "Kim",
      "Shaik"
    ],
    "review": "In this submission, an interesting approach to character-based language modeling is pursued that retains word-level representations both in the context, and optionally also in the output. However, the approach is not new, cf. (Kim et al. 2015) as cited in the submission, as well as (Jozefowicz et al. 2016). Both Kim and Jozefowicz already go beyond this submission by applying the approach using RNNs/LSTMs. Also, Jozefowicz et al. provide a comparative discussion of different approaches to character-level modeling, which I am missing here, at least by discussing this existing work. THe remaining novelty of the approach then would be its application to machine translation, although it remains somewhat unclear, inhowfar reranking of N-best lists can handle the OOV problem - the translation-related part of the OVV problem should be elaborated here. That said, some of the claims of this submission seems somewhat exaggerated, like the statement in Sec. 2.3: \"making the notion of vocabulary obsolete\", whereas the authors e.g. express doubts concerning the interpretation of perplexity w/o an explicit output vocabulary. For example modeling of especially frequent word forms still can be expected to contribute, as shown in e.g. arXiv:1609.08144\n\nSec. 2.3: You claim that the objective requires a finite vocabulary. This statement only is correct if the units considered are limited to full word forms. However, using subwords and even individual characters, implicitly larger and even infinite vocabularies can be covered with the log-likelihood criterion. Even though this require a model different from the one proposed here, the corresponding statement should qualified in this respect.\n\nThe way character embeddings are used for the output should be clarified. The description in Sec. 2.4 is not explicit enough in my view.\n\nConcerning the configuration of NCE, it would be desirable to get a better idea of how you arrived at your specific configuration and parameterization described in Sec. 3.4.\n\nSec. 4.1: you might want to mention that (Kim et al. 2015) came to similar conclusions w.r.t. the performance of using character embeddings at the output, and discuss the suggestions for possible improvements given therein.\n\nSec. 4.2: there are ways to calculate and interpret perplexity for unknown words, cf. (Shaik et al. IWSLT 2013).\n\nSec. 4.4 and Table 4: the size of the full training vocabulary should be provided here.\n\nMinor comments:\np. 2, bottom: three different input layer -> three different input layers (plural)\nFig. 1: fonts within the figure are way too small\np. 3, first item below Fig. 1: that we will note WE -> that we will denote WE\nSec. 2.3: the parameters estimation -> the parameter estimation (or: the parameters' estimation)\np. 5, first paragraph: in factored way -> in a factored way\np. 5, second paragraph: a n-best list, a nk-best list -> an n-best list, an nk-best list\nSec. 4.2, last sentence: Despite adaptive gradient, -> verb and article missing\n"
  },
  {
    "people": [
      "Shaik"
    ],
    "review": "In Sec. 4.2 you mention that perplexity is hard to interpret for models not using an explicit output vocabulary. When analysing open vocabulary approaches, perplexity can also be renormalized to character level, cf. e.g. Shaik et al. IWLST 2013. Did you consider this?"
  },
  {
    "people": [
      "Kim"
    ],
    "review": "Also Kim et al. AAAI 2015 got the similar conclusions w.r.t. the performance of character-level embeddings and also provided a discussion with suggestions for improvements. Did you consider these?"
  },
  {
    "people": [
      "Kim"
    ],
    "review": "Can you confirm that the character-level word embedding used here is the same as in the google paper by Kim et al. AAAI 2015? It is not cited in Sec. 2.1.\n\n"
  },
  {
    "people": [
      "Snoek",
      "Larochelle",
      "Adams"
    ],
    "review": "A well written paper and an interesting construction - I thoroughly enjoyed reading it. \n\nI found the formalism a bit hard to follow without specific examples- that is, it wasn't clear to me at first what the specific components in figure 1A were. What constitutes the controller, a control, the optimizer, what was being optimized, etc., in specific cases. Algorithm boxes may have been helpful, especially in the case of your experiments. A description of existing models that fall under your conceptual framework might help as well.\n\nIn Practical Bayesian Optimization of Machine Learning Algorithms, Snoek, Larochelle and Adams propose optimizing with respect to expected improvement per second to balance computation cost and performance loss. It might be interesting to see how this falls into your framework.\n\nExperimental results were presented clearly and well illustrated the usefulness of the metacontroller. I'm curious to see the results of using more metaexperts."
  },
  {
    "people": [
      "Snoek",
      "Larochelle",
      "Adams"
    ],
    "review": "A well written paper and an interesting construction - I thoroughly enjoyed reading it. \n\nI found the formalism a bit hard to follow without specific examples- that is, it wasn't clear to me at first what the specific components in figure 1A were. What constitutes the controller, a control, the optimizer, what was being optimized, etc., in specific cases. Algorithm boxes may have been helpful, especially in the case of your experiments. A description of existing models that fall under your conceptual framework might help as well.\n\nIn Practical Bayesian Optimization of Machine Learning Algorithms, Snoek, Larochelle and Adams propose optimizing with respect to expected improvement per second to balance computation cost and performance loss. It might be interesting to see how this falls into your framework.\n\nExperimental results were presented clearly and well illustrated the usefulness of the metacontroller. I'm curious to see the results of using more metaexperts."
  },
  {
    "people": [
      "Tamar"
    ],
    "review": "The approach here looks at learning policies that are robust over a parameterized class of MDPs (in the sense the probability that the policy doesn't perform well is small over this class. The idea is fairly straightforward, but the algorithm seems novel and the results show that the approach does seem to provide substantial benefit. The reviewers were all in agreement that the paper is worth accepting.\n \n Pros:\n + Nice application of robust (really stochastic, since these are chance constraints) optimization to policy search\n + Compelling demonstration of the improved range of good performance over methods like vanilla TRPO\n \n Cons:\n - The question of how to parameterize a class of MDPs for real-world scenarios is still somewhat unclear\n - The description of the method as optimizing CVaR seems incorrect, since they appear to be using an actual chance constraint, whereas CVaR is essentially a convex relaxation ... this may be related to the work in (Tamar, 2015), but needs to be better explained if so."
  },
  {
    "people": [
      "Tamar"
    ],
    "review": "The approach here looks at learning policies that are robust over a parameterized class of MDPs (in the sense the probability that the policy doesn't perform well is small over this class. The idea is fairly straightforward, but the algorithm seems novel and the results show that the approach does seem to provide substantial benefit. The reviewers were all in agreement that the paper is worth accepting.\n \n Pros:\n + Nice application of robust (really stochastic, since these are chance constraints) optimization to policy search\n + Compelling demonstration of the improved range of good performance over methods like vanilla TRPO\n \n Cons:\n - The question of how to parameterize a class of MDPs for real-world scenarios is still somewhat unclear\n - The description of the method as optimizing CVaR seems incorrect, since they appear to be using an actual chance constraint, whereas CVaR is essentially a convex relaxation ... this may be related to the work in (Tamar, 2015), but needs to be better explained if so."
  },
  {
    "people": [
      "Scarselli"
    ],
    "review": "This paper studies neural models that can be applied to set-structured inputs and thus require permutation invariance or equivariance. After a first section that introduces necessary and sufficient conditions for permutation invariance/equivariance, the authors present experiments in supervised and semi-supervised learning on point-cloud data as well as cosmology data.\n \n The reviewers agreed that this is a very promising line of work and acknowledged the effort of the authors to improve their paper after the initial discussion phase. However, they also agree that the work appears to be missing more convincing numerical experiments and insights on the choice of neural architectures in the class of permutation-covariant. \n \n In light of these reviews, the AC invites their work to the workshop track. \n Also, I would like to emphasize an aspect of this work that I think should be addressed in the subsequent revision.\n \n As the authors rightfully show (thm 2.1), permutation equivariance puts very strong constraints in the class of 1-layer networks. This theorem, while rigorous, reflects a simple algebraic property of matrices that commute with permutation matrices. It is therefore not very surprising, and the resulting architecture relatively obvious. So much so that it already exists in the literature. In fact, it is a particular instance of the graph neural network model of Scarselli et al. '09 ("
  },
  {
    "people": [
      "N. Bourbaki"
    ],
    "review": "This review is only an informed guess - unfortunately I cannot assess the paper due to my lack of understanding of the paper. \nI have spent several hours trying to read this paper - but it has not been possible for me to follow - partially due to my own limitations, but also I think due to an overly abstract level of presentation. The paper is clearly written, but in the same way that a N. Bourbaki book is clearly written.\n\nI would prefer to leave the accept/reject decision to the other reviewers who may have a better understanding - even if the authors had made a serious mistake, I would not be able to tell. My proposal is positive because the paper is apparently clearly written and the empirical evaluation is quite promising. But some effort will be needed in order to address the broader audience that could potentially be interested in the topic. \n\nI therefore would like to provide feedback only at the level of presentation. \n\nMy main source of problems is that the authors do not try to ground their abstract formalism with concrete examples; when the examples show up it is by \"revelation\" rather than by explaining how they connect to the previous concepts. \n\nThe one example that could unlock most people's understanding is how convolution, or inner product operations connect with the setting described here. For what I know convolution is tied with space (or time) and is understood as an equivariant operation - shifting the signal shifts the output. \nIt is not explained how the '(x, x')' pairs used by the authors in order to build relations, structures and then to define invariance relate to this setting. \nGoing from sets, to relations, to functions, to operators, and then to shift-invariant operators (convolutions) involves many steps, and some hand-holding is needed.\n\nWhy is the 3x3 convolution associated to 9 relations? \nAre these relations referring to the input at a given coordinate and its contribution to the output? (w_{offset} x_{i-offset})? In that case, why is there a backward arrow from the center node to the other nodes? And why are there arrows across nodes? \nWhat is a Cardinal and what is a Cartesian convolution in signal processing terms? (clearly these are not standard terms). \nAre we talking about separable filters? \nWhat are the X and Square symbols in Figure 2? And what are the horizontal and vertical sub-graphs standing for? What is x_1 and what is x_{11},x_{1,2},x_{1,3} and what is the relationship between them?\n\nI realize that to the authors these questions may seem to be trivial and left as  homework for the reader. But I think part of publishing a paper is doing a big part of the homework for the readers so that it becomes easy to get the idea. \n\nClearly the authors target the more general case - but spending some time to explain how the particular case is an instance of the the general case would be a good use of space. \n\nI would propose that the authors explain what are  x, x_{I}, and x_{S} for the simplest possible example, e.g. convolving a 1x5 signal with a 1x3 filter, how the convolution filter parameters show up in the function f, as well as how the spatial invariance (or, equivariance) of convolution is reflected here. "
  },
  {
    "people": [
      "Scarselli"
    ],
    "review": "This paper studies neural models that can be applied to set-structured inputs and thus require permutation invariance or equivariance. After a first section that introduces necessary and sufficient conditions for permutation invariance/equivariance, the authors present experiments in supervised and semi-supervised learning on point-cloud data as well as cosmology data.\n \n The reviewers agreed that this is a very promising line of work and acknowledged the effort of the authors to improve their paper after the initial discussion phase. However, they also agree that the work appears to be missing more convincing numerical experiments and insights on the choice of neural architectures in the class of permutation-covariant. \n \n In light of these reviews, the AC invites their work to the workshop track. \n Also, I would like to emphasize an aspect of this work that I think should be addressed in the subsequent revision.\n \n As the authors rightfully show (thm 2.1), permutation equivariance puts very strong constraints in the class of 1-layer networks. This theorem, while rigorous, reflects a simple algebraic property of matrices that commute with permutation matrices. It is therefore not very surprising, and the resulting architecture relatively obvious. So much so that it already exists in the literature. In fact, it is a particular instance of the graph neural network model of Scarselli et al. '09 ("
  },
  {
    "people": [
      "N. Bourbaki"
    ],
    "review": "This review is only an informed guess - unfortunately I cannot assess the paper due to my lack of understanding of the paper. \nI have spent several hours trying to read this paper - but it has not been possible for me to follow - partially due to my own limitations, but also I think due to an overly abstract level of presentation. The paper is clearly written, but in the same way that a N. Bourbaki book is clearly written.\n\nI would prefer to leave the accept/reject decision to the other reviewers who may have a better understanding - even if the authors had made a serious mistake, I would not be able to tell. My proposal is positive because the paper is apparently clearly written and the empirical evaluation is quite promising. But some effort will be needed in order to address the broader audience that could potentially be interested in the topic. \n\nI therefore would like to provide feedback only at the level of presentation. \n\nMy main source of problems is that the authors do not try to ground their abstract formalism with concrete examples; when the examples show up it is by \"revelation\" rather than by explaining how they connect to the previous concepts. \n\nThe one example that could unlock most people's understanding is how convolution, or inner product operations connect with the setting described here. For what I know convolution is tied with space (or time) and is understood as an equivariant operation - shifting the signal shifts the output. \nIt is not explained how the '(x, x')' pairs used by the authors in order to build relations, structures and then to define invariance relate to this setting. \nGoing from sets, to relations, to functions, to operators, and then to shift-invariant operators (convolutions) involves many steps, and some hand-holding is needed.\n\nWhy is the 3x3 convolution associated to 9 relations? \nAre these relations referring to the input at a given coordinate and its contribution to the output? (w_{offset} x_{i-offset})? In that case, why is there a backward arrow from the center node to the other nodes? And why are there arrows across nodes? \nWhat is a Cardinal and what is a Cartesian convolution in signal processing terms? (clearly these are not standard terms). \nAre we talking about separable filters? \nWhat are the X and Square symbols in Figure 2? And what are the horizontal and vertical sub-graphs standing for? What is x_1 and what is x_{11},x_{1,2},x_{1,3} and what is the relationship between them?\n\nI realize that to the authors these questions may seem to be trivial and left as  homework for the reader. But I think part of publishing a paper is doing a big part of the homework for the readers so that it becomes easy to get the idea. \n\nClearly the authors target the more general case - but spending some time to explain how the particular case is an instance of the the general case would be a good use of space. \n\nI would propose that the authors explain what are  x, x_{I}, and x_{S} for the simplest possible example, e.g. convolving a 1x5 signal with a 1x3 filter, how the convolution filter parameters show up in the function f, as well as how the spatial invariance (or, equivariance) of convolution is reflected here. "
  },
  {
    "people": [
      "Albrecht",
      "Joshua",
      "Rebecca Hwa"
    ],
    "review": "Noting the authors' concern about one of the reviewers, I read the paper myself and offer my own brief review.\n \n Evaluation is an extremely important question that does not get enough attention in the machine learning community, so the authors' effort is welcomed. The task that the authors are trying to evaluate is especially hard; in fact, it is not even clear to me how humans make these judgments. The low kappa scores on some of the non-\"overall\" dimensions, and only moderate agreement on \"overall,\" are quite worrisome. What makes a good \"chat\" dialogue? The authors seem not to have qualitatively grappled with this key question, rather defining it empirically as \"whatever our human judges think it is.\" This is, I think the deepest flaw of the work; the authors are rushing to automate evaluation without taking the time to ponder what good performance actually is. \n \n That aside, I think the idea of automatic evaluation as a modeling problem is worth studying. The authors note that this has been done for other problems, such as machine translation. They give only a cursory discussion of this prior work, however, and miss the seminal reference, \"Regression for sentence-level MT evaluation with pseudo references,\" Albrecht, Joshua, and Rebecca Hwa, ACL 2007.\n \n The paper would be much stronger with some robustness analysis; does the quality of the evaluation hold up if the design decisions are made differently, if less data are used to estimate the evaluation model, etc.? How does it hold up across datasets, and across different types of dialogue systems? As a methodological note, there are a lot of significance tests here and no mention of any attempt to correct for this (e.g., Bonferroni, FDR, etc.).\n \n As interesting as the ideas here are, I can't see the dialogue community rushing to adopt this approach to evaluation based on the findings in this paper. I do think that the ideas it presents should be hashed out in a public forum sooner rather than later, and therefore recommend it as one of a few papers to be presented at the workshop."
  },
  {
    "people": [
      "Liu"
    ],
    "review": "We thank the reviewers again for their feedback. While our detailed rebuttal to each reviewer is written as as a direct response to the review, we have now incorporated many of the reviewers' comments into an updated version of the paper. In particular, we have addressed the following items:\n\n1) We have made several alterations to alleviate possible misunderstandings from reading the paper. Most notably, we have added a paragraph to Section 4 explaining why the ADEM model is not a dialogue retrieval model. \n\n2) As recommended by Reviewer #1, we have produced two additional results that help clarify the workings of different parts of the model. In Table 3, we show correlations for the ADEM model when it can only compare the model response to the context (C-ADEM), and when it can only compare the model response to the reference response (R-ADEM). It seems that ADEM is mostly using the comparison to the reference response in order to assign its score. This makes sense, since the reference response is often closer semantically to the model response than the context, and goes to further illustrate how our model is different from a dialogue retrieval model (which could only use the context, i.e. C-ADEM). \n\n3) We provide new correlation results for word overlap metrics on the dataset from Liu et al. (2016). In particular, we standardized the pre-processing procedure by removing the "
  },
  {
    "people": [
      "Albrecht",
      "Joshua",
      "Rebecca Hwa"
    ],
    "review": "Noting the authors' concern about one of the reviewers, I read the paper myself and offer my own brief review.\n \n Evaluation is an extremely important question that does not get enough attention in the machine learning community, so the authors' effort is welcomed. The task that the authors are trying to evaluate is especially hard; in fact, it is not even clear to me how humans make these judgments. The low kappa scores on some of the non-\"overall\" dimensions, and only moderate agreement on \"overall,\" are quite worrisome. What makes a good \"chat\" dialogue? The authors seem not to have qualitatively grappled with this key question, rather defining it empirically as \"whatever our human judges think it is.\" This is, I think the deepest flaw of the work; the authors are rushing to automate evaluation without taking the time to ponder what good performance actually is. \n \n That aside, I think the idea of automatic evaluation as a modeling problem is worth studying. The authors note that this has been done for other problems, such as machine translation. They give only a cursory discussion of this prior work, however, and miss the seminal reference, \"Regression for sentence-level MT evaluation with pseudo references,\" Albrecht, Joshua, and Rebecca Hwa, ACL 2007.\n \n The paper would be much stronger with some robustness analysis; does the quality of the evaluation hold up if the design decisions are made differently, if less data are used to estimate the evaluation model, etc.? How does it hold up across datasets, and across different types of dialogue systems? As a methodological note, there are a lot of significance tests here and no mention of any attempt to correct for this (e.g., Bonferroni, FDR, etc.).\n \n As interesting as the ideas here are, I can't see the dialogue community rushing to adopt this approach to evaluation based on the findings in this paper. I do think that the ideas it presents should be hashed out in a public forum sooner rather than later, and therefore recommend it as one of a few papers to be presented at the workshop."
  },
  {
    "people": [
      "Liu"
    ],
    "review": "We thank the reviewers again for their feedback. While our detailed rebuttal to each reviewer is written as as a direct response to the review, we have now incorporated many of the reviewers' comments into an updated version of the paper. In particular, we have addressed the following items:\n\n1) We have made several alterations to alleviate possible misunderstandings from reading the paper. Most notably, we have added a paragraph to Section 4 explaining why the ADEM model is not a dialogue retrieval model. \n\n2) As recommended by Reviewer #1, we have produced two additional results that help clarify the workings of different parts of the model. In Table 3, we show correlations for the ADEM model when it can only compare the model response to the context (C-ADEM), and when it can only compare the model response to the reference response (R-ADEM). It seems that ADEM is mostly using the comparison to the reference response in order to assign its score. This makes sense, since the reference response is often closer semantically to the model response than the context, and goes to further illustrate how our model is different from a dialogue retrieval model (which could only use the context, i.e. C-ADEM). \n\n3) We provide new correlation results for word overlap metrics on the dataset from Liu et al. (2016). In particular, we standardized the pre-processing procedure by removing the "
  },
  {
    "people": [
      "Sukhbaatar"
    ],
    "review": "This paper introduces a simulator and a set of synthetic tasks for evaluating a dialogue agent's ability to learn from user feedback. For solving these tasks, the paper uses memory networks (Sukhbaatar et al., 2015) learned through previously proposed supervised learning and reinforcement learning methods. In this setup, it is demonstrated that the agent learning from feedback (e.g. through question asking or question clarification) performs better.\n\nThe motivation for the paper is excellent; dialogue agents which learn directly from unstructured human feedback (as opposed to reward signals alone) could be very useful in real-world applications. However, the paper falls short on the execution. All the numerous experiments presented are based on the synthetic dialogue simulator, which is highly artificial and different from real-world dialogues. The simulator is based on a simple factoid question-answering framework, which normally is not considered dialogue and which appears to be solvable with a few hand-crafted rules. The framework also assumes that the user's feedback is always correct and is given in one of a handful of forms (e.g. paraphrase of original question without typos) and that the agent can learn from examples of another agent asking questions or making clarifications, which simplifies the task even further.\n\nBecause of the artificial setting and limited scope of the experiments, it seems difficult to draw conclusions about how to learn from unstructured user feedback. To test the hypothesis that it is possible to learn from such user feedback, I would strongly recommend the authors to continue working on this project by carrying out experiments with real human users (even in the factoid question answering domain, if necessary). This would provide much stronger evidence that a dialogue agent can learn from such feedback.\n\n\nOther comments:\n- The abstract uses the phrase \"interactive dialogue agents\". What is meant by \"interactive\" dialogue agents? All dialogue agents interact with the user, so isn't it redundant to call them interactive?\n- A major limitation of the experiments is that the questions the agent can ask are specified a priori. If I understand correctly, in the supervised learning setting the agent is trained to imitate the questions of another rule-based agent. While in the RL setting, the paper states \"For each dialogue, the bot takes two sequential actions $(a_1 , a_2)$: to ask or not to ask a question (denoted as a_1 ); and guessing the final answer (denoted as a_2)\". This means the agent learns *when* to ask questions but not *what* questions to ask.\n- Related to the previous comment, in the sub-section \"ONLINE REINFORCEMENT LEARNING (RL)\" the paper states \"We also explored scenarios where the student learns the ability to decide when to ask a question and what to ask.\". Please clarify this by removing the part \"what to ask\".\n- The paper presents an overwhelming amount of results. I understand the benefit of synthetic tasks is precisely the ability to measure many aspects of model performance, but in this case it confuses the reader to present so many results. For example, what was the reason for including the \"TrainAQ(+FP)\" and \"TrainMix\" training settings? How do these results help validate the original hypothesis? If they don't, they should be taken out or moved to the appendix.\n- Since the contribution of the paper lies in the tasks and evaluation, it might be better to move either the vanilla-MemN2N (Table 2) to the appendix or to move the Cont-MemN2N results (Table 3) to the appendix.\n\n--- UPDATE ---\n\nFollowing the discussion below and the additional experiments provided by the authors, I have increased my score to 8."
  },
  {
    "people": [
      "Sukhbaatar"
    ],
    "review": "This paper introduces a simulator and a set of synthetic tasks for evaluating a dialogue agent's ability to learn from user feedback. For solving these tasks, the paper uses memory networks (Sukhbaatar et al., 2015) learned through previously proposed supervised learning and reinforcement learning methods. In this setup, it is demonstrated that the agent learning from feedback (e.g. through question asking or question clarification) performs better.\n\nThe motivation for the paper is excellent; dialogue agents which learn directly from unstructured human feedback (as opposed to reward signals alone) could be very useful in real-world applications. However, the paper falls short on the execution. All the numerous experiments presented are based on the synthetic dialogue simulator, which is highly artificial and different from real-world dialogues. The simulator is based on a simple factoid question-answering framework, which normally is not considered dialogue and which appears to be solvable with a few hand-crafted rules. The framework also assumes that the user's feedback is always correct and is given in one of a handful of forms (e.g. paraphrase of original question without typos) and that the agent can learn from examples of another agent asking questions or making clarifications, which simplifies the task even further.\n\nBecause of the artificial setting and limited scope of the experiments, it seems difficult to draw conclusions about how to learn from unstructured user feedback. To test the hypothesis that it is possible to learn from such user feedback, I would strongly recommend the authors to continue working on this project by carrying out experiments with real human users (even in the factoid question answering domain, if necessary). This would provide much stronger evidence that a dialogue agent can learn from such feedback.\n\n\nOther comments:\n- The abstract uses the phrase \"interactive dialogue agents\". What is meant by \"interactive\" dialogue agents? All dialogue agents interact with the user, so isn't it redundant to call them interactive?\n- A major limitation of the experiments is that the questions the agent can ask are specified a priori. If I understand correctly, in the supervised learning setting the agent is trained to imitate the questions of another rule-based agent. While in the RL setting, the paper states \"For each dialogue, the bot takes two sequential actions $(a_1 , a_2)$: to ask or not to ask a question (denoted as a_1 ); and guessing the final answer (denoted as a_2)\". This means the agent learns *when* to ask questions but not *what* questions to ask.\n- Related to the previous comment, in the sub-section \"ONLINE REINFORCEMENT LEARNING (RL)\" the paper states \"We also explored scenarios where the student learns the ability to decide when to ask a question and what to ask.\". Please clarify this by removing the part \"what to ask\".\n- The paper presents an overwhelming amount of results. I understand the benefit of synthetic tasks is precisely the ability to measure many aspects of model performance, but in this case it confuses the reader to present so many results. For example, what was the reason for including the \"TrainAQ(+FP)\" and \"TrainMix\" training settings? How do these results help validate the original hypothesis? If they don't, they should be taken out or moved to the appendix.\n- Since the contribution of the paper lies in the tasks and evaluation, it might be better to move either the vanilla-MemN2N (Table 2) to the appendix or to move the Cont-MemN2N results (Table 3) to the appendix.\n\n--- UPDATE ---\n\nFollowing the discussion below and the additional experiments provided by the authors, I have increased my score to 8."
  },
  {
    "people": [
      "Sukhbaatar"
    ],
    "review": "This paper introduces a simulator and a set of synthetic tasks for evaluating a dialogue agent's ability to learn from user feedback. For solving these tasks, the paper uses memory networks (Sukhbaatar et al., 2015) learned through previously proposed supervised learning and reinforcement learning methods. In this setup, it is demonstrated that the agent learning from feedback (e.g. through question asking or question clarification) performs better.\n\nThe motivation for the paper is excellent; dialogue agents which learn directly from unstructured human feedback (as opposed to reward signals alone) could be very useful in real-world applications. However, the paper falls short on the execution. All the numerous experiments presented are based on the synthetic dialogue simulator, which is highly artificial and different from real-world dialogues. The simulator is based on a simple factoid question-answering framework, which normally is not considered dialogue and which appears to be solvable with a few hand-crafted rules. The framework also assumes that the user's feedback is always correct and is given in one of a handful of forms (e.g. paraphrase of original question without typos) and that the agent can learn from examples of another agent asking questions or making clarifications, which simplifies the task even further.\n\nBecause of the artificial setting and limited scope of the experiments, it seems difficult to draw conclusions about how to learn from unstructured user feedback. To test the hypothesis that it is possible to learn from such user feedback, I would strongly recommend the authors to continue working on this project by carrying out experiments with real human users (even in the factoid question answering domain, if necessary). This would provide much stronger evidence that a dialogue agent can learn from such feedback.\n\n\nOther comments:\n- The abstract uses the phrase \"interactive dialogue agents\". What is meant by \"interactive\" dialogue agents? All dialogue agents interact with the user, so isn't it redundant to call them interactive?\n- A major limitation of the experiments is that the questions the agent can ask are specified a priori. If I understand correctly, in the supervised learning setting the agent is trained to imitate the questions of another rule-based agent. While in the RL setting, the paper states \"For each dialogue, the bot takes two sequential actions $(a_1 , a_2)$: to ask or not to ask a question (denoted as a_1 ); and guessing the final answer (denoted as a_2)\". This means the agent learns *when* to ask questions but not *what* questions to ask.\n- Related to the previous comment, in the sub-section \"ONLINE REINFORCEMENT LEARNING (RL)\" the paper states \"We also explored scenarios where the student learns the ability to decide when to ask a question and what to ask.\". Please clarify this by removing the part \"what to ask\".\n- The paper presents an overwhelming amount of results. I understand the benefit of synthetic tasks is precisely the ability to measure many aspects of model performance, but in this case it confuses the reader to present so many results. For example, what was the reason for including the \"TrainAQ(+FP)\" and \"TrainMix\" training settings? How do these results help validate the original hypothesis? If they don't, they should be taken out or moved to the appendix.\n- Since the contribution of the paper lies in the tasks and evaluation, it might be better to move either the vanilla-MemN2N (Table 2) to the appendix or to move the Cont-MemN2N results (Table 3) to the appendix.\n\n--- UPDATE ---\n\nFollowing the discussion below and the additional experiments provided by the authors, I have increased my score to 8."
  },
  {
    "people": [
      "Sukhbaatar"
    ],
    "review": "This paper introduces a simulator and a set of synthetic tasks for evaluating a dialogue agent's ability to learn from user feedback. For solving these tasks, the paper uses memory networks (Sukhbaatar et al., 2015) learned through previously proposed supervised learning and reinforcement learning methods. In this setup, it is demonstrated that the agent learning from feedback (e.g. through question asking or question clarification) performs better.\n\nThe motivation for the paper is excellent; dialogue agents which learn directly from unstructured human feedback (as opposed to reward signals alone) could be very useful in real-world applications. However, the paper falls short on the execution. All the numerous experiments presented are based on the synthetic dialogue simulator, which is highly artificial and different from real-world dialogues. The simulator is based on a simple factoid question-answering framework, which normally is not considered dialogue and which appears to be solvable with a few hand-crafted rules. The framework also assumes that the user's feedback is always correct and is given in one of a handful of forms (e.g. paraphrase of original question without typos) and that the agent can learn from examples of another agent asking questions or making clarifications, which simplifies the task even further.\n\nBecause of the artificial setting and limited scope of the experiments, it seems difficult to draw conclusions about how to learn from unstructured user feedback. To test the hypothesis that it is possible to learn from such user feedback, I would strongly recommend the authors to continue working on this project by carrying out experiments with real human users (even in the factoid question answering domain, if necessary). This would provide much stronger evidence that a dialogue agent can learn from such feedback.\n\n\nOther comments:\n- The abstract uses the phrase \"interactive dialogue agents\". What is meant by \"interactive\" dialogue agents? All dialogue agents interact with the user, so isn't it redundant to call them interactive?\n- A major limitation of the experiments is that the questions the agent can ask are specified a priori. If I understand correctly, in the supervised learning setting the agent is trained to imitate the questions of another rule-based agent. While in the RL setting, the paper states \"For each dialogue, the bot takes two sequential actions $(a_1 , a_2)$: to ask or not to ask a question (denoted as a_1 ); and guessing the final answer (denoted as a_2)\". This means the agent learns *when* to ask questions but not *what* questions to ask.\n- Related to the previous comment, in the sub-section \"ONLINE REINFORCEMENT LEARNING (RL)\" the paper states \"We also explored scenarios where the student learns the ability to decide when to ask a question and what to ask.\". Please clarify this by removing the part \"what to ask\".\n- The paper presents an overwhelming amount of results. I understand the benefit of synthetic tasks is precisely the ability to measure many aspects of model performance, but in this case it confuses the reader to present so many results. For example, what was the reason for including the \"TrainAQ(+FP)\" and \"TrainMix\" training settings? How do these results help validate the original hypothesis? If they don't, they should be taken out or moved to the appendix.\n- Since the contribution of the paper lies in the tasks and evaluation, it might be better to move either the vanilla-MemN2N (Table 2) to the appendix or to move the Cont-MemN2N results (Table 3) to the appendix.\n\n--- UPDATE ---\n\nFollowing the discussion below and the additional experiments provided by the authors, I have increased my score to 8."
  },
  {
    "people": [
      "Vicente O",
      "Joseph R"
    ],
    "review": "This paper presents a ternary quantization method for convolutional neural networks. All weights are represented by ternary values multiplied by two scaling coefficients. Both ternary weights and the scaling coefficients are updated using back-propagation. This is useful for CNN model compression. Experiments on AlexNet show that the proposed method is superior Ternary-Weight-Networks (TWN) and DoReFa-Net. This work has the following strengths and weaknesses.\n\nStrengths:\n(1). Good results are shown on CIFAR-10 dataset.\n\n(2). Massive energy saving of the ternary weights comparing to 32-bit weights.\n\n(3). It is well written, and easy to understand.\n\nWeaknesses:\n\n(1). It seems that this work is an incremental improvement on the existing works. The main difference from Binary-Weight-Networks (BWN) proposed in XNOR-net[1] is using ternary weights instead of binary weights, while ternary weights have been used by many previous works. Both BWN and the proposed method in this paper learn the scaling factors during training. Comparing to Ternary-Weight-Networks (TWN), the main difference is that two independent quantization factors are used for positive and negative weights, while TWN utilizes the same scaling factor for all weights. \n\n(2). In the experiment, the authors did not process the first conv layer and the last fully-connected layer. The results of processing all layers should be given for fair comparison. \n\n(3). In the experiment, comparison with BWN is not reported. In [1], the top-1 and top-5 error of AlexNet on ImageNet of BWN is 43.2% and 20.6%, which is comparable with the method proposed in this paper (42.5% and 20.3%). However, the BWN only uses binary weights and all layers are binarized including the first conv layer and the last fully-connected layer. \n\n(4). For the baseline method of full precision alexnet (with BN), the reported accuracy seems to be too low (44.1% top-1 error). Commonly, using batch normalization can boost the accuracy, while the reported accuracy are much lower than alexnet without batch normalization. On the other hand, the error rates of pre-trained model of alexnet (with BN) reported by the official MatConvNet [2] are 41.8% and 19.2%. \n\n(5). The proposed method should be evaluated on the original AlexNet, whose accuracy is publicly available for almost all deep learning frameworks like caffe. Moreover, more experiments should be added on ImageNet, like VGG-S, VGG-16, GoogleNet or ResNet.\n\n(6). In previous methods such as XNOR-net and TWN, most of the 32-bit multiply operation can be replaced by addition by using binary or ternary weights. However, the proposed method in this paper utilizes independent scaling factors for positive and negative weights. Thus it seems that the multiply operation can not be replaced by addition. \n\n\nReferences:\n[1] Mohammad R, Vicente O, Joseph R, Ali F. XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks. ECCV 2016\n[2] "
  },
  {
    "people": [
      "Vicente O",
      "Joseph R"
    ],
    "review": "This paper presents a ternary quantization method for convolutional neural networks. All weights are represented by ternary values multiplied by two scaling coefficients. Both ternary weights and the scaling coefficients are updated using back-propagation. This is useful for CNN model compression. Experiments on AlexNet show that the proposed method is superior Ternary-Weight-Networks (TWN) and DoReFa-Net. This work has the following strengths and weaknesses.\n\nStrengths:\n(1). Good results are shown on CIFAR-10 dataset.\n\n(2). Massive energy saving of the ternary weights comparing to 32-bit weights.\n\n(3). It is well written, and easy to understand.\n\nWeaknesses:\n\n(1). It seems that this work is an incremental improvement on the existing works. The main difference from Binary-Weight-Networks (BWN) proposed in XNOR-net[1] is using ternary weights instead of binary weights, while ternary weights have been used by many previous works. Both BWN and the proposed method in this paper learn the scaling factors during training. Comparing to Ternary-Weight-Networks (TWN), the main difference is that two independent quantization factors are used for positive and negative weights, while TWN utilizes the same scaling factor for all weights. \n\n(2). In the experiment, the authors did not process the first conv layer and the last fully-connected layer. The results of processing all layers should be given for fair comparison. \n\n(3). In the experiment, comparison with BWN is not reported. In [1], the top-1 and top-5 error of AlexNet on ImageNet of BWN is 43.2% and 20.6%, which is comparable with the method proposed in this paper (42.5% and 20.3%). However, the BWN only uses binary weights and all layers are binarized including the first conv layer and the last fully-connected layer. \n\n(4). For the baseline method of full precision alexnet (with BN), the reported accuracy seems to be too low (44.1% top-1 error). Commonly, using batch normalization can boost the accuracy, while the reported accuracy are much lower than alexnet without batch normalization. On the other hand, the error rates of pre-trained model of alexnet (with BN) reported by the official MatConvNet [2] are 41.8% and 19.2%. \n\n(5). The proposed method should be evaluated on the original AlexNet, whose accuracy is publicly available for almost all deep learning frameworks like caffe. Moreover, more experiments should be added on ImageNet, like VGG-S, VGG-16, GoogleNet or ResNet.\n\n(6). In previous methods such as XNOR-net and TWN, most of the 32-bit multiply operation can be replaced by addition by using binary or ternary weights. However, the proposed method in this paper utilizes independent scaling factors for positive and negative weights. Thus it seems that the multiply operation can not be replaced by addition. \n\n\nReferences:\n[1] Mohammad R, Vicente O, Joseph R, Ali F. XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks. ECCV 2016\n[2] "
  },
  {
    "people": [
      "Sainath",
      "Yu",
      "Deng",
      "L.",
      "Dahl",
      "Dahl",
      "Yu",
      "Deng",
      "L.",
      "Acero"
    ],
    "review": "This paper aims at attacking the problem of preselecting deep learning model structures for new domains. It reported a series of experiments on various small tasks and feed-forward DNNs. It claims that some ranking algorithm can be learned based on these results to guide the selection of model structures for new domains.\n\nAlthough the goal is interesting I found their conclusion is neither convincing nor useful in practice for several reasons:\n\n1. They only explored really simple networks (feed-forward DNNs). While this significantly limited the search space, it also limited the value of the experiments. In fact, the best model architecture is highly task (domain) dependent and the type of model (DNN vs CNN vs LSTM) is often much more important than size of the network itself.\n2. Their experiments were conduced with some important hyper parameters (e.g., learning rate schedule) fixed. However, it is well known  that learning rate often is the most important hyper parameter during training. Without adjusting these important hyper parameters the conclusion on the best model architecture is not convincing.\n3. Their experiments seem to indicate that the training data difference is not important. However, this is unlikely to be true as you would definitely want to use larger models (total number of parameters) when your training set is magnitude larger (i.e., log(datasize) can be an important feature). This is likely because they did not run experiments on large datasets.\n\nIn addition, I think the title of the paper does not accurately reflect what the paper is about and should be modified. Also, this paper cited Sainath et al. 2015 as the work that leads to breakthrough in speech recognition. However, the breakthrough in ASR happened much earlier. The first paper with all three key components was published in 2010:\n\nYu, D., Deng, L. and Dahl, G., 2010, December. Roles of pre-training and fine-tuning in context-dependent DBN-HMMs for real-world speech recognition. In Proc. NIPS Workshop on Deep Learning and Unsupervised Feature Learning.\n\nand the more detailed paper was published in 2012\n\nDahl, G.E., Yu, D., Deng, L. and Acero, A., 2012. Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. IEEE Transactions on Audio, Speech, and Language Processing, 20(1), pp.30-42.\n\nAs a conclusion, this paper presented some very preliminary result. Although it's interesting it's not ready for publishing."
  },
  {
    "people": [
      "Sainath",
      "Yu",
      "Deng",
      "L.",
      "Dahl",
      "Dahl",
      "Yu",
      "Deng",
      "L.",
      "Acero"
    ],
    "review": "This paper aims at attacking the problem of preselecting deep learning model structures for new domains. It reported a series of experiments on various small tasks and feed-forward DNNs. It claims that some ranking algorithm can be learned based on these results to guide the selection of model structures for new domains.\n\nAlthough the goal is interesting I found their conclusion is neither convincing nor useful in practice for several reasons:\n\n1. They only explored really simple networks (feed-forward DNNs). While this significantly limited the search space, it also limited the value of the experiments. In fact, the best model architecture is highly task (domain) dependent and the type of model (DNN vs CNN vs LSTM) is often much more important than size of the network itself.\n2. Their experiments were conduced with some important hyper parameters (e.g., learning rate schedule) fixed. However, it is well known  that learning rate often is the most important hyper parameter during training. Without adjusting these important hyper parameters the conclusion on the best model architecture is not convincing.\n3. Their experiments seem to indicate that the training data difference is not important. However, this is unlikely to be true as you would definitely want to use larger models (total number of parameters) when your training set is magnitude larger (i.e., log(datasize) can be an important feature). This is likely because they did not run experiments on large datasets.\n\nIn addition, I think the title of the paper does not accurately reflect what the paper is about and should be modified. Also, this paper cited Sainath et al. 2015 as the work that leads to breakthrough in speech recognition. However, the breakthrough in ASR happened much earlier. The first paper with all three key components was published in 2010:\n\nYu, D., Deng, L. and Dahl, G., 2010, December. Roles of pre-training and fine-tuning in context-dependent DBN-HMMs for real-world speech recognition. In Proc. NIPS Workshop on Deep Learning and Unsupervised Feature Learning.\n\nand the more detailed paper was published in 2012\n\nDahl, G.E., Yu, D., Deng, L. and Acero, A., 2012. Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. IEEE Transactions on Audio, Speech, and Language Processing, 20(1), pp.30-42.\n\nAs a conclusion, this paper presented some very preliminary result. Although it's interesting it's not ready for publishing."
  },
  {
    "people": [
      "Ogawa",
      "Tetsuji",
      "Hermansky",
      "Hynek",
      "Variani",
      "Ehsan"
    ],
    "review": "The authors propose the use of statistics of softmax outputs to estimate the probability of error and probability of a test sample being out-of-domain. They contrast the performance of the proposed method with directly using the softmax output probabilities, and not their statistics, as a measure of confidence.\n\nIt would be great if the authors elaborate on the idea of ignoring the logit of the blank symbol.\n\nIt would be interesting to see the performance of the proposed methods in more confusable settings, ie., in cases where the out-of-domain examples are more similar to the in-domain examples. e.g., in the case of speech recognition this might correspond to using a different language's speech with an ASR system trained in a particular language. Here the acoustic characteristics of the speech signals from two different languages might be more similar as compared to the noisy and clean speech signals from the same language.\n\nIn section 4, the description of the auxiliary decoder setup might benefit from more detail.\n\nThere has been recent work on performance monitoring and accuracy prediction in the area of speech recognition, some of this work is listed below. \n1. Ogawa, Tetsuji, et al. \"Delta-M measure for accuracy prediction and its application to multi-stream based unsupervised adaptation.\" Proceedings of ICASSP. 2015.\n\n2. Hermansky, Hynek, et al. \"Towards machines that know when they do not know.\" Proceedings of ICASSP, 2015.\n\n3. Variani, Ehsan et al. \"Multi-stream recognition of noisy speech with performance monitoring.\" INTERSPEECH. 2013."
  },
  {
    "people": [
      "Ogawa",
      "Tetsuji",
      "Hermansky",
      "Hynek",
      "Variani",
      "Ehsan"
    ],
    "review": "The authors propose the use of statistics of softmax outputs to estimate the probability of error and probability of a test sample being out-of-domain. They contrast the performance of the proposed method with directly using the softmax output probabilities, and not their statistics, as a measure of confidence.\n\nIt would be great if the authors elaborate on the idea of ignoring the logit of the blank symbol.\n\nIt would be interesting to see the performance of the proposed methods in more confusable settings, ie., in cases where the out-of-domain examples are more similar to the in-domain examples. e.g., in the case of speech recognition this might correspond to using a different language's speech with an ASR system trained in a particular language. Here the acoustic characteristics of the speech signals from two different languages might be more similar as compared to the noisy and clean speech signals from the same language.\n\nIn section 4, the description of the auxiliary decoder setup might benefit from more detail.\n\nThere has been recent work on performance monitoring and accuracy prediction in the area of speech recognition, some of this work is listed below. \n1. Ogawa, Tetsuji, et al. \"Delta-M measure for accuracy prediction and its application to multi-stream based unsupervised adaptation.\" Proceedings of ICASSP. 2015.\n\n2. Hermansky, Hynek, et al. \"Towards machines that know when they do not know.\" Proceedings of ICASSP, 2015.\n\n3. Variani, Ehsan et al. \"Multi-stream recognition of noisy speech with performance monitoring.\" INTERSPEECH. 2013."
  },
  {
    "people": [
      "Bahdanau",
      "Xu",
      "Pan"
    ],
    "review": "This paper addresses video captioning with a TEM-HAM architecture, where a HAM module attends over attended outputs of the TEM module when generating the description. This gives a kind of 2-level attention. The model is evaluated on the Charades and MSVD datasets.\n\n1. Quality/Clarify: I found this paper to be poorly written and relatively hard to understand. As far as I can tell the TEM module of Section 3.1 is a straight-forward attention frame encoder of Bahdanau et al. 2015 or Xu et al. 2015. The decoder of Section 3.3 is a standard LSTM with log likelihood. The HAM module of Section 3.2 is the novel module but is not very well described. It looks to be an attention LSTM where the attention is over the TEM LSTM outputs, but the attention weights are additionally conditioned on the decoder state. There are a lot of small problems with the description, such as notational discrepancy in using \\textbf in equations and then not using it in the text. Also, I spent a long time trying to understand what f_m is. The authors say: \n\n\"In order to let the network remember what has been attended before and the temporal\nstructure of a video, we propose f_m to memorize the previous attention and encoded version of an\ninput video with language model. Using f_m not only enables the network to memorize previous\nattention and frames, but also to learn multi-layer attention over an input video and corresponding\nlanguage.\"\n\nWhere one f_m is bold and the other f_m is not. Due to words such as \"we propose f_m\" assumed this was some kind of a novel technical contribution I couldn't find any details about but it is specified later in Section 3.3 at the end that f_m is in fact just an LSTM. It's not clear why this piece of information is in Section 3.3, which discusses the decoder. The paper is sloppy in other parts. For example in Table 1 some numbers have 1 significant digit and some have 2. The semantics of the horizontal line in Table 2 are not explained in text. \n\n2. Experimental results: The ablation study shows mixed results when adding TEM and HAM to the model. Looking at METEOR which was shown to have the highest correlation to humans in the COCO paper compared to the other evaluation criteria, adding TEM+HAM improves the model from 31.20 to 31.70. It is not clear how significant this improvement is, especially given that the test set is only 670 videos. I have doubts over this result. In Table 2, the METEOR score of Pan et al. 2016a is higher [33.10 vs. 31.80], but this discrepancy is not addressed in text. This is surprising because the authors explicitly claim \"state of the art results\".\n\n3. Originality/Significance: The paper introduces an additional layer of attention over a more standard sequence to sequence setup, which is argued to alleviate the burden on the LSTM's memory. This is moderately novel but I don't believe that the experimental results make it sufficiently clear that it is also worth doing. If the paper made the standard model somehow simpler instead of more complex I would be more inclined to judge it favorably.\n\n\nMinor:\nIn response to the author's comment \"not sure what causes to think of RBM. We don't model any part of our architecture using RBM. We'd be appreciated if you please elaborate more about your confusion about figure 1 so we can address it accordingly.\", I created a diagram to hopefully make this more clear:"
  },
  {
    "people": [
      "Bahdanau"
    ],
    "review": "The authors propose a \"hierarchical\" attention model for video captioning.  They introduce a model composed of three parts: the temporal modeler (TEM) that takes as input the video sequence and outputs a sequential representation of the video to the HAM; the hierarchical attention/memory mechanism (HAM) implements a soft-attention mechanism over the sequential video representation; and finally a decoder that generates a caption. \n\nRelated to the second series of questions above, it seems as though the authors have chosen to refer to their use of an LSTM (or equivalent RNN) as the output of the Bahdanau et al (2015) attention mechanism as a hierarchical memory mechanism. I am actually sympathetic to this terminology in the sense that the recent popularity of memory-based models seems to neglect the memory implicit in the LSTM state vector, but that said, this seems to seriously misrepresent the significance fo the contribution of this paper. \n\nI appreciate the ablation study presented in Table 1. Not enough researchers bother with this kind of analysis. But it does show that the value of the contributions is not actually clear. In particular the case for the TEM is quite weak.\n\nRegarding the quantitative evaluation presented in Table 2, the authors are carving out a fairly specific set of features to describe the set of \"fair\" comparators from the literature. Given the variability of the models and alternate training datasets that are in use, I would find it more compelling if the authors just set about trying to achieve the best results they can, if that includes the fine-tuning of the frame model, so be it. The value of this work is as an application paper, so the discovery and incorporation of elements that can significantly improve performance would seems warranted. \n\nOverall, at this point, I do not see a sufficient contribution to warrant publication in ICLR.\n"
  },
  {
    "people": [
      "Xu",
      "Xu",
      "Yao",
      "Xu",
      "Ballas",
      "Yao",
      "Xu",
      "Pan",
      "Yu",
      "Charades",
      "Charades"
    ],
    "review": "The paper proposes an attention-based approach for video description. The approach uses three LSTMs and two attention mechanisms to sequentially predict words from a sequence of frames.\nIn the LSTM-encoder of the frames (TEM), the first attention approach predicts a spatial attention per frame, and computes the weighted average. The second LSTM (HAM) predicts an attention over the hidden states of the encoder LSTM.\nThe third LSTM which run temporally in parallel to the second LSTM generates the sentence, one word at a time.\n\n\nStrength:\n===============\n\n-\tThe paper works on a relevant and interesting problem.\n-\tUsing 2 layers of attention in the proposed way have to my knowledge not been used before for video description. The exact architecture is thus novel (but the work claims much more without sufficient attribution, see blow)\n-\tThe experiments are evaluated on two datasets, MSVD and Charades, showing performance on the level of related work for MSVD and improvements for Charades.\n\nWeaknesses:\n===============\n\n1.\tClaims about the contribution/novelty of the model seem not to hold: \n1.1.\tOne of the main contributions is the Hierarchical Attention/Memory (HAM):\n1.1.1.\tIt is not clear to me how the presented model (Eq 6-8), are significantly different from the presented model in Xu et al / Yao et al. While Xu et al. attends over spatial image locations and Yao et al. attend over frames, this model attends over encoded video representations h_v^i. A slight difference might be that Xu et al. use the same LSTM to generate, while this model uses an additional LSTM for the decoding.\n1.1.2.\tThe paper states in section 3.2 \u201cwe propose f_m to memorize the previous attention\u201d, however H_m^{t\u2019-1} only consist of the last hidden state. Furthermore, the model f_m does not have access to the \u201cattention\u201d \\alpha. This was also discussed in comments by others, but remains unclear.\n1.1.3.\tIn the discussion of comments the authors claim that \u201cattention not only is a function a current time step but also a function of all previous attentions and network states.\u201d: While it is true that there is a dependency but that is true also for any LSTM, however the model does not have access to the previous network states as H_g^{t\u2019-1} only consist of the last hidden state, as well as H_m^{t\u2019-1} [at least that is what the formulas say and what Figure 1 suggests]. \n1.1.4.\tThe authors claim to have multi-layer attention in HAM, however it remains unclear where the multi-layer comes from.\n1.2.\tThe paper states that in section 3.1. \u201c[CNN] features tend to discard the low level information useful in modeling the motion in the video (Ballas et al., 2016).\u201d This suggests that the approach which follows attacks this problem. However, it cannot model motion as attention \\rho between frames is not available when predicting the next frame. Also, it is not clear how the model can capture anything \u201clow level\u201d as it operates on rather high level VGG conv 5 features.\n\n2.\tRelated work: The difference of HAM to Yao et al. and Xu et al. should be made more clear / or these papers should be cited in the HAM section.\n\n3.\tConceptual Limitation of the model: The model has two independent attention mechanisms, a spatial one, and a temporal one. The spatial (within a frame) is independent of the sentence generation. It thus cannot attend to different aspects of the frames for different words which would make sense. E.g. if the sentence is \u201cthe dog jumps on the trampoline\u201d, the model should focus on the dog when saying \u201cdog\u201d and on the trampoline when saying \u201ctrampoline\u201d, however, as the spatial attention is fixed this is difficult. Also, the encoder model does not have an explicitly way to look at different aspects in the frame during the encoding so might likely get stuck and always predict the same spatial attention for all frames (or it might e.g. always attend to the dog which moves around, but never on the scene).\n\n4.\tEq 11 contradicts Fig 1: How is the model exactly receiving the previous word as input. Eq. 11 suggests it is the softmax. If this is the case, the authors should emphasize this in the text as this is unusual. More common would be to use the ground truth previous word during training (which Fig 11 suggests) and the \u201chardmax\u201d, i.e. the highest predicted previous word encoded as one-hot vector at test time.\n\n5.\tClarity:\n5.1.\tIt would be helpful if the same notation would be used in Eq 2-5 and 6-9. Why is a different notation required?\n5.2.\tIt would be helpful if Fig 1 could contain more details or additional figures for the corresponding parts would be added. If space is a problem, e.g. the well-known equations for LSTM, softmax (Eq 2), and log likelihood loss (Eq 12) could be omitted or inlined.\n\n6.\tEvaluation:\n6.1.\tThe paper claims that the \u201cthe proposed architecture outperforms all previously proposed methods and leads to a new state of the art results\u201d.\n6.1.1.\tFor the MSVD dataset this clearly is wrong, even given the same feature representation. Pan et al. (2016 a) in Table 2 achieve higher METEOR (33.10).\n6.1.2.\tFor this strong claim, I would also expect that it outperforms all previous results independent of the features used, which is also wrong again, Yu et al achieve higher performance in all compared metrics.\n6.1.3.\tFor Charades dataset, this claim is also too bold as hardly any methods have been evaluated on this dataset, so at least all the ablations reported in Table 1 should also be reported for the Charades dataset, to make for this dataset any stronger claims.\n6.2.\tMissing qualitative results of attention: The authors should show qualitative results of the attention, for both attention mechanisms to understand if anything sensible is happening there. How diverse is the spatial and the temporal attention? Is it peaky or rather uniform?\n6.3.\tPerformance improvement is not significant over model ablations: The improvements over Att+No TEM is only 0.5 Meteor, 0.7 Blue@4 and the performance drops for CIDEr by 1.7.\n6.4.\tMissing human evaluation: I disagree with the authors that a human evaluation is not feasible. 1. An evaluation on a subset of the test data is not so difficult. 2. Even if other authors do not provide their code/model [and some do], they are typically happy to share the predicted sentences which is sufficient and even better for human evaluation [if not I would explicitly mention that some authors did not share sentences, as this seems clearly wrong]. 3. For model ablations the sentences are available to the authors.\n\n7.\tSeveral of the comments raised by reviewers/others have not yet been incorporated in a revised version of the paper and/or are still not clear from the explanations given. E.g. including SPICE evaluation and making fixes seems trivial.\t\n\n8.\tHyperparameters are inconsistent: Why are the hyperparemters inconsistent between the ablation analysis (40 frames are sampled) and the performance comparison (8 frames)? Should this not be selected on the validation set? What is the performance of all the ablations with 8 frames?\n\nOther (minor/discussion points)\n-\tEquation 10: what happens with h_m, and h_g, the LSTM formulas provided only handle two inputs. Are h_m and h_g concatenated.\n-\tThere is a section 4.1 but no 4.2.\n-\tThe paper states in section 4.1 \u201cour proposed architecture can alone not only learn a representation for video that can model the temporal structure of a video sequence, but also a representation that can effectively map visual space to the language space.\u201d However, this seems to be true also for many/most other approaches, e.g. [Venugopalan et al. 2015 ICCV]\n\nSummary:\n===============\n\nWhile the paper makes strong claims w.r.t. to the approach and results, the approach lacks novelty and the results are not convincing over related work and ablations. Furthermore, improved clarity and visualizations of the model and attention results would benefit the paper.\n"
  },
  {
    "people": [
      "Bahdanau",
      "Xu",
      "Pan"
    ],
    "review": "\nThis paper addresses video captioning with a TEM-HAM architecture, where a HAM module attends over attended outputs of the TEM module when generating the description. This gives a kind of 2-level attention. The model is evaluated on the Charades and MSVD datasets.\n\n1. Quality/Clarify: I found this paper to be poorly written and relatively hard to understand. As far as I can tell the TEM module of Section 3.1 is a straight-forward attention frame encoder of Bahdanau et al. 2015 or Xu et al. 2015. The decoder of Section 3.3 is a standard LSTM with log likelihood. The HAM module of Section 3.2 is the novel module but is not very well described. It looks to be an attention LSTM where the attention is over the TEM LSTM outputs, but the attention weights are additionally conditioned on the decoder state. There are a lot of small problems with the description, such as notational discrepancy in using \\textbf in equations and then not using it in the text. Also, I spent a long time trying to understand what f_m is. The authors say: \n\n\"In order to let the network remember what has been attended before and the temporal\nstructure of a video, we propose f_m to memorize the previous attention and encoded version of an\ninput video with language model. Using f_m not only enables the network to memorize previous\nattention and frames, but also to learn multi-layer attention over an input video and corresponding\nlanguage.\"\n\nWhere one f_m is bold and the other f_m is not. Due to words such as \"we propose f_m\" assumed this was some kind of a novel technical contribution I couldn't find any details about but it is specified later in Section 3.3 at the end that f_m is in fact just an LSTM. It's not clear why this piece of information is in Section 3.3, which discusses the decoder. The paper is sloppy in other parts. For example in Table 1 some numbers have 1 significant digit and some have 2. The semantics of the horizontal line in Table 2 are not explained in text. \n\n2. Experimental results: The ablation study shows mixed results when adding TEM and HAM to the model. Looking at METEOR which was shown to have the highest correlation to humans in the COCO paper compared to the other evaluation criteria, adding TEM+HAM improves the model from 31.20 to 31.70. It is not clear how significant this improvement is, especially given that the test set is only 670 videos. I have doubts over this result. In Table 2, the METEOR score of Pan et al. 2016a is higher [33.10 vs. 31.80], but this discrepancy is not addressed in text. This is surprising because the authors explicitly claim \"state of the art results\".\n\n3. Originality/Significance: The paper introduces an additional layer of attention over a more standard sequence to sequence setup, which is argued to alleviate the burden on the LSTM's memory. This is moderately novel but I don't believe that the experimental results make it sufficiently clear that it is also worth doing. If the paper made the standard model somehow simpler instead of more complex I would be more inclined to judge it favorably.\n\n\nMinor:\nIn response to the author's comment \"not sure what causes to think of RBM. We don't model any part of our architecture using RBM. We'd be appreciated if you please elaborate more about your confusion about figure 1 so we can address it accordingly.\", I created a diagram to hopefully make this more clear:  "
  },
  {
    "people": [
      "Bahdanau",
      "Xu",
      "Pan"
    ],
    "review": "This paper addresses video captioning with a TEM-HAM architecture, where a HAM module attends over attended outputs of the TEM module when generating the description. This gives a kind of 2-level attention. The model is evaluated on the Charades and MSVD datasets.\n\n1. Quality/Clarify: I found this paper to be poorly written and relatively hard to understand. As far as I can tell the TEM module of Section 3.1 is a straight-forward attention frame encoder of Bahdanau et al. 2015 or Xu et al. 2015. The decoder of Section 3.3 is a standard LSTM with log likelihood. The HAM module of Section 3.2 is the novel module but is not very well described. It looks to be an attention LSTM where the attention is over the TEM LSTM outputs, but the attention weights are additionally conditioned on the decoder state. There are a lot of small problems with the description, such as notational discrepancy in using \\textbf in equations and then not using it in the text. Also, I spent a long time trying to understand what f_m is. The authors say: \n\n\"In order to let the network remember what has been attended before and the temporal\nstructure of a video, we propose f_m to memorize the previous attention and encoded version of an\ninput video with language model. Using f_m not only enables the network to memorize previous\nattention and frames, but also to learn multi-layer attention over an input video and corresponding\nlanguage.\"\n\nWhere one f_m is bold and the other f_m is not. Due to words such as \"we propose f_m\" assumed this was some kind of a novel technical contribution I couldn't find any details about but it is specified later in Section 3.3 at the end that f_m is in fact just an LSTM. It's not clear why this piece of information is in Section 3.3, which discusses the decoder. The paper is sloppy in other parts. For example in Table 1 some numbers have 1 significant digit and some have 2. The semantics of the horizontal line in Table 2 are not explained in text. \n\n2. Experimental results: The ablation study shows mixed results when adding TEM and HAM to the model. Looking at METEOR which was shown to have the highest correlation to humans in the COCO paper compared to the other evaluation criteria, adding TEM+HAM improves the model from 31.20 to 31.70. It is not clear how significant this improvement is, especially given that the test set is only 670 videos. I have doubts over this result. In Table 2, the METEOR score of Pan et al. 2016a is higher [33.10 vs. 31.80], but this discrepancy is not addressed in text. This is surprising because the authors explicitly claim \"state of the art results\".\n\n3. Originality/Significance: The paper introduces an additional layer of attention over a more standard sequence to sequence setup, which is argued to alleviate the burden on the LSTM's memory. This is moderately novel but I don't believe that the experimental results make it sufficiently clear that it is also worth doing. If the paper made the standard model somehow simpler instead of more complex I would be more inclined to judge it favorably.\n\n\nMinor:\nIn response to the author's comment \"not sure what causes to think of RBM. We don't model any part of our architecture using RBM. We'd be appreciated if you please elaborate more about your confusion about figure 1 so we can address it accordingly.\", I created a diagram to hopefully make this more clear:"
  },
  {
    "people": [
      "Bahdanau"
    ],
    "review": "The authors propose a \"hierarchical\" attention model for video captioning.  They introduce a model composed of three parts: the temporal modeler (TEM) that takes as input the video sequence and outputs a sequential representation of the video to the HAM; the hierarchical attention/memory mechanism (HAM) implements a soft-attention mechanism over the sequential video representation; and finally a decoder that generates a caption. \n\nRelated to the second series of questions above, it seems as though the authors have chosen to refer to their use of an LSTM (or equivalent RNN) as the output of the Bahdanau et al (2015) attention mechanism as a hierarchical memory mechanism. I am actually sympathetic to this terminology in the sense that the recent popularity of memory-based models seems to neglect the memory implicit in the LSTM state vector, but that said, this seems to seriously misrepresent the significance fo the contribution of this paper. \n\nI appreciate the ablation study presented in Table 1. Not enough researchers bother with this kind of analysis. But it does show that the value of the contributions is not actually clear. In particular the case for the TEM is quite weak.\n\nRegarding the quantitative evaluation presented in Table 2, the authors are carving out a fairly specific set of features to describe the set of \"fair\" comparators from the literature. Given the variability of the models and alternate training datasets that are in use, I would find it more compelling if the authors just set about trying to achieve the best results they can, if that includes the fine-tuning of the frame model, so be it. The value of this work is as an application paper, so the discovery and incorporation of elements that can significantly improve performance would seems warranted. \n\nOverall, at this point, I do not see a sufficient contribution to warrant publication in ICLR.\n"
  },
  {
    "people": [
      "Xu",
      "Xu",
      "Yao",
      "Xu",
      "Ballas",
      "Yao",
      "Xu",
      "Pan",
      "Yu",
      "Charades",
      "Charades"
    ],
    "review": "The paper proposes an attention-based approach for video description. The approach uses three LSTMs and two attention mechanisms to sequentially predict words from a sequence of frames.\nIn the LSTM-encoder of the frames (TEM), the first attention approach predicts a spatial attention per frame, and computes the weighted average. The second LSTM (HAM) predicts an attention over the hidden states of the encoder LSTM.\nThe third LSTM which run temporally in parallel to the second LSTM generates the sentence, one word at a time.\n\n\nStrength:\n===============\n\n-\tThe paper works on a relevant and interesting problem.\n-\tUsing 2 layers of attention in the proposed way have to my knowledge not been used before for video description. The exact architecture is thus novel (but the work claims much more without sufficient attribution, see blow)\n-\tThe experiments are evaluated on two datasets, MSVD and Charades, showing performance on the level of related work for MSVD and improvements for Charades.\n\nWeaknesses:\n===============\n\n1.\tClaims about the contribution/novelty of the model seem not to hold: \n1.1.\tOne of the main contributions is the Hierarchical Attention/Memory (HAM):\n1.1.1.\tIt is not clear to me how the presented model (Eq 6-8), are significantly different from the presented model in Xu et al / Yao et al. While Xu et al. attends over spatial image locations and Yao et al. attend over frames, this model attends over encoded video representations h_v^i. A slight difference might be that Xu et al. use the same LSTM to generate, while this model uses an additional LSTM for the decoding.\n1.1.2.\tThe paper states in section 3.2 \u201cwe propose f_m to memorize the previous attention\u201d, however H_m^{t\u2019-1} only consist of the last hidden state. Furthermore, the model f_m does not have access to the \u201cattention\u201d \\alpha. This was also discussed in comments by others, but remains unclear.\n1.1.3.\tIn the discussion of comments the authors claim that \u201cattention not only is a function a current time step but also a function of all previous attentions and network states.\u201d: While it is true that there is a dependency but that is true also for any LSTM, however the model does not have access to the previous network states as H_g^{t\u2019-1} only consist of the last hidden state, as well as H_m^{t\u2019-1} [at least that is what the formulas say and what Figure 1 suggests]. \n1.1.4.\tThe authors claim to have multi-layer attention in HAM, however it remains unclear where the multi-layer comes from.\n1.2.\tThe paper states that in section 3.1. \u201c[CNN] features tend to discard the low level information useful in modeling the motion in the video (Ballas et al., 2016).\u201d This suggests that the approach which follows attacks this problem. However, it cannot model motion as attention \\rho between frames is not available when predicting the next frame. Also, it is not clear how the model can capture anything \u201clow level\u201d as it operates on rather high level VGG conv 5 features.\n\n2.\tRelated work: The difference of HAM to Yao et al. and Xu et al. should be made more clear / or these papers should be cited in the HAM section.\n\n3.\tConceptual Limitation of the model: The model has two independent attention mechanisms, a spatial one, and a temporal one. The spatial (within a frame) is independent of the sentence generation. It thus cannot attend to different aspects of the frames for different words which would make sense. E.g. if the sentence is \u201cthe dog jumps on the trampoline\u201d, the model should focus on the dog when saying \u201cdog\u201d and on the trampoline when saying \u201ctrampoline\u201d, however, as the spatial attention is fixed this is difficult. Also, the encoder model does not have an explicitly way to look at different aspects in the frame during the encoding so might likely get stuck and always predict the same spatial attention for all frames (or it might e.g. always attend to the dog which moves around, but never on the scene).\n\n4.\tEq 11 contradicts Fig 1: How is the model exactly receiving the previous word as input. Eq. 11 suggests it is the softmax. If this is the case, the authors should emphasize this in the text as this is unusual. More common would be to use the ground truth previous word during training (which Fig 11 suggests) and the \u201chardmax\u201d, i.e. the highest predicted previous word encoded as one-hot vector at test time.\n\n5.\tClarity:\n5.1.\tIt would be helpful if the same notation would be used in Eq 2-5 and 6-9. Why is a different notation required?\n5.2.\tIt would be helpful if Fig 1 could contain more details or additional figures for the corresponding parts would be added. If space is a problem, e.g. the well-known equations for LSTM, softmax (Eq 2), and log likelihood loss (Eq 12) could be omitted or inlined.\n\n6.\tEvaluation:\n6.1.\tThe paper claims that the \u201cthe proposed architecture outperforms all previously proposed methods and leads to a new state of the art results\u201d.\n6.1.1.\tFor the MSVD dataset this clearly is wrong, even given the same feature representation. Pan et al. (2016 a) in Table 2 achieve higher METEOR (33.10).\n6.1.2.\tFor this strong claim, I would also expect that it outperforms all previous results independent of the features used, which is also wrong again, Yu et al achieve higher performance in all compared metrics.\n6.1.3.\tFor Charades dataset, this claim is also too bold as hardly any methods have been evaluated on this dataset, so at least all the ablations reported in Table 1 should also be reported for the Charades dataset, to make for this dataset any stronger claims.\n6.2.\tMissing qualitative results of attention: The authors should show qualitative results of the attention, for both attention mechanisms to understand if anything sensible is happening there. How diverse is the spatial and the temporal attention? Is it peaky or rather uniform?\n6.3.\tPerformance improvement is not significant over model ablations: The improvements over Att+No TEM is only 0.5 Meteor, 0.7 Blue@4 and the performance drops for CIDEr by 1.7.\n6.4.\tMissing human evaluation: I disagree with the authors that a human evaluation is not feasible. 1. An evaluation on a subset of the test data is not so difficult. 2. Even if other authors do not provide their code/model [and some do], they are typically happy to share the predicted sentences which is sufficient and even better for human evaluation [if not I would explicitly mention that some authors did not share sentences, as this seems clearly wrong]. 3. For model ablations the sentences are available to the authors.\n\n7.\tSeveral of the comments raised by reviewers/others have not yet been incorporated in a revised version of the paper and/or are still not clear from the explanations given. E.g. including SPICE evaluation and making fixes seems trivial.\t\n\n8.\tHyperparameters are inconsistent: Why are the hyperparemters inconsistent between the ablation analysis (40 frames are sampled) and the performance comparison (8 frames)? Should this not be selected on the validation set? What is the performance of all the ablations with 8 frames?\n\nOther (minor/discussion points)\n-\tEquation 10: what happens with h_m, and h_g, the LSTM formulas provided only handle two inputs. Are h_m and h_g concatenated.\n-\tThere is a section 4.1 but no 4.2.\n-\tThe paper states in section 4.1 \u201cour proposed architecture can alone not only learn a representation for video that can model the temporal structure of a video sequence, but also a representation that can effectively map visual space to the language space.\u201d However, this seems to be true also for many/most other approaches, e.g. [Venugopalan et al. 2015 ICCV]\n\nSummary:\n===============\n\nWhile the paper makes strong claims w.r.t. to the approach and results, the approach lacks novelty and the results are not convincing over related work and ablations. Furthermore, improved clarity and visualizations of the model and attention results would benefit the paper.\n"
  },
  {
    "people": [
      "Bahdanau",
      "Xu",
      "Pan"
    ],
    "review": "\nThis paper addresses video captioning with a TEM-HAM architecture, where a HAM module attends over attended outputs of the TEM module when generating the description. This gives a kind of 2-level attention. The model is evaluated on the Charades and MSVD datasets.\n\n1. Quality/Clarify: I found this paper to be poorly written and relatively hard to understand. As far as I can tell the TEM module of Section 3.1 is a straight-forward attention frame encoder of Bahdanau et al. 2015 or Xu et al. 2015. The decoder of Section 3.3 is a standard LSTM with log likelihood. The HAM module of Section 3.2 is the novel module but is not very well described. It looks to be an attention LSTM where the attention is over the TEM LSTM outputs, but the attention weights are additionally conditioned on the decoder state. There are a lot of small problems with the description, such as notational discrepancy in using \\textbf in equations and then not using it in the text. Also, I spent a long time trying to understand what f_m is. The authors say: \n\n\"In order to let the network remember what has been attended before and the temporal\nstructure of a video, we propose f_m to memorize the previous attention and encoded version of an\ninput video with language model. Using f_m not only enables the network to memorize previous\nattention and frames, but also to learn multi-layer attention over an input video and corresponding\nlanguage.\"\n\nWhere one f_m is bold and the other f_m is not. Due to words such as \"we propose f_m\" assumed this was some kind of a novel technical contribution I couldn't find any details about but it is specified later in Section 3.3 at the end that f_m is in fact just an LSTM. It's not clear why this piece of information is in Section 3.3, which discusses the decoder. The paper is sloppy in other parts. For example in Table 1 some numbers have 1 significant digit and some have 2. The semantics of the horizontal line in Table 2 are not explained in text. \n\n2. Experimental results: The ablation study shows mixed results when adding TEM and HAM to the model. Looking at METEOR which was shown to have the highest correlation to humans in the COCO paper compared to the other evaluation criteria, adding TEM+HAM improves the model from 31.20 to 31.70. It is not clear how significant this improvement is, especially given that the test set is only 670 videos. I have doubts over this result. In Table 2, the METEOR score of Pan et al. 2016a is higher [33.10 vs. 31.80], but this discrepancy is not addressed in text. This is surprising because the authors explicitly claim \"state of the art results\".\n\n3. Originality/Significance: The paper introduces an additional layer of attention over a more standard sequence to sequence setup, which is argued to alleviate the burden on the LSTM's memory. This is moderately novel but I don't believe that the experimental results make it sufficiently clear that it is also worth doing. If the paper made the standard model somehow simpler instead of more complex I would be more inclined to judge it favorably.\n\n\nMinor:\nIn response to the author's comment \"not sure what causes to think of RBM. We don't model any part of our architecture using RBM. We'd be appreciated if you please elaborate more about your confusion about figure 1 so we can address it accordingly.\", I created a diagram to hopefully make this more clear:  "
  },
  {
    "people": [
      "Zaremba"
    ],
    "review": "The paper reports that \"[a]fter the controller RNN is done training, we take the best RNN cell according to the lowest validation perplexity and then run a grid search over learning rate, weight initialization, dropout rates and decay epoch. The best cell found was then run with three different configurations and sizes to increase its capacity.\"\n\nIs it possible for you to include (or provide here) the hyperparameters and type of dropout (i.e. recurrent dropout, embedding dropout, ...) used? Without them, replication would at best require a great deal of trial and error. As with \"Recurrent Neural Network Regularization\" (Zaremba et al., 2014), releasing a base set of hyper parameters greatly assists in the future work of the field.\n\nThis will likely also be desired for the other experiments, such as character LM."
  },
  {
    "people": [
      "Zaremba"
    ],
    "review": "The paper reports that \"[a]fter the controller RNN is done training, we take the best RNN cell according to the lowest validation perplexity and then run a grid search over learning rate, weight initialization, dropout rates and decay epoch. The best cell found was then run with three different configurations and sizes to increase its capacity.\"\n\nIs it possible for you to include (or provide here) the hyperparameters and type of dropout (i.e. recurrent dropout, embedding dropout, ...) used? Without them, replication would at best require a great deal of trial and error. As with \"Recurrent Neural Network Regularization\" (Zaremba et al., 2014), releasing a base set of hyper parameters greatly assists in the future work of the field.\n\nThis will likely also be desired for the other experiments, such as character LM."
  },
  {
    "people": [
      "Leroux",
      "Leroux"
    ],
    "review": "This paper presents a theoretical and empirical approach to the problem of understanding the expressivity of deep networks.\n\nRandom networks (deep networks with random Gaussian weights, hard tanh or ReLU activation) are studied according to several criterions: number of neutron transitions, activation patterns, dichotomies and trajectory length.\n\nThere doesn't seem to be a solid justification for why the newly introduced measures of expressivity really measure expressivity.\nFor instance the trajectory length seems a very discutable measure of expressivity. The only justification given for why it should be a good measure of expressivity is proportionality with other measures of expressivity in the specific case of random networks.\n\nThe paper is too obscure and too long. The work may have some interesting ideas but it does not seem to be properly replaced in context.\n\nSome findings seem trivial.\n\ndetailed comments\n\np2 \n\n\"Much of the work examining achievable functions relies on unrealistic architectural assumptions such as layers being exponentially wide\"\n\nI don\u2019t think so. In \"Deep Belief Networks are Compact Universal Approximators\" by Leroux et al., proof is given that deep but narrow feed-forward neural networks with sigmoidal units can represent any Boolean expression i.e. A neural network with 2n\u22121 + 1 layers of n units (with n the number of input neutron).\n\n\u201cComparing architectures in such a fashion limits the generality of the conclusions\u201d\n\nTo my knowledge much of the previous work has focused on mathematical proof, and has led to very general conclusions on the representative power of deep networks (one example being Leroux et al again).\n\nIt is much harder to generalise the approach you propose, based on random networks which are not used in practice.\n\n\u201c[we study] a family of networks arising in practice: the behaviour of networks after random initialisation\u201d\n\nThese networks arise in practice as an intermediate step that is not used to perform computations; this means that the representative power of such intermediate networks is a priori irrelevant. You would need to justify why it is not.\n\n\u201cresults on random networks provide natural baselines to compare trained networks with\u201d\n\nrandom networks are not \u201cnatural\u201d for the study of expressivity of deep networks. It is not clear how the representative power of random networks (what kind of random networks seems an important question here) is linked to the representative power of (i) of the whole class of networks or (ii) the class of networks after training. Those two classes of networks are the ones we would a priori care about and you would need to justify why the study of random networks helps in understanding either (i) or (ii).\n\np5\n\n\u201cAs FW is a random neural network [\u2026] it would suggest that points far enough away from each other would have independent signs, i.e. a direct proportionality between the length of z(n)(t) and the number of times it crosses the decision boundary.\u201d\n\nAs you say, it seems that proportionality of the two measures depends on the network being random. This seems to invalidate generalisation to other networks, i.e. if the networks are not random, one would assume that path lengths are not proportional.\n\np6\n\nthe expressivity w.r.t. remaining depth seems a trivial concerns, completely equivalent to the expressivity w.r.t. depth. This makes the remark in figure 5 that the number of achievable dichotomies only depends *only* on the number of layers above the layer swept seem trivial\n\np7\n\nin figure 6 a network width of 100 for MNIST seems much too small. Accordingly performance is very poor and it is difficult to generalise the results to relevant situations.\n"
  },
  {
    "people": [
      "Leroux",
      "Leroux"
    ],
    "review": "This paper presents a theoretical and empirical approach to the problem of understanding the expressivity of deep networks.\n\nRandom networks (deep networks with random Gaussian weights, hard tanh or ReLU activation) are studied according to several criterions: number of neutron transitions, activation patterns, dichotomies and trajectory length.\n\nThere doesn't seem to be a solid justification for why the newly introduced measures of expressivity really measure expressivity.\nFor instance the trajectory length seems a very discutable measure of expressivity. The only justification given for why it should be a good measure of expressivity is proportionality with other measures of expressivity in the specific case of random networks.\n\nThe paper is too obscure and too long. The work may have some interesting ideas but it does not seem to be properly replaced in context.\n\nSome findings seem trivial.\n\ndetailed comments\n\np2 \n\n\"Much of the work examining achievable functions relies on unrealistic architectural assumptions such as layers being exponentially wide\"\n\nI don\u2019t think so. In \"Deep Belief Networks are Compact Universal Approximators\" by Leroux et al., proof is given that deep but narrow feed-forward neural networks with sigmoidal units can represent any Boolean expression i.e. A neural network with 2n\u22121 + 1 layers of n units (with n the number of input neutron).\n\n\u201cComparing architectures in such a fashion limits the generality of the conclusions\u201d\n\nTo my knowledge much of the previous work has focused on mathematical proof, and has led to very general conclusions on the representative power of deep networks (one example being Leroux et al again).\n\nIt is much harder to generalise the approach you propose, based on random networks which are not used in practice.\n\n\u201c[we study] a family of networks arising in practice: the behaviour of networks after random initialisation\u201d\n\nThese networks arise in practice as an intermediate step that is not used to perform computations; this means that the representative power of such intermediate networks is a priori irrelevant. You would need to justify why it is not.\n\n\u201cresults on random networks provide natural baselines to compare trained networks with\u201d\n\nrandom networks are not \u201cnatural\u201d for the study of expressivity of deep networks. It is not clear how the representative power of random networks (what kind of random networks seems an important question here) is linked to the representative power of (i) of the whole class of networks or (ii) the class of networks after training. Those two classes of networks are the ones we would a priori care about and you would need to justify why the study of random networks helps in understanding either (i) or (ii).\n\np5\n\n\u201cAs FW is a random neural network [\u2026] it would suggest that points far enough away from each other would have independent signs, i.e. a direct proportionality between the length of z(n)(t) and the number of times it crosses the decision boundary.\u201d\n\nAs you say, it seems that proportionality of the two measures depends on the network being random. This seems to invalidate generalisation to other networks, i.e. if the networks are not random, one would assume that path lengths are not proportional.\n\np6\n\nthe expressivity w.r.t. remaining depth seems a trivial concerns, completely equivalent to the expressivity w.r.t. depth. This makes the remark in figure 5 that the number of achievable dichotomies only depends *only* on the number of layers above the layer swept seem trivial\n\np7\n\nin figure 6 a network width of 100 for MNIST seems much too small. Accordingly performance is very poor and it is difficult to generalise the results to relevant situations.\n"
  },
  {
    "people": [
      "Han"
    ],
    "review": "Summary: There are many different pruning techniques to reduce memory footprint of CNN models, and those techniques have different granularities (layer, maps, kernel or intra kernel), pruning ratio and sparsity of representation. The work proposes a method to choose the best pruning masks out to many trials. Tested on CIFAR-10, SVHN and MNIST.\n\nPros:\nProposes a method to choose pruning mask out of N trials. \nAnalysis on different pruning methods.\n\nCons & Questions:\n\u201cThe proposed strategy selects the best pruned network through N random pruning trials. This approach enables one to select pruning mask in one shot and is simpler than the multi-step technique.\u201d How can one get the best pruning mask in one shot if you ran N random pruning trials? (answered)\nMissing tests of the approach with bigger CNN: like AlexNet, VGG, GoogLeNet or ResNet. (extended to VGG ok)\nSince reducing model size for embedded systems is the final goal, then showing how much memory space in MB is saved with the proposed technique compared with other approaches like Han et al. (2015) would be good.\n\nMisc:\nTypo in figure 6 a) caption: \u201cFeatuer\u201d (corrected)"
  },
  {
    "people": [
      "Wen",
      "Wei",
      "Lebedev",
      "Vadim",
      "Victor Lempitsky",
      "Alvarez",
      "Jose M.",
      "Mathieu Salzmann"
    ],
    "review": "1) Wen, Wei, et al. \"Learning structured sparsity in deep neural networks.\" Advances in Neural Information Processing Systems. 2016.\n2) Lebedev, Vadim, and Victor Lempitsky. \"Fast convnets using group-wise brain damage.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.\n3) Alvarez, Jose M., and Mathieu Salzmann. \"Learning the Number of Neurons in Deep Networks.\" Advances in Neural Information Processing Systems. 2016."
  },
  {
    "people": [
      "Han"
    ],
    "review": "Summary: There are many different pruning techniques to reduce memory footprint of CNN models, and those techniques have different granularities (layer, maps, kernel or intra kernel), pruning ratio and sparsity of representation. The work proposes a method to choose the best pruning masks out to many trials. Tested on CIFAR-10, SVHN and MNIST.\n\nPros:\nProposes a method to choose pruning mask out of N trials. \nAnalysis on different pruning methods.\n\nCons & Questions:\n\u201cThe proposed strategy selects the best pruned network through N random pruning trials. This approach enables one to select pruning mask in one shot and is simpler than the multi-step technique.\u201d How can one get the best pruning mask in one shot if you ran N random pruning trials? (answered)\nMissing tests of the approach with bigger CNN: like AlexNet, VGG, GoogLeNet or ResNet. (extended to VGG ok)\nSince reducing model size for embedded systems is the final goal, then showing how much memory space in MB is saved with the proposed technique compared with other approaches like Han et al. (2015) would be good.\n\nMisc:\nTypo in figure 6 a) caption: \u201cFeatuer\u201d (corrected)\n"
  },
  {
    "people": [
      "Han"
    ],
    "review": "Summary: There are many different pruning techniques to reduce memory footprint of CNN models, and those techniques have different granularities (layer, maps, kernel or intra kernel), pruning ratio and sparsity of representation. The work proposes a method to choose the best pruning masks out to many trials. Tested on CIFAR-10, SVHN and MNIST.\n\nPros:\nProposes a method to choose pruning mask out of N trials. \nAnalysis on different pruning methods.\n\nCons & Questions:\n\u201cThe proposed strategy selects the best pruned network through N random pruning trials. This approach enables one to select pruning mask in one shot and is simpler than the multi-step technique.\u201d How can one get the best pruning mask in one shot if you ran N random pruning trials? (answered)\nMissing tests of the approach with bigger CNN: like AlexNet, VGG, GoogLeNet or ResNet. (extended to VGG ok)\nSince reducing model size for embedded systems is the final goal, then showing how much memory space in MB is saved with the proposed technique compared with other approaches like Han et al. (2015) would be good.\n\nMisc:\nTypo in figure 6 a) caption: \u201cFeatuer\u201d (corrected)"
  },
  {
    "people": [
      "Wen",
      "Wei",
      "Lebedev",
      "Vadim",
      "Victor Lempitsky",
      "Alvarez",
      "Jose M.",
      "Mathieu Salzmann"
    ],
    "review": "1) Wen, Wei, et al. \"Learning structured sparsity in deep neural networks.\" Advances in Neural Information Processing Systems. 2016.\n2) Lebedev, Vadim, and Victor Lempitsky. \"Fast convnets using group-wise brain damage.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.\n3) Alvarez, Jose M., and Mathieu Salzmann. \"Learning the Number of Neurons in Deep Networks.\" Advances in Neural Information Processing Systems. 2016."
  },
  {
    "people": [
      "Han"
    ],
    "review": "Summary: There are many different pruning techniques to reduce memory footprint of CNN models, and those techniques have different granularities (layer, maps, kernel or intra kernel), pruning ratio and sparsity of representation. The work proposes a method to choose the best pruning masks out to many trials. Tested on CIFAR-10, SVHN and MNIST.\n\nPros:\nProposes a method to choose pruning mask out of N trials. \nAnalysis on different pruning methods.\n\nCons & Questions:\n\u201cThe proposed strategy selects the best pruned network through N random pruning trials. This approach enables one to select pruning mask in one shot and is simpler than the multi-step technique.\u201d How can one get the best pruning mask in one shot if you ran N random pruning trials? (answered)\nMissing tests of the approach with bigger CNN: like AlexNet, VGG, GoogLeNet or ResNet. (extended to VGG ok)\nSince reducing model size for embedded systems is the final goal, then showing how much memory space in MB is saved with the proposed technique compared with other approaches like Han et al. (2015) would be good.\n\nMisc:\nTypo in figure 6 a) caption: \u201cFeatuer\u201d (corrected)\n"
  },
  {
    "people": [
      "Hinton"
    ],
    "review": "Dear reviewers,\n\n\nWe would like to announce our final update, main changes include:\n\n(A) A Pseudo-Algorithm for more clarity of the process\n\n(B) Clarified description of the method and experiment section in particular.\n\n(C) Results for wide ResNets on CIFAR-10.\n\n(D) Proposal for prior update given extremely many parameters such as VGG in Appendix C.\n\n\n\nWe also would like to make some final comments on the procedure.\n\nWe do believe that that our method is principled and will achieve high compression rates even when the format of storing changes, because we optimize the lower bound directly. One could imagine a scenario where we store noisy weights as proposed by Hinton et al. (1992).\n\nHowever, there is to say that the process involves a lot of hyper parameters that are hard to tune.\nInteresting, also, that there seems to be a regime where the empirical prior helps improving upon pretrained results (often very significantly). For us that encourages using empirical priors for neural network training in general such as training from scratch, training in a teacher student setting or training networks with little data."
  },
  {
    "people": [
      "Han",
      "Nowlan",
      "Hinton"
    ],
    "review": "Thank you very much for the carefully crafted reviews. We agree with most points you are making. We would like to comment on them and give an outlook of future efforts.\n \nVGG: We ran our algorithm that was successful for MNIST on ImageNet using the pretrained VGG network. However, due to some implementation inefficiencies we were only able to run it for a few epochs. Therefore, we have strong reasons to believe the network has not converged the way that LeNet had. In order to quickly verify that our method works on natural images we ran it on CIFAR10/100 and the results look in line with our LeNet experiments. Our computational bottleneck seems to stem from the updating of the prior parameters. In the future, we will be updating prior parameters with influence of less weights.  \n\nClarity: The methods and experiments section will be improved and extended to enhance its clarity. In particular, we will add an explicit algorithm. Furthermore, we will put our code up online.\n\nSpearmint: We agree with AnonReviewer1 that there is no theoretical justification for a linear relationship of accuracy and compression rate. We can derive some theoretical results under the assumption that the amount of pruned weights and the accuracy have a known relationship and considering only the storage format proposed by Han et al. (2016).\nWe will improve that in the next upcoming version.\n\nIn response to: \"In Figure 2 I am noticing two things: On the left, there is a large number of points with improved accuracy which is not the case for LeNet5-Caffe. Is there any intuition for why that's the case?\"\nNowlan and Hinton (1992) did originally propose the method to improve generalization. They offered it as an alternative to convolutional weight-sharing. I think for this fully connected architecture that is exactly what is happening. The network has more room for generalization because there has been no other form of regularization.\n\nWe hope to be able to report more soon ... next year :).\n\nTill than happy Christmas and New Year."
  },
  {
    "people": [
      "Han",
      "Bayes",
      "Snoek",
      "Han",
      "Guo"
    ],
    "review": "This paper proposes to use an empirical Bayesian approach to learn the parameters of a neural network, and their priors.\nA mixture model prior over the weights leads to a clustering effect in the weight posterior distributions (which are approximated with delta peaks). \nThis clustering effect can exploited for parameter quantisation and compression of the network parameters.\nThe authors show that this leads to compression rates and predictive accuracy comparable to related approaches. \n\nEarlier work [Han et al. 2015] is based on a three-stage process of pruning small magnitude weights, clustering the remaining ones, and updating the cluster centres to optimise performance. The current work provides a more principled approach that does not have such an ad-hoc multi-stage structure, but a single iterative optimisation process.\n\nA first experiment, described in section 6.1 shows that an empirical Bayes\u2019 approach, without the use of hyper priors, already leads to a pronounced clustering effect and to setting many weights to zero. \nIn particular a compression rate of 64.2 is obtained on the LeNet300-100 model.\nIn section 6.1 the text refers to figure C, I suppose this should be figure 1.\n\nSection 6.2 describes an experiment where hyper-priors are used, and the parameters of these distributions, as well as other hyper-parameters such as the learning rates, are being optimised using Spearmint (Snoek et al., 2012). Figure 2 shows the performance of the  different points in the hyper-parameter space that have been evaluated (each trained network gives an accuracy-compressionrate point in the graph). The text claims that best results lie on a line, this seems a little opportunistic interpretation given the limited data. Moreover, it would be useful to add a small discussion on whether such a linear relationship would be expected or not. Currently the results of this experiment lack interpretation.\n\nSection 6.3 describes results obtained for both CNN models and compares results to the recent results of (Han et al., 2015) and (Guo et al., 2016).\nComparable results are obtained in terms of compression rate and accuracy. \nThe authors state that their current algorithm is too slow to be useful for larger models such as VGG-19, but they do briefly report some results obtained for this model (but do not compare to related work). It would be useful here to explain what slows the training down with respect to standard training without the weight clustering approach, and how the proposed algorithm scales in terms of the relevant quantities of the data and the model.\n\nThe contribution of this paper is mostly experimental, leveraging fairly standard ideas from empirical Bayesian learning to introduce weight clustering effects in CNN training.\nThis being said, it is an interesting result that such a relatively straightforward approach leads to results that are on par with state-of-the-art, but more ad-hoc, network compression techniques.\nThe paper could be improved by clearly describing the algorithm used for training, and how it scales to large networks and datasets.\nAnother point that would deserve further discussion is how the hyper-parameter search is performed ( not using test data I assume), and how the compared methods dealt with the search over hyper-parameters to determine the accuracy-compression tradeoff. Ideally, I think, methods should be evaluated across different points on this trade-off.\n\n"
  },
  {
    "people": [
      "Hinton"
    ],
    "review": "Dear reviewers,\n\n\nWe would like to announce our final update, main changes include:\n\n(A) A Pseudo-Algorithm for more clarity of the process\n\n(B) Clarified description of the method and experiment section in particular.\n\n(C) Results for wide ResNets on CIFAR-10.\n\n(D) Proposal for prior update given extremely many parameters such as VGG in Appendix C.\n\n\n\nWe also would like to make some final comments on the procedure.\n\nWe do believe that that our method is principled and will achieve high compression rates even when the format of storing changes, because we optimize the lower bound directly. One could imagine a scenario where we store noisy weights as proposed by Hinton et al. (1992).\n\nHowever, there is to say that the process involves a lot of hyper parameters that are hard to tune.\nInteresting, also, that there seems to be a regime where the empirical prior helps improving upon pretrained results (often very significantly). For us that encourages using empirical priors for neural network training in general such as training from scratch, training in a teacher student setting or training networks with little data."
  },
  {
    "people": [
      "Han",
      "Nowlan",
      "Hinton"
    ],
    "review": "Thank you very much for the carefully crafted reviews. We agree with most points you are making. We would like to comment on them and give an outlook of future efforts.\n \nVGG: We ran our algorithm that was successful for MNIST on ImageNet using the pretrained VGG network. However, due to some implementation inefficiencies we were only able to run it for a few epochs. Therefore, we have strong reasons to believe the network has not converged the way that LeNet had. In order to quickly verify that our method works on natural images we ran it on CIFAR10/100 and the results look in line with our LeNet experiments. Our computational bottleneck seems to stem from the updating of the prior parameters. In the future, we will be updating prior parameters with influence of less weights.  \n\nClarity: The methods and experiments section will be improved and extended to enhance its clarity. In particular, we will add an explicit algorithm. Furthermore, we will put our code up online.\n\nSpearmint: We agree with AnonReviewer1 that there is no theoretical justification for a linear relationship of accuracy and compression rate. We can derive some theoretical results under the assumption that the amount of pruned weights and the accuracy have a known relationship and considering only the storage format proposed by Han et al. (2016).\nWe will improve that in the next upcoming version.\n\nIn response to: \"In Figure 2 I am noticing two things: On the left, there is a large number of points with improved accuracy which is not the case for LeNet5-Caffe. Is there any intuition for why that's the case?\"\nNowlan and Hinton (1992) did originally propose the method to improve generalization. They offered it as an alternative to convolutional weight-sharing. I think for this fully connected architecture that is exactly what is happening. The network has more room for generalization because there has been no other form of regularization.\n\nWe hope to be able to report more soon ... next year :).\n\nTill than happy Christmas and New Year."
  },
  {
    "people": [
      "Han",
      "Bayes",
      "Snoek",
      "Han",
      "Guo"
    ],
    "review": "This paper proposes to use an empirical Bayesian approach to learn the parameters of a neural network, and their priors.\nA mixture model prior over the weights leads to a clustering effect in the weight posterior distributions (which are approximated with delta peaks). \nThis clustering effect can exploited for parameter quantisation and compression of the network parameters.\nThe authors show that this leads to compression rates and predictive accuracy comparable to related approaches. \n\nEarlier work [Han et al. 2015] is based on a three-stage process of pruning small magnitude weights, clustering the remaining ones, and updating the cluster centres to optimise performance. The current work provides a more principled approach that does not have such an ad-hoc multi-stage structure, but a single iterative optimisation process.\n\nA first experiment, described in section 6.1 shows that an empirical Bayes\u2019 approach, without the use of hyper priors, already leads to a pronounced clustering effect and to setting many weights to zero. \nIn particular a compression rate of 64.2 is obtained on the LeNet300-100 model.\nIn section 6.1 the text refers to figure C, I suppose this should be figure 1.\n\nSection 6.2 describes an experiment where hyper-priors are used, and the parameters of these distributions, as well as other hyper-parameters such as the learning rates, are being optimised using Spearmint (Snoek et al., 2012). Figure 2 shows the performance of the  different points in the hyper-parameter space that have been evaluated (each trained network gives an accuracy-compressionrate point in the graph). The text claims that best results lie on a line, this seems a little opportunistic interpretation given the limited data. Moreover, it would be useful to add a small discussion on whether such a linear relationship would be expected or not. Currently the results of this experiment lack interpretation.\n\nSection 6.3 describes results obtained for both CNN models and compares results to the recent results of (Han et al., 2015) and (Guo et al., 2016).\nComparable results are obtained in terms of compression rate and accuracy. \nThe authors state that their current algorithm is too slow to be useful for larger models such as VGG-19, but they do briefly report some results obtained for this model (but do not compare to related work). It would be useful here to explain what slows the training down with respect to standard training without the weight clustering approach, and how the proposed algorithm scales in terms of the relevant quantities of the data and the model.\n\nThe contribution of this paper is mostly experimental, leveraging fairly standard ideas from empirical Bayesian learning to introduce weight clustering effects in CNN training.\nThis being said, it is an interesting result that such a relatively straightforward approach leads to results that are on par with state-of-the-art, but more ad-hoc, network compression techniques.\nThe paper could be improved by clearly describing the algorithm used for training, and how it scales to large networks and datasets.\nAnother point that would deserve further discussion is how the hyper-parameter search is performed ( not using test data I assume), and how the compared methods dealt with the search over hyper-parameters to determine the accuracy-compression tradeoff. Ideally, I think, methods should be evaluated across different points on this trade-off.\n\n"
  },
  {
    "people": [
      "Munkhdalai",
      "Tsendsuren",
      "Munkhdalai",
      "Tsendsuren"
    ],
    "review": "The idea of shared memory in context of memory augmented neural networks is not novel. Neural Semantic Encoders previously introduced shared and multiple memory accesses [1, 2]. Please discuss the connection between Implicit ReasoNet and Neural Semantic Encoders in your manuscript.\n\nThanks,\n\n\nRef:\n\n1. Munkhdalai, Tsendsuren, and Hong Yu. \"Neural Semantic Encoders.\" arXiv preprint arXiv:1607.04315 (2016).\n2. Munkhdalai, Tsendsuren, and Hong Yu. \"Reasoning with Memory Augmented Neural Networks for Language Comprehension.\" arXiv preprint arXiv:1610.06454 (2016)."
  },
  {
    "people": [
      "Munkhdalai",
      "Tsendsuren",
      "Munkhdalai",
      "Tsendsuren"
    ],
    "review": "The idea of shared memory in context of memory augmented neural networks is not novel. Neural Semantic Encoders previously introduced shared and multiple memory accesses [1, 2]. Please discuss the connection between Implicit ReasoNet and Neural Semantic Encoders in your manuscript.\n\nThanks,\n\n\nRef:\n\n1. Munkhdalai, Tsendsuren, and Hong Yu. \"Neural Semantic Encoders.\" arXiv preprint arXiv:1607.04315 (2016).\n2. Munkhdalai, Tsendsuren, and Hong Yu. \"Reasoning with Memory Augmented Neural Networks for Language Comprehension.\" arXiv preprint arXiv:1610.06454 (2016)."
  },
  {
    "people": [
      "Donahue"
    ],
    "review": "1) Summary\n\nThis paper proposes an end-to-end hybrid architecture to predict the local linear trends of time series. A temporal convnet on raw data extracts short-term features. In parallel, long term representations are learned via a LSTM on piecewise linear approximations of the time series. Both representations are combined using a MLP with one hidden layer (in two parts, one for each stream), and the entire architecture is trained end-to-end by minimizing (using Adam) the (l2-regularized) euclidean loss w.r.t. ground truth local trend durations and slopes.\n \n2) Contributions\n\n+ Interesting end-to-end architecture decoupling short-term and long-term representation learning in two separate streams in the first part of the architecture.\n+ Comparison to deep and shallow baselines.\n\n3) Suggestions for improvement\n\nAdd a LRCN baseline and discussion:\nThe benefits of decoupling short-term and long-term representation learning need to be assessed by comparing to the popular \"long-term recurrent convolutional network\" (LRCN) of Donahue et al ("
  },
  {
    "people": [
      "J. Hernandez-Lobato",
      "D. Hernandez-Lobato"
    ],
    "review": "Updated review: the authors did an admirable job of responding to and incorporating reviewer feedback. In particular, they put a lot of effort into additional experiments, even incorporating a new and much stronger baseline (the ConvNet -> LSTM baseline requested by multiple reviewers). I still have two lingering concerns previously stated -- that each model's architecture (# hidden units, etc.) should be tuned independently and that a pure time series forecasting baselines (without the trend preprocessing) should be tried. I'm going to bump up my score from a clear rejection to a borderline.\n\n-----\n\nThis paper is concerned with time series prediction problems for which the prediction targets include the slope and duration of upcoming local trends. This setting is of great interest in several real world problem settings (e.g., financial markets) where decisions (e.g., buy or sell) are often driven by local changes and trends. The primary challenge in these problems is distinguishing true changes and trends (i.e., a downturn in share price) from noise. The authors tackle this with an interesting hybrid architecture (TreNet) with four parts: (1) preprocessing to extract trends, (2) an LSTM that accepts those trends as inputs to ostensibly capture long term dependencies, (3) a ConvNet that accepts a local window of raw data as its input at each time step, and (4) a higher \"feature fusion\" (i.e., dense) layer to combine the LSTM's and ConvNet's outputs. On three univariate time series data sets, the TreNet outperforms the competing baselines including those based on its constituent parts (LSTM + trend inputs, CNN).\n\nStrengths:\n- A very interesting problem setting that can plausibly be argued to differ from other sequential modeling problems in deep learning (e.g., video classification). This is a nice example of fairly thoughtful task-driven machine learning.\n- Accepting the author's assumptions as true for the moment, the proposed architecture seems intuitive and well-designed.\n\nWeaknesses:\n- Although this is an interesting problem setting (decisions driven by trends and changes), the authors did not make a strong argument for why they formulated the machine learning task as they did. Trend targets are not provided from \"on high\" (by data oracle) but extracted from raw data using a deterministic algorithm. Thus, one could just easily formulate this as plain time series forecasting problem in which we forecast the next 100 steps and then apply the trend extractor to convert those predictions into a trend. If the forecasts are accurate, so will be the extracted trends.\n- The proposed architecture, while interesting, is not justified, in particular the choice to feed the extracted trends and raw data into separate LSTM and ConvNet layers that are only combined at the end by a shallow MLP. An equally straightforward but more intuitive choice would have been to feed the output of the ConvNet into the LSTM, perhaps augmented by the trend input. Without a solid rationale, this unconventional choice comes across as arbitrary.\n- Following up on that point, the raw->ConvNet->LSTM and {raw->ConvNet,trends}->LSTM architectures are natural baselines for experiments.\n- The paper presupposes, rather than argues, the value of the extracted trends and durations as inputs. It is not unreasonable to think that, with enough training data, a sufficiently powerful ConvNet->LSTM architecture should be able to learn to detect these trends in raw data, if they are predictive.\n- Following up on that point, two other obvious baselines that were omitted: raw->LSTM and {raw->ConvNet,trends}->MLP. Basically, the authors propose a complex architecture without demonstrating the value of each part (trend extraction, LSTM, ConvNet, MLP). The baselines are unnecessarily weak.\n\nOne thing I am uncertain about in general: the validity of the practice of using the same LSTM and ConvNet architectures in both the baselines and the TreNet. This *sounds* like an apples-to-apples comparison, but in the world of hyperparameter tuning, it could in fact disadvantage either. It seems like a more thorough approach would be to optimize each architecture independently.\n\nRegarding related work and baselines: I think it is fair to limit the scope of in-depth analysis and experiments to a set of reasonable, representative baselines, at least in a conference paper submitted to a deep learning conference. That said, the authors ignored a large body of work on financial time series modeling using probabilistic models and related techniques. This is another way to frame the above \"separate trends from noise\" problem: treat the observations as noisy. One semi-recent example: J. Hernandez-Lobato, J. Lloyds, and D. Hernandez-Lobato. Gaussian process conditional copulas with applications to financial time series. NIPS 2013.\n\nI appreciate this research direction in general, but at the moment, I believe that the work described in this manuscript is not suitable for inclusion at ICLR. My policy for interactive review is to keep an open mind and willingness to change my score, but a large revision is unlikely. I would encourage the authors to instead use their time and energy -- and reviewer feedback -- in order to prepare for a future conference deadline (e.g., ICML)."
  },
  {
    "people": [
      "Donahue"
    ],
    "review": "1) Summary\n\nThis paper proposes an end-to-end hybrid architecture to predict the local linear trends of time series. A temporal convnet on raw data extracts short-term features. In parallel, long term representations are learned via a LSTM on piecewise linear approximations of the time series. Both representations are combined using a MLP with one hidden layer (in two parts, one for each stream), and the entire architecture is trained end-to-end by minimizing (using Adam) the (l2-regularized) euclidean loss w.r.t. ground truth local trend durations and slopes.\n \n2) Contributions\n\n+ Interesting end-to-end architecture decoupling short-term and long-term representation learning in two separate streams in the first part of the architecture.\n+ Comparison to deep and shallow baselines.\n\n3) Suggestions for improvement\n\nAdd a LRCN baseline and discussion:\nThe benefits of decoupling short-term and long-term representation learning need to be assessed by comparing to the popular \"long-term recurrent convolutional network\" (LRCN) of Donahue et al ("
  },
  {
    "people": [
      "Bengio",
      "Samy",
      "Erhan",
      "Dumitru",
      "Donahue",
      "Jeff",
      "Hendricks",
      "Lisa Anne",
      "Guadarrama",
      "Sergio",
      "Rohrbach",
      "Marcus",
      "Venugopalan",
      "Subhashini",
      "Saenko",
      "Kate",
      "Darrell",
      "Trevor",
      "Karpathy",
      "Andrej",
      "Toderici",
      "George",
      "Shetty",
      "Sanketh",
      "Leung",
      "Thomas",
      "Sukthankar",
      "Rahul",
      "Fei-Fei, Li"
    ],
    "review": "Revision of the review:\nThe authors did a commendable job of including additional references and baseline experiments.\n\n---\n\nThis paper presents a hybrid architecture for time series prediction, focusing on the slope and duration of linear trends. The architecture consists of combining a 1D convnet for local time series and an LSTM for time series of trend descriptors. The convnet and LSTM features are combined into an MLP for predicting either the slope or the duration of the next trend in a 1D time series. The method is evaluated on 3 small datasets.\n\nSummary:\nThis paper, while relative well written and presenting an interesting approach, has several methodology flaws, that should be handled by new experiments.\n\nPros:\nThe idea of extracting upward or downward trends from time series - although these should, ideally be learned, not rely on an ad-hoc technique, given that this is a submission for ICLR.\n\nMethodology:\n* In section 3, what do you mean by predicting \u201ceither [the duration] $\\hat l_t$ or [slope] $\\hat s_t$\u201d of the trend? Predictions are valid only if those two predictions are done jointly. The two losses should be combined during training.\n* In the entire paper, the trend slope and duration need to be predicted jointly. Predicting a time series without specifying the horizon of the prediction is meaningless. If the duration of the trends is short, the time series could go up or down alternatively; if the duration of the trend is long, the slope might be close to zero. Predictions at specific horizons are needed.\n* In general, time series prediction for such applications as trading and load forecasting is pointless if no decision is made. A trading strategy would be radically different for short-term and noisy oscillations or from long-term, stable upward or downward trend. An actual evaluation in terms of trading profit/loss should be added for each of the baselines, including the na\u00efve baselines.\n* As mentioned earlier in the pre-review questions, an important baseline is missing: feeding the local time series to the convnet and connecting the convnet directly to the LSTM, without ad-hoc trend extraction.\n* The convnet -> LSTM architecture would need an analysis of the convnet filters and trend prediction representation.\n* Trend prediction/segmentation by the convnet could be an extra supervised loss.\n* The detailed analysis of the trend extraction technique is missing.\n* In section 5, the SVM baselines have local trend and local time series vectors concatenated. Why isn\u2019t the same approach used for LSTM baselines (as a multivariate input) and why the convnet operates only on local \n* An important, \u201cna\u00efve\u201d baseline is missing: next local trend slope and duration = previous local trend slope and duration.\n\nMissing references:\nThe related work section is very partial and omits important work in hybrid convnet + LSTM architectures, in particular:\nVinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Erhan, Dumitru. Show and tell: A neural image caption generator. CoRR, abs/1411.4555, 2014.\nDonahue, Jeff, Hendricks, Lisa Anne, Guadarrama, Sergio, Rohrbach, Marcus, Venugopalan, Subhashini, Saenko, Kate, and Darrell, Trevor. Long-term recurrent convolutional networks for visual recognition and description. CoRR, abs/1411.4389, 2014.\nKarpathy, Andrej, Toderici, George, Shetty, Sanketh, Leung, Thomas, Sukthankar, Rahul, and Fei-Fei, Li. Large-scale video classification with convolutional neural networks. In CVPR, 2014.\n\nThe organization of the paper needs improvement:\n* Section 3 does not explain the selection of the maximal tolerable variance in each trend segment. The appendix should be moved to the core part of the paper.\n* Section 4 is unnecessarily long and gives well known details and equations about convnets and LSTMs. The only variation from standard algorithm descriptions is that $l_k$ $s_k$ are concatenated. The feature fusion layer can be expressed by a simple MLP on the concatenation of R(T(l)) and C(L(t)). Details could be moved to the appendix.\n\nAdditional questions:\n*In section 5, how many datapoints are there in each dataset? Listing only the number of local trends is uninformative.\n\nTypos:\n* p. 5, top \u201cduration and slop\u201d\n"
  },
  {
    "people": [
      "Donahue"
    ],
    "review": "1) Summary\n\nThis paper proposes an end-to-end hybrid architecture to predict the local linear trends of time series. A temporal convnet on raw data extracts short-term features. In parallel, long term representations are learned via a LSTM on piecewise linear approximations of the time series. Both representations are combined using a MLP with one hidden layer (in two parts, one for each stream), and the entire architecture is trained end-to-end by minimizing (using Adam) the (l2-regularized) euclidean loss w.r.t. ground truth local trend durations and slopes.\n \n2) Contributions\n\n+ Interesting end-to-end architecture decoupling short-term and long-term representation learning in two separate streams in the first part of the architecture.\n+ Comparison to deep and shallow baselines.\n\n3) Suggestions for improvement\n\nAdd a LRCN baseline and discussion:\nThe benefits of decoupling short-term and long-term representation learning need to be assessed by comparing to the popular \"long-term recurrent convolutional network\" (LRCN) of Donahue et al ("
  },
  {
    "people": [
      "J. Hernandez-Lobato",
      "D. Hernandez-Lobato"
    ],
    "review": "Updated review: the authors did an admirable job of responding to and incorporating reviewer feedback. In particular, they put a lot of effort into additional experiments, even incorporating a new and much stronger baseline (the ConvNet -> LSTM baseline requested by multiple reviewers). I still have two lingering concerns previously stated -- that each model's architecture (# hidden units, etc.) should be tuned independently and that a pure time series forecasting baselines (without the trend preprocessing) should be tried. I'm going to bump up my score from a clear rejection to a borderline.\n\n-----\n\nThis paper is concerned with time series prediction problems for which the prediction targets include the slope and duration of upcoming local trends. This setting is of great interest in several real world problem settings (e.g., financial markets) where decisions (e.g., buy or sell) are often driven by local changes and trends. The primary challenge in these problems is distinguishing true changes and trends (i.e., a downturn in share price) from noise. The authors tackle this with an interesting hybrid architecture (TreNet) with four parts: (1) preprocessing to extract trends, (2) an LSTM that accepts those trends as inputs to ostensibly capture long term dependencies, (3) a ConvNet that accepts a local window of raw data as its input at each time step, and (4) a higher \"feature fusion\" (i.e., dense) layer to combine the LSTM's and ConvNet's outputs. On three univariate time series data sets, the TreNet outperforms the competing baselines including those based on its constituent parts (LSTM + trend inputs, CNN).\n\nStrengths:\n- A very interesting problem setting that can plausibly be argued to differ from other sequential modeling problems in deep learning (e.g., video classification). This is a nice example of fairly thoughtful task-driven machine learning.\n- Accepting the author's assumptions as true for the moment, the proposed architecture seems intuitive and well-designed.\n\nWeaknesses:\n- Although this is an interesting problem setting (decisions driven by trends and changes), the authors did not make a strong argument for why they formulated the machine learning task as they did. Trend targets are not provided from \"on high\" (by data oracle) but extracted from raw data using a deterministic algorithm. Thus, one could just easily formulate this as plain time series forecasting problem in which we forecast the next 100 steps and then apply the trend extractor to convert those predictions into a trend. If the forecasts are accurate, so will be the extracted trends.\n- The proposed architecture, while interesting, is not justified, in particular the choice to feed the extracted trends and raw data into separate LSTM and ConvNet layers that are only combined at the end by a shallow MLP. An equally straightforward but more intuitive choice would have been to feed the output of the ConvNet into the LSTM, perhaps augmented by the trend input. Without a solid rationale, this unconventional choice comes across as arbitrary.\n- Following up on that point, the raw->ConvNet->LSTM and {raw->ConvNet,trends}->LSTM architectures are natural baselines for experiments.\n- The paper presupposes, rather than argues, the value of the extracted trends and durations as inputs. It is not unreasonable to think that, with enough training data, a sufficiently powerful ConvNet->LSTM architecture should be able to learn to detect these trends in raw data, if they are predictive.\n- Following up on that point, two other obvious baselines that were omitted: raw->LSTM and {raw->ConvNet,trends}->MLP. Basically, the authors propose a complex architecture without demonstrating the value of each part (trend extraction, LSTM, ConvNet, MLP). The baselines are unnecessarily weak.\n\nOne thing I am uncertain about in general: the validity of the practice of using the same LSTM and ConvNet architectures in both the baselines and the TreNet. This *sounds* like an apples-to-apples comparison, but in the world of hyperparameter tuning, it could in fact disadvantage either. It seems like a more thorough approach would be to optimize each architecture independently.\n\nRegarding related work and baselines: I think it is fair to limit the scope of in-depth analysis and experiments to a set of reasonable, representative baselines, at least in a conference paper submitted to a deep learning conference. That said, the authors ignored a large body of work on financial time series modeling using probabilistic models and related techniques. This is another way to frame the above \"separate trends from noise\" problem: treat the observations as noisy. One semi-recent example: J. Hernandez-Lobato, J. Lloyds, and D. Hernandez-Lobato. Gaussian process conditional copulas with applications to financial time series. NIPS 2013.\n\nI appreciate this research direction in general, but at the moment, I believe that the work described in this manuscript is not suitable for inclusion at ICLR. My policy for interactive review is to keep an open mind and willingness to change my score, but a large revision is unlikely. I would encourage the authors to instead use their time and energy -- and reviewer feedback -- in order to prepare for a future conference deadline (e.g., ICML)."
  },
  {
    "people": [
      "Donahue"
    ],
    "review": "1) Summary\n\nThis paper proposes an end-to-end hybrid architecture to predict the local linear trends of time series. A temporal convnet on raw data extracts short-term features. In parallel, long term representations are learned via a LSTM on piecewise linear approximations of the time series. Both representations are combined using a MLP with one hidden layer (in two parts, one for each stream), and the entire architecture is trained end-to-end by minimizing (using Adam) the (l2-regularized) euclidean loss w.r.t. ground truth local trend durations and slopes.\n \n2) Contributions\n\n+ Interesting end-to-end architecture decoupling short-term and long-term representation learning in two separate streams in the first part of the architecture.\n+ Comparison to deep and shallow baselines.\n\n3) Suggestions for improvement\n\nAdd a LRCN baseline and discussion:\nThe benefits of decoupling short-term and long-term representation learning need to be assessed by comparing to the popular \"long-term recurrent convolutional network\" (LRCN) of Donahue et al ("
  },
  {
    "people": [
      "Bengio",
      "Samy",
      "Erhan",
      "Dumitru",
      "Donahue",
      "Jeff",
      "Hendricks",
      "Lisa Anne",
      "Guadarrama",
      "Sergio",
      "Rohrbach",
      "Marcus",
      "Venugopalan",
      "Subhashini",
      "Saenko",
      "Kate",
      "Darrell",
      "Trevor",
      "Karpathy",
      "Andrej",
      "Toderici",
      "George",
      "Shetty",
      "Sanketh",
      "Leung",
      "Thomas",
      "Sukthankar",
      "Rahul",
      "Fei-Fei, Li"
    ],
    "review": "Revision of the review:\nThe authors did a commendable job of including additional references and baseline experiments.\n\n---\n\nThis paper presents a hybrid architecture for time series prediction, focusing on the slope and duration of linear trends. The architecture consists of combining a 1D convnet for local time series and an LSTM for time series of trend descriptors. The convnet and LSTM features are combined into an MLP for predicting either the slope or the duration of the next trend in a 1D time series. The method is evaluated on 3 small datasets.\n\nSummary:\nThis paper, while relative well written and presenting an interesting approach, has several methodology flaws, that should be handled by new experiments.\n\nPros:\nThe idea of extracting upward or downward trends from time series - although these should, ideally be learned, not rely on an ad-hoc technique, given that this is a submission for ICLR.\n\nMethodology:\n* In section 3, what do you mean by predicting \u201ceither [the duration] $\\hat l_t$ or [slope] $\\hat s_t$\u201d of the trend? Predictions are valid only if those two predictions are done jointly. The two losses should be combined during training.\n* In the entire paper, the trend slope and duration need to be predicted jointly. Predicting a time series without specifying the horizon of the prediction is meaningless. If the duration of the trends is short, the time series could go up or down alternatively; if the duration of the trend is long, the slope might be close to zero. Predictions at specific horizons are needed.\n* In general, time series prediction for such applications as trading and load forecasting is pointless if no decision is made. A trading strategy would be radically different for short-term and noisy oscillations or from long-term, stable upward or downward trend. An actual evaluation in terms of trading profit/loss should be added for each of the baselines, including the na\u00efve baselines.\n* As mentioned earlier in the pre-review questions, an important baseline is missing: feeding the local time series to the convnet and connecting the convnet directly to the LSTM, without ad-hoc trend extraction.\n* The convnet -> LSTM architecture would need an analysis of the convnet filters and trend prediction representation.\n* Trend prediction/segmentation by the convnet could be an extra supervised loss.\n* The detailed analysis of the trend extraction technique is missing.\n* In section 5, the SVM baselines have local trend and local time series vectors concatenated. Why isn\u2019t the same approach used for LSTM baselines (as a multivariate input) and why the convnet operates only on local \n* An important, \u201cna\u00efve\u201d baseline is missing: next local trend slope and duration = previous local trend slope and duration.\n\nMissing references:\nThe related work section is very partial and omits important work in hybrid convnet + LSTM architectures, in particular:\nVinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Erhan, Dumitru. Show and tell: A neural image caption generator. CoRR, abs/1411.4555, 2014.\nDonahue, Jeff, Hendricks, Lisa Anne, Guadarrama, Sergio, Rohrbach, Marcus, Venugopalan, Subhashini, Saenko, Kate, and Darrell, Trevor. Long-term recurrent convolutional networks for visual recognition and description. CoRR, abs/1411.4389, 2014.\nKarpathy, Andrej, Toderici, George, Shetty, Sanketh, Leung, Thomas, Sukthankar, Rahul, and Fei-Fei, Li. Large-scale video classification with convolutional neural networks. In CVPR, 2014.\n\nThe organization of the paper needs improvement:\n* Section 3 does not explain the selection of the maximal tolerable variance in each trend segment. The appendix should be moved to the core part of the paper.\n* Section 4 is unnecessarily long and gives well known details and equations about convnets and LSTMs. The only variation from standard algorithm descriptions is that $l_k$ $s_k$ are concatenated. The feature fusion layer can be expressed by a simple MLP on the concatenation of R(T(l)) and C(L(t)). Details could be moved to the appendix.\n\nAdditional questions:\n*In section 5, how many datapoints are there in each dataset? Listing only the number of local trends is uninformative.\n\nTypos:\n* p. 5, top \u201cduration and slop\u201d\n"
  },
  {
    "people": [
      "Weston",
      "Bordes",
      "Weston"
    ],
    "review": "This work describes \n\n1: a two stage encoding of stories in bAbI like setups, where a GRU is used to encode a sentence, word by word, conditioned on a sentence level GRU, and the sentence level GRU keeps track of a sentence level encoding.  Each is used\n\n2: modifying the bAbI tasks so it is necessary to ask a question to correctly solve the problem\n\nI am not convinced by the papers results:\n\n1:   The new architecture does not do significantly better than DMN+, and in my view, is similar to DMN+.   What problem with DMN+ does your architecture solve?   \n\n2:  There are now several papers doing the second thing, for example \"Dialog-based Language Learning\" by Weston and  \"Learning End-to-End Goal-Oriented Dialog\" by Bordes and Weston, and I think doing it more carefully and in more compelling ways.   In the current work, the correct answer to the question seems given independent of the what the agent asks, so any model that can output \"unknown\" and then input the extra response has an advantage.  Essentially all of the architectures that are used to solve bAbI can be modified to do this...  Indeed, the enc-dec* accuracies in appendix A show that this sort of module can be appended to any other model.  All of the standard models can be trained to output questions as a sequence of words.    Furthermore, I suspect you could generate the questions  in the authors' setting just by enumerating all the questions that occur in training, and taking a softmax over them, instead of generating word-by-word.\n"
  },
  {
    "people": [
      "Weston",
      "Bordes",
      "Weston"
    ],
    "review": "This work describes \n\n1: a two stage encoding of stories in bAbI like setups, where a GRU is used to encode a sentence, word by word, conditioned on a sentence level GRU, and the sentence level GRU keeps track of a sentence level encoding.  Each is used\n\n2: modifying the bAbI tasks so it is necessary to ask a question to correctly solve the problem\n\nI am not convinced by the papers results:\n\n1:   The new architecture does not do significantly better than DMN+, and in my view, is similar to DMN+.   What problem with DMN+ does your architecture solve?   \n\n2:  There are now several papers doing the second thing, for example \"Dialog-based Language Learning\" by Weston and  \"Learning End-to-End Goal-Oriented Dialog\" by Bordes and Weston, and I think doing it more carefully and in more compelling ways.   In the current work, the correct answer to the question seems given independent of the what the agent asks, so any model that can output \"unknown\" and then input the extra response has an advantage.  Essentially all of the architectures that are used to solve bAbI can be modified to do this...  Indeed, the enc-dec* accuracies in appendix A show that this sort of module can be appended to any other model.  All of the standard models can be trained to output questions as a sequence of words.    Furthermore, I suspect you could generate the questions  in the authors' setting just by enumerating all the questions that occur in training, and taking a softmax over them, instead of generating word-by-word.\n"
  },
  {
    "people": [
      "Kingma"
    ],
    "review": "This is a very interesting paper. It combines the best of both world: PixelCNN and VAE. State-of-the-art LL and visually appealing images.\n\nBelow are some comments and suggestions.\n\n1. In van dern Oord et al. 2016b they use an auto-encoder architecture with conditional PixelCNN as the decoder. It is not a variational one, though. It would be interesting to give a comparison between the model in this paper and their model. Are there any other differences, besides the KLD loss on latent variables and the (optional) hierarchical extension?\n\n2. The introduction of latent varibles z definitely helps modeling global structure, however it also breaks the chain rule so the exact likelihood cannnot be derived.\n\n3. How is the quality of representations learned by the encoder? Will the PixelCNN decoder help the encoder learn better representations, compared to Kingma et. al., \"Semi-supervised learning with deep generative models\"?\n\n3. Also, it would be great if you can show some reconstruction results, in addition to generated samples.\n\nBut overall I like this paper very much."
  },
  {
    "people": [
      "Kingma"
    ],
    "review": "This is a very interesting paper. It combines the best of both world: PixelCNN and VAE. State-of-the-art LL and visually appealing images.\n\nBelow are some comments and suggestions.\n\n1. In van dern Oord et al. 2016b they use an auto-encoder architecture with conditional PixelCNN as the decoder. It is not a variational one, though. It would be interesting to give a comparison between the model in this paper and their model. Are there any other differences, besides the KLD loss on latent variables and the (optional) hierarchical extension?\n\n2. The introduction of latent varibles z definitely helps modeling global structure, however it also breaks the chain rule so the exact likelihood cannnot be derived.\n\n3. How is the quality of representations learned by the encoder? Will the PixelCNN decoder help the encoder learn better representations, compared to Kingma et. al., \"Semi-supervised learning with deep generative models\"?\n\n3. Also, it would be great if you can show some reconstruction results, in addition to generated samples.\n\nBut overall I like this paper very much."
  },
  {
    "people": [
      "Judd",
      "Judd",
      "Judd",
      "Judd"
    ],
    "review": "Summary:\n\nThe paper describes how the DaDianNao (DaDN) DNN accelerator can be improved by employing bit serial arithmetic.  They replace the bit-parallel multipliers in DaDN with multipliers that accept the weights in parallel but the activations serially (serial x parallel multipliers).  They increase the number of units keeping the total number of adders constant.  This enables them to tailor the time and energy consumed to the number of bits used to represent activations.  They show how their configuration can be used to process both fully-connected and convolutional layers of DNNs.\n\nStrengths:\n\nUsing variable precision for each layer of the network is useful - but was previously reported in Judd (2015)\n\nGood evaluation including synthesis - but not place and route - of the units.  Also this evaluation is identical to that in Judd (2016b)\n\nWeaknesses:\n\nThe idea of combining bit-serial arithmetic with the DaDN architecture is a small one.\n\nThe authors have already published almost everything that is in this paper at Micro 2016 in Judd (2016b).  The increment here is the analysis of the architecture on fully-connected layers.  Everything else is in the previous publication.\n\nThe energy gains are small - because the additional flip-flop energy of shifting the activations in almost offsets the energy saved on reducing the precision of the arithmetic.\n\nThe authors don\u2019t compare to more conventional approaches to variable precision - using bit-parallel arithmetic units but data gating the LSBs so that only the relevant portion of the arithmetic units toggle.  This would not provide any speedup, but would likely provide better energy gains than the bit-serial x bit-parallel approach.\n\nOverall:\n\nThe Tartan and Stripes architectures are interesting but the incremental contribution of this paper (adding support for fully-connected layers) over the three previous publications on this topic, and in particular Judd (2016b) is very small.  This idea is worth one good paper, not four."
  },
  {
    "people": [
      "Judd",
      "Judd",
      "Judd",
      "Judd"
    ],
    "review": "Summary:\n\nThe paper describes how the DaDianNao (DaDN) DNN accelerator can be improved by employing bit serial arithmetic.  They replace the bit-parallel multipliers in DaDN with multipliers that accept the weights in parallel but the activations serially (serial x parallel multipliers).  They increase the number of units keeping the total number of adders constant.  This enables them to tailor the time and energy consumed to the number of bits used to represent activations.  They show how their configuration can be used to process both fully-connected and convolutional layers of DNNs.\n\nStrengths:\n\nUsing variable precision for each layer of the network is useful - but was previously reported in Judd (2015)\n\nGood evaluation including synthesis - but not place and route - of the units.  Also this evaluation is identical to that in Judd (2016b)\n\nWeaknesses:\n\nThe idea of combining bit-serial arithmetic with the DaDN architecture is a small one.\n\nThe authors have already published almost everything that is in this paper at Micro 2016 in Judd (2016b).  The increment here is the analysis of the architecture on fully-connected layers.  Everything else is in the previous publication.\n\nThe energy gains are small - because the additional flip-flop energy of shifting the activations in almost offsets the energy saved on reducing the precision of the arithmetic.\n\nThe authors don\u2019t compare to more conventional approaches to variable precision - using bit-parallel arithmetic units but data gating the LSBs so that only the relevant portion of the arithmetic units toggle.  This would not provide any speedup, but would likely provide better energy gains than the bit-serial x bit-parallel approach.\n\nOverall:\n\nThe Tartan and Stripes architectures are interesting but the incremental contribution of this paper (adding support for fully-connected layers) over the three previous publications on this topic, and in particular Judd (2016b) is very small.  This idea is worth one good paper, not four."
  },
  {
    "people": [
      "Reed"
    ],
    "review": "This paper argues that being able to handle recursion is very important for neural programming architectures \u2014 that handling recursion allows for strong generalization to out of domain test cases and learning from smaller amounts of training data.  Most of the paper is a riff on the Reed & de Freitas paper on Neural Programmer Interpreters from ICLR 2016 which learns from program traces \u2014 this paper trains NPI models on traces that have recursive calls.  The authors show how to verify correctness by evaluating the learned program on only a small set of base cases and reduction rules and impressively, show that the NPI architecture is able to perfectly infer Bubblesort and the Tower of Hanoi problems.  \n\nWhat I like is that the idea is super simple and as the authors even mention, the only change is to the execution traces that the training pipeline gets to see.  I\u2019m actually not sure what the right take-away is \u2014 does this mean that we have effectively solved the neural programming problem when the execution traces are available? (and was the problem too easy to begin with?).    For example, a larger input domain (as one of the reviewers also mentions) is MNIST digits and we can imagine a problem where the NPI must infer how to sort MNIST digits from highest to lowest.  In this setting, having execution traces would effectively decouple the problem of recognizing the digits from that of inferring the program logic \u2014 and so the problem would be no harder than learning to recognize MNIST digits and learning to bubble sort from symbols.  What is a problem where we have access to execution traces but cannot infer it using the proposed method?\n"
  },
  {
    "people": [
      "Reed"
    ],
    "review": "This paper argues that being able to handle recursion is very important for neural programming architectures \u2014 that handling recursion allows for strong generalization to out of domain test cases and learning from smaller amounts of training data.  Most of the paper is a riff on the Reed & de Freitas paper on Neural Programmer Interpreters from ICLR 2016 which learns from program traces \u2014 this paper trains NPI models on traces that have recursive calls.  The authors show how to verify correctness by evaluating the learned program on only a small set of base cases and reduction rules and impressively, show that the NPI architecture is able to perfectly infer Bubblesort and the Tower of Hanoi problems.  \n\nWhat I like is that the idea is super simple and as the authors even mention, the only change is to the execution traces that the training pipeline gets to see.  I\u2019m actually not sure what the right take-away is \u2014 does this mean that we have effectively solved the neural programming problem when the execution traces are available? (and was the problem too easy to begin with?).    For example, a larger input domain (as one of the reviewers also mentions) is MNIST digits and we can imagine a problem where the NPI must infer how to sort MNIST digits from highest to lowest.  In this setting, having execution traces would effectively decouple the problem of recognizing the digits from that of inferring the program logic \u2014 and so the problem would be no harder than learning to recognize MNIST digits and learning to bubble sort from symbols.  What is a problem where we have access to execution traces but cannot infer it using the proposed method?\n"
  },
  {
    "people": [
      "Zhu",
      "Tai"
    ],
    "review": "As part of this meta-review, I read the paper and found some surprising claims, such as the somewhat poorly motivated claim that coercing the output of a sub-network be a unit vector my dividing it by its L2 norm is close to layer normalisation which is mathematically almost true, if the mean of of the activations is 0, and we accept a fixed offset in the calculation of the stddev, but conceptually a different form of normalisation. It is also curious that other methods of obtaining stable training in recursive networks, such as TreeLSTM (Zhu et al. 2015, Tai et al. 2015), were not compared to. None of these problems is particularly damning but it is slightly disappointing not to see these issues discussed in the review process.\n\nOverall, the reviews, which I found superficial in comparison to the other papers I am chairing, found the method proposed here sound, although some details lacked explanation. The consensus was that the general problem being addressed is interesting and timely, given the attention the topics of program induction and interpretation have been receiving in the community recently. There was also consensus that the setting the model was evaluated on was far too simple and unnatural, and that there is need for a more complex, task involving symbolic interpretation to validate the model. It is hard to tell, given all the design decisions made (l2-normalisation vs layer norm, softmax not working), whether the end product is tailored to the task at hand, and whether it will tell us something useful about how this approach generalises. I am inclined, on the basis of the reviewer's opinions of the setting and my own concerns outlined above, to recommend redirection to the workshop track."
  },
  {
    "people": [
      "Socher",
      "Socher"
    ],
    "review": "The goal of this paper is to learn vector representation of boolean and polynomial expressions, such that equivalent expressions have similar representations.\n\nThe model proposed in the paper is based on recursive neural network, as introduced by Socher et al. (2012). Given the syntactic parse tree of a formula (either boolean or polynomial), the representation for a node is obtained by applying a MLP on the representation of the children. This process is applied recursively to obtain the representation of the full expression. Contrary to Socher et al. (2012), this paper proposes to use more than one layer (this is especially important to capture XOR operation, which is not surprising at all). The paper also introduces a reconstruction error (called SubexpForce), so that the expression of children can be recovered from the expression of the parent (if I understood correctly). The model is trained using a classification loss, where the label of a given expression corresponds to its equivalence class. The method is then evaluated on randomly generated data, and compared to baselines such as tf-idf, GRU RNN or standard recursive neural network.\n\nWhile I do agree with the authors that learning good representation for symbolic expressions (and to capture compositionality) is an important task, I am not entirely convinced by the experimental setting proposed in this paper. Indeed, as stated in the paper, the task of deciding if two boolean expressions are equivalent is NP-hard, and I do not understand if the model can do better than implicitly computing the truth table of expressions. While sometimes a bit hard to follow, the paper is technically sound. In particular, the proposed model is well adapted to the problem and outperforms the baselines.\n\npros:\n - the model is relatively simple and sound.\n - using a classification loss over equivalence classes (should be compared with using similarity).\n\ncons:\n - not convinced by the setting: I do not believe that you can really do better than the truth table for boolean expr (or computing the value of the polynomial expression for randomly chosen points in [0, 1]^n).\n - some part of the paper are a bit hard to follow (e.g. justification of the SubexpForce, discussion of why softmax does not work, etc...).\n - comparison between classification loss and similarity loss is missing."
  },
  {
    "people": [
      "Zhu",
      "Tai"
    ],
    "review": "As part of this meta-review, I read the paper and found some surprising claims, such as the somewhat poorly motivated claim that coercing the output of a sub-network be a unit vector my dividing it by its L2 norm is close to layer normalisation which is mathematically almost true, if the mean of of the activations is 0, and we accept a fixed offset in the calculation of the stddev, but conceptually a different form of normalisation. It is also curious that other methods of obtaining stable training in recursive networks, such as TreeLSTM (Zhu et al. 2015, Tai et al. 2015), were not compared to. None of these problems is particularly damning but it is slightly disappointing not to see these issues discussed in the review process.\n\nOverall, the reviews, which I found superficial in comparison to the other papers I am chairing, found the method proposed here sound, although some details lacked explanation. The consensus was that the general problem being addressed is interesting and timely, given the attention the topics of program induction and interpretation have been receiving in the community recently. There was also consensus that the setting the model was evaluated on was far too simple and unnatural, and that there is need for a more complex, task involving symbolic interpretation to validate the model. It is hard to tell, given all the design decisions made (l2-normalisation vs layer norm, softmax not working), whether the end product is tailored to the task at hand, and whether it will tell us something useful about how this approach generalises. I am inclined, on the basis of the reviewer's opinions of the setting and my own concerns outlined above, to recommend redirection to the workshop track."
  },
  {
    "people": [
      "Socher",
      "Socher"
    ],
    "review": "The goal of this paper is to learn vector representation of boolean and polynomial expressions, such that equivalent expressions have similar representations.\n\nThe model proposed in the paper is based on recursive neural network, as introduced by Socher et al. (2012). Given the syntactic parse tree of a formula (either boolean or polynomial), the representation for a node is obtained by applying a MLP on the representation of the children. This process is applied recursively to obtain the representation of the full expression. Contrary to Socher et al. (2012), this paper proposes to use more than one layer (this is especially important to capture XOR operation, which is not surprising at all). The paper also introduces a reconstruction error (called SubexpForce), so that the expression of children can be recovered from the expression of the parent (if I understood correctly). The model is trained using a classification loss, where the label of a given expression corresponds to its equivalence class. The method is then evaluated on randomly generated data, and compared to baselines such as tf-idf, GRU RNN or standard recursive neural network.\n\nWhile I do agree with the authors that learning good representation for symbolic expressions (and to capture compositionality) is an important task, I am not entirely convinced by the experimental setting proposed in this paper. Indeed, as stated in the paper, the task of deciding if two boolean expressions are equivalent is NP-hard, and I do not understand if the model can do better than implicitly computing the truth table of expressions. While sometimes a bit hard to follow, the paper is technically sound. In particular, the proposed model is well adapted to the problem and outperforms the baselines.\n\npros:\n - the model is relatively simple and sound.\n - using a classification loss over equivalence classes (should be compared with using similarity).\n\ncons:\n - not convinced by the setting: I do not believe that you can really do better than the truth table for boolean expr (or computing the value of the polynomial expression for randomly chosen points in [0, 1]^n).\n - some part of the paper are a bit hard to follow (e.g. justification of the SubexpForce, discussion of why softmax does not work, etc...).\n - comparison between classification loss and similarity loss is missing."
  },
  {
    "people": [
      "Bartlett",
      "Sayedi"
    ],
    "review": "The paper proposes new bounds on the misclassification error. The bounds lead to training classifiers with an adaptive loss function, and the algorithm operates in successive steps: the parameters are trained by minimizing the log-loss weighted by the probability of the observed class as given by the parameters of the previous steps. The bound improves on standard log-likelihood when outliers/underfitting prevents the learning algorithm to properly optimize the true classification error. Experiments are performed to confirm the therotical intuition and motivation. They show different cases where the new algorithm leads to improved classification error because underfitting occurs when using standard log-loss, and other cases where the new bounds do not lead to any improvement because the log-loss is sufficient to fit the dataset.\n\nThe paper also discusses the relationship between the proposed idea and reinforcement learning, as well as with classifiers that have an \"uncertain\" label. \n\nWhile the paper is easy to read and well-written overall, in a second read I found it difficult to fully understand because two problems are somewhat mixed together (here considering only binary classification for simplicity): \n(a) the optimization of the classification error of a *randomized* classifier, which predicts 1 with probability P(1|x, theta), and \n(b) the optimization of the deterministic classifier, which predicts sign(P(1|x, theta) - 0.5), in a way that is robust to outliers/underfitting. \n\nThe reason why I am confused is that \"The standard approach to supervised classification\", as is mentioned in the abstract, is to use deterministic classifiers at test time, and the log-loss (up to constants) is an upper bound on the classification error of the deterministic classifier. However, the bounds discussed in the paper only concern the randomized classifier.\n\n=== question:\nIn the experiments, what kind of classifier is used? The randomized one (as would the sentence in the first page suggest \"Assuming the class is chosen according to p(y|X, \u03b8)\"), or the more standard deterministic classifier argmax_y P(y|x, theta) ?\n\nAs far as I can see, there are two cases: either (i) the paper deals with learning randomized classifiers, in which case it should compare the performances with the deterministic counterparts that people use in practice, or (ii) the paper makes sense as soon as we accept that the optimization of criterion (a) is a good surrogate for (b). In both cases,  I think the write-up should be made clearer (because in case (ii) the algorithm does not minimize an upper bound on the classification error, and in case (i) what is done does not correspond to what is usually done in binary classification). \n\n=== comments:\n- The section \"allowing uncertainty in the decision\" may be improved by adding some references, e.g. Bartlett & Wegkamp (2008) \"Classification with a Reject Option using a Hinge Loss\" or Sayedi et al. (2010) \"Trading off Mistakes and Don\u2019t Know Predictions\".\n\n- there seems to be a \"-\" sign missing in the P(1|x, theta) in L(theta, lambda) in Section 3.\n\n- The idea presented in the paper is interesting and original. While I give a relatively low score for now, I am willing to increase this score if the clarifications are made.\n\nFinal comments:\nI think the paper is clear enough in its current form, even though there should still be improvement in the justification of why and to what extent the error of the randomized classifier is a good surrogate for the error of the true classifier. While the \"smoothed\" version of the 0/1 loss is an acceptable explanation in the standard classification setup, it is less clear in the section dealing with an additional \"uncertain\" label. I increase my score from 5 to 6."
  },
  {
    "people": [
      "Bartlett",
      "Sayedi"
    ],
    "review": "The paper proposes new bounds on the misclassification error. The bounds lead to training classifiers with an adaptive loss function, and the algorithm operates in successive steps: the parameters are trained by minimizing the log-loss weighted by the probability of the observed class as given by the parameters of the previous steps. The bound improves on standard log-likelihood when outliers/underfitting prevents the learning algorithm to properly optimize the true classification error. Experiments are performed to confirm the therotical intuition and motivation. They show different cases where the new algorithm leads to improved classification error because underfitting occurs when using standard log-loss, and other cases where the new bounds do not lead to any improvement because the log-loss is sufficient to fit the dataset.\n\nThe paper also discusses the relationship between the proposed idea and reinforcement learning, as well as with classifiers that have an \"uncertain\" label. \n\nWhile the paper is easy to read and well-written overall, in a second read I found it difficult to fully understand because two problems are somewhat mixed together (here considering only binary classification for simplicity): \n(a) the optimization of the classification error of a *randomized* classifier, which predicts 1 with probability P(1|x, theta), and \n(b) the optimization of the deterministic classifier, which predicts sign(P(1|x, theta) - 0.5), in a way that is robust to outliers/underfitting. \n\nThe reason why I am confused is that \"The standard approach to supervised classification\", as is mentioned in the abstract, is to use deterministic classifiers at test time, and the log-loss (up to constants) is an upper bound on the classification error of the deterministic classifier. However, the bounds discussed in the paper only concern the randomized classifier.\n\n=== question:\nIn the experiments, what kind of classifier is used? The randomized one (as would the sentence in the first page suggest \"Assuming the class is chosen according to p(y|X, \u03b8)\"), or the more standard deterministic classifier argmax_y P(y|x, theta) ?\n\nAs far as I can see, there are two cases: either (i) the paper deals with learning randomized classifiers, in which case it should compare the performances with the deterministic counterparts that people use in practice, or (ii) the paper makes sense as soon as we accept that the optimization of criterion (a) is a good surrogate for (b). In both cases,  I think the write-up should be made clearer (because in case (ii) the algorithm does not minimize an upper bound on the classification error, and in case (i) what is done does not correspond to what is usually done in binary classification). \n\n=== comments:\n- The section \"allowing uncertainty in the decision\" may be improved by adding some references, e.g. Bartlett & Wegkamp (2008) \"Classification with a Reject Option using a Hinge Loss\" or Sayedi et al. (2010) \"Trading off Mistakes and Don\u2019t Know Predictions\".\n\n- there seems to be a \"-\" sign missing in the P(1|x, theta) in L(theta, lambda) in Section 3.\n\n- The idea presented in the paper is interesting and original. While I give a relatively low score for now, I am willing to increase this score if the clarifications are made.\n\nFinal comments:\nI think the paper is clear enough in its current form, even though there should still be improvement in the justification of why and to what extent the error of the randomized classifier is a good surrogate for the error of the true classifier. While the \"smoothed\" version of the 0/1 loss is an acceptable explanation in the standard classification setup, it is less clear in the section dealing with an additional \"uncertain\" label. I increase my score from 5 to 6."
  },
  {
    "people": [
      "Huang"
    ],
    "review": "We thank all reviewers and readers for the helpful comments! We have updated the paper with the following changes:\n\n1) Provide more details about the experimental settings in section 4.1.\n2) Add a section \"Related Work\" (section 2) and move the detailed discussion of related works in Introduction to this section, so that the Introduction focuses more on our contributions.\n3) Update the Introduction section to highlight the differences between our work and previous works, and emphasize our contributions. Also, we provide more explanation of the term \"cross-model\" in Introduction section (at the top of page 2) to avoid confusion.\n4) Add the discussion of following related works: Huang et al. ("
  },
  {
    "people": [
      "Li",
      "Li",
      "Szegedy"
    ],
    "review": "The paper compares several defense mechanisms against adversarial attacks: retraining, two kinds of autoencoders and distillation with the conclusion that the retraining methodology proposed by Li et al. works best of those approaches.\n\nThe paper documents a series of experiments on making models robust against adversarial examples. The methods proposed here are not all too original, RAD was proposed by Li et al, distillation was proposed in Goodfellow et al's \"Explaining and harnessing adversarial examples\", stacked autoencoders were proposed by Szegedy et al's \"Intriguing Properties of Neural Networks\". The most original part of the paper is the improved version of autoencoders proposed in this paper.\n\nThe paper establishes experimental evidence that the RAD framework provides the best defense mechanism against adversarial attacks which makes the introduction of the improved autoencoder mechanism less appealing.\n\nAlthough the paper establishes interesting measurement points and therefore it has the potential for being cited as a reference, its relative lack of originality decreases its significance."
  },
  {
    "people": [
      "Huang"
    ],
    "review": "We thank all reviewers and readers for the helpful comments! We have updated the paper with the following changes:\n\n1) Provide more details about the experimental settings in section 4.1.\n2) Add a section \"Related Work\" (section 2) and move the detailed discussion of related works in Introduction to this section, so that the Introduction focuses more on our contributions.\n3) Update the Introduction section to highlight the differences between our work and previous works, and emphasize our contributions. Also, we provide more explanation of the term \"cross-model\" in Introduction section (at the top of page 2) to avoid confusion.\n4) Add the discussion of following related works: Huang et al. ("
  },
  {
    "people": [
      "Li",
      "Li",
      "Szegedy"
    ],
    "review": "The paper compares several defense mechanisms against adversarial attacks: retraining, two kinds of autoencoders and distillation with the conclusion that the retraining methodology proposed by Li et al. works best of those approaches.\n\nThe paper documents a series of experiments on making models robust against adversarial examples. The methods proposed here are not all too original, RAD was proposed by Li et al, distillation was proposed in Goodfellow et al's \"Explaining and harnessing adversarial examples\", stacked autoencoders were proposed by Szegedy et al's \"Intriguing Properties of Neural Networks\". The most original part of the paper is the improved version of autoencoders proposed in this paper.\n\nThe paper establishes experimental evidence that the RAD framework provides the best defense mechanism against adversarial attacks which makes the introduction of the improved autoencoder mechanism less appealing.\n\nAlthough the paper establishes interesting measurement points and therefore it has the potential for being cited as a reference, its relative lack of originality decreases its significance."
  },
  {
    "people": [
      "Barbara",
      "Lena",
      "Mandrill",
      "Peppers"
    ],
    "review": "We have uploaded a revised version of the paper.  Major changes are as follows:\n\n* Introduction/Discussion: we\u2019ve made some adjustments to the text, further motivating the use of MSE (instead of perceptual error), emphasizing that the addition of uniform noise is used only for optimization (all compression results are based on quantized and entropy-coded values), and elaborating on the visual appearance of the compressed results.\n\n* Images: we\u2019ve now run tests on additional images, including the \u201cclassic\u201d examples {Barbara, Lena, Mandrill, Peppers}, as well as more of our own photographs.  All examples have been added to the online set, which has been consolidated at "
  },
  {
    "people": [
      "Barbara",
      "Lena",
      "Mandrill",
      "Peppers"
    ],
    "review": "We have uploaded a revised version of the paper.  Major changes are as follows:\n\n* Introduction/Discussion: we\u2019ve made some adjustments to the text, further motivating the use of MSE (instead of perceptual error), emphasizing that the addition of uniform noise is used only for optimization (all compression results are based on quantized and entropy-coded values), and elaborating on the visual appearance of the compressed results.\n\n* Images: we\u2019ve now run tests on additional images, including the \u201cclassic\u201d examples {Barbara, Lena, Mandrill, Peppers}, as well as more of our own photographs.  All examples have been added to the online set, which has been consolidated at "
  },
  {
    "people": [
      "Graves",
      "Plamondon",
      "Graves",
      "Plamondon"
    ],
    "review": "This paper takes a model based on that of Graves and retrofits it with a representation derived from the work of Plamondon. \npart of the goal of deep learning has been to avoid the use of hand-crafted features and have the network learn from raw feature representations, so this paper is somewhat against the grain. \n\nThe paper relies on some qualitative examples as demonstration of the system, and doesn't seem to provide a strong motivation for there being any progress here. \nThe paper does not provide true text-conditional handwriting synthesis as shown in Graves' original work. \n\nBe more consistent about your bibliography (e.g. variants of Plamondon's own name, use of \"et al.\" in the bibliography etc.)"
  },
  {
    "people": [
      "Graves"
    ],
    "review": "This paper has no machine learning algorithmic contribution: it just uses the the same combination  of LSTM and bivariate mixture density network as Graves, and the detailed explanation in the appendix even misses one key essential point: how are the Gaussian parameters obtained as a transformation of the output of the LSTM.\nThere are also no numerical evaluation suggesting that the algorithm is some form of improvement over the state-of-the-art.\n\nSo I do not think such a paper is appropriate for a conference like ICLR. The part describing the handwriting tasks and the data transformation is well written and interesting to read, it could be valuable work for a conference more focused on handwriting recognition, but I am no expert in the field."
  },
  {
    "people": [
      "Plamondon"
    ],
    "review": "The paper presents a method for sequence generation with a known method applied to feature extracted from another existing method. The paper is heavily oriented towards to chosen technologies and lacks in literature on sequence generation. In principle, rich literature on motion prediction for various applications could be relevant here. Recent models exist for sequence prediction (from primed inputs) for various applications, e.g. for skeleton data. These models learn complex motion w/o any pre-processing. \n\nEvaluation is a big concern. There is no quantitative evaluation. There is no comparision with other methods.\n\nI still wonder whether the intermediate representation (developed by Plamondon et al.) is useful in this context of a fully trained sequence generation model and whether the model could pick up the necessary transformations itself. This should be evaluated.\n\nDetails:\n\nThere are several typos and word omissions, which can be found by carefully rereading the paper.\n\nAt the beginning of section 3, it is still unclear what the application is. Prediction of dynamic parameters? What for? Section 3 should give a better motivation of the work.\n\nConcerning the following paragraph\n\n\"While such methods are superior for handwriting analysis and biometric purposes, we opt for a less precise method (Berio & Leymarie, 2015) that is less sensitive to sampling quality and is aimed at generating virtual target sequences that remain perceptually similar to the original trace. \n\"\nThis method has not been explained. A paper should be self-contained.\n\nThe authors mentioned that the \"V2V-model is conditioned on (...)\"; but not enough details are given. \n\nGenerally speaking, more efforts could be made to make the paper more self-contained.\n"
  },
  {
    "people": [
      "Graves",
      "Plamondon",
      "Graves",
      "Plamondon"
    ],
    "review": "This paper takes a model based on that of Graves and retrofits it with a representation derived from the work of Plamondon. \npart of the goal of deep learning has been to avoid the use of hand-crafted features and have the network learn from raw feature representations, so this paper is somewhat against the grain. \n\nThe paper relies on some qualitative examples as demonstration of the system, and doesn't seem to provide a strong motivation for there being any progress here. \nThe paper does not provide true text-conditional handwriting synthesis as shown in Graves' original work. \n\nBe more consistent about your bibliography (e.g. variants of Plamondon's own name, use of \"et al.\" in the bibliography etc.) "
  },
  {
    "people": [
      "Graves",
      "Plamondon",
      "Graves",
      "Plamondon"
    ],
    "review": "This paper takes a model based on that of Graves and retrofits it with a representation derived from the work of Plamondon. \npart of the goal of deep learning has been to avoid the use of hand-crafted features and have the network learn from raw feature representations, so this paper is somewhat against the grain. \n\nThe paper relies on some qualitative examples as demonstration of the system, and doesn't seem to provide a strong motivation for there being any progress here. \nThe paper does not provide true text-conditional handwriting synthesis as shown in Graves' original work. \n\nBe more consistent about your bibliography (e.g. variants of Plamondon's own name, use of \"et al.\" in the bibliography etc.)"
  },
  {
    "people": [
      "Graves"
    ],
    "review": "This paper has no machine learning algorithmic contribution: it just uses the the same combination  of LSTM and bivariate mixture density network as Graves, and the detailed explanation in the appendix even misses one key essential point: how are the Gaussian parameters obtained as a transformation of the output of the LSTM.\nThere are also no numerical evaluation suggesting that the algorithm is some form of improvement over the state-of-the-art.\n\nSo I do not think such a paper is appropriate for a conference like ICLR. The part describing the handwriting tasks and the data transformation is well written and interesting to read, it could be valuable work for a conference more focused on handwriting recognition, but I am no expert in the field."
  },
  {
    "people": [
      "Plamondon"
    ],
    "review": "The paper presents a method for sequence generation with a known method applied to feature extracted from another existing method. The paper is heavily oriented towards to chosen technologies and lacks in literature on sequence generation. In principle, rich literature on motion prediction for various applications could be relevant here. Recent models exist for sequence prediction (from primed inputs) for various applications, e.g. for skeleton data. These models learn complex motion w/o any pre-processing. \n\nEvaluation is a big concern. There is no quantitative evaluation. There is no comparision with other methods.\n\nI still wonder whether the intermediate representation (developed by Plamondon et al.) is useful in this context of a fully trained sequence generation model and whether the model could pick up the necessary transformations itself. This should be evaluated.\n\nDetails:\n\nThere are several typos and word omissions, which can be found by carefully rereading the paper.\n\nAt the beginning of section 3, it is still unclear what the application is. Prediction of dynamic parameters? What for? Section 3 should give a better motivation of the work.\n\nConcerning the following paragraph\n\n\"While such methods are superior for handwriting analysis and biometric purposes, we opt for a less precise method (Berio & Leymarie, 2015) that is less sensitive to sampling quality and is aimed at generating virtual target sequences that remain perceptually similar to the original trace. \n\"\nThis method has not been explained. A paper should be self-contained.\n\nThe authors mentioned that the \"V2V-model is conditioned on (...)\"; but not enough details are given. \n\nGenerally speaking, more efforts could be made to make the paper more self-contained.\n"
  },
  {
    "people": [
      "Graves",
      "Plamondon",
      "Graves",
      "Plamondon"
    ],
    "review": "This paper takes a model based on that of Graves and retrofits it with a representation derived from the work of Plamondon. \npart of the goal of deep learning has been to avoid the use of hand-crafted features and have the network learn from raw feature representations, so this paper is somewhat against the grain. \n\nThe paper relies on some qualitative examples as demonstration of the system, and doesn't seem to provide a strong motivation for there being any progress here. \nThe paper does not provide true text-conditional handwriting synthesis as shown in Graves' original work. \n\nBe more consistent about your bibliography (e.g. variants of Plamondon's own name, use of \"et al.\" in the bibliography etc.) "
  },
  {
    "people": [
      "Svore"
    ],
    "review": "This paper provides two RNN-based architectures for extractive document summarization. The first, \"Classify\", reads in the whole document and traverses the sentences a second time to decide whether to include them or not (0/1 decisions). The second, \"Select\",  reads in the whole document and picks the most relevant sentence one at the time. The models assume that oracle extractive summaries exist, and a pseudo ground-truth generation procedure is used, which mimics Svore et al. (2007) among others. \n\nOverall, this paper seems a small increment over Cheng & Lapata (2016) and performance is similar or worse to that paper. The problem of single document extractive summarization is not particularly exciting since in DUC 2002 (14 years ago) existing models could not beat the lead baseline (which selects the first sentences of the document). It's a pity that this paper doesn't address the most interesting problems of abstractive summarization or apply the proposed approach to multi-document summarization. It's also a little disappointing that the maximum sentence length had to be capped to 50, which suggests the model has some trouble to scale."
  },
  {
    "people": [
      "Cheng",
      "Lapata"
    ],
    "review": "Reviewers found this paper clear to read, but leaned negative on in terms of impact and originality of the work. Main complaint is that the paper is neither significantly novel in terms of modeling (pointing to Cheng & Lapata), nor significantly more performative on this task (\"only slightly better\"). One reviewer also has a side complaint that the task itself is also somewhat simplistic and simplified, and suggests other tasks. This comment is perhaps harsh, but reflects a mandate for revisiting \"old\" problems to provide significant improvements in accuracy or novel modeling."
  },
  {
    "people": [
      "Cheng",
      "Lapata",
      "Lapata",
      "Lapata",
      "Cheng",
      "Lapata"
    ],
    "review": "This paper presents two RNN architectures for extractive document summarization. The first one, Classifier, takes into account the order in which sentences appear in the original document, whereas the second one, Selector, chooses sentences in an arbitrary order. For each architecture, the concatenated RNN hidden state from a sentence forward and backward pass  is used as features to compute a score that captures content richness, salience, positional importance, and redundancy. Both models are trained in a supervised manner, so the authors used \"pseudo-ground truth generation\" to create training data from abstractive summaries. Experiments show that the Classifier model performs better, and it achieves near state-of-the-art performance for some evaluation metrics.\n\nThe proposed model is in general an extension of Cheng and Lapata, 2016. Unfortunately, the performance is only slightly better or sometimes even worse. The authors mentioned that one key difference how they transform abstractive summaries to become gold labels for the supervised method. However, in the experiment results, the authors described that one potential reason their models do not consistently outperform the extractive model of Cheng & Lapata, 2016 is that the unsupervised greedy approximation may generate noisier ground truth labels than Cheng & Lapata. Is there a reason to construct the training data similar to Cheng & Lapata, if that turns out to be a better method?\nIn order for the proposed models to be convincing, they need to outperform this baseline that's very similar to the proposed methods more consistently, since the main contribution is improved neural architectures for extractive document summarization.\n"
  },
  {
    "people": [
      "Svore"
    ],
    "review": "This paper provides two RNN-based architectures for extractive document summarization. The first, \"Classify\", reads in the whole document and traverses the sentences a second time to decide whether to include them or not (0/1 decisions). The second, \"Select\",  reads in the whole document and picks the most relevant sentence one at the time. The models assume that oracle extractive summaries exist, and a pseudo ground-truth generation procedure is used, which mimics Svore et al. (2007) among others. \n\nOverall, this paper seems a small increment over Cheng & Lapata (2016) and performance is similar or worse to that paper. The problem of single document extractive summarization is not particularly exciting since in DUC 2002 (14 years ago) existing models could not beat the lead baseline (which selects the first sentences of the document). It's a pity that this paper doesn't address the most interesting problems of abstractive summarization or apply the proposed approach to multi-document summarization. It's also a little disappointing that the maximum sentence length had to be capped to 50, which suggests the model has some trouble to scale."
  },
  {
    "people": [
      "Svore"
    ],
    "review": "This paper provides two RNN-based architectures for extractive document summarization. The first, \"Classify\", reads in the whole document and traverses the sentences a second time to decide whether to include them or not (0/1 decisions). The second, \"Select\",  reads in the whole document and picks the most relevant sentence one at the time. The models assume that oracle extractive summaries exist, and a pseudo ground-truth generation procedure is used, which mimics Svore et al. (2007) among others. \n\nOverall, this paper seems a small increment over Cheng & Lapata (2016) and performance is similar or worse to that paper. The problem of single document extractive summarization is not particularly exciting since in DUC 2002 (14 years ago) existing models could not beat the lead baseline (which selects the first sentences of the document). It's a pity that this paper doesn't address the most interesting problems of abstractive summarization or apply the proposed approach to multi-document summarization. It's also a little disappointing that the maximum sentence length had to be capped to 50, which suggests the model has some trouble to scale."
  },
  {
    "people": [
      "Cheng",
      "Lapata"
    ],
    "review": "Reviewers found this paper clear to read, but leaned negative on in terms of impact and originality of the work. Main complaint is that the paper is neither significantly novel in terms of modeling (pointing to Cheng & Lapata), nor significantly more performative on this task (\"only slightly better\"). One reviewer also has a side complaint that the task itself is also somewhat simplistic and simplified, and suggests other tasks. This comment is perhaps harsh, but reflects a mandate for revisiting \"old\" problems to provide significant improvements in accuracy or novel modeling."
  },
  {
    "people": [
      "Cheng",
      "Lapata",
      "Lapata",
      "Lapata",
      "Cheng",
      "Lapata"
    ],
    "review": "This paper presents two RNN architectures for extractive document summarization. The first one, Classifier, takes into account the order in which sentences appear in the original document, whereas the second one, Selector, chooses sentences in an arbitrary order. For each architecture, the concatenated RNN hidden state from a sentence forward and backward pass  is used as features to compute a score that captures content richness, salience, positional importance, and redundancy. Both models are trained in a supervised manner, so the authors used \"pseudo-ground truth generation\" to create training data from abstractive summaries. Experiments show that the Classifier model performs better, and it achieves near state-of-the-art performance for some evaluation metrics.\n\nThe proposed model is in general an extension of Cheng and Lapata, 2016. Unfortunately, the performance is only slightly better or sometimes even worse. The authors mentioned that one key difference how they transform abstractive summaries to become gold labels for the supervised method. However, in the experiment results, the authors described that one potential reason their models do not consistently outperform the extractive model of Cheng & Lapata, 2016 is that the unsupervised greedy approximation may generate noisier ground truth labels than Cheng & Lapata. Is there a reason to construct the training data similar to Cheng & Lapata, if that turns out to be a better method?\nIn order for the proposed models to be convincing, they need to outperform this baseline that's very similar to the proposed methods more consistently, since the main contribution is improved neural architectures for extractive document summarization.\n"
  },
  {
    "people": [
      "Svore"
    ],
    "review": "This paper provides two RNN-based architectures for extractive document summarization. The first, \"Classify\", reads in the whole document and traverses the sentences a second time to decide whether to include them or not (0/1 decisions). The second, \"Select\",  reads in the whole document and picks the most relevant sentence one at the time. The models assume that oracle extractive summaries exist, and a pseudo ground-truth generation procedure is used, which mimics Svore et al. (2007) among others. \n\nOverall, this paper seems a small increment over Cheng & Lapata (2016) and performance is similar or worse to that paper. The problem of single document extractive summarization is not particularly exciting since in DUC 2002 (14 years ago) existing models could not beat the lead baseline (which selects the first sentences of the document). It's a pity that this paper doesn't address the most interesting problems of abstractive summarization or apply the proposed approach to multi-document summarization. It's also a little disappointing that the maximum sentence length had to be capped to 50, which suggests the model has some trouble to scale."
  },
  {
    "people": [
      "Lenc",
      "Vedaldi",
      "Krizhevsky"
    ],
    "review": "This paper empirically studies the invariance, equivariance and equivalence properties of representations learned by convolutional networks under various kinds of data augmentation. Additional loss terms are presented which can make a representation more invariant or equivariant.\n\nThe idea of measuring invariance, equivariance and equivalence of representations is not new (Lenc & Vedaldi). The authors are the first to systematically study the effect of data augmentation on these properties, but it is unclear in what way the results are surprising, interesting, or useful. It is not really surprising that data augmentation increases invariance, or that training with the same augmentation leads to more similar representations than training with different augmentations.\n\nRegarding the presented method to increase invariance and equivariance: while it could be that a representation will generalize better if it is invariant or equivariant, it is not clear why one would want to increase in/equivariance if it does not indeed lead to improvements in performance. The paper presents no evidence that training for increased invariance / equivariance leads to substantial improvements in performance. Combined with the fact that the loss (eq. 6) would substantially increase the computational burden, I don\u2019t think this technique will be very useful.\n\nMinor comments:\n-R^{nxn} should be R^{n \\times n}\n-In eq. 2: \u2018equivaraince\u2019\n-In 3.3, argmax is not properly formatted\n-I think data augmentation was already considered essential before Krizhevsky et al. Not really correct to attribute this to them.\n- About the claim \u201cThis is related to the idea of whether CNNs collapse (invariance) or linearize (equivariance) view manifolds of 3D objects\u201d. The idea that equivariance means that the manifold (orbit) is linearized, is incorrect. A linear representation M_g can create nonlinear manifolds. A simple example is given by a rotation matrix in 2D (clearly linear), generating a nonlinear manifold (the circle). \n- Equivariance in eq. 2 should be called \u201cnon-equivariance\u201d. If the value is low, the representation is equivariant, while if it is high it is non-equivariant.\n- \u201cEq. 2  also uses the paradigm that\u201d, uses the word paradigm in a strange manner\n- In the definition of x\u2019_ij, should one of the g_j be inverted? Otherwise it seems like the transformation is applied twice, instead of being undone.\n"
  },
  {
    "people": [
      "Decoste",
      "Scholkopf"
    ],
    "review": "This work presents an empirical study of the influence of different types of data augmentation on the performance of CNNs. It also proposes to incorporate additional loss functions to encourage approximate invariance or equivariance, and shows there are some benefits.\n\nThe paper reads well and the objectives are clear. The study of invariances in CNNs is a very important topic, and advances in this area are greatly appreciated. The paper splits itself in two very different parts -- the empirical study of equivariances in existing CNNs, and the proposal of equivariance objectives. However, taken separately each of these two parts could be better executed.\n\nOn the empirical study, its breath is relatively limited, and it's hard to draw any far-reaching conclusions from it:\n- Only one network is studied; at least one other architecture would have made for better generalization.\n- Only one layer (fc7) is studied; this presents issues as the top layer is the most invariant. At least one convolutional layer (possibly more) should have been considered.\n- The reliance on the scanned text dataset does not help; however the ImageNet results are definitely very encouraging.\n\nIt is nice to see how performance degrades with the degree of transformations, and the authors do interpret the results, but it would be better to see more analysis. There is only a limited set of conclusions that can be drawn from evaluating networks with jittered data. If the authors could propose some other interesting ways to assess the invariance and equivariance, they would potentially draw more insightful conclusions from it.\n\nOn the proposed loss function, only a very quick treatment of it is given (Section 4, half a page). It does not differ too much from known invariance/equivariance objectives studied in the literature previously, e.g. Decoste and Scholkopf, \"Training Invariant Support Vector Machines\", Machine Learning, 2002.\n\nI'm not sure that dividing the paper into these two different contributions is the best approach; they both feel a bit incomplete, and a full treatment of only one of them would make for an overall better paper.\n\n"
  },
  {
    "people": [
      "Lenc",
      "Vedaldi",
      "Krizhevsky"
    ],
    "review": "This paper empirically studies the invariance, equivariance and equivalence properties of representations learned by convolutional networks under various kinds of data augmentation. Additional loss terms are presented which can make a representation more invariant or equivariant.\n\nThe idea of measuring invariance, equivariance and equivalence of representations is not new (Lenc & Vedaldi). The authors are the first to systematically study the effect of data augmentation on these properties, but it is unclear in what way the results are surprising, interesting, or useful. It is not really surprising that data augmentation increases invariance, or that training with the same augmentation leads to more similar representations than training with different augmentations.\n\nRegarding the presented method to increase invariance and equivariance: while it could be that a representation will generalize better if it is invariant or equivariant, it is not clear why one would want to increase in/equivariance if it does not indeed lead to improvements in performance. The paper presents no evidence that training for increased invariance / equivariance leads to substantial improvements in performance. Combined with the fact that the loss (eq. 6) would substantially increase the computational burden, I don\u2019t think this technique will be very useful.\n\nMinor comments:\n-R^{nxn} should be R^{n \\times n}\n-In eq. 2: \u2018equivaraince\u2019\n-In 3.3, argmax is not properly formatted\n-I think data augmentation was already considered essential before Krizhevsky et al. Not really correct to attribute this to them.\n- About the claim \u201cThis is related to the idea of whether CNNs collapse (invariance) or linearize (equivariance) view manifolds of 3D objects\u201d. The idea that equivariance means that the manifold (orbit) is linearized, is incorrect. A linear representation M_g can create nonlinear manifolds. A simple example is given by a rotation matrix in 2D (clearly linear), generating a nonlinear manifold (the circle). \n- Equivariance in eq. 2 should be called \u201cnon-equivariance\u201d. If the value is low, the representation is equivariant, while if it is high it is non-equivariant.\n- \u201cEq. 2  also uses the paradigm that\u201d, uses the word paradigm in a strange manner\n- In the definition of x\u2019_ij, should one of the g_j be inverted? Otherwise it seems like the transformation is applied twice, instead of being undone.\n"
  },
  {
    "people": [
      "Decoste",
      "Scholkopf"
    ],
    "review": "This work presents an empirical study of the influence of different types of data augmentation on the performance of CNNs. It also proposes to incorporate additional loss functions to encourage approximate invariance or equivariance, and shows there are some benefits.\n\nThe paper reads well and the objectives are clear. The study of invariances in CNNs is a very important topic, and advances in this area are greatly appreciated. The paper splits itself in two very different parts -- the empirical study of equivariances in existing CNNs, and the proposal of equivariance objectives. However, taken separately each of these two parts could be better executed.\n\nOn the empirical study, its breath is relatively limited, and it's hard to draw any far-reaching conclusions from it:\n- Only one network is studied; at least one other architecture would have made for better generalization.\n- Only one layer (fc7) is studied; this presents issues as the top layer is the most invariant. At least one convolutional layer (possibly more) should have been considered.\n- The reliance on the scanned text dataset does not help; however the ImageNet results are definitely very encouraging.\n\nIt is nice to see how performance degrades with the degree of transformations, and the authors do interpret the results, but it would be better to see more analysis. There is only a limited set of conclusions that can be drawn from evaluating networks with jittered data. If the authors could propose some other interesting ways to assess the invariance and equivariance, they would potentially draw more insightful conclusions from it.\n\nOn the proposed loss function, only a very quick treatment of it is given (Section 4, half a page). It does not differ too much from known invariance/equivariance objectives studied in the literature previously, e.g. Decoste and Scholkopf, \"Training Invariant Support Vector Machines\", Machine Learning, 2002.\n\nI'm not sure that dividing the paper into these two different contributions is the best approach; they both feel a bit incomplete, and a full treatment of only one of them would make for an overall better paper.\n\n"
  },
  {
    "people": [
      "Jonschkowski",
      "Brock",
      "Jonschkowki",
      "Brock",
      "Finn",
      "Finn"
    ],
    "review": "This paper implements the method of Jonschkowski & Brock to learn a low-dimensional state representation represented as the last layer of a neural network. The experiments apply the method for learning a one-dimensional state representation of a simulated robot\u2019s head position from synthetic images.\n\nLearning state representations is an active and useful area of research for learning representations in interactive domains such as robotics. However, there seems to be no novelty in the method, over Jonschkowki & Brock. The primary contribution is the experimental evaluation performed on one task, where the paper evaluates the correlation between the learned state representation and the ideal state representation for the task (which is the robot\u2019s head position).\n\nAs acknowledged by the authors, the experiments are very preliminary, only showing one simple task with a one-dimensional learned representation and a two-dimensional discrete action space. To make the experiments compelling, there need to be comparisons to prior methods such as Lange et al. \u201912, Watter et al. NIPS \u201915, and Finn et al. ICRA \u201916 which also learn state representations from raw images. PCA on the images would also be a useful comparison, especially for simple tasks. Without these comparisons, it is impossible to evaluate the effectiveness of the method.\n\nLastly, as mentioned in the pre-review questions, the related work should include a discussion of other state representation learning methods such as Watter et al. NIPS \u201915, Finn et al. ICRA \u201916, and van Hoof et al. IROS \u201916.\n\nIn summary, this paper lacks novelty and significance, as the paper implements an existing method and demonstrates results on only one simple task. Without comparisons, the results are impossible to interpret. More challenging tasks and experimental comparisons would significantly improve the paper. Additionally, this paper does not introduce any novel contributions to state representation learning for solving challenges in this domain. One pro is that the paper is generally written clearly."
  },
  {
    "people": [
      "Jonschkowski",
      "Brock",
      "Jonschkowki",
      "Brock",
      "Finn",
      "Finn"
    ],
    "review": "This paper implements the method of Jonschkowski & Brock to learn a low-dimensional state representation represented as the last layer of a neural network. The experiments apply the method for learning a one-dimensional state representation of a simulated robot\u2019s head position from synthetic images.\n\nLearning state representations is an active and useful area of research for learning representations in interactive domains such as robotics. However, there seems to be no novelty in the method, over Jonschkowki & Brock. The primary contribution is the experimental evaluation performed on one task, where the paper evaluates the correlation between the learned state representation and the ideal state representation for the task (which is the robot\u2019s head position).\n\nAs acknowledged by the authors, the experiments are very preliminary, only showing one simple task with a one-dimensional learned representation and a two-dimensional discrete action space. To make the experiments compelling, there need to be comparisons to prior methods such as Lange et al. \u201912, Watter et al. NIPS \u201915, and Finn et al. ICRA \u201916 which also learn state representations from raw images. PCA on the images would also be a useful comparison, especially for simple tasks. Without these comparisons, it is impossible to evaluate the effectiveness of the method.\n\nLastly, as mentioned in the pre-review questions, the related work should include a discussion of other state representation learning methods such as Watter et al. NIPS \u201915, Finn et al. ICRA \u201916, and van Hoof et al. IROS \u201916.\n\nIn summary, this paper lacks novelty and significance, as the paper implements an existing method and demonstrates results on only one simple task. Without comparisons, the results are impossible to interpret. More challenging tasks and experimental comparisons would significantly improve the paper. Additionally, this paper does not introduce any novel contributions to state representation learning for solving challenges in this domain. One pro is that the paper is generally written clearly."
  },
  {
    "people": [
      "Koutnik"
    ],
    "review": "This is a very interesting and timely paper, with multiple contributions. \n- it proposes a setup for dealing with combinatorial perception and action-spaces that generalizes to an arbitrary number of units and opponent units,\n- it establishes some deep RL baseline results on a collection of Starcraft subdomains,\n- it proposes a new algorithm that is a hybrid between black-box optimization REINFORCE, and which facilitates consistent exploration.\n\n\nAs mentioned in an earlier comment, I don\u2019t see why the \u201cgradient of the average cumulative reward\u201d is a reasonable choice, as compared to just the average reward? This over-weights late rewards at the expense of early ones, so the updates are not matching the measured objective. The authors state that they \u201cdid not observe a large difference in preliminary experiments\u201d -- so if that is the case, then why not choose the correct objective?\n\nDPQ is characterized incorrectly: despite its name, it does not \u201ccollect traces by following deterministic policies\u201d, instead it follows a stochastic behavior policy and learns off-policy about the deterministic policy. Please revise this. \n\nGradient-free optimization is also characterized incorrectly (\u201cit only scales to few parameters\u201d), recent work has shown that this can be overcome (e.g. the TORCS paper by Koutnik et al, 2013). This also suggests that your \u201cpreliminary experiments with direct exploration in the parameter space\u201d may not have followed best practices in neuroevolution? Did you try out some of the recent variants of NEAT for example, which have been applied to similar domains in the past?\n\nOn the specific results, I\u2019m wondering about the DQN transfer from m15v16 to m5v5, obtaining the best win rate of 96% in transfer, despite only reaching 13% (the worst) on the training domain? Is this a typo, or how can you explain that?"
  },
  {
    "people": [
      "Koutnik"
    ],
    "review": "This is a very interesting and timely paper, with multiple contributions. \n- it proposes a setup for dealing with combinatorial perception and action-spaces that generalizes to an arbitrary number of units and opponent units,\n- it establishes some deep RL baseline results on a collection of Starcraft subdomains,\n- it proposes a new algorithm that is a hybrid between black-box optimization REINFORCE, and which facilitates consistent exploration.\n\n\nAs mentioned in an earlier comment, I don\u2019t see why the \u201cgradient of the average cumulative reward\u201d is a reasonable choice, as compared to just the average reward? This over-weights late rewards at the expense of early ones, so the updates are not matching the measured objective. The authors state that they \u201cdid not observe a large difference in preliminary experiments\u201d -- so if that is the case, then why not choose the correct objective?\n\nDPQ is characterized incorrectly: despite its name, it does not \u201ccollect traces by following deterministic policies\u201d, instead it follows a stochastic behavior policy and learns off-policy about the deterministic policy. Please revise this. \n\nGradient-free optimization is also characterized incorrectly (\u201cit only scales to few parameters\u201d), recent work has shown that this can be overcome (e.g. the TORCS paper by Koutnik et al, 2013). This also suggests that your \u201cpreliminary experiments with direct exploration in the parameter space\u201d may not have followed best practices in neuroevolution? Did you try out some of the recent variants of NEAT for example, which have been applied to similar domains in the past?\n\nOn the specific results, I\u2019m wondering about the DQN transfer from m15v16 to m5v5, obtaining the best win rate of 96% in transfer, despite only reaching 13% (the worst) on the training domain? Is this a typo, or how can you explain that?"
  },
  {
    "people": [
      "Zhang",
      "Zha",
      "Brand",
      "Verbeek",
      "Roweis",
      "Vlassis",
      "Bengio",
      "van der Maaten"
    ],
    "review": "The paper presents an analysis of the ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds. Specifically, the paper focuses on what the authors call \"monotonic chains of linear segments\", which are essentially sets of intersecting tangent planes. The paper presents a construction that efficiently models such manifolds in a deep net, and presents a basic error analysis of the resulting construction.\n\nWhile the presented results are novel to the best of my knowledge, they are hardly surprising (1) given what we already know about the representational power of deep networks and (2) given that the study selects a deep network architecture and a data structure that are very \"compatible\". In particular, I have three main concerns with respect to the results presented in this paper:\n\n(1) In the last decade, there has been quite a bit of work on learning data representations from sets of local tangent planes. Examples that spring to mind are local tangent space analysis of Zhang & Zha (2002), manifold charting by Brand (2002) and alignment of local models by Verbeek, Roweis, and Vlassis (2003). None of this work is referred to in related work, even though it seems highly relevant to the analysis presented here. For instance, it would be interesting to see how these old techniques compare to the deep network trained to produce the embedding of Figure 6. This may provide some insight into the inductive biases the deep net introduces: does it learn better representations that non-parametric techniques because it has better inductive biases, or does it learn worse representations because the loss being optimized is non-convex?\n\n(2) It is difficult to see how the analysis generalizes to more complex data in which local linearity assumptions on the data manifold are vacuous given the sparsity of data in high-dimensional space, or how it generalizes to deep network architectures that are not pure ReLU networks. For instance, most modern networks use a variant of batch normalization; this already appears to break the presented analyses.\n\n(3) The error bound presented in Section 4 appears vacuous for any practical setting, as the upper bound on the error is exponential in the total curvature (a quantity that will be quite large in most practical settings). This is underlined by the analysis of the Swiss roll dataset, of which the authors state that the \"bound for this case is very loose\". The fact that the bound is already so loose for this arguably very simple manifold makes that the error analysis may tell us very little about the representational power of deep nets.\n\nI would encourage the authors to address issue (1) in the revision of the paper. Issue (2) and (3) may be harder to address, but is essential that they are addressed for the line of work pioneered by this paper to have an impact on our understanding of deep learning.\n\n\nMinor comments: \n\n- In prior work, the authors only refer to fully supervised siamese network approaches. These approaches differ from that taken by the authors, as their approach is unsupervised. It should be noted that the authors are not the first to study unsupervised representation learners parametrized by deep networks: other important examples are deep autoencoders (Hinton & Salakhutdinov, 2006 and work on denoising autoencoders from Bengio's group) and parametric t-SNE (van der Maaten, 2009).\n- What loss do the authors use in their experiments? Using \"the difference between the ground truth distance ... and the distance computed by the network\" seems odd, because it encourages the network to produce infinitely large distances (to get a loss of minus infinity). Is the difference squared?"
  },
  {
    "people": [
      "Zhang",
      "Zha",
      "Brand",
      "Verbeek",
      "Roweis",
      "Vlassis",
      "Bengio",
      "van der Maaten"
    ],
    "review": "The paper presents an analysis of the ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds. Specifically, the paper focuses on what the authors call \"monotonic chains of linear segments\", which are essentially sets of intersecting tangent planes. The paper presents a construction that efficiently models such manifolds in a deep net, and presents a basic error analysis of the resulting construction.\n\nWhile the presented results are novel to the best of my knowledge, they are hardly surprising (1) given what we already know about the representational power of deep networks and (2) given that the study selects a deep network architecture and a data structure that are very \"compatible\". In particular, I have three main concerns with respect to the results presented in this paper:\n\n(1) In the last decade, there has been quite a bit of work on learning data representations from sets of local tangent planes. Examples that spring to mind are local tangent space analysis of Zhang & Zha (2002), manifold charting by Brand (2002) and alignment of local models by Verbeek, Roweis, and Vlassis (2003). None of this work is referred to in related work, even though it seems highly relevant to the analysis presented here. For instance, it would be interesting to see how these old techniques compare to the deep network trained to produce the embedding of Figure 6. This may provide some insight into the inductive biases the deep net introduces: does it learn better representations that non-parametric techniques because it has better inductive biases, or does it learn worse representations because the loss being optimized is non-convex?\n\n(2) It is difficult to see how the analysis generalizes to more complex data in which local linearity assumptions on the data manifold are vacuous given the sparsity of data in high-dimensional space, or how it generalizes to deep network architectures that are not pure ReLU networks. For instance, most modern networks use a variant of batch normalization; this already appears to break the presented analyses.\n\n(3) The error bound presented in Section 4 appears vacuous for any practical setting, as the upper bound on the error is exponential in the total curvature (a quantity that will be quite large in most practical settings). This is underlined by the analysis of the Swiss roll dataset, of which the authors state that the \"bound for this case is very loose\". The fact that the bound is already so loose for this arguably very simple manifold makes that the error analysis may tell us very little about the representational power of deep nets.\n\nI would encourage the authors to address issue (1) in the revision of the paper. Issue (2) and (3) may be harder to address, but is essential that they are addressed for the line of work pioneered by this paper to have an impact on our understanding of deep learning.\n\n\nMinor comments: \n\n- In prior work, the authors only refer to fully supervised siamese network approaches. These approaches differ from that taken by the authors, as their approach is unsupervised. It should be noted that the authors are not the first to study unsupervised representation learners parametrized by deep networks: other important examples are deep autoencoders (Hinton & Salakhutdinov, 2006 and work on denoising autoencoders from Bengio's group) and parametric t-SNE (van der Maaten, 2009).\n- What loss do the authors use in their experiments? Using \"the difference between the ground truth distance ... and the distance computed by the network\" seems odd, because it encourages the network to produce infinitely large distances (to get a loss of minus infinity). Is the difference squared?"
  },
  {
    "people": [
      "Peano",
      "Das",
      "Wiles",
      "Graves",
      "Joulin"
    ],
    "review": "The paper presents a way to \"learn\" approximate data structures. They train neural networks (ConvNets here) to perform as an approximate abstract data structure by having an L2 loss (for the unrolled NN) on respecting the axioms of the data structure they want the NN to learn. E.g. you NN.push(8), NN.push(6), NN.push(4), the loss is proportional to the distance with what is NN.pop()ed three times and 4, 6, 8 (this example is the one of Figure 1).\n\nThere are several flaws:\n - In the case of the stack: I do not see a difference between this and a seq-to-seq RNN trained with e.g. 8, 6, 4 as input sequence, to predict 4, 6, 8.\n - While some of the previous work is adequately cited, there is an important body of previous work (some from the 90s) on learning Peano's axioms, stacks, queues, etc. that is not cited nor compared to. For instance [Das et al. 1992], [Wiles & Elman 1995], and more recently [Graves et al. 2014], [Joulin & Mikolov 2015], [Kaiser & Sutskever 2016]...\n - Using MNIST digits, and not e.g. a categorical distribution on numbers, is adding complexity for no reason.\n - (Probably the biggest flaw) The experimental section is too weak to support the claims. The figures are adequate, but there is no comparison to anything. There is also no description nor attempt to quantify a form of \"success rate\" of learning such data structures, for instance w.r.t the number of examples, or w.r.t to the size of the input sequences. The current version of the paper (December 9th 2016) provides, at best, anecdotal experimental evidence to support the claims of the rest of the paper.\n\nWhile an interesting direction of research, I think that this paper is not experimentally sound enough for ICLR."
  },
  {
    "people": [
      "Peano",
      "Das",
      "Wiles",
      "Graves",
      "Joulin"
    ],
    "review": "The paper presents a way to \"learn\" approximate data structures. They train neural networks (ConvNets here) to perform as an approximate abstract data structure by having an L2 loss (for the unrolled NN) on respecting the axioms of the data structure they want the NN to learn. E.g. you NN.push(8), NN.push(6), NN.push(4), the loss is proportional to the distance with what is NN.pop()ed three times and 4, 6, 8 (this example is the one of Figure 1).\n\nThere are several flaws:\n - In the case of the stack: I do not see a difference between this and a seq-to-seq RNN trained with e.g. 8, 6, 4 as input sequence, to predict 4, 6, 8.\n - While some of the previous work is adequately cited, there is an important body of previous work (some from the 90s) on learning Peano's axioms, stacks, queues, etc. that is not cited nor compared to. For instance [Das et al. 1992], [Wiles & Elman 1995], and more recently [Graves et al. 2014], [Joulin & Mikolov 2015], [Kaiser & Sutskever 2016]...\n - Using MNIST digits, and not e.g. a categorical distribution on numbers, is adding complexity for no reason.\n - (Probably the biggest flaw) The experimental section is too weak to support the claims. The figures are adequate, but there is no comparison to anything. There is also no description nor attempt to quantify a form of \"success rate\" of learning such data structures, for instance w.r.t the number of examples, or w.r.t to the size of the input sequences. The current version of the paper (December 9th 2016) provides, at best, anecdotal experimental evidence to support the claims of the rest of the paper.\n\nWhile an interesting direction of research, I think that this paper is not experimentally sound enough for ICLR."
  },
  {
    "people": [
      "Peano",
      "Das",
      "Wiles",
      "Graves",
      "Joulin"
    ],
    "review": "The paper presents a way to \"learn\" approximate data structures. They train neural networks (ConvNets here) to perform as an approximate abstract data structure by having an L2 loss (for the unrolled NN) on respecting the axioms of the data structure they want the NN to learn. E.g. you NN.push(8), NN.push(6), NN.push(4), the loss is proportional to the distance with what is NN.pop()ed three times and 4, 6, 8 (this example is the one of Figure 1).\n\nThere are several flaws:\n - In the case of the stack: I do not see a difference between this and a seq-to-seq RNN trained with e.g. 8, 6, 4 as input sequence, to predict 4, 6, 8.\n - While some of the previous work is adequately cited, there is an important body of previous work (some from the 90s) on learning Peano's axioms, stacks, queues, etc. that is not cited nor compared to. For instance [Das et al. 1992], [Wiles & Elman 1995], and more recently [Graves et al. 2014], [Joulin & Mikolov 2015], [Kaiser & Sutskever 2016]...\n - Using MNIST digits, and not e.g. a categorical distribution on numbers, is adding complexity for no reason.\n - (Probably the biggest flaw) The experimental section is too weak to support the claims. The figures are adequate, but there is no comparison to anything. There is also no description nor attempt to quantify a form of \"success rate\" of learning such data structures, for instance w.r.t the number of examples, or w.r.t to the size of the input sequences. The current version of the paper (December 9th 2016) provides, at best, anecdotal experimental evidence to support the claims of the rest of the paper.\n\nWhile an interesting direction of research, I think that this paper is not experimentally sound enough for ICLR."
  },
  {
    "people": [
      "Peano",
      "Das",
      "Wiles",
      "Graves",
      "Joulin"
    ],
    "review": "The paper presents a way to \"learn\" approximate data structures. They train neural networks (ConvNets here) to perform as an approximate abstract data structure by having an L2 loss (for the unrolled NN) on respecting the axioms of the data structure they want the NN to learn. E.g. you NN.push(8), NN.push(6), NN.push(4), the loss is proportional to the distance with what is NN.pop()ed three times and 4, 6, 8 (this example is the one of Figure 1).\n\nThere are several flaws:\n - In the case of the stack: I do not see a difference between this and a seq-to-seq RNN trained with e.g. 8, 6, 4 as input sequence, to predict 4, 6, 8.\n - While some of the previous work is adequately cited, there is an important body of previous work (some from the 90s) on learning Peano's axioms, stacks, queues, etc. that is not cited nor compared to. For instance [Das et al. 1992], [Wiles & Elman 1995], and more recently [Graves et al. 2014], [Joulin & Mikolov 2015], [Kaiser & Sutskever 2016]...\n - Using MNIST digits, and not e.g. a categorical distribution on numbers, is adding complexity for no reason.\n - (Probably the biggest flaw) The experimental section is too weak to support the claims. The figures are adequate, but there is no comparison to anything. There is also no description nor attempt to quantify a form of \"success rate\" of learning such data structures, for instance w.r.t the number of examples, or w.r.t to the size of the input sequences. The current version of the paper (December 9th 2016) provides, at best, anecdotal experimental evidence to support the claims of the rest of the paper.\n\nWhile an interesting direction of research, I think that this paper is not experimentally sound enough for ICLR."
  },
  {
    "people": [
      "Deift"
    ],
    "review": "The idea of universality that is independent of input distribution and dimension, depending only on the algorithm is an appealing one. However, as an empirical study, this paper comes up somewhat short:\n\n1. Exactly one algorithm is shown for the deep learning example. It would have been more convincing to compare distributions with one or more algorithms. \n\n2. The definition (1),  and much of the work of Section 2.1 seems to have already been covered in Deift (2014), Section 1.3. In that paper, a number of different algorithms for the solution of linear systems are considered, and then the concept of universality becomes more plausible. I do not see enough of such algorithmic comparisons in this paper (same problem setup, different algorithms).\n\n3.  It seems to me that what practitioners might care about in practice are both the mean and variance in running times; these quantities are buried in (1). So I question how useful the distribution itself might be for algorithm tuning. \n\nAt the least, many more empirical comparisons should be provided to convince me that the universality holds across a broad range of algorithms."
  },
  {
    "people": [
      "Deift"
    ],
    "review": "The idea of universality that is independent of input distribution and dimension, depending only on the algorithm is an appealing one. However, as an empirical study, this paper comes up somewhat short:\n\n1. Exactly one algorithm is shown for the deep learning example. It would have been more convincing to compare distributions with one or more algorithms. \n\n2. The definition (1),  and much of the work of Section 2.1 seems to have already been covered in Deift (2014), Section 1.3. In that paper, a number of different algorithms for the solution of linear systems are considered, and then the concept of universality becomes more plausible. I do not see enough of such algorithmic comparisons in this paper (same problem setup, different algorithms).\n\n3.  It seems to me that what practitioners might care about in practice are both the mean and variance in running times; these quantities are buried in (1). So I question how useful the distribution itself might be for algorithm tuning. \n\nAt the least, many more empirical comparisons should be provided to convince me that the universality holds across a broad range of algorithms."
  },
  {
    "people": [
      "Deift"
    ],
    "review": "The idea of universality that is independent of input distribution and dimension, depending only on the algorithm is an appealing one. However, as an empirical study, this paper comes up somewhat short:\n\n1. Exactly one algorithm is shown for the deep learning example. It would have been more convincing to compare distributions with one or more algorithms. \n\n2. The definition (1),  and much of the work of Section 2.1 seems to have already been covered in Deift (2014), Section 1.3. In that paper, a number of different algorithms for the solution of linear systems are considered, and then the concept of universality becomes more plausible. I do not see enough of such algorithmic comparisons in this paper (same problem setup, different algorithms).\n\n3.  It seems to me that what practitioners might care about in practice are both the mean and variance in running times; these quantities are buried in (1). So I question how useful the distribution itself might be for algorithm tuning. \n\nAt the least, many more empirical comparisons should be provided to convince me that the universality holds across a broad range of algorithms."
  },
  {
    "people": [
      "Deift"
    ],
    "review": "The idea of universality that is independent of input distribution and dimension, depending only on the algorithm is an appealing one. However, as an empirical study, this paper comes up somewhat short:\n\n1. Exactly one algorithm is shown for the deep learning example. It would have been more convincing to compare distributions with one or more algorithms. \n\n2. The definition (1),  and much of the work of Section 2.1 seems to have already been covered in Deift (2014), Section 1.3. In that paper, a number of different algorithms for the solution of linear systems are considered, and then the concept of universality becomes more plausible. I do not see enough of such algorithmic comparisons in this paper (same problem setup, different algorithms).\n\n3.  It seems to me that what practitioners might care about in practice are both the mean and variance in running times; these quantities are buried in (1). So I question how useful the distribution itself might be for algorithm tuning. \n\nAt the least, many more empirical comparisons should be provided to convince me that the universality holds across a broad range of algorithms."
  },
  {
    "people": [
      "Zhu",
      "Jampani"
    ],
    "review": "Two things I really liked about this paper:\n1. The whole idea of having a data-dependent proposal distribution for MCMC. I wasn't familiar with this, although it apparently was previously published. I went back: the (Zhu, 2000) paper was unreadable. The (Jampani, 2014) paper on informed sampling was good. So, perhaps this isn't a good reason for accepting to ICLR.\n\n2. The results are quite impressive. The rough rule-of-thumb is that optimization can help you speed up code by 10%. The standard MCMC results presented on the paper on randomly-generated programs roughly matches this (15%). The fact that the proposed algorithm get ~33% speedup is quite surprising, and worth publishing.\n\nThe argument against accepting this paper is that it doesn't match the goals of ICLR. I don't go to ICLR to hear about generic machine learning papers (we have NIPS and ICML for that). Instead, I go to learn about how to automatically represent data and models. Now, maybe this paper talks about how to represent (generated) programs, so it tangentially lives under the umbrella of ICLR. But it will compete against more relevant papers in the conference -- it may just be a poster. Sending this to a programming language conference may have more eventual impact.\n\nNonetheless, I give this paper an \"accept\", because I learned something valuable and the results are very good."
  },
  {
    "people": [
      "AnonReviewer2",
      "Stoke"
    ],
    "review": "We thank the reviewers for their helpful feedback. We uploaded a new version of\nthe paper. The elements that have changed are:\n\nClarity of experiments:\n- The complexity of each model in term of number of parameters is given in Table 1 and the hyperparameters / architecture were added as Appendix A.\n- Addition of a better representation of the consequences of a learned bias (Figure 3). We compare Stoke (uniform distribution)  running for (200,400) iterations with the learned optimizer running for (100, 200) iterations (at training time, the horizon is set at 200 iterations). Even with four times less iterations, a better distribution of the score is achieved.\n- The numbers for the impact on throughput of the learned proposal distribution have been added (Table 3). This measures not just the time for the proposal (which we gave as answer to the question of AnonReviewer2) but the whole time of an MCMC iterations which represent more accurately the throughput of the algorithm.\n\nExplanations:\n- We added the citation that AnonReviewer3 suggested (Section 2 - Related Works).\n- We added our argument that superoptimization provides a good benchmark to estimate representation of programs to the introduction (Section 1 - Introduction, 1st paragraph).\n- Clarified throughout the paper that the \"Uniform\" model is Stoke (Section 3.2.2-Parametrization and Section 4.1-Models, Table 1).\n- Fixed some typos"
  },
  {
    "people": [
      "Schkufza"
    ],
    "review": "This work builds on top of STOKE (Schkufza et al., 2013), which is a superoptimization engine for program binaries. It works by starting with an existing program, and proposing modifications to it according to a proposal distribution. Proposals are accepted according to the Metropolis-Hastings criteria. The acceptance criteria takes into account the correctness of the program, and performance of the new program. Thus, the MCMC process is likely to converge to correct programs with high performance. Typically, the proposal distribution is fixed. The contribution of this work is to learn the proposal distribution as a function of the features of the program (bag of words of all the opcodes in the program). The experiments compare with the baselines of uniform proposal distribution, and a baseline where one just learns the weights of the proposal distribution but without conditioning on the features of the program. The evaluation shows that the proposed method has slightly better performance than the compared baselines.\n\nThe significance of this work at ICLR seems to be quite low., both because this is not a progress in learning representations, but a straightforward application of neural networks and REINFORCE to yet another task which has non-differentiable components. The task itself (superoptimization) is not of significant interest to ICLR readers/attendees. A conference like AAAI/UAI seem a better fit for this work.\n\nThe proposed method is seemingly novel. Typical MCMC-based synthesis methods are lacking due to their being no learning components in them. However, to make this work compelling, the authors should consider demonstrating the proposed method in other synthesis tasks, or even more generally, other tasks where MH-MCMC is used, and a learnt proposal distribution can be beneficial. Superoptimization alone (esp with small improvements over baselines) is not compelling enough.\n\nIt is also not clear if there is any significant representation learning is going on. Since a BoW feature is used to represent the programs, the neural network cannot possibly learn anything more than just correlations between presence of opcodes and good moves. Such a model cannot possibly understand the program semantics in any way. It would have been a more interesting contribution if the authors had used a model (such as Tree-LSTM) which attempts to learn the semantics the program. The quite naive method of learning makes this paper not a favorable candidate for acceptance."
  },
  {
    "people": [
      "Salimans"
    ],
    "review": "This is an interesting and pleasant paper on superoptimization, that extends the  problem approached by the stochastic search STOKE to a learned stochastic search, where the STOKE proposals are the output of a neural network which takes some program embedding as an input. The authors then use REINFORCE to learn an MCMC scheme with the objective of minimizing the final program cost.\n\nThe writing is clear and results highlight the efficacy of the method.\n\ncomments / questions:\n- Am I correct in understanding that of the entire stochastic computation graph, only the features->proposal part is learned. The rest is still effectively the stoke MCMC scheme? Does that imply that the 'uniform' model is effectively Stoke and is your baseline (this should probably be made explicit )\n\n- Did the authors consider learning the features instead of using out of the box features (could be difficult given the relatively small amount of data - the feature extractor might not generalize).\n\n- In a different context, 'Markov Chain Monte Carlo and Variational Inference:Bridging the Gap' by Salimans et al. suggests considering a MCMC scheme as a stochastic computation graph and optimizing using a variational (i.e. RL) criterion. The problem is different, it uses HMC instead of MCMC, but it might be worth citing as a similar approach to 'meta-optimized' MCMC algorithms.\n\n"
  },
  {
    "people": [
      "Zhu",
      "Jampani"
    ],
    "review": "Two things I really liked about this paper:\n1. The whole idea of having a data-dependent proposal distribution for MCMC. I wasn't familiar with this, although it apparently was previously published. I went back: the (Zhu, 2000) paper was unreadable. The (Jampani, 2014) paper on informed sampling was good. So, perhaps this isn't a good reason for accepting to ICLR.\n\n2. The results are quite impressive. The rough rule-of-thumb is that optimization can help you speed up code by 10%. The standard MCMC results presented on the paper on randomly-generated programs roughly matches this (15%). The fact that the proposed algorithm get ~33% speedup is quite surprising, and worth publishing.\n\nThe argument against accepting this paper is that it doesn't match the goals of ICLR. I don't go to ICLR to hear about generic machine learning papers (we have NIPS and ICML for that). Instead, I go to learn about how to automatically represent data and models. Now, maybe this paper talks about how to represent (generated) programs, so it tangentially lives under the umbrella of ICLR. But it will compete against more relevant papers in the conference -- it may just be a poster. Sending this to a programming language conference may have more eventual impact.\n\nNonetheless, I give this paper an \"accept\", because I learned something valuable and the results are very good. "
  },
  {
    "people": [
      "Zhu",
      "Jampani"
    ],
    "review": "Two things I really liked about this paper:\n1. The whole idea of having a data-dependent proposal distribution for MCMC. I wasn't familiar with this, although it apparently was previously published. I went back: the (Zhu, 2000) paper was unreadable. The (Jampani, 2014) paper on informed sampling was good. So, perhaps this isn't a good reason for accepting to ICLR.\n\n2. The results are quite impressive. The rough rule-of-thumb is that optimization can help you speed up code by 10%. The standard MCMC results presented on the paper on randomly-generated programs roughly matches this (15%). The fact that the proposed algorithm get ~33% speedup is quite surprising, and worth publishing.\n\nThe argument against accepting this paper is that it doesn't match the goals of ICLR. I don't go to ICLR to hear about generic machine learning papers (we have NIPS and ICML for that). Instead, I go to learn about how to automatically represent data and models. Now, maybe this paper talks about how to represent (generated) programs, so it tangentially lives under the umbrella of ICLR. But it will compete against more relevant papers in the conference -- it may just be a poster. Sending this to a programming language conference may have more eventual impact.\n\nNonetheless, I give this paper an \"accept\", because I learned something valuable and the results are very good."
  },
  {
    "people": [
      "AnonReviewer2",
      "Stoke"
    ],
    "review": "We thank the reviewers for their helpful feedback. We uploaded a new version of\nthe paper. The elements that have changed are:\n\nClarity of experiments:\n- The complexity of each model in term of number of parameters is given in Table 1 and the hyperparameters / architecture were added as Appendix A.\n- Addition of a better representation of the consequences of a learned bias (Figure 3). We compare Stoke (uniform distribution)  running for (200,400) iterations with the learned optimizer running for (100, 200) iterations (at training time, the horizon is set at 200 iterations). Even with four times less iterations, a better distribution of the score is achieved.\n- The numbers for the impact on throughput of the learned proposal distribution have been added (Table 3). This measures not just the time for the proposal (which we gave as answer to the question of AnonReviewer2) but the whole time of an MCMC iterations which represent more accurately the throughput of the algorithm.\n\nExplanations:\n- We added the citation that AnonReviewer3 suggested (Section 2 - Related Works).\n- We added our argument that superoptimization provides a good benchmark to estimate representation of programs to the introduction (Section 1 - Introduction, 1st paragraph).\n- Clarified throughout the paper that the \"Uniform\" model is Stoke (Section 3.2.2-Parametrization and Section 4.1-Models, Table 1).\n- Fixed some typos"
  },
  {
    "people": [
      "Schkufza"
    ],
    "review": "This work builds on top of STOKE (Schkufza et al., 2013), which is a superoptimization engine for program binaries. It works by starting with an existing program, and proposing modifications to it according to a proposal distribution. Proposals are accepted according to the Metropolis-Hastings criteria. The acceptance criteria takes into account the correctness of the program, and performance of the new program. Thus, the MCMC process is likely to converge to correct programs with high performance. Typically, the proposal distribution is fixed. The contribution of this work is to learn the proposal distribution as a function of the features of the program (bag of words of all the opcodes in the program). The experiments compare with the baselines of uniform proposal distribution, and a baseline where one just learns the weights of the proposal distribution but without conditioning on the features of the program. The evaluation shows that the proposed method has slightly better performance than the compared baselines.\n\nThe significance of this work at ICLR seems to be quite low., both because this is not a progress in learning representations, but a straightforward application of neural networks and REINFORCE to yet another task which has non-differentiable components. The task itself (superoptimization) is not of significant interest to ICLR readers/attendees. A conference like AAAI/UAI seem a better fit for this work.\n\nThe proposed method is seemingly novel. Typical MCMC-based synthesis methods are lacking due to their being no learning components in them. However, to make this work compelling, the authors should consider demonstrating the proposed method in other synthesis tasks, or even more generally, other tasks where MH-MCMC is used, and a learnt proposal distribution can be beneficial. Superoptimization alone (esp with small improvements over baselines) is not compelling enough.\n\nIt is also not clear if there is any significant representation learning is going on. Since a BoW feature is used to represent the programs, the neural network cannot possibly learn anything more than just correlations between presence of opcodes and good moves. Such a model cannot possibly understand the program semantics in any way. It would have been a more interesting contribution if the authors had used a model (such as Tree-LSTM) which attempts to learn the semantics the program. The quite naive method of learning makes this paper not a favorable candidate for acceptance."
  },
  {
    "people": [
      "Salimans"
    ],
    "review": "This is an interesting and pleasant paper on superoptimization, that extends the  problem approached by the stochastic search STOKE to a learned stochastic search, where the STOKE proposals are the output of a neural network which takes some program embedding as an input. The authors then use REINFORCE to learn an MCMC scheme with the objective of minimizing the final program cost.\n\nThe writing is clear and results highlight the efficacy of the method.\n\ncomments / questions:\n- Am I correct in understanding that of the entire stochastic computation graph, only the features->proposal part is learned. The rest is still effectively the stoke MCMC scheme? Does that imply that the 'uniform' model is effectively Stoke and is your baseline (this should probably be made explicit )\n\n- Did the authors consider learning the features instead of using out of the box features (could be difficult given the relatively small amount of data - the feature extractor might not generalize).\n\n- In a different context, 'Markov Chain Monte Carlo and Variational Inference:Bridging the Gap' by Salimans et al. suggests considering a MCMC scheme as a stochastic computation graph and optimizing using a variational (i.e. RL) criterion. The problem is different, it uses HMC instead of MCMC, but it might be worth citing as a similar approach to 'meta-optimized' MCMC algorithms.\n\n"
  },
  {
    "people": [
      "Zhu",
      "Jampani"
    ],
    "review": "Two things I really liked about this paper:\n1. The whole idea of having a data-dependent proposal distribution for MCMC. I wasn't familiar with this, although it apparently was previously published. I went back: the (Zhu, 2000) paper was unreadable. The (Jampani, 2014) paper on informed sampling was good. So, perhaps this isn't a good reason for accepting to ICLR.\n\n2. The results are quite impressive. The rough rule-of-thumb is that optimization can help you speed up code by 10%. The standard MCMC results presented on the paper on randomly-generated programs roughly matches this (15%). The fact that the proposed algorithm get ~33% speedup is quite surprising, and worth publishing.\n\nThe argument against accepting this paper is that it doesn't match the goals of ICLR. I don't go to ICLR to hear about generic machine learning papers (we have NIPS and ICML for that). Instead, I go to learn about how to automatically represent data and models. Now, maybe this paper talks about how to represent (generated) programs, so it tangentially lives under the umbrella of ICLR. But it will compete against more relevant papers in the conference -- it may just be a poster. Sending this to a programming language conference may have more eventual impact.\n\nNonetheless, I give this paper an \"accept\", because I learned something valuable and the results are very good. "
  },
  {
    "people": [
      "Tang"
    ],
    "review": "This paper presents a hierarchical attention-based method for document classification. \nThe main idea is to first run a bidirectional LSTM to get global context vector, and then run another attention-based bidirectional LSTM that uses the final hidden state from the first pass to weight local context vectors (TS-ATT). \nA simpler architecture that removes the first LSTM and uses the output of the second LSTM as the global context vector is also proposed (SS-ATT). \nExperiments on three datasets are presented, however the results are mostly not state-of-the-art.\n\nI think the idea is nice, but the experiment results are not convincing enough to justify this new model architecture. \nWhy is your Yelp 2013 dataset smaller than the original Tang et al, 2015 paper that has ~300k documents? \nI noticed your other datasets are also quite small. Is it because your model is difficult to scale to large datasets?\nYou should also include results from Tang et al., 2015 in Table 2 that achieves 65.1% accuracy on Yelp 2013 (why is your number so much lower?)\nI also suggest removing phrases such as \"Learning to Understand\" when presenting their model.\nOverall, I think that this submission is a better fit for the workshop.\n\nMinor comments:\n- gloal -> global\n- Not needing a pretrained embeddings, while of course nice, is not that big of a deal. Various models will work just fine without pretrained embeddings."
  },
  {
    "people": [
      "Kumar"
    ],
    "review": "The authors did not bother responding or fixing any of the pre-review comments. Hence I repeat here:\n\nPlease do not make incredibly unscientific statements like this one:\n\"The working procedure of this model is just like how we human beings read a text and then answer a related question. \"\nReally, \"humans beings\" have an LSTM like model to read a text? Can you cite an actual neuroscience paper for such a claim? The answer is no, so please delete such statements from future drafts.\n\nGenerally, your experiments are about simple classification and the methods you're competing against are simple models like NB-SVM. So I would change the title, abstract ad introduction accordingly and not attempt hyperbole like \"Learning to Understand\" in the title.\n\nLastly, your attention level approach seems similar to dynamic memory networks by Kumar et al. they also have experiments for sentiment and it would be interesting to understand the differences to your model and compare to them.\n\nOther reviewers included further missing related work and fitting this paper into the context of current literature.\nGiven that no efforts were made to fix the pre-review questions and feedback, I doubt this will become ready in time for publication."
  },
  {
    "people": [
      "Luong",
      "Shen",
      "Lee"
    ],
    "review": "The paper proposes to enhance the attention mechanism for sentiment classification by using global context computed by a Bi-LSTM. The proposed models outperform many existing models in the literature on 3 sentiment analysis datasets. \n\nThe key idea of using Bi-LSTM to compute global context for attention is actually not novel, as proposed several times in the literature, e.g., Luong et al (2015) and Shen & Lee (2016). Especially, Luong et al (2015) already proposed to combine global context with local context for attention.\n\nRegarding to the experiments, of course it would be nice if the model can work well without the need of tricks like dropout or pre-trained word embeddings. However, it would be even better if the model can work well using those tricks. The authors should show results of the models using those tricks and compare them to the results in the literature.  \n\n\nRef:\nLuong et al. Effective Approaches to Attention-based Neural Machine Translation. EMNLP 2015"
  },
  {
    "people": [
      "Tang"
    ],
    "review": "This paper presents a hierarchical attention-based method for document classification. \nThe main idea is to first run a bidirectional LSTM to get global context vector, and then run another attention-based bidirectional LSTM that uses the final hidden state from the first pass to weight local context vectors (TS-ATT). \nA simpler architecture that removes the first LSTM and uses the output of the second LSTM as the global context vector is also proposed (SS-ATT). \nExperiments on three datasets are presented, however the results are mostly not state-of-the-art.\n\nI think the idea is nice, but the experiment results are not convincing enough to justify this new model architecture. \nWhy is your Yelp 2013 dataset smaller than the original Tang et al, 2015 paper that has ~300k documents? \nI noticed your other datasets are also quite small. Is it because your model is difficult to scale to large datasets?\nYou should also include results from Tang et al., 2015 in Table 2 that achieves 65.1% accuracy on Yelp 2013 (why is your number so much lower?)\nI also suggest removing phrases such as \"Learning to Understand\" when presenting their model.\nOverall, I think that this submission is a better fit for the workshop.\n\nMinor comments:\n- gloal -> global\n- Not needing a pretrained embeddings, while of course nice, is not that big of a deal. Various models will work just fine without pretrained embeddings."
  },
  {
    "people": [
      "Tang"
    ],
    "review": "This paper presents a hierarchical attention-based method for document classification. \nThe main idea is to first run a bidirectional LSTM to get global context vector, and then run another attention-based bidirectional LSTM that uses the final hidden state from the first pass to weight local context vectors (TS-ATT). \nA simpler architecture that removes the first LSTM and uses the output of the second LSTM as the global context vector is also proposed (SS-ATT). \nExperiments on three datasets are presented, however the results are mostly not state-of-the-art.\n\nI think the idea is nice, but the experiment results are not convincing enough to justify this new model architecture. \nWhy is your Yelp 2013 dataset smaller than the original Tang et al, 2015 paper that has ~300k documents? \nI noticed your other datasets are also quite small. Is it because your model is difficult to scale to large datasets?\nYou should also include results from Tang et al., 2015 in Table 2 that achieves 65.1% accuracy on Yelp 2013 (why is your number so much lower?)\nI also suggest removing phrases such as \"Learning to Understand\" when presenting their model.\nOverall, I think that this submission is a better fit for the workshop.\n\nMinor comments:\n- gloal -> global\n- Not needing a pretrained embeddings, while of course nice, is not that big of a deal. Various models will work just fine without pretrained embeddings."
  },
  {
    "people": [
      "Kumar"
    ],
    "review": "The authors did not bother responding or fixing any of the pre-review comments. Hence I repeat here:\n\nPlease do not make incredibly unscientific statements like this one:\n\"The working procedure of this model is just like how we human beings read a text and then answer a related question. \"\nReally, \"humans beings\" have an LSTM like model to read a text? Can you cite an actual neuroscience paper for such a claim? The answer is no, so please delete such statements from future drafts.\n\nGenerally, your experiments are about simple classification and the methods you're competing against are simple models like NB-SVM. So I would change the title, abstract ad introduction accordingly and not attempt hyperbole like \"Learning to Understand\" in the title.\n\nLastly, your attention level approach seems similar to dynamic memory networks by Kumar et al. they also have experiments for sentiment and it would be interesting to understand the differences to your model and compare to them.\n\nOther reviewers included further missing related work and fitting this paper into the context of current literature.\nGiven that no efforts were made to fix the pre-review questions and feedback, I doubt this will become ready in time for publication."
  },
  {
    "people": [
      "Luong",
      "Shen",
      "Lee"
    ],
    "review": "The paper proposes to enhance the attention mechanism for sentiment classification by using global context computed by a Bi-LSTM. The proposed models outperform many existing models in the literature on 3 sentiment analysis datasets. \n\nThe key idea of using Bi-LSTM to compute global context for attention is actually not novel, as proposed several times in the literature, e.g., Luong et al (2015) and Shen & Lee (2016). Especially, Luong et al (2015) already proposed to combine global context with local context for attention.\n\nRegarding to the experiments, of course it would be nice if the model can work well without the need of tricks like dropout or pre-trained word embeddings. However, it would be even better if the model can work well using those tricks. The authors should show results of the models using those tricks and compare them to the results in the literature.  \n\n\nRef:\nLuong et al. Effective Approaches to Attention-based Neural Machine Translation. EMNLP 2015"
  },
  {
    "people": [
      "Tang"
    ],
    "review": "This paper presents a hierarchical attention-based method for document classification. \nThe main idea is to first run a bidirectional LSTM to get global context vector, and then run another attention-based bidirectional LSTM that uses the final hidden state from the first pass to weight local context vectors (TS-ATT). \nA simpler architecture that removes the first LSTM and uses the output of the second LSTM as the global context vector is also proposed (SS-ATT). \nExperiments on three datasets are presented, however the results are mostly not state-of-the-art.\n\nI think the idea is nice, but the experiment results are not convincing enough to justify this new model architecture. \nWhy is your Yelp 2013 dataset smaller than the original Tang et al, 2015 paper that has ~300k documents? \nI noticed your other datasets are also quite small. Is it because your model is difficult to scale to large datasets?\nYou should also include results from Tang et al., 2015 in Table 2 that achieves 65.1% accuracy on Yelp 2013 (why is your number so much lower?)\nI also suggest removing phrases such as \"Learning to Understand\" when presenting their model.\nOverall, I think that this submission is a better fit for the workshop.\n\nMinor comments:\n- gloal -> global\n- Not needing a pretrained embeddings, while of course nice, is not that big of a deal. Various models will work just fine without pretrained embeddings."
  },
  {
    "people": [
      "Grosse"
    ],
    "review": "The paper describes a method to evaluate generative models such as VAE, GAN and GMMN. This is very much needed in our community where we still eyeball generated images to judge the quality of a model. However, the technical increment over the NIPS 16 paper: \u201cMeasuring the reliability of MCMC inference with bidirectional Monte Carlo\u201d is very small, or nonexistent (but please correct me if I am wrong!).  (Grosse et al). The relative contribution of this paper is the application of this method to generative models. \nIn section 2.3 the authors seem to make a mistake. They write E[p\u2019(x)] <= p(x) but I think they mean: E[log p\u2019(x)] <= log E[p\u2019(x)] = log p(x). Also,  for what value of x? If p(x) is normalized it can\u2019t be true for all values of x. Anyways, I think there are typos here and there and the equations could be more precise.\nOn page 5 top of the page it is said that the AIS procedure can be initialized with q(z|x) instead of p(z). However, it is unclear what value of x is then picked? Is it perhaps Ep(x)[q(z|x)] ?\nI am confused with the use of the term overfitting (p8 bottom). Does a model A overfit relative to a another model B if the test accuracy of A is higher than that of B even though the gap between train and test accuracy is also higher for B than for A. I think not. Perhaps the last sentence on page 8 should say that VAE-50 underfits less than GMMN-50?\nThe experimental results are interesting in that it exposes the fact that GANs and GMMNs seem to have much lover test accuracy than VAE despite the fact that their samples look great."
  },
  {
    "people": [
      "Salakhutdinov",
      "Murray",
      "Desjardins"
    ],
    "review": "# Review\nThis paper proposes a quantitative evaluation for decoder-based generative models that use Annealed Importance Sampling (AIS) to estimate log-likelihoods. Quantitative evaluations are indeed much needed since for some models, like Generative Adversarial Networks (GANs) and Generative Moment Matching Networks (GMMNs), qualitative evaluation of samples is still frequently used to assess their generative capability. Even though, there exist quantitative evaluations like Kernel Density Estimation (KDE), the authors show how AIS is more accurate than KDE and how it can be used to perform fine-grained comparison between generative models (GAN, GMMs and Variational Autoencoders (VAE)).\n\nThe authors report empirical results comparing two different decoder architectures that were both trained, on the continuous MNIST dataset, using the VAE, GAN and GMMN objectives. They also trained an Importance Weighted Autoencoder (IWAE) on binarized MNIST and show that, in this case, the IWAE bound underestimates the true log-likelihoods by at least 1 nat (which is significant for this dataset) according to the AIS evaluation of the same model.\n\n\n# Pros\nTheir evaluation framework is public and is definitely a nice contribution to the community.\n\nThis paper gives some insights about how GAN behaves from log-likelihood perspective. The authors disconfirm the commonly proposed hypothesis that GAN are memorizing training data. The authors also observed that GANs miss important modes of the data distribution.\n\n\n# Cons/Questions\nIt is not clear for me why sometimes the experiments were done using different number of examples (100, 1000, 10000) coming from different sources (trainset, validset, testset or simulation/generated by the model). For instance, in Table 2 why results were not reported using all 10,000 examples of the testing set?\n\nIt is not clear why in Figure 2c, AIS is slower than AIS+encoder? Is the number of intermediate distributions the same in both?\n\n16 independent chains for AIS seems a bit low from what I saw in the literature (e.g. in [Salakhutdinov & Murray, 2008] or [Desjardins etal., 2011], they used 100 chains). Could it be that increasing the number of chains helps tighten the confidence interval reported in Table 2?\n\nI would have like the authors to give their intuitions as to why GAN50 has a BDMC gap of 10 nats, i.e. 1 order of magnitude compared to the others?\n\n\n# Minor comments\nTable 1 is not referenced in the text and lacks description of what the different columns represent.\nFigure 2(a), are the reported values represents the average log-likelihood of 100 (each or total?) training and validation examples of MNIST (as described in Section 5.3.2).\nFigure 2(c), I'm guessing it is on binarized MNIST? Also, why are there fewer points for AIS compared to IWAE and AIS+encoder?\nAre the BDMC gaps mentioned in Section 5.3.1 the same as the ones reported in Table2 ?\nTypo in caption of Figure 3: \"(c) GMMN-10\" but actually showing GMMN-50 according to the graph title and subcaption."
  },
  {
    "people": [
      "MacKay",
      "MacKay"
    ],
    "review": "Summary:\nThis paper describes how to estimate log-likelihoods of currently popular decoder-based generative models using annealed importance sampling (AIS) and HMC. It validates the method using bidirectional Monte Carlo on the example of MNIST, and compares the performance of GANs and VAEs.\n\n\nReview:\nAlthough this seems like a fairly straight-forward application of AIS to me (correct me if I missed an important trick to make this work), I very much appreciate the educational value and empirical contributions of this paper. It should lead to clarity in debates around the density estimation performance of GANs, and should enable more people to use AIS.\n\nSpace permitting, it might be a good idea to try to expand the description of AIS. All the components of AIS are mentioned and a basic description of the algorithm is given, but the paper doesn\u2019t explain well \u201cwhy\u201d the algorithm does what it does/why it works.\n\nI was initially confused by the widely different numbers in Figure 2. On first glance my expectation was that this Figure is comparing GAN, GMMN and IWAE (because of the labeling at the bottom and because of the leading words in the caption\u2019s descriptions). Perhaps mention in the caption that (a) and (b) use continuous MNIST and (c) uses discrete MNIST. \u201cGMMN-50\u201d should probably be \u201cGMMN-10\u201d.\n\n\nUsing reconstructions for evaluation of models may be a necessary but is not sufficient condition for a good model. Depending on the likelihood, a posterior sample might have very low density under the prior, for example. It would be great if the authors could point out and discuss the limitations of this test a bit more.\n\n\nMinor:\n\nPerhaps add a reference to MacKay\u2019s density networks (MacKay, 1995) for decoder-based generative models.\n\nIn Section 2.2, the authors write \u201cthe prior over z can be drastically different than the true posterior p(z|x), especially in high dimension\u201d. I think the flow of the paper could be improved here, especially for people less familiar with importance sampling/AIS. I don\u2019t think the relevance of the posterior for importance sampling is clear at this point in the paper.\n\nIn Section 2.3 the authors claim that is often more \u201cmeaningful\u201d to estimate p(x) in log-space because of underflow problems. \u201cMeaningful\u201d seems like the wrong word here. Perhaps revise to say that it\u2019s more practical to estimate log p(x) because of underflow problems, or to say that it\u2019s more meaningful to estimate log p(x) because of its connection to compression/surprise/entropy."
  },
  {
    "people": [
      "Grosse"
    ],
    "review": "The paper describes a method to evaluate generative models such as VAE, GAN and GMMN. This is very much needed in our community where we still eyeball generated images to judge the quality of a model. However, the technical increment over the NIPS 16 paper: \u201cMeasuring the reliability of MCMC inference with bidirectional Monte Carlo\u201d is very small, or nonexistent (but please correct me if I am wrong!).  (Grosse et al). The relative contribution of this paper is the application of this method to generative models. \nIn section 2.3 the authors seem to make a mistake. They write E[p\u2019(x)] <= p(x) but I think they mean: E[log p\u2019(x)] <= log E[p\u2019(x)] = log p(x). Also,  for what value of x? If p(x) is normalized it can\u2019t be true for all values of x. Anyways, I think there are typos here and there and the equations could be more precise.\nOn page 5 top of the page it is said that the AIS procedure can be initialized with q(z|x) instead of p(z). However, it is unclear what value of x is then picked? Is it perhaps Ep(x)[q(z|x)] ?\nI am confused with the use of the term overfitting (p8 bottom). Does a model A overfit relative to a another model B if the test accuracy of A is higher than that of B even though the gap between train and test accuracy is also higher for B than for A. I think not. Perhaps the last sentence on page 8 should say that VAE-50 underfits less than GMMN-50?\nThe experimental results are interesting in that it exposes the fact that GANs and GMMNs seem to have much lover test accuracy than VAE despite the fact that their samples look great. \n"
  },
  {
    "people": [
      "Grosse"
    ],
    "review": "The paper describes a method to evaluate generative models such as VAE, GAN and GMMN. This is very much needed in our community where we still eyeball generated images to judge the quality of a model. However, the technical increment over the NIPS 16 paper: \u201cMeasuring the reliability of MCMC inference with bidirectional Monte Carlo\u201d is very small, or nonexistent (but please correct me if I am wrong!).  (Grosse et al). The relative contribution of this paper is the application of this method to generative models. \nIn section 2.3 the authors seem to make a mistake. They write E[p\u2019(x)] <= p(x) but I think they mean: E[log p\u2019(x)] <= log E[p\u2019(x)] = log p(x). Also,  for what value of x? If p(x) is normalized it can\u2019t be true for all values of x. Anyways, I think there are typos here and there and the equations could be more precise.\nOn page 5 top of the page it is said that the AIS procedure can be initialized with q(z|x) instead of p(z). However, it is unclear what value of x is then picked? Is it perhaps Ep(x)[q(z|x)] ?\nI am confused with the use of the term overfitting (p8 bottom). Does a model A overfit relative to a another model B if the test accuracy of A is higher than that of B even though the gap between train and test accuracy is also higher for B than for A. I think not. Perhaps the last sentence on page 8 should say that VAE-50 underfits less than GMMN-50?\nThe experimental results are interesting in that it exposes the fact that GANs and GMMNs seem to have much lover test accuracy than VAE despite the fact that their samples look great."
  },
  {
    "people": [
      "Salakhutdinov",
      "Murray",
      "Desjardins"
    ],
    "review": "# Review\nThis paper proposes a quantitative evaluation for decoder-based generative models that use Annealed Importance Sampling (AIS) to estimate log-likelihoods. Quantitative evaluations are indeed much needed since for some models, like Generative Adversarial Networks (GANs) and Generative Moment Matching Networks (GMMNs), qualitative evaluation of samples is still frequently used to assess their generative capability. Even though, there exist quantitative evaluations like Kernel Density Estimation (KDE), the authors show how AIS is more accurate than KDE and how it can be used to perform fine-grained comparison between generative models (GAN, GMMs and Variational Autoencoders (VAE)).\n\nThe authors report empirical results comparing two different decoder architectures that were both trained, on the continuous MNIST dataset, using the VAE, GAN and GMMN objectives. They also trained an Importance Weighted Autoencoder (IWAE) on binarized MNIST and show that, in this case, the IWAE bound underestimates the true log-likelihoods by at least 1 nat (which is significant for this dataset) according to the AIS evaluation of the same model.\n\n\n# Pros\nTheir evaluation framework is public and is definitely a nice contribution to the community.\n\nThis paper gives some insights about how GAN behaves from log-likelihood perspective. The authors disconfirm the commonly proposed hypothesis that GAN are memorizing training data. The authors also observed that GANs miss important modes of the data distribution.\n\n\n# Cons/Questions\nIt is not clear for me why sometimes the experiments were done using different number of examples (100, 1000, 10000) coming from different sources (trainset, validset, testset or simulation/generated by the model). For instance, in Table 2 why results were not reported using all 10,000 examples of the testing set?\n\nIt is not clear why in Figure 2c, AIS is slower than AIS+encoder? Is the number of intermediate distributions the same in both?\n\n16 independent chains for AIS seems a bit low from what I saw in the literature (e.g. in [Salakhutdinov & Murray, 2008] or [Desjardins etal., 2011], they used 100 chains). Could it be that increasing the number of chains helps tighten the confidence interval reported in Table 2?\n\nI would have like the authors to give their intuitions as to why GAN50 has a BDMC gap of 10 nats, i.e. 1 order of magnitude compared to the others?\n\n\n# Minor comments\nTable 1 is not referenced in the text and lacks description of what the different columns represent.\nFigure 2(a), are the reported values represents the average log-likelihood of 100 (each or total?) training and validation examples of MNIST (as described in Section 5.3.2).\nFigure 2(c), I'm guessing it is on binarized MNIST? Also, why are there fewer points for AIS compared to IWAE and AIS+encoder?\nAre the BDMC gaps mentioned in Section 5.3.1 the same as the ones reported in Table2 ?\nTypo in caption of Figure 3: \"(c) GMMN-10\" but actually showing GMMN-50 according to the graph title and subcaption."
  },
  {
    "people": [
      "MacKay",
      "MacKay"
    ],
    "review": "Summary:\nThis paper describes how to estimate log-likelihoods of currently popular decoder-based generative models using annealed importance sampling (AIS) and HMC. It validates the method using bidirectional Monte Carlo on the example of MNIST, and compares the performance of GANs and VAEs.\n\n\nReview:\nAlthough this seems like a fairly straight-forward application of AIS to me (correct me if I missed an important trick to make this work), I very much appreciate the educational value and empirical contributions of this paper. It should lead to clarity in debates around the density estimation performance of GANs, and should enable more people to use AIS.\n\nSpace permitting, it might be a good idea to try to expand the description of AIS. All the components of AIS are mentioned and a basic description of the algorithm is given, but the paper doesn\u2019t explain well \u201cwhy\u201d the algorithm does what it does/why it works.\n\nI was initially confused by the widely different numbers in Figure 2. On first glance my expectation was that this Figure is comparing GAN, GMMN and IWAE (because of the labeling at the bottom and because of the leading words in the caption\u2019s descriptions). Perhaps mention in the caption that (a) and (b) use continuous MNIST and (c) uses discrete MNIST. \u201cGMMN-50\u201d should probably be \u201cGMMN-10\u201d.\n\n\nUsing reconstructions for evaluation of models may be a necessary but is not sufficient condition for a good model. Depending on the likelihood, a posterior sample might have very low density under the prior, for example. It would be great if the authors could point out and discuss the limitations of this test a bit more.\n\n\nMinor:\n\nPerhaps add a reference to MacKay\u2019s density networks (MacKay, 1995) for decoder-based generative models.\n\nIn Section 2.2, the authors write \u201cthe prior over z can be drastically different than the true posterior p(z|x), especially in high dimension\u201d. I think the flow of the paper could be improved here, especially for people less familiar with importance sampling/AIS. I don\u2019t think the relevance of the posterior for importance sampling is clear at this point in the paper.\n\nIn Section 2.3 the authors claim that is often more \u201cmeaningful\u201d to estimate p(x) in log-space because of underflow problems. \u201cMeaningful\u201d seems like the wrong word here. Perhaps revise to say that it\u2019s more practical to estimate log p(x) because of underflow problems, or to say that it\u2019s more meaningful to estimate log p(x) because of its connection to compression/surprise/entropy."
  },
  {
    "people": [
      "Grosse"
    ],
    "review": "The paper describes a method to evaluate generative models such as VAE, GAN and GMMN. This is very much needed in our community where we still eyeball generated images to judge the quality of a model. However, the technical increment over the NIPS 16 paper: \u201cMeasuring the reliability of MCMC inference with bidirectional Monte Carlo\u201d is very small, or nonexistent (but please correct me if I am wrong!).  (Grosse et al). The relative contribution of this paper is the application of this method to generative models. \nIn section 2.3 the authors seem to make a mistake. They write E[p\u2019(x)] <= p(x) but I think they mean: E[log p\u2019(x)] <= log E[p\u2019(x)] = log p(x). Also,  for what value of x? If p(x) is normalized it can\u2019t be true for all values of x. Anyways, I think there are typos here and there and the equations could be more precise.\nOn page 5 top of the page it is said that the AIS procedure can be initialized with q(z|x) instead of p(z). However, it is unclear what value of x is then picked? Is it perhaps Ep(x)[q(z|x)] ?\nI am confused with the use of the term overfitting (p8 bottom). Does a model A overfit relative to a another model B if the test accuracy of A is higher than that of B even though the gap between train and test accuracy is also higher for B than for A. I think not. Perhaps the last sentence on page 8 should say that VAE-50 underfits less than GMMN-50?\nThe experimental results are interesting in that it exposes the fact that GANs and GMMNs seem to have much lover test accuracy than VAE despite the fact that their samples look great. \n"
  },
  {
    "people": [
      "Gatys",
      "Gatys",
      "Gatys",
      "Liu"
    ],
    "review": "The paper introduces a variation to the CNN-based texture synthesis procedure of Gatys et al. that matches correlations between spatially shifted feature responses in addition to the correlations between feature responses at the same position in the feature maps. \nThe paper claims that this \na) improves texture synthesis for textures with long-range regular structures, that are not preserved with the Gatys et al. method\nb) improves performance on texture inpainting tasks compared to the Gatys et al. method\nc) improves results in season transfer when combined with the style transfer method by Gatys et al. \nFurthermore the paper shows that\nd) by matching correlations between spatially flipped feature maps, symmetry properties around the flipping axis can be preserved.\n\nI agree with claim a). However, the generated textures still have some issues such as greyish regions so the problem is not solved. Additionally, the procedure proposed is very costly which makes an already slow texture synthesis method substantially slower. For example, in comparison, the concurrent work by Liu et al. ("
  },
  {
    "people": [
      "Gatys",
      "Gatys",
      "Gatys",
      "Liu"
    ],
    "review": "The paper introduces a variation to the CNN-based texture synthesis procedure of Gatys et al. that matches correlations between spatially shifted feature responses in addition to the correlations between feature responses at the same position in the feature maps. \nThe paper claims that this \na) improves texture synthesis for textures with long-range regular structures, that are not preserved with the Gatys et al. method\nb) improves performance on texture inpainting tasks compared to the Gatys et al. method\nc) improves results in season transfer when combined with the style transfer method by Gatys et al. \nFurthermore the paper shows that\nd) by matching correlations between spatially flipped feature maps, symmetry properties around the flipping axis can be preserved.\n\nI agree with claim a). However, the generated textures still have some issues such as greyish regions so the problem is not solved. Additionally, the procedure proposed is very costly which makes an already slow texture synthesis method substantially slower. For example, in comparison, the concurrent work by Liu et al. ("
  },
  {
    "people": [
      "Gatys",
      "Gatys",
      "Gatys",
      "Gatys"
    ],
    "review": "The paper investigates a simple extension of Gatys et al. CNN-based texture descriptors for image generation. Similar to Gatys et al., the method uses as texture descriptor the empirical intra-channel correlation matrix of the CNN feature response at some layer of a deep network. Differently from Gatys et al., longer range correlations are measured by introducing a shift between the correlated feature responses, which translates in a simple modification of the original architecture.\n\nThe idea is simple but has interesting effects on the generated textures and can be extended to transformations other than translation. While longer range correlations could be accounted for by considering the response of deeper CNN features in the original method by Gatys et al., the authors show that modelling them explicitly using shallower features is more effective, which is reasonable.\n\nAn important limitation that this work shares with most of its peers is the lack of a principled quantitative evaluation protocol, such that judging the effectiveness of the approach remains almost entirely a qualitative affair. While this should not be considered a significant drawback of the paper due to the objective difficulty of solving this open issue, nevertheless it is somewhat limiting that no principled evaluation method could be devised and implemented. The authors suggest that, as future work, a possible evaluation method could be based on a classification task -- this is a potentially interesting approach that merits some further investigation.\n"
  },
  {
    "people": [
      "Gatys",
      "Gatys"
    ],
    "review": "This paper proposes a modification of the parametric texture synthesis model of Gatys et al. to take into account long-range correlations of textures. To this end the authors add the Gram matrices between spatially shifted feature vectors to the synthesis loss. Some of the synthesised textures are visually superior to the original Gatys et al. method, in particular on textures with very structured long-range correlations (such as bricks).\n\nThe paper is well written, the method and intuitions are clearly exposed and the authors perform quite a wide range of synthesis experiments on different textures.\n\nMy only concern, which is true for all methods including Gatys et al., is the variability of the samples. Clearly the global minimum of the proposed objective is the original image itself. This issue is partially circumvented by performing inpainting experiments, by which the synthesised paths needs to stay coherent with the borders (as the authors did). There are no additional insights into this problem in this paper, which would have been a plus.\n\nAll in all, this work is a simple and nice modification of Gatys at al. which is worth publishing but does not constitute a major breakthrough.\n\n\n"
  },
  {
    "people": [
      "Gatys",
      "Gatys",
      "Gatys",
      "Liu"
    ],
    "review": "The paper introduces a variation to the CNN-based texture synthesis procedure of Gatys et al. that matches correlations between spatially shifted feature responses in addition to the correlations between feature responses at the same position in the feature maps. \nThe paper claims that this \na) improves texture synthesis for textures with long-range regular structures, that are not preserved with the Gatys et al. method\nb) improves performance on texture inpainting tasks compared to the Gatys et al. method\nc) improves results in season transfer when combined with the style transfer method by Gatys et al. \nFurthermore the paper shows that\nd) by matching correlations between spatially flipped feature maps, symmetry properties around the flipping axis can be preserved.\n\nI agree with claim a). However, the generated textures still have some issues such as greyish regions so the problem is not solved. Additionally, the procedure proposed is very costly which makes an already slow texture synthesis method substantially slower. For example, in comparison, the concurrent work by Liu et al. ("
  },
  {
    "people": [
      "Gatys",
      "Gatys",
      "Gatys",
      "Liu"
    ],
    "review": "The paper introduces a variation to the CNN-based texture synthesis procedure of Gatys et al. that matches correlations between spatially shifted feature responses in addition to the correlations between feature responses at the same position in the feature maps. \nThe paper claims that this \na) improves texture synthesis for textures with long-range regular structures, that are not preserved with the Gatys et al. method\nb) improves performance on texture inpainting tasks compared to the Gatys et al. method\nc) improves results in season transfer when combined with the style transfer method by Gatys et al. \nFurthermore the paper shows that\nd) by matching correlations between spatially flipped feature maps, symmetry properties around the flipping axis can be preserved.\n\nI agree with claim a). However, the generated textures still have some issues such as greyish regions so the problem is not solved. Additionally, the procedure proposed is very costly which makes an already slow texture synthesis method substantially slower. For example, in comparison, the concurrent work by Liu et al. ("
  },
  {
    "people": [
      "Gatys",
      "Gatys",
      "Gatys",
      "Gatys"
    ],
    "review": "The paper investigates a simple extension of Gatys et al. CNN-based texture descriptors for image generation. Similar to Gatys et al., the method uses as texture descriptor the empirical intra-channel correlation matrix of the CNN feature response at some layer of a deep network. Differently from Gatys et al., longer range correlations are measured by introducing a shift between the correlated feature responses, which translates in a simple modification of the original architecture.\n\nThe idea is simple but has interesting effects on the generated textures and can be extended to transformations other than translation. While longer range correlations could be accounted for by considering the response of deeper CNN features in the original method by Gatys et al., the authors show that modelling them explicitly using shallower features is more effective, which is reasonable.\n\nAn important limitation that this work shares with most of its peers is the lack of a principled quantitative evaluation protocol, such that judging the effectiveness of the approach remains almost entirely a qualitative affair. While this should not be considered a significant drawback of the paper due to the objective difficulty of solving this open issue, nevertheless it is somewhat limiting that no principled evaluation method could be devised and implemented. The authors suggest that, as future work, a possible evaluation method could be based on a classification task -- this is a potentially interesting approach that merits some further investigation.\n"
  },
  {
    "people": [
      "Gatys",
      "Gatys"
    ],
    "review": "This paper proposes a modification of the parametric texture synthesis model of Gatys et al. to take into account long-range correlations of textures. To this end the authors add the Gram matrices between spatially shifted feature vectors to the synthesis loss. Some of the synthesised textures are visually superior to the original Gatys et al. method, in particular on textures with very structured long-range correlations (such as bricks).\n\nThe paper is well written, the method and intuitions are clearly exposed and the authors perform quite a wide range of synthesis experiments on different textures.\n\nMy only concern, which is true for all methods including Gatys et al., is the variability of the samples. Clearly the global minimum of the proposed objective is the original image itself. This issue is partially circumvented by performing inpainting experiments, by which the synthesised paths needs to stay coherent with the borders (as the authors did). There are no additional insights into this problem in this paper, which would have been a plus.\n\nAll in all, this work is a simple and nice modification of Gatys at al. which is worth publishing but does not constitute a major breakthrough.\n\n\n"
  },
  {
    "people": [
      "Salimans"
    ],
    "review": "After reading the rebuttal, I decided to increase my score. I think ALI somehow stabilizes the GAN training as demonstrated in Fig. 8 and learns a reasonable inference network.\n\n---------------\nInitial Review:\n\nThis paper proposes a new method for learning an inference network in the GAN framework. ALI's objective is to match the joint distribution of hidden and visible units imposed by an encoder and decoder network. ALI is trained on multiple datasets, and it seems to have a good reconstruction even though it does not have an explicit reconstruction term in the cost function. This shows it is learning a decent inference network for GAN.\n\nThere are currently many ways to learn an inference network for GANs: One can learn an inference network after training the GAN by sampling from the GAN and learning a separate network to map X to Z. There is also the infoGAN approach (not cited) which trains the inference network at the same time with the generative path. I think this paper should have an extensive comparison with these other methods and have a discussion for why ALI's inference network is superior to previous works.\n\nSince ALI's inference network is stochastic, it would be great if different reconstructions of a same image is included. I believe the inference network of the BiGAN paper is deterministic which is the main difference with this work. So maybe it is worth highlighting this difference.\n\nThe quality of samples is very good, but there is no quantitative experiment to compare ALI's samples with other GAN variants. So I am not sure if learning an inference network has contributed to better generative samples. Maybe including an inception score for comparison can help.\n\nThere are two sets of semi-supervised results: \nThe first one concatenate the hidden layers of the inference network and uses an L2-SVM afterwards. Ideally, concatenating feature maps is not the best way for semi-supervised learning and one would want to train the semi-supervised path at the same time with the generative path. It would have been much more interesting if part of the hidden code was a categorical distribution and another part of it was a continuous distribution like Gaussian, and the inference network on the categorical latent variable was used directly for classification (like semi-supervised VAE). In this case, the inference network would be trained at the same time with the generative path. Also if the authors can show that ALI can disentangle factors of variations with a discrete latent variable like infoGAN, it will significantly improve the quality of the paper.\n\nThe second semi-supervised learning results show that ALI can match the state-of-the-art. But my impression is that the significant gain is mainly coming from the adaptation of Salimans et al. (2016) in which the discriminator is used for classification. It is unclear to me why learning an inference network help the discriminator do a better job in classification. How do we know the proposed method is improving the stability of the GAN? My understanding is that one of the main points of learning an inference network is to learn a mapping from the image to the high-level features such as class labels. So it would have been more interesting if the inference path was directly used for semi-supervised learning as I explained above."
  },
  {
    "people": [
      "Salimans"
    ],
    "review": "After reading the rebuttal, I decided to increase my score. I think ALI somehow stabilizes the GAN training as demonstrated in Fig. 8 and learns a reasonable inference network.\n\n---------------\nInitial Review:\n\nThis paper proposes a new method for learning an inference network in the GAN framework. ALI's objective is to match the joint distribution of hidden and visible units imposed by an encoder and decoder network. ALI is trained on multiple datasets, and it seems to have a good reconstruction even though it does not have an explicit reconstruction term in the cost function. This shows it is learning a decent inference network for GAN.\n\nThere are currently many ways to learn an inference network for GANs: One can learn an inference network after training the GAN by sampling from the GAN and learning a separate network to map X to Z. There is also the infoGAN approach (not cited) which trains the inference network at the same time with the generative path. I think this paper should have an extensive comparison with these other methods and have a discussion for why ALI's inference network is superior to previous works.\n\nSince ALI's inference network is stochastic, it would be great if different reconstructions of a same image is included. I believe the inference network of the BiGAN paper is deterministic which is the main difference with this work. So maybe it is worth highlighting this difference.\n\nThe quality of samples is very good, but there is no quantitative experiment to compare ALI's samples with other GAN variants. So I am not sure if learning an inference network has contributed to better generative samples. Maybe including an inception score for comparison can help.\n\nThere are two sets of semi-supervised results: \nThe first one concatenate the hidden layers of the inference network and uses an L2-SVM afterwards. Ideally, concatenating feature maps is not the best way for semi-supervised learning and one would want to train the semi-supervised path at the same time with the generative path. It would have been much more interesting if part of the hidden code was a categorical distribution and another part of it was a continuous distribution like Gaussian, and the inference network on the categorical latent variable was used directly for classification (like semi-supervised VAE). In this case, the inference network would be trained at the same time with the generative path. Also if the authors can show that ALI can disentangle factors of variations with a discrete latent variable like infoGAN, it will significantly improve the quality of the paper.\n\nThe second semi-supervised learning results show that ALI can match the state-of-the-art. But my impression is that the significant gain is mainly coming from the adaptation of Salimans et al. (2016) in which the discriminator is used for classification. It is unclear to me why learning an inference network help the discriminator do a better job in classification. How do we know the proposed method is improving the stability of the GAN? My understanding is that one of the main points of learning an inference network is to learn a mapping from the image to the high-level features such as class labels. So it would have been more interesting if the inference path was directly used for semi-supervised learning as I explained above."
  },
  {
    "people": [
      "Salimans"
    ],
    "review": "After reading the rebuttal, I decided to increase my score. I think ALI somehow stabilizes the GAN training as demonstrated in Fig. 8 and learns a reasonable inference network.\n\n---------------\nInitial Review:\n\nThis paper proposes a new method for learning an inference network in the GAN framework. ALI's objective is to match the joint distribution of hidden and visible units imposed by an encoder and decoder network. ALI is trained on multiple datasets, and it seems to have a good reconstruction even though it does not have an explicit reconstruction term in the cost function. This shows it is learning a decent inference network for GAN.\n\nThere are currently many ways to learn an inference network for GANs: One can learn an inference network after training the GAN by sampling from the GAN and learning a separate network to map X to Z. There is also the infoGAN approach (not cited) which trains the inference network at the same time with the generative path. I think this paper should have an extensive comparison with these other methods and have a discussion for why ALI's inference network is superior to previous works.\n\nSince ALI's inference network is stochastic, it would be great if different reconstructions of a same image is included. I believe the inference network of the BiGAN paper is deterministic which is the main difference with this work. So maybe it is worth highlighting this difference.\n\nThe quality of samples is very good, but there is no quantitative experiment to compare ALI's samples with other GAN variants. So I am not sure if learning an inference network has contributed to better generative samples. Maybe including an inception score for comparison can help.\n\nThere are two sets of semi-supervised results: \nThe first one concatenate the hidden layers of the inference network and uses an L2-SVM afterwards. Ideally, concatenating feature maps is not the best way for semi-supervised learning and one would want to train the semi-supervised path at the same time with the generative path. It would have been much more interesting if part of the hidden code was a categorical distribution and another part of it was a continuous distribution like Gaussian, and the inference network on the categorical latent variable was used directly for classification (like semi-supervised VAE). In this case, the inference network would be trained at the same time with the generative path. Also if the authors can show that ALI can disentangle factors of variations with a discrete latent variable like infoGAN, it will significantly improve the quality of the paper.\n\nThe second semi-supervised learning results show that ALI can match the state-of-the-art. But my impression is that the significant gain is mainly coming from the adaptation of Salimans et al. (2016) in which the discriminator is used for classification. It is unclear to me why learning an inference network help the discriminator do a better job in classification. How do we know the proposed method is improving the stability of the GAN? My understanding is that one of the main points of learning an inference network is to learn a mapping from the image to the high-level features such as class labels. So it would have been more interesting if the inference path was directly used for semi-supervised learning as I explained above."
  },
  {
    "people": [
      "Salimans"
    ],
    "review": "After reading the rebuttal, I decided to increase my score. I think ALI somehow stabilizes the GAN training as demonstrated in Fig. 8 and learns a reasonable inference network.\n\n---------------\nInitial Review:\n\nThis paper proposes a new method for learning an inference network in the GAN framework. ALI's objective is to match the joint distribution of hidden and visible units imposed by an encoder and decoder network. ALI is trained on multiple datasets, and it seems to have a good reconstruction even though it does not have an explicit reconstruction term in the cost function. This shows it is learning a decent inference network for GAN.\n\nThere are currently many ways to learn an inference network for GANs: One can learn an inference network after training the GAN by sampling from the GAN and learning a separate network to map X to Z. There is also the infoGAN approach (not cited) which trains the inference network at the same time with the generative path. I think this paper should have an extensive comparison with these other methods and have a discussion for why ALI's inference network is superior to previous works.\n\nSince ALI's inference network is stochastic, it would be great if different reconstructions of a same image is included. I believe the inference network of the BiGAN paper is deterministic which is the main difference with this work. So maybe it is worth highlighting this difference.\n\nThe quality of samples is very good, but there is no quantitative experiment to compare ALI's samples with other GAN variants. So I am not sure if learning an inference network has contributed to better generative samples. Maybe including an inception score for comparison can help.\n\nThere are two sets of semi-supervised results: \nThe first one concatenate the hidden layers of the inference network and uses an L2-SVM afterwards. Ideally, concatenating feature maps is not the best way for semi-supervised learning and one would want to train the semi-supervised path at the same time with the generative path. It would have been much more interesting if part of the hidden code was a categorical distribution and another part of it was a continuous distribution like Gaussian, and the inference network on the categorical latent variable was used directly for classification (like semi-supervised VAE). In this case, the inference network would be trained at the same time with the generative path. Also if the authors can show that ALI can disentangle factors of variations with a discrete latent variable like infoGAN, it will significantly improve the quality of the paper.\n\nThe second semi-supervised learning results show that ALI can match the state-of-the-art. But my impression is that the significant gain is mainly coming from the adaptation of Salimans et al. (2016) in which the discriminator is used for classification. It is unclear to me why learning an inference network help the discriminator do a better job in classification. How do we know the proposed method is improving the stability of the GAN? My understanding is that one of the main points of learning an inference network is to learn a mapping from the image to the high-level features such as class labels. So it would have been more interesting if the inference path was directly used for semi-supervised learning as I explained above."
  },
  {
    "people": [
      "Worringen"
    ],
    "review": "The primary point made by this paper is that given certain architectural characteristics of multi-GPU systems, namely the use of bi-directional PCI-E for communication and the integration of two independent DMA engines on recent GPU devices (providing support for simultaneous independent communications), and given the characteristics of the communications patterns required by synchronous SGD trainers for deep neural networks, namely that the messages are large, dense, and have a fixed length, it makes sense to design communication collectives such as broadcast, reduce, and allreduce specifically for the use case of synchronous SGD training on a multi-GPU system.  The paper describes the implementation of these three collectives (broadcast, reduce, and allreduce) using a linear pipelining (LP) scheme on a (logical) ring topology.  The paper compares the LP collectives to two alternatives:  collectives based on a minimal spanning tree (MST) topology and collectives based on bidirectional exchange (BE).  First, a theoretical comparison is made using a standard cost model used in the high performance computing community.  When assumptions based on multi-GPU system architecture (very low latency for messages) and on the communication characteristics of synchronous SGD training (very large messages) are integrated into the model, the paper finds that the LP collectives should be less costly than BE collectives by a factor of 2 and less costly than MST collectives by a factor of log(p), where p is the number of GPUs being used.  Second, an empirical comparison is performed in which (1) the time required to perform each of the different collectives on a 4-device (k40m) system is measured as a function of message size and (2) the time required to perform each of the different collectives with a 200 MB message length is measured as a function of the number of devices in the system.  These measurements show that the LP-based collectives are consistently the fastest.  Third, DNN training experiments with AlexNet and GoogLeNet are performed on a 4-device system using three different synchronous SGD algorithms with the different implementations of the collectives (a total of 6 different algorithms in all).  Measurements of the communication and computation costs show that the LP collectives reduce communication costs without affecting computation costs (as expected).  Measurements of the convergence of the training loss as a function of time for the two DNN architectures show that use of the LP collectives leads to faster training.\n\nWhile the theory says that the costs of LP collectives should be invariant to the number of devices in a multi-GPU system, the empirical work shows that in practice this does not hold going from 4 to 5 devices (in the tested configuration) because in a 5-device system messages must traverse the QPI.  Are there other practical considerations that the authors are aware of that affect the scaling of the LP collectives?  If so, these should be mentioned in the paper.\n\nIn the sentence \"Worringen (2003) proposed a pipeline collective model in shared memory environment for CPU data, but communications of different MPI processes sharing the same CPU memory bus within the same CPU socket.\" I really can't figure out what the words after \"but communications of different MPI processes\" are trying to convey.  This sentence is not comprehensible.\n\n\"Please note the latency term is log p\u03b1, which is the smallest among algorithms in Table.1. Therefore, MST only suits for high frequent short messages.\"  The claim that MST collectives are only suitable for high-frequency, short messages does not follow from the statement that MST collectives have the smallest latency term.  You also need to consider the way the cost scales with message size (the bandwidth term).  If the MST collectives had a better bandwidth term than the other collectives, then they would also be superior for large messages.\n\n\"Let\u2019s take an appropriate block size b to ensure n/b \u226a \u03b1.\"  This looks wrong, since n > b.  Should it be b/n \u226a \u03b1?\n\n\"However, the parameters are not consistent after several iterations due to the precision issues of float multiplications in Gradient Update.\"  Are you sure the inconsistency in weight estimates across devices is due to multiplication?  I would expect that it would be due to gradients being accumulated in different orders; that is, because floating point addition is not commutative.\n\nI recommend replacing the term \"sub-gradients\" in this paper with \"partial gradients.\"  In the optimization literature, the term \"sub-gradient\" has a very specific meaning that differs from this paper's use of the term (see"
  },
  {
    "people": [
      "Worringen"
    ],
    "review": "The primary point made by this paper is that given certain architectural characteristics of multi-GPU systems, namely the use of bi-directional PCI-E for communication and the integration of two independent DMA engines on recent GPU devices (providing support for simultaneous independent communications), and given the characteristics of the communications patterns required by synchronous SGD trainers for deep neural networks, namely that the messages are large, dense, and have a fixed length, it makes sense to design communication collectives such as broadcast, reduce, and allreduce specifically for the use case of synchronous SGD training on a multi-GPU system.  The paper describes the implementation of these three collectives (broadcast, reduce, and allreduce) using a linear pipelining (LP) scheme on a (logical) ring topology.  The paper compares the LP collectives to two alternatives:  collectives based on a minimal spanning tree (MST) topology and collectives based on bidirectional exchange (BE).  First, a theoretical comparison is made using a standard cost model used in the high performance computing community.  When assumptions based on multi-GPU system architecture (very low latency for messages) and on the communication characteristics of synchronous SGD training (very large messages) are integrated into the model, the paper finds that the LP collectives should be less costly than BE collectives by a factor of 2 and less costly than MST collectives by a factor of log(p), where p is the number of GPUs being used.  Second, an empirical comparison is performed in which (1) the time required to perform each of the different collectives on a 4-device (k40m) system is measured as a function of message size and (2) the time required to perform each of the different collectives with a 200 MB message length is measured as a function of the number of devices in the system.  These measurements show that the LP-based collectives are consistently the fastest.  Third, DNN training experiments with AlexNet and GoogLeNet are performed on a 4-device system using three different synchronous SGD algorithms with the different implementations of the collectives (a total of 6 different algorithms in all).  Measurements of the communication and computation costs show that the LP collectives reduce communication costs without affecting computation costs (as expected).  Measurements of the convergence of the training loss as a function of time for the two DNN architectures show that use of the LP collectives leads to faster training.\n\nWhile the theory says that the costs of LP collectives should be invariant to the number of devices in a multi-GPU system, the empirical work shows that in practice this does not hold going from 4 to 5 devices (in the tested configuration) because in a 5-device system messages must traverse the QPI.  Are there other practical considerations that the authors are aware of that affect the scaling of the LP collectives?  If so, these should be mentioned in the paper.\n\nIn the sentence \"Worringen (2003) proposed a pipeline collective model in shared memory environment for CPU data, but communications of different MPI processes sharing the same CPU memory bus within the same CPU socket.\" I really can't figure out what the words after \"but communications of different MPI processes\" are trying to convey.  This sentence is not comprehensible.\n\n\"Please note the latency term is log p\u03b1, which is the smallest among algorithms in Table.1. Therefore, MST only suits for high frequent short messages.\"  The claim that MST collectives are only suitable for high-frequency, short messages does not follow from the statement that MST collectives have the smallest latency term.  You also need to consider the way the cost scales with message size (the bandwidth term).  If the MST collectives had a better bandwidth term than the other collectives, then they would also be superior for large messages.\n\n\"Let\u2019s take an appropriate block size b to ensure n/b \u226a \u03b1.\"  This looks wrong, since n > b.  Should it be b/n \u226a \u03b1?\n\n\"However, the parameters are not consistent after several iterations due to the precision issues of float multiplications in Gradient Update.\"  Are you sure the inconsistency in weight estimates across devices is due to multiplication?  I would expect that it would be due to gradients being accumulated in different orders; that is, because floating point addition is not commutative.\n\nI recommend replacing the term \"sub-gradients\" in this paper with \"partial gradients.\"  In the optimization literature, the term \"sub-gradient\" has a very specific meaning that differs from this paper's use of the term (see "
  },
  {
    "people": [
      "Worringen"
    ],
    "review": "The primary point made by this paper is that given certain architectural characteristics of multi-GPU systems, namely the use of bi-directional PCI-E for communication and the integration of two independent DMA engines on recent GPU devices (providing support for simultaneous independent communications), and given the characteristics of the communications patterns required by synchronous SGD trainers for deep neural networks, namely that the messages are large, dense, and have a fixed length, it makes sense to design communication collectives such as broadcast, reduce, and allreduce specifically for the use case of synchronous SGD training on a multi-GPU system.  The paper describes the implementation of these three collectives (broadcast, reduce, and allreduce) using a linear pipelining (LP) scheme on a (logical) ring topology.  The paper compares the LP collectives to two alternatives:  collectives based on a minimal spanning tree (MST) topology and collectives based on bidirectional exchange (BE).  First, a theoretical comparison is made using a standard cost model used in the high performance computing community.  When assumptions based on multi-GPU system architecture (very low latency for messages) and on the communication characteristics of synchronous SGD training (very large messages) are integrated into the model, the paper finds that the LP collectives should be less costly than BE collectives by a factor of 2 and less costly than MST collectives by a factor of log(p), where p is the number of GPUs being used.  Second, an empirical comparison is performed in which (1) the time required to perform each of the different collectives on a 4-device (k40m) system is measured as a function of message size and (2) the time required to perform each of the different collectives with a 200 MB message length is measured as a function of the number of devices in the system.  These measurements show that the LP-based collectives are consistently the fastest.  Third, DNN training experiments with AlexNet and GoogLeNet are performed on a 4-device system using three different synchronous SGD algorithms with the different implementations of the collectives (a total of 6 different algorithms in all).  Measurements of the communication and computation costs show that the LP collectives reduce communication costs without affecting computation costs (as expected).  Measurements of the convergence of the training loss as a function of time for the two DNN architectures show that use of the LP collectives leads to faster training.\n\nWhile the theory says that the costs of LP collectives should be invariant to the number of devices in a multi-GPU system, the empirical work shows that in practice this does not hold going from 4 to 5 devices (in the tested configuration) because in a 5-device system messages must traverse the QPI.  Are there other practical considerations that the authors are aware of that affect the scaling of the LP collectives?  If so, these should be mentioned in the paper.\n\nIn the sentence \"Worringen (2003) proposed a pipeline collective model in shared memory environment for CPU data, but communications of different MPI processes sharing the same CPU memory bus within the same CPU socket.\" I really can't figure out what the words after \"but communications of different MPI processes\" are trying to convey.  This sentence is not comprehensible.\n\n\"Please note the latency term is log p\u03b1, which is the smallest among algorithms in Table.1. Therefore, MST only suits for high frequent short messages.\"  The claim that MST collectives are only suitable for high-frequency, short messages does not follow from the statement that MST collectives have the smallest latency term.  You also need to consider the way the cost scales with message size (the bandwidth term).  If the MST collectives had a better bandwidth term than the other collectives, then they would also be superior for large messages.\n\n\"Let\u2019s take an appropriate block size b to ensure n/b \u226a \u03b1.\"  This looks wrong, since n > b.  Should it be b/n \u226a \u03b1?\n\n\"However, the parameters are not consistent after several iterations due to the precision issues of float multiplications in Gradient Update.\"  Are you sure the inconsistency in weight estimates across devices is due to multiplication?  I would expect that it would be due to gradients being accumulated in different orders; that is, because floating point addition is not commutative.\n\nI recommend replacing the term \"sub-gradients\" in this paper with \"partial gradients.\"  In the optimization literature, the term \"sub-gradient\" has a very specific meaning that differs from this paper's use of the term (see"
  },
  {
    "people": [
      "Worringen"
    ],
    "review": "The primary point made by this paper is that given certain architectural characteristics of multi-GPU systems, namely the use of bi-directional PCI-E for communication and the integration of two independent DMA engines on recent GPU devices (providing support for simultaneous independent communications), and given the characteristics of the communications patterns required by synchronous SGD trainers for deep neural networks, namely that the messages are large, dense, and have a fixed length, it makes sense to design communication collectives such as broadcast, reduce, and allreduce specifically for the use case of synchronous SGD training on a multi-GPU system.  The paper describes the implementation of these three collectives (broadcast, reduce, and allreduce) using a linear pipelining (LP) scheme on a (logical) ring topology.  The paper compares the LP collectives to two alternatives:  collectives based on a minimal spanning tree (MST) topology and collectives based on bidirectional exchange (BE).  First, a theoretical comparison is made using a standard cost model used in the high performance computing community.  When assumptions based on multi-GPU system architecture (very low latency for messages) and on the communication characteristics of synchronous SGD training (very large messages) are integrated into the model, the paper finds that the LP collectives should be less costly than BE collectives by a factor of 2 and less costly than MST collectives by a factor of log(p), where p is the number of GPUs being used.  Second, an empirical comparison is performed in which (1) the time required to perform each of the different collectives on a 4-device (k40m) system is measured as a function of message size and (2) the time required to perform each of the different collectives with a 200 MB message length is measured as a function of the number of devices in the system.  These measurements show that the LP-based collectives are consistently the fastest.  Third, DNN training experiments with AlexNet and GoogLeNet are performed on a 4-device system using three different synchronous SGD algorithms with the different implementations of the collectives (a total of 6 different algorithms in all).  Measurements of the communication and computation costs show that the LP collectives reduce communication costs without affecting computation costs (as expected).  Measurements of the convergence of the training loss as a function of time for the two DNN architectures show that use of the LP collectives leads to faster training.\n\nWhile the theory says that the costs of LP collectives should be invariant to the number of devices in a multi-GPU system, the empirical work shows that in practice this does not hold going from 4 to 5 devices (in the tested configuration) because in a 5-device system messages must traverse the QPI.  Are there other practical considerations that the authors are aware of that affect the scaling of the LP collectives?  If so, these should be mentioned in the paper.\n\nIn the sentence \"Worringen (2003) proposed a pipeline collective model in shared memory environment for CPU data, but communications of different MPI processes sharing the same CPU memory bus within the same CPU socket.\" I really can't figure out what the words after \"but communications of different MPI processes\" are trying to convey.  This sentence is not comprehensible.\n\n\"Please note the latency term is log p\u03b1, which is the smallest among algorithms in Table.1. Therefore, MST only suits for high frequent short messages.\"  The claim that MST collectives are only suitable for high-frequency, short messages does not follow from the statement that MST collectives have the smallest latency term.  You also need to consider the way the cost scales with message size (the bandwidth term).  If the MST collectives had a better bandwidth term than the other collectives, then they would also be superior for large messages.\n\n\"Let\u2019s take an appropriate block size b to ensure n/b \u226a \u03b1.\"  This looks wrong, since n > b.  Should it be b/n \u226a \u03b1?\n\n\"However, the parameters are not consistent after several iterations due to the precision issues of float multiplications in Gradient Update.\"  Are you sure the inconsistency in weight estimates across devices is due to multiplication?  I would expect that it would be due to gradients being accumulated in different orders; that is, because floating point addition is not commutative.\n\nI recommend replacing the term \"sub-gradients\" in this paper with \"partial gradients.\"  In the optimization literature, the term \"sub-gradient\" has a very specific meaning that differs from this paper's use of the term (see "
  },
  {
    "people": [
      "Oseledests",
      "Lubich"
    ],
    "review": "The paper presents an application of a tensor factorization to linear models, which allows to consider higher-order interactions between variables in classification (and regression) problems, and that maintains computational feasibility, being linear in the dimension. The factorization employed is based on the TT format, first proposed by Oseledests (2011). The authors also propose the adoption of a Riemannian optimization scheme to explicit consider the geometry of the tensor manifold, and thus speed up convergence.\n\nThe paper in general is well written, it presents an interesting application of the TT tensor format for linear models (together with an application of Riemannian optimization), which in my opinion is quite interesting since it has a wide range of possible applications in different algorithms in machine learning.\n\nOn the other side, I have some concerns are about the experimental part, which I consider not at the level of the rest of the paper, for instance in terms of number of experiments on real datasets, role of dropout in real datasets, comparison with other algorithms on real datasets. Moreover the authors do not take into account explicitly the problem of the choice of the rank to be used in the experiments. In general the experimental section seems a collection of preliminary experiments where different aspects have been tested by not in a organic way.\n\nI think the paper is close to a weak acceptance / weak rejection, I don't rate it as a full acceptance paper, mainly due to the non-satisfactory experiment setting. In case of extra experiments confirming the goodness of the approach, I believe the paper could have much better scores.\n\nSome minor comments:\n-formula 2: Obvious comment: learning the parameters of the model in (1) can be done as in (2), but also in other ways, depending on the approach you are using.\n-the fact that the rank is bounded by 2r, before formula 9, is explained in Lubich et al., 2015?\n-after formula 10: why the N projections in total they cost O(dr^2(r+N)), it should be O(Ndr^2(r+1)), no? since each of the elements of the summation has rank 1, and the cost for each of them is O(dr^2(r+TT_rank(Z)^2)), where TT-rank(Z)=1. Am I wrong?\n-section 6.2: can you explain why the random initialization freezes the convergence? This seems interesting but not motivated. Any guess?\n-section 6.3: you adopt dropout: can you comment in particular on the advantages it gives in the context of the exponential machines? did you use it on real datasets?\n-how do you choose r_0 in you experiments? with a validation set?\n-in section 7: why you don't have x_1 x_2 among the variables?\n-section 8: there is a typo in \"experiments\"\n-section 8.1: \"We simplicity, we binarized\" I think there's a problem with the English language in this sentence\n-section 8.3: \"we report that dropout helps\".. this is quite general statement, only tested on a synthetic dataset\n-section 8.5: can you provide more results for this dataset, for instance in terms of training and inference time? or test wrt other algorithms?"
  },
  {
    "people": [
      "Alexander",
      "Livni"
    ],
    "review": "Hi Alexander,\n\nYour paper is an interesting addition to the literature on low-rank polynomial models. Good work!\n\nI agree with a previous comment that it would be worth adding more comments on the difference between FMs and EMs in Section 9. For instance, EMs model all d-combinations while FMs model combinations up to some degree. It would also be informative to compare the total model size of both models.\n\nI am not sure I agree that \"TT-format allows for Riemannian optimization\" is an advantage of tensor trains. For example, it should be possible to train FMs over the positive definite matrix manifold.\n\nAdmittedly, the literature is fairly recent but here are a few relevant prior works:\n\n- \"On the Computational Efficiency of Training Neural Networks\" by Livni et al. "
  },
  {
    "people": [
      "Oseledests",
      "Lubich"
    ],
    "review": "The paper presents an application of a tensor factorization to linear models, which allows to consider higher-order interactions between variables in classification (and regression) problems, and that maintains computational feasibility, being linear in the dimension. The factorization employed is based on the TT format, first proposed by Oseledests (2011). The authors also propose the adoption of a Riemannian optimization scheme to explicit consider the geometry of the tensor manifold, and thus speed up convergence.\n\nThe paper in general is well written, it presents an interesting application of the TT tensor format for linear models (together with an application of Riemannian optimization), which in my opinion is quite interesting since it has a wide range of possible applications in different algorithms in machine learning.\n\nOn the other side, I have some concerns are about the experimental part, which I consider not at the level of the rest of the paper, for instance in terms of number of experiments on real datasets, role of dropout in real datasets, comparison with other algorithms on real datasets. Moreover the authors do not take into account explicitly the problem of the choice of the rank to be used in the experiments. In general the experimental section seems a collection of preliminary experiments where different aspects have been tested by not in a organic way.\n\nI think the paper is close to a weak acceptance / weak rejection, I don't rate it as a full acceptance paper, mainly due to the non-satisfactory experiment setting. In case of extra experiments confirming the goodness of the approach, I believe the paper could have much better scores.\n\nSome minor comments:\n-formula 2: Obvious comment: learning the parameters of the model in (1) can be done as in (2), but also in other ways, depending on the approach you are using.\n-the fact that the rank is bounded by 2r, before formula 9, is explained in Lubich et al., 2015?\n-after formula 10: why the N projections in total they cost O(dr^2(r+N)), it should be O(Ndr^2(r+1)), no? since each of the elements of the summation has rank 1, and the cost for each of them is O(dr^2(r+TT_rank(Z)^2)), where TT-rank(Z)=1. Am I wrong?\n-section 6.2: can you explain why the random initialization freezes the convergence? This seems interesting but not motivated. Any guess?\n-section 6.3: you adopt dropout: can you comment in particular on the advantages it gives in the context of the exponential machines? did you use it on real datasets?\n-how do you choose r_0 in you experiments? with a validation set?\n-in section 7: why you don't have x_1 x_2 among the variables?\n-section 8: there is a typo in \"experiments\"\n-section 8.1: \"We simplicity, we binarized\" I think there's a problem with the English language in this sentence\n-section 8.3: \"we report that dropout helps\".. this is quite general statement, only tested on a synthetic dataset\n-section 8.5: can you provide more results for this dataset, for instance in terms of training and inference time? or test wrt other algorithms?"
  },
  {
    "people": [
      "Alexander",
      "Livni"
    ],
    "review": "Hi Alexander,\n\nYour paper is an interesting addition to the literature on low-rank polynomial models. Good work!\n\nI agree with a previous comment that it would be worth adding more comments on the difference between FMs and EMs in Section 9. For instance, EMs model all d-combinations while FMs model combinations up to some degree. It would also be informative to compare the total model size of both models.\n\nI am not sure I agree that \"TT-format allows for Riemannian optimization\" is an advantage of tensor trains. For example, it should be possible to train FMs over the positive definite matrix manifold.\n\nAdmittedly, the literature is fairly recent but here are a few relevant prior works:\n\n- \"On the Computational Efficiency of Training Neural Networks\" by Livni et al. "
  },
  {
    "people": [
      "Yu"
    ],
    "review": "I would like first to apologize for the delay.\n\nSummary: A framework for two-samples statistical test using binary\nclassification is proposed. It allows multi-dimensional sample testing and\nan interpretability that other tests lack. A theoritical analysis is\nprovided and various empirical tests reported.\n\nA very interesting approach. I have however two main concerns.\n\nThe clarity of the presentation is obscured by too much content. It would\nbe more interesting if the presentation could be somewhat\nself-contained. You could consider making 2 papers out of this paper.\n\nSeriously, you cram a lot of experiments in this paper. But the setting\nof the experiments is not really explained. We are supposed to have read\nJitkrittum et al., 2016, Radford et al., 2016, Yu et al., 2015, etc. All \nthis is okay but reduces your public to a very few.\n\nFor example, if I am not mistaken, you never explained what SCF is, despite\nthe fact that its performances are reported. \n\nAs a second point, given also that the number of submissions to this conference are exploding,\nI would like to challenge you with the following question:\n\nWhy is this work significant to the representation learning community?\n"
  },
  {
    "people": [
      "Arthur Gretton's"
    ],
    "review": "The submission considers the setting of 2-sample testing from the perspective of evaluating a classifier.  For a classifier between two samples from the same distribution, the distribution of the classification accuracy follows a simple form under the null hypothesis.  As such, a straightforward threshold can be derived for any classifier.  Finding a more powerful test then amounts to training a better classifier.  One may then focus efforts, e.g. on deep neural networks, for which statistics such as the MMD may be very difficult to characterize.\n\n+ The approach is sound and very general\n+ The paper is timely in that deep learning has had huge impacts in classification and other prediction settings, but has not had as big an impact on statistical hypothesis testing as kernel methods have\n\n- The discussion of the relationship to kernel-MMD has not always been as realistic as it could have been.  For example, the kernel-MMD can also be seen as a classifier based approach, so a more fair discussion could be provided.  Also, the form of kernel-MMD used in the comparisons is a bit contradictory to the discussion as well\n * The linear kernel-MMD is used which is less powerful than the quadradic kernel-MMD (the authors have justified this from the perspective of computation time)\n * The kernel-MMD is argued against due to its unwieldy distribution under the null, but the linear time kernel-MMD (see also Zaremba et al., NIPS 2013) has a Gaussian distribution under the null.\n\nArthur Gretton's comment from Dec 14 during the discussion period was very insightful and helpful.  If these insights and additional experiments comparing the kernel-MMD to the classifier threshold on the blobs dataset could be included, that would be very helpful for understanding the paper.  The open review format gives an excellent opportunity to assign proper credit for these experiments and insights by citing the comment."
  },
  {
    "people": [
      "Gretton",
      "Chwialkowski",
      "Arthur Gretton"
    ],
    "review": "We are writing both to provide a perspective on the proposed \u201cclassification approach\u201d to testing, and to expand on the reviews of AnonReviewer3 and AnonReviewer1, which suggested considering the relation between the MMD and classification. We discuss this link with reference to some earlier work, and then look into the broader implications for testing using classification error.\n\n\nFirst, the MMD can indeed be thought of as a classifier. In Section 2 of the paper:\n\n\nSriperumbudur, B., Fukumizu, K., Gretton, A., Lanckriet, G., and Schoelkopf, B., Kernel choice and classifiability for RKHS embeddings of probability distributions, NIPS 2009.\n\n\nwe show that the MMD is the negative of the optimal risk corresponding to a linear loss function, associated with the kernel classifier. The \u201cwitness function\u201d of the MMD can then be thought of as the decision function for that classifier, returning labels +1 for positive values (class P), and -1 for negative values (class Q). See figure 1 in Gretton et al (2012a) for an illustration of the witness function.\n\n\nThis is a particularly useful result, since it allows the direct comparison between two alternative tests:\n\n\n1) the original MMD test, based on the asymptotic distribution of the norm of the witness function under the null distribution (obtained in practice by permutation)\n\n\n2)  the proposed test strategy, based on the expected chance level performance of this classifier on a held-out test set, where the test threshold is set to the appropriate quantile of the binomial distribution. \n\n\nWe look at the 2-D blobs dataset from the paper of Chwialkowski et al. (NIPS 2015), for 1000 samples, and the same kernel bandwidth for each approach. The results are:\n\n\nOriginal MMD test: 85% true positives\nClassification-based approach: 63% true positives.\n\n\nThis can easily be understood, for two reasons: first, binomial variables have relatively high variance, hence the resulting test may be conservative. A more direct way to detect chance level would be to measure that the decision boundary is as \u201cflat\u201d as one would expect if there were no differences in the distributions generating the samples, as done for the original MMD test. Second, a classification test uses half its samples to create a decision function, and the other half for testing, whereas the MMD test uses the entire sample for testing.\n\n\nAll this being said, the approach proposed by the paper remains a very interesting one, and well worth publishing. When a classifier is highly complex and/or expensive to train, it may not be possible to cheaply obtain a suitable test threshold based on the decision function.  The proposed binomial test threshold might then be the only way to construct a test. In this case, the gain in test sensitivity by using a powerful classifier could be more important the loss in power due to the two issues raised in the paragraph above.\n\n\nArthur Gretton, Wittawat Jitkrittum"
  },
  {
    "people": [
      "Bowman"
    ],
    "review": "This is interesting work. Another reference for Section 5 of this paper: the use of binary classifiers to evaluate generative models was also used in Bowman et al. 2016 ("
  },
  {
    "people": [
      "Yu"
    ],
    "review": "I would like first to apologize for the delay.\n\nSummary: A framework for two-samples statistical test using binary\nclassification is proposed. It allows multi-dimensional sample testing and\nan interpretability that other tests lack. A theoritical analysis is\nprovided and various empirical tests reported.\n\nA very interesting approach. I have however two main concerns.\n\nThe clarity of the presentation is obscured by too much content. It would\nbe more interesting if the presentation could be somewhat\nself-contained. You could consider making 2 papers out of this paper.\n\nSeriously, you cram a lot of experiments in this paper. But the setting\nof the experiments is not really explained. We are supposed to have read\nJitkrittum et al., 2016, Radford et al., 2016, Yu et al., 2015, etc. All \nthis is okay but reduces your public to a very few.\n\nFor example, if I am not mistaken, you never explained what SCF is, despite\nthe fact that its performances are reported. \n\nAs a second point, given also that the number of submissions to this conference are exploding,\nI would like to challenge you with the following question:\n\nWhy is this work significant to the representation learning community?\n"
  },
  {
    "people": [
      "Arthur Gretton's"
    ],
    "review": "The submission considers the setting of 2-sample testing from the perspective of evaluating a classifier.  For a classifier between two samples from the same distribution, the distribution of the classification accuracy follows a simple form under the null hypothesis.  As such, a straightforward threshold can be derived for any classifier.  Finding a more powerful test then amounts to training a better classifier.  One may then focus efforts, e.g. on deep neural networks, for which statistics such as the MMD may be very difficult to characterize.\n\n+ The approach is sound and very general\n+ The paper is timely in that deep learning has had huge impacts in classification and other prediction settings, but has not had as big an impact on statistical hypothesis testing as kernel methods have\n\n- The discussion of the relationship to kernel-MMD has not always been as realistic as it could have been.  For example, the kernel-MMD can also be seen as a classifier based approach, so a more fair discussion could be provided.  Also, the form of kernel-MMD used in the comparisons is a bit contradictory to the discussion as well\n * The linear kernel-MMD is used which is less powerful than the quadradic kernel-MMD (the authors have justified this from the perspective of computation time)\n * The kernel-MMD is argued against due to its unwieldy distribution under the null, but the linear time kernel-MMD (see also Zaremba et al., NIPS 2013) has a Gaussian distribution under the null.\n\nArthur Gretton's comment from Dec 14 during the discussion period was very insightful and helpful.  If these insights and additional experiments comparing the kernel-MMD to the classifier threshold on the blobs dataset could be included, that would be very helpful for understanding the paper.  The open review format gives an excellent opportunity to assign proper credit for these experiments and insights by citing the comment."
  },
  {
    "people": [
      "Gretton",
      "Chwialkowski",
      "Arthur Gretton"
    ],
    "review": "We are writing both to provide a perspective on the proposed \u201cclassification approach\u201d to testing, and to expand on the reviews of AnonReviewer3 and AnonReviewer1, which suggested considering the relation between the MMD and classification. We discuss this link with reference to some earlier work, and then look into the broader implications for testing using classification error.\n\n\nFirst, the MMD can indeed be thought of as a classifier. In Section 2 of the paper:\n\n\nSriperumbudur, B., Fukumizu, K., Gretton, A., Lanckriet, G., and Schoelkopf, B., Kernel choice and classifiability for RKHS embeddings of probability distributions, NIPS 2009.\n\n\nwe show that the MMD is the negative of the optimal risk corresponding to a linear loss function, associated with the kernel classifier. The \u201cwitness function\u201d of the MMD can then be thought of as the decision function for that classifier, returning labels +1 for positive values (class P), and -1 for negative values (class Q). See figure 1 in Gretton et al (2012a) for an illustration of the witness function.\n\n\nThis is a particularly useful result, since it allows the direct comparison between two alternative tests:\n\n\n1) the original MMD test, based on the asymptotic distribution of the norm of the witness function under the null distribution (obtained in practice by permutation)\n\n\n2)  the proposed test strategy, based on the expected chance level performance of this classifier on a held-out test set, where the test threshold is set to the appropriate quantile of the binomial distribution. \n\n\nWe look at the 2-D blobs dataset from the paper of Chwialkowski et al. (NIPS 2015), for 1000 samples, and the same kernel bandwidth for each approach. The results are:\n\n\nOriginal MMD test: 85% true positives\nClassification-based approach: 63% true positives.\n\n\nThis can easily be understood, for two reasons: first, binomial variables have relatively high variance, hence the resulting test may be conservative. A more direct way to detect chance level would be to measure that the decision boundary is as \u201cflat\u201d as one would expect if there were no differences in the distributions generating the samples, as done for the original MMD test. Second, a classification test uses half its samples to create a decision function, and the other half for testing, whereas the MMD test uses the entire sample for testing.\n\n\nAll this being said, the approach proposed by the paper remains a very interesting one, and well worth publishing. When a classifier is highly complex and/or expensive to train, it may not be possible to cheaply obtain a suitable test threshold based on the decision function.  The proposed binomial test threshold might then be the only way to construct a test. In this case, the gain in test sensitivity by using a powerful classifier could be more important the loss in power due to the two issues raised in the paragraph above.\n\n\nArthur Gretton, Wittawat Jitkrittum"
  },
  {
    "people": [
      "Bowman"
    ],
    "review": "This is interesting work. Another reference for Section 5 of this paper: the use of binary classifiers to evaluate generative models was also used in Bowman et al. 2016 ("
  },
  {
    "people": [
      "Mi"
    ],
    "review": "This paper conducts a comprehensive series of experiments on vocabulary selection strategies to reduce the computational cost of neural machine translation.\n\nA range of techniques are investigated, ranging from very simple methods such as word co-occurences, to the relatively complex use of SVMs.\n\nThe experiments are solid, comprehensive and very useful in practical terms.  It is good to see that the best vocabulary selection method is very effective at achieving a very high proportion of the coverage of the full-vocabulary model (fig 3).  However, I feel that the experiments in section 4.3 (vocabulary selection during training) was rather limited in their scope - I would have liked to see more experiments here.\n\nA major criticism I have with this paper is that there is little novelty here.  The techniques are mostly standard methods and rather simple, and in particular, there it seems that there is not much additional material beyond the work of Mi et al (2016).  So although the work is solid, the lack of originality lets it down.\n\nMinor comments: in 2.1, the word co-occurence measure - was any smoothing used to make this measure more robust to low counts?\n\n\n\n\n\n\n\n\n"
  },
  {
    "people": [
      "Mauser",
      "Ha",
      "Mi"
    ],
    "review": "This paper compares several strategies for guessing a short list of vocabulary for the target language in neural machine translation. The primary findings are that word alignment dictionaries work better than a variety of other techniques.\n\nMy take on this paper is that to have a significant impact, it needs to make the case for why one might want vocabulary rather than characters or sub word units like BPE. I think there are likely many very good reasons to do this that could be argued for (synthesize morphology, deal with transliteration, etc), but most of these would suggest some particular models and experiments, which are of course not in this paper. As it is, I think this paper is a useful but minor contribution that shows that word alignment is a good way of getting short lists, but it does not strongly make the case that we should abandon work in other directions.\n\nMinor comments:\nIn addition to the SVM approach for modeling vocabulary, the discriminative word lexicon of Mauser et al. (2009) and the neural version of Ha et al. (2014) are also worth mentioning.\n\nIt would be useful to know what the coverage rate of the actual full vocabulary would be (rather than the 100k \u201cfull vocabulary\u201d). Since presumably this technique could be used to work with much larger vocabularies.\n\nWhen reducing the vocabulary size for training, the Mi et al. (2016) technique of taking the union of all the vocabularies in a mini batch seems like a rather strange objective. If the vocabulary of a single sentence is used, the probabilistic semantics of the translation model can still be preserved since p(e | f, vocab(f)) = p(e | f) if p(vocab(f) | f) = 1, i.e., is deterministic, which it is here. Whereas the objective is no longer a sensible probability model in the mini batch vocabulary case. Thus, while it may be a bit more difficult to implement, it seems like it would at least be a sensible comparison to make."
  },
  {
    "people": [
      "Mi"
    ],
    "review": "In this paper, the authors present several strategies to select a small subset of target vocabulary to work with per source sentence, which results in significant speedup. The results are convincing and I think this paper offers practical values to general seq2seq approaches to language tasks. However, there is little novelty in this work: the authors further mostly extend the work of (Mi et al., 2016) with more vocabulary selection strategies and thorough experiments. This paper will fit better in an NLP venue."
  },
  {
    "people": [
      "Mi"
    ],
    "review": "This paper conducts a comprehensive series of experiments on vocabulary selection strategies to reduce the computational cost of neural machine translation.\n\nA range of techniques are investigated, ranging from very simple methods such as word co-occurences, to the relatively complex use of SVMs.\n\nThe experiments are solid, comprehensive and very useful in practical terms.  It is good to see that the best vocabulary selection method is very effective at achieving a very high proportion of the coverage of the full-vocabulary model (fig 3).  However, I feel that the experiments in section 4.3 (vocabulary selection during training) was rather limited in their scope - I would have liked to see more experiments here.\n\nA major criticism I have with this paper is that there is little novelty here.  The techniques are mostly standard methods and rather simple, and in particular, there it seems that there is not much additional material beyond the work of Mi et al (2016).  So although the work is solid, the lack of originality lets it down.\n\nMinor comments: in 2.1, the word co-occurence measure - was any smoothing used to make this measure more robust to low counts?\n\n\n\n\n\n\n\n\n"
  },
  {
    "people": [
      "Mauser",
      "Ha",
      "Mi"
    ],
    "review": "This paper compares several strategies for guessing a short list of vocabulary for the target language in neural machine translation. The primary findings are that word alignment dictionaries work better than a variety of other techniques.\n\nMy take on this paper is that to have a significant impact, it needs to make the case for why one might want vocabulary rather than characters or sub word units like BPE. I think there are likely many very good reasons to do this that could be argued for (synthesize morphology, deal with transliteration, etc), but most of these would suggest some particular models and experiments, which are of course not in this paper. As it is, I think this paper is a useful but minor contribution that shows that word alignment is a good way of getting short lists, but it does not strongly make the case that we should abandon work in other directions.\n\nMinor comments:\nIn addition to the SVM approach for modeling vocabulary, the discriminative word lexicon of Mauser et al. (2009) and the neural version of Ha et al. (2014) are also worth mentioning.\n\nIt would be useful to know what the coverage rate of the actual full vocabulary would be (rather than the 100k \u201cfull vocabulary\u201d). Since presumably this technique could be used to work with much larger vocabularies.\n\nWhen reducing the vocabulary size for training, the Mi et al. (2016) technique of taking the union of all the vocabularies in a mini batch seems like a rather strange objective. If the vocabulary of a single sentence is used, the probabilistic semantics of the translation model can still be preserved since p(e | f, vocab(f)) = p(e | f) if p(vocab(f) | f) = 1, i.e., is deterministic, which it is here. Whereas the objective is no longer a sensible probability model in the mini batch vocabulary case. Thus, while it may be a bit more difficult to implement, it seems like it would at least be a sensible comparison to make."
  },
  {
    "people": [
      "Mi"
    ],
    "review": "In this paper, the authors present several strategies to select a small subset of target vocabulary to work with per source sentence, which results in significant speedup. The results are convincing and I think this paper offers practical values to general seq2seq approaches to language tasks. However, there is little novelty in this work: the authors further mostly extend the work of (Mi et al., 2016) with more vocabulary selection strategies and thorough experiments. This paper will fit better in an NLP venue."
  },
  {
    "people": [
      "Ghavamzadeh"
    ],
    "review": "This paper studies the problem of abstract hierarchical multiagent RL with policy sketches, high level descriptions of abstract actions. The work is related to much previous work in hierarchical RL, and adds some new elements by using neural implementations of prior work on hierarchical learning and skill representations. \n\nSketches are sequences of high level symbolic labels drawn from some fixed vocabulary, which initially are devoid of any meaning. Eventually the sketches get mapped into real policies and enable policy transfer and temporal abstraction. Learning occurs through a variant of the standard actor critic architecture. \n\nExperiments are provided through a standard game like domain (maze, minecraft etc.). \n\nThe paper as written suffers from two problems. One, the idea of policy sketches is nice, but not sufficiently fleshed out to have any real impact. It would have been useful to see this spelled out in the context of abstract SMDP models to see what they bring to the table. What one gets here is some specialized invocation of this idea in the context of the specific approach proposed here. Second, the experiments are not thorough enough in terms of comparing with all the related work. For example, Ghavamzadeh et al. explored the use of MAXQ like abstractions in the context of mulitagent RL. It would be great to get a more detailed comparison to MAXQ based multiagent RL approaches, where the value function is explicitly decomposed. "
  },
  {
    "people": [
      "Ghavamzadeh"
    ],
    "review": "This paper studies the problem of abstract hierarchical multiagent RL with policy sketches, high level descriptions of abstract actions. The work is related to much previous work in hierarchical RL, and adds some new elements by using neural implementations of prior work on hierarchical learning and skill representations. \n\nSketches are sequences of high level symbolic labels drawn from some fixed vocabulary, which initially are devoid of any meaning. Eventually the sketches get mapped into real policies and enable policy transfer and temporal abstraction. Learning occurs through a variant of the standard actor critic architecture. \n\nExperiments are provided through a standard game like domain (maze, minecraft etc.). \n\nThe paper as written suffers from two problems. One, the idea of policy sketches is nice, but not sufficiently fleshed out to have any real impact. It would have been useful to see this spelled out in the context of abstract SMDP models to see what they bring to the table. What one gets here is some specialized invocation of this idea in the context of the specific approach proposed here. Second, the experiments are not thorough enough in terms of comparing with all the related work. For example, Ghavamzadeh et al. explored the use of MAXQ like abstractions in the context of mulitagent RL. It would be great to get a more detailed comparison to MAXQ based multiagent RL approaches, where the value function is explicitly decomposed. "
  },
  {
    "people": [
      "Fernandez",
      "Veloso",
      "Parisotto",
      "Mnih",
      "Pong"
    ],
    "review": "In this paper a well known soft mixture of experts model is adapted for, and applied to, a specific type of transfer learning problem in reinforcement learning (RL), namely transfer of action policies and value functions between similar tasks. Although not treated as such, the experimental setup is reminiscent of hierarchical RL works, an aspect which the paper does not consider at length, regrettably.\nOne possible implication of this work is that architecture and even learning algorithm choices could simply be stated in terms of the objective of the target task, rather than being hand-engineered by the experimenter. This is clearly an interesting direction of future work which the paper illuminates.\n\n\nPros:\nThe paper diligently explains how the network architecture fits in with various widely used reinforcement learning setups, which does facilitate continuation of this work.\nThe experiments are good proofs of concept, but do not go beyond that i.m.h.o. \nEven so, this work provides convincing clues that collections of deep networks, which were trained on not entirely different tasks, generalize better to related tasks when used together rather than through conventional transfer learning (e.g. fine-tuning).\n\nCons:\nAs the paper well recounts in the related work section, libraries of fixed policies have long been formally proposed for reuse while learning similar tasks. Indeed, it is well understood in hierarchical RL literature that it can be beneficial to reuse libraries of fixed (Fernandez & Veloso 2006) or jointly learned policies which may not apply to the entire state space, e.g. options (Pricop et. al). What is not well understood is how to build such libraries, and this paper does not convincingly shed light in that direction, as far as I can tell.\nThe transfer tasks have been picked to effectively illustrate the potential of the proposed architecture, but the paper does not tackle negative transfer or compositional reuse in well known challenging situations outlined in previous work (e.g. Parisotto et. al 2015, Rusu el. al 2015, 2016).\nSince the main contributions are of an empirical nature, I am curious how the results shown in figures 6 & 7 look plotted against wall-clock time, since relatively low data efficiency is not a limitation for achieving perfect play in Pong (see Mnih. et al, 2015). It would be more illuminating to consider tasks where final performance is plausibly limited by data availability. It would also be interesting if the presented results were achieved with reduced amounts of computation, or reduced representation sizes compared to learning from scratch, especially when one of the useful source tasks is an actual policy trained on the target task.\nFinally, it is perhaps underwhelming that it takes a quarter of the data required for learning Pong from scratch just to figure out that a perfect Pong policy is already in the expert library. Simply evaluating each expert for 10 episodes and using an  average-score-weighted majority vote to mix action choices would probably achieve the same final performance for a smaller fraction of the data."
  },
  {
    "people": [
      "Fernandez",
      "Veloso",
      "Parisotto",
      "Mnih"
    ],
    "review": "In this paper a well known soft mixture of experts model is adapted for, and applied to, a specific type of transfer learning problem in reinforcement learning (RL), namely transfer of action policies and value functions between similar tasks. Although not treated as such, the experimental setup is reminiscent of hierarchical RL works, an aspect which the paper does not consider at length, regrettably.\nOne possible implication of this work is that architecture and even learning algorithm choices could simply be stated in terms of the objective of the target task, rather than being hand-engineered by the experimenter. This is clearly an interesting direction of future work which the paper illuminates.\n\n\nPros:\nThe paper diligently explains how the network architecture fits in with various widely used reinforcement learning setups, which does facilitate continuation of this work.\nThe experiments are good proofs of concept, but do not go beyond that i.m.h.o. \nEven so, this work provides convincing clues that collections of deep networks, which were trained on not entirely different tasks, generalize better to related tasks when used together rather than through conventional transfer learning (e.g. fine-tuning).\n\nCons:\nAs the paper well recounts in the related work section, libraries of fixed policies have long been formally proposed for reuse while learning similar tasks. Indeed, it is well understood in hierarchical RL literature that it can be beneficial to reuse libraries of fixed (Fernandez & Veloso 2006) or jointly learned policies which may not apply to the entire state space, e.g. options (Pricop et. al). What is not well understood is how to build such libraries, and this paper does not convincingly shed light in that direction, as far as I can tell.\nThe transfer tasks have been picked to effectively illustrate the potential of the proposed architecture, but the paper does not tackle negative transfer or compositional reuse in well known challenging situations outlined in previous work (e.g. Parisotto et. al 2015, Rusu el. al 2015, 2016).\nSince the main contributions are of an empirical nature, I am curious how the results shown in figures 6 & 7 look plotted against wall-clock time, since relatively low data efficiency is not a limitation for achieving perfect play in Pong (see Mnih. et al, 2015). It would be more illuminating to consider tasks where final performance is plausibly limited by data availability. It would also be interesting if the presented results were achieved with reduced amounts of computation, or reduced representation sizes compared to learning from scratch, especially when one of the useful source tasks is an actual policy trained on the target task.\nFinally, it is perhaps underwhelming that it takes a quarter of the data required for learning Pong from scratch just to figure out that a perfect Pong policy is already in the expert library. Simply evaluating each expert for 10 episodes and using an  average-score-weighted majority vote to mix action choices would probably achieve the same final performance for a smaller fraction of the data. \n"
  },
  {
    "people": [
      "Fernandez",
      "Veloso",
      "Parisotto",
      "Mnih",
      "Pong"
    ],
    "review": "In this paper a well known soft mixture of experts model is adapted for, and applied to, a specific type of transfer learning problem in reinforcement learning (RL), namely transfer of action policies and value functions between similar tasks. Although not treated as such, the experimental setup is reminiscent of hierarchical RL works, an aspect which the paper does not consider at length, regrettably.\nOne possible implication of this work is that architecture and even learning algorithm choices could simply be stated in terms of the objective of the target task, rather than being hand-engineered by the experimenter. This is clearly an interesting direction of future work which the paper illuminates.\n\n\nPros:\nThe paper diligently explains how the network architecture fits in with various widely used reinforcement learning setups, which does facilitate continuation of this work.\nThe experiments are good proofs of concept, but do not go beyond that i.m.h.o. \nEven so, this work provides convincing clues that collections of deep networks, which were trained on not entirely different tasks, generalize better to related tasks when used together rather than through conventional transfer learning (e.g. fine-tuning).\n\nCons:\nAs the paper well recounts in the related work section, libraries of fixed policies have long been formally proposed for reuse while learning similar tasks. Indeed, it is well understood in hierarchical RL literature that it can be beneficial to reuse libraries of fixed (Fernandez & Veloso 2006) or jointly learned policies which may not apply to the entire state space, e.g. options (Pricop et. al). What is not well understood is how to build such libraries, and this paper does not convincingly shed light in that direction, as far as I can tell.\nThe transfer tasks have been picked to effectively illustrate the potential of the proposed architecture, but the paper does not tackle negative transfer or compositional reuse in well known challenging situations outlined in previous work (e.g. Parisotto et. al 2015, Rusu el. al 2015, 2016).\nSince the main contributions are of an empirical nature, I am curious how the results shown in figures 6 & 7 look plotted against wall-clock time, since relatively low data efficiency is not a limitation for achieving perfect play in Pong (see Mnih. et al, 2015). It would be more illuminating to consider tasks where final performance is plausibly limited by data availability. It would also be interesting if the presented results were achieved with reduced amounts of computation, or reduced representation sizes compared to learning from scratch, especially when one of the useful source tasks is an actual policy trained on the target task.\nFinally, it is perhaps underwhelming that it takes a quarter of the data required for learning Pong from scratch just to figure out that a perfect Pong policy is already in the expert library. Simply evaluating each expert for 10 episodes and using an  average-score-weighted majority vote to mix action choices would probably achieve the same final performance for a smaller fraction of the data."
  },
  {
    "people": [
      "Fernandez",
      "Veloso",
      "Parisotto",
      "Mnih"
    ],
    "review": "In this paper a well known soft mixture of experts model is adapted for, and applied to, a specific type of transfer learning problem in reinforcement learning (RL), namely transfer of action policies and value functions between similar tasks. Although not treated as such, the experimental setup is reminiscent of hierarchical RL works, an aspect which the paper does not consider at length, regrettably.\nOne possible implication of this work is that architecture and even learning algorithm choices could simply be stated in terms of the objective of the target task, rather than being hand-engineered by the experimenter. This is clearly an interesting direction of future work which the paper illuminates.\n\n\nPros:\nThe paper diligently explains how the network architecture fits in with various widely used reinforcement learning setups, which does facilitate continuation of this work.\nThe experiments are good proofs of concept, but do not go beyond that i.m.h.o. \nEven so, this work provides convincing clues that collections of deep networks, which were trained on not entirely different tasks, generalize better to related tasks when used together rather than through conventional transfer learning (e.g. fine-tuning).\n\nCons:\nAs the paper well recounts in the related work section, libraries of fixed policies have long been formally proposed for reuse while learning similar tasks. Indeed, it is well understood in hierarchical RL literature that it can be beneficial to reuse libraries of fixed (Fernandez & Veloso 2006) or jointly learned policies which may not apply to the entire state space, e.g. options (Pricop et. al). What is not well understood is how to build such libraries, and this paper does not convincingly shed light in that direction, as far as I can tell.\nThe transfer tasks have been picked to effectively illustrate the potential of the proposed architecture, but the paper does not tackle negative transfer or compositional reuse in well known challenging situations outlined in previous work (e.g. Parisotto et. al 2015, Rusu el. al 2015, 2016).\nSince the main contributions are of an empirical nature, I am curious how the results shown in figures 6 & 7 look plotted against wall-clock time, since relatively low data efficiency is not a limitation for achieving perfect play in Pong (see Mnih. et al, 2015). It would be more illuminating to consider tasks where final performance is plausibly limited by data availability. It would also be interesting if the presented results were achieved with reduced amounts of computation, or reduced representation sizes compared to learning from scratch, especially when one of the useful source tasks is an actual policy trained on the target task.\nFinally, it is perhaps underwhelming that it takes a quarter of the data required for learning Pong from scratch just to figure out that a perfect Pong policy is already in the expert library. Simply evaluating each expert for 10 episodes and using an  average-score-weighted majority vote to mix action choices would probably achieve the same final performance for a smaller fraction of the data. \n"
  },
  {
    "people": [
      "Han"
    ],
    "review": "Here is a summary of the reviews:\n \n Strengths\n Experiments are done on state-of-the-art networks, on a real speech recognition problem (R3, R1)\n Networks themselves are of a very large size (R3)\n Computational gains are substantial (R3, R4)\n Paper is clear (R1)\n \n Weaknesses\n Experiments are all done on a private dataset (R3)\n No comparison to other pruning approaches (e.g. Han et al.) (R3); AC notes that reviewers added new results which compare to an existing pruning method\n No comparison to distillation techniques (R1)\n Paper doesn't present much novelty in terms of ideas (R3)\n \n The AC encouraged feedback from the reviewers following author rebuttal and paper improvements. Reviewers stated that the improvements made to the paper made it publishable but was still closer to the threshold. R1 who had originally rated the paper 3: a \"clear reject\" updated the score to 6 (just above acceptance).\n \n Considering the reviews and discussions, the AC thinks that this paper is a poster accept. There are no serious flaws, the improvements made to the paper during the discussion paper have satisfied the reviewers, and this is an important topic with practical benefits; evaluated on a real large-scale problem."
  },
  {
    "people": [
      "Yu"
    ],
    "review": "I have uploaded a new revision which includes results using the pruning method proposed in \"Exploiting sparseness in deep neural networks for large vocabulary speech recognition\" by Yu et. al. The results show that gradual pruning used in our approach performs better (in terms of accuracy) than hard pruning. We thank the reviewers for the suggestion to compare our method with this pruning approach. "
  },
  {
    "people": [
      "Han",
      "Han",
      "Han",
      "Song",
      "Pool",
      "Jeff",
      "Tran",
      "John",
      "Dally",
      "William J."
    ],
    "review": "The paper proposes a method for pruning weights in neural networks during training to obtain sparse solutions. The approach is applied to an RNN-based system which is trained and evaluated on a speech recognition dataset. The results indicate that large savings in test-time computations can be obtained without affecting the task performance too much. In some cases the method can actually improve the evaluation performance.\n\nThe experiments are done using a state-of-the-art RNN system and the methodology of those experiments seems sound. I like that the effect of the pruning is investigated for networks of very large sizes. The computational gains are clearly substantial. It is a bit unfortunate that all experiments are done using a private dataset. Even with private training data, it would have been nice to see an evaluation on a known test set like the HUB5 for conversational speech. It would also have been nice to see a comparison with some other pruning approaches given the similarity of the proposed method to the work by Han et al. [2] to verify the relative merit of the proposed pruning scheme. While single-stage training looks more elegant at first sight, it may not save much time if more experiments are needed to find good hyperparameter settings for the threshold adaptation scheme. Finally, the dense baseline would have been more convincing if it involved some model compression tricks like training on the soft targets provided by a bigger network.\n\nOverall, the paper is easy to read. The table and figure captions could be a bit more detailed but they are still clear enough. The discussion of potential future speed-ups of sparse recurrent neural networks and memory savings is interesting but not specific to the proposed pruning algorithm. The paper doesn\u2019t motivate the details of the method very well. It\u2019s not clear to me why the threshold has to ramp up after a certain period time for example. If this is based on preliminary findings, the paper should mention that.\n\nSparse neural networks have been the subject of research for a long time and this includes recurrent neural networks (e.g., sparse recurrent weight matrices were standard for echo-state networks [1]). The proposed method is also very similar to the work by Han et al. [2], where a threshold is used to prune weights after training, followed by a retraining phase of the remaining weights. While I think that it is certainly more elegant to replace this three stage procedure with a single training phase, the proposed scheme still contains multiple regimes that resemble such a process by first training without pruning followed by pruning at two different rates and finally training without further pruning again. The main novelty of the work would be the application of such a scheme to RNNs, which are typically more tricky to train than feedforward nets.\n\nImproving scalability is an important driving force of the progress in neural network research. While I don\u2019t think the paper presents much novelty in ideas or scientific insight, it does show that weight pruning can be successfully applied to large practical RNN systems without sacrificing much in performance. The fact that this is possible with such a simple heuristic is a result worth sharing.\n\n\nPros:\nThe proposed method is successful at reducing the number of parameters in RNNs substantially without sacrificing too much in performance.\nThe experiments are done using a state-of-the-art system for a practical application.\n\nCons:\nThe proposed method is very similar to earlier work and barely novel.\nThere is no comparison with other pruning methods.\nThe data is private and this prevents others from replicating the results.\n\n\n[1] Jaeger, H. (2001). The \u201cecho state\u201d approach to analyzing and training recurrent neural networks-with an erratum note. Bonn, Germany: German National Research Center for Information Technology GMD Technical Report, 148, 34.\n\n\n[2] Han, Song, Pool, Jeff, Tran, John, and Dally, William J. Learning both weights and connections for efficient neural networks. In Advances in Neural Information Processing Systems, 2015."
  },
  {
    "people": [
      "Yu"
    ],
    "review": "Updated review: 18 Jan. 2017\n\nThanks to the authors for including a comparison to the previously published sparsity method of Yu et al., 2012.  The comparison is plausible, though it would be clearer if the authors were to state that the best comparison for the results in Table 4 is the \"RNN Sparse 1760\" result in Table 3.\n\nI have updated my review to reflect my evaluation of the revised paper, although I am also leaving the original review in place to preserve the history of the paper.\n\nThis paper has three main contributions.  (1) It proposes an approach to training sparse RNNs in which weights falling below a given threshold are masked to zero, and a schedule is used for the threshold in which pruning is only applied after a certain number of iterations have been performed and the threshold increases over the course of training.  (2) It provides experimental results on a Baidu-internal task with the Deep Speech 2 network architecture showing that applying the sparsification to a large model can lead to a final, trained model which has better performance and fewer non-zero parameters than a dense baseline model.  (3) It provides results from timing experiments with the cuSPARSE library showing that there is some potential for faster model evaluation with sufficiently sparse models, but that the current cuSPARSE implementation may not be optimal.\n\nPros\n+ The paper is mostly clear and easy to understand.\n+ The paper tackles an important, practical problem in deep learning:  how to successfully deploy models at the lowest possible computational and memory cost.\n\nCons\n- As a second baseline, this paper should compare to \"distillation\" approaches (e.g., "
  },
  {
    "people": [
      "Han",
      "Han",
      "Han",
      "Han"
    ],
    "review": "In introduction: \"... unlike previous approaches\nsuch as in Han et al. (2015). State of the art results in speech recognition generally require between\ndays and weeks of training time, so a further 3-4\u00d7 increase in training time is undesirable.\"\n\nBut, according to Han et al. (2015), \"Huffman coding doesn\u2019t require training and is implemented\noffline after all the fine-tuning is finished.\"\n\nBoth yours and Han et al. (2015) use a weight pruning technique. Intuitively, they should have similar training time for LSTM models.\nWhere does 3-4x extra training time comes from Han et al. (2015) but doesn't have in your approach?"
  },
  {
    "people": [
      "Han"
    ],
    "review": "Here is a summary of the reviews:\n \n Strengths\n Experiments are done on state-of-the-art networks, on a real speech recognition problem (R3, R1)\n Networks themselves are of a very large size (R3)\n Computational gains are substantial (R3, R4)\n Paper is clear (R1)\n \n Weaknesses\n Experiments are all done on a private dataset (R3)\n No comparison to other pruning approaches (e.g. Han et al.) (R3); AC notes that reviewers added new results which compare to an existing pruning method\n No comparison to distillation techniques (R1)\n Paper doesn't present much novelty in terms of ideas (R3)\n \n The AC encouraged feedback from the reviewers following author rebuttal and paper improvements. Reviewers stated that the improvements made to the paper made it publishable but was still closer to the threshold. R1 who had originally rated the paper 3: a \"clear reject\" updated the score to 6 (just above acceptance).\n \n Considering the reviews and discussions, the AC thinks that this paper is a poster accept. There are no serious flaws, the improvements made to the paper during the discussion paper have satisfied the reviewers, and this is an important topic with practical benefits; evaluated on a real large-scale problem."
  },
  {
    "people": [
      "Yu"
    ],
    "review": "I have uploaded a new revision which includes results using the pruning method proposed in \"Exploiting sparseness in deep neural networks for large vocabulary speech recognition\" by Yu et. al. The results show that gradual pruning used in our approach performs better (in terms of accuracy) than hard pruning. We thank the reviewers for the suggestion to compare our method with this pruning approach. "
  },
  {
    "people": [
      "Han",
      "Han",
      "Han",
      "Song",
      "Pool",
      "Jeff",
      "Tran",
      "John",
      "Dally",
      "William J."
    ],
    "review": "The paper proposes a method for pruning weights in neural networks during training to obtain sparse solutions. The approach is applied to an RNN-based system which is trained and evaluated on a speech recognition dataset. The results indicate that large savings in test-time computations can be obtained without affecting the task performance too much. In some cases the method can actually improve the evaluation performance.\n\nThe experiments are done using a state-of-the-art RNN system and the methodology of those experiments seems sound. I like that the effect of the pruning is investigated for networks of very large sizes. The computational gains are clearly substantial. It is a bit unfortunate that all experiments are done using a private dataset. Even with private training data, it would have been nice to see an evaluation on a known test set like the HUB5 for conversational speech. It would also have been nice to see a comparison with some other pruning approaches given the similarity of the proposed method to the work by Han et al. [2] to verify the relative merit of the proposed pruning scheme. While single-stage training looks more elegant at first sight, it may not save much time if more experiments are needed to find good hyperparameter settings for the threshold adaptation scheme. Finally, the dense baseline would have been more convincing if it involved some model compression tricks like training on the soft targets provided by a bigger network.\n\nOverall, the paper is easy to read. The table and figure captions could be a bit more detailed but they are still clear enough. The discussion of potential future speed-ups of sparse recurrent neural networks and memory savings is interesting but not specific to the proposed pruning algorithm. The paper doesn\u2019t motivate the details of the method very well. It\u2019s not clear to me why the threshold has to ramp up after a certain period time for example. If this is based on preliminary findings, the paper should mention that.\n\nSparse neural networks have been the subject of research for a long time and this includes recurrent neural networks (e.g., sparse recurrent weight matrices were standard for echo-state networks [1]). The proposed method is also very similar to the work by Han et al. [2], where a threshold is used to prune weights after training, followed by a retraining phase of the remaining weights. While I think that it is certainly more elegant to replace this three stage procedure with a single training phase, the proposed scheme still contains multiple regimes that resemble such a process by first training without pruning followed by pruning at two different rates and finally training without further pruning again. The main novelty of the work would be the application of such a scheme to RNNs, which are typically more tricky to train than feedforward nets.\n\nImproving scalability is an important driving force of the progress in neural network research. While I don\u2019t think the paper presents much novelty in ideas or scientific insight, it does show that weight pruning can be successfully applied to large practical RNN systems without sacrificing much in performance. The fact that this is possible with such a simple heuristic is a result worth sharing.\n\n\nPros:\nThe proposed method is successful at reducing the number of parameters in RNNs substantially without sacrificing too much in performance.\nThe experiments are done using a state-of-the-art system for a practical application.\n\nCons:\nThe proposed method is very similar to earlier work and barely novel.\nThere is no comparison with other pruning methods.\nThe data is private and this prevents others from replicating the results.\n\n\n[1] Jaeger, H. (2001). The \u201cecho state\u201d approach to analyzing and training recurrent neural networks-with an erratum note. Bonn, Germany: German National Research Center for Information Technology GMD Technical Report, 148, 34.\n\n\n[2] Han, Song, Pool, Jeff, Tran, John, and Dally, William J. Learning both weights and connections for efficient neural networks. In Advances in Neural Information Processing Systems, 2015."
  },
  {
    "people": [
      "Yu"
    ],
    "review": "Updated review: 18 Jan. 2017\n\nThanks to the authors for including a comparison to the previously published sparsity method of Yu et al., 2012.  The comparison is plausible, though it would be clearer if the authors were to state that the best comparison for the results in Table 4 is the \"RNN Sparse 1760\" result in Table 3.\n\nI have updated my review to reflect my evaluation of the revised paper, although I am also leaving the original review in place to preserve the history of the paper.\n\nThis paper has three main contributions.  (1) It proposes an approach to training sparse RNNs in which weights falling below a given threshold are masked to zero, and a schedule is used for the threshold in which pruning is only applied after a certain number of iterations have been performed and the threshold increases over the course of training.  (2) It provides experimental results on a Baidu-internal task with the Deep Speech 2 network architecture showing that applying the sparsification to a large model can lead to a final, trained model which has better performance and fewer non-zero parameters than a dense baseline model.  (3) It provides results from timing experiments with the cuSPARSE library showing that there is some potential for faster model evaluation with sufficiently sparse models, but that the current cuSPARSE implementation may not be optimal.\n\nPros\n+ The paper is mostly clear and easy to understand.\n+ The paper tackles an important, practical problem in deep learning:  how to successfully deploy models at the lowest possible computational and memory cost.\n\nCons\n- As a second baseline, this paper should compare to \"distillation\" approaches (e.g., "
  },
  {
    "people": [
      "Han",
      "Han",
      "Han",
      "Han"
    ],
    "review": "In introduction: \"... unlike previous approaches\nsuch as in Han et al. (2015). State of the art results in speech recognition generally require between\ndays and weeks of training time, so a further 3-4\u00d7 increase in training time is undesirable.\"\n\nBut, according to Han et al. (2015), \"Huffman coding doesn\u2019t require training and is implemented\noffline after all the fine-tuning is finished.\"\n\nBoth yours and Han et al. (2015) use a weight pruning technique. Intuitively, they should have similar training time for LSTM models.\nWhere does 3-4x extra training time comes from Han et al. (2015) but doesn't have in your approach?"
  },
  {
    "people": [
      "Narasimhan",
      "Karthik Narasimhan",
      "Tejas Kulkarni",
      "Regina Barzilay"
    ],
    "review": "Hi, \nVery nice work, with a neat jump in performance! Just had a couple questions:\n1. I understand the idea behind 'pixel control' but I don't quite get the motivation behind adding the 'feature control' task. Could you provide some intuition as to why maximizing the hidden unit activations would help learning? Also, from figure 5(c), it looks like this doesn't help much?\n2. Have you tried playing with the skewed sampling parameter (0.5)? We had experimented with a very similar scheme, prioritized sampling (Narasimhan et al., 2015), and found that tuning this parameter did give us some gains. \n\nReferences:\nKarthik Narasimhan, Tejas Kulkarni, Regina Barzilay. Language Understanding for Text-based Games Using Deep Reinforcement Learning. Proceedings of EMNLP, 2015 "
  },
  {
    "people": [
      "Narasimhan",
      "Karthik Narasimhan",
      "Tejas Kulkarni",
      "Regina Barzilay"
    ],
    "review": "Hi, \nVery nice work, with a neat jump in performance! Just had a couple questions:\n1. I understand the idea behind 'pixel control' but I don't quite get the motivation behind adding the 'feature control' task. Could you provide some intuition as to why maximizing the hidden unit activations would help learning? Also, from figure 5(c), it looks like this doesn't help much?\n2. Have you tried playing with the skewed sampling parameter (0.5)? We had experimented with a very similar scheme, prioritized sampling (Narasimhan et al., 2015), and found that tuning this parameter did give us some gains. \n\nReferences:\nKarthik Narasimhan, Tejas Kulkarni, Regina Barzilay. Language Understanding for Text-based Games Using Deep Reinforcement Learning. Proceedings of EMNLP, 2015 "
  },
  {
    "people": [
      "Kingma"
    ],
    "review": "The paper attempts to combine Variational Auto-Encoders with the Stick-Breaking process. The motivation is to tackle the component collapsing and have a representation with stochastic dimensionality. To demonstrate the merit of their approach, the authors test this model on MNIST and SVHN in an unsupervised and semi-supervised fashion.\nAfter reading the paper in more detail, I find that the claim that the dimensionality of the latent variable is stochastic does not seem quite correct: all latent variables are \"used\" (which actually enable backpropagation) but the latent variables are parametrized differently (into $\\pi$) and the decoding process is altered as to give the impression of sparsity. The way all these latent variables are used does not involve any marginalization but is very similar to the common soft-gating mechanism already used in LSTM or attentional model.\nWith respect to the Figure 5b showing the decoder input weights: component collapsing probably does not have the same effect as Gaussian prior. $\\pi$ is positive therefore having a very small average value might mean that its value is close to zero most of the time, not requiring any update on the weight. For the standard Gaussian prior, component collapsing means having a very noisy input with no signal involved, which forces the decoder to shut down this channel, i.e. have small incoming weights from this collapsed variable.\nAdding a histogram of the latent variables in addition to that might help decide if the associated weights are relatively large because they are actually used or if it's because the inputs are zero anyway.\nThe semi-supervised results are better than a weaker version of the model used in (Kingma et al., 2014), but as to have a fairer comparison, the results should be compared with the M1+M2 model in that paper, even if that requires also using two VAEs."
  },
  {
    "people": [
      "Kumaraswamy",
      "Blei"
    ],
    "review": "This paper presents an approach which modifies the variational auto-encoder (VAE) framework so as to use stochastic latent dimensionality. This is achieved by using an inherently infinite prior, the stick-breaking process. This is coupled with inference tailored to this model, specifically the Kumaraswamy distribution as an approximate variational posterior. The resulting model is named the SB-VAE which also has a semi-supervised extension, in similar vein to the original VAE paper.\n\nThere's a lot of interest in VAEs these days; many lines of work seek to achieve automatic \"black-box\" inference in these models. For example, the authors themselves mention parallel work by Blei's lab (also others) towards this direction. However, there's a lot of merit in investigating more bespoke solutions to new models, which is what the authors are doing in this paper. Indeed, a (useful) side-effect of providing efficient inference for the SB-VAE is drawing attention to the use of the Kumaraswamy distribution which hasn't been popular in ML.\n\nAlthough the paper is in general well structured, I found it confusing at parts. I think the major source of confusion comes from the fact that the model specification and model inference are discussed in a somehow mixed manner. The pre-review questions clarified most parts.\n\nI have two main concerns regarding the methodology and motivation of this paper. Firstly, conditioning the model directly on the stick-breaking weights seems a little odd. I initially thought that there was some mixture probabilistic model involved, but this is not the case. To be fair, the authors discuss about this issue (which became clearer to me after the pre-review questions), and explain that they're investigating the apparently challenging problem of using a base distribution G_0. The question is whether their relaxation is still useful. From the experiments it seems that the method is at least competitive, so the answer is yes. Hopefully an extension will come in the future, as the authors mention.\n\nThe second concern is about the motivation of this method. It seems that the paper fails to clearly explain in a convincing way why it is beneficial to reformulate the VAE as a SB-VAE. I understand that the non-parametric property induced by the prior might result in better capacity control, however I feel that this advantage (and potentially others which are still unclear to me) is not sufficiently explained and demonstrated. Perhaps some comparison with a dropout approach or a more thorough discussion related to dropout would make this clearer.\n\nOverall, I found this to be an interesting paper, it would be a good fit for ICLR.\n"
  },
  {
    "people": [
      "Kingma"
    ],
    "review": "The paper attempts to combine Variational Auto-Encoders with the Stick-Breaking process. The motivation is to tackle the component collapsing and have a representation with stochastic dimensionality. To demonstrate the merit of their approach, the authors test this model on MNIST and SVHN in an unsupervised and semi-supervised fashion.\nAfter reading the paper in more detail, I find that the claim that the dimensionality of the latent variable is stochastic does not seem quite correct: all latent variables are \"used\" (which actually enable backpropagation) but the latent variables are parametrized differently (into $\\pi$) and the decoding process is altered as to give the impression of sparsity. The way all these latent variables are used does not involve any marginalization but is very similar to the common soft-gating mechanism already used in LSTM or attentional model.\nWith respect to the Figure 5b showing the decoder input weights: component collapsing probably does not have the same effect as Gaussian prior. $\\pi$ is positive therefore having a very small average value might mean that its value is close to zero most of the time, not requiring any update on the weight. For the standard Gaussian prior, component collapsing means having a very noisy input with no signal involved, which forces the decoder to shut down this channel, i.e. have small incoming weights from this collapsed variable.\nAdding a histogram of the latent variables in addition to that might help decide if the associated weights are relatively large because they are actually used or if it's because the inputs are zero anyway.\nThe semi-supervised results are better than a weaker version of the model used in (Kingma et al., 2014), but as to have a fairer comparison, the results should be compared with the M1+M2 model in that paper, even if that requires also using two VAEs."
  },
  {
    "people": [
      "Kingma"
    ],
    "review": "The paper attempts to combine Variational Auto-Encoders with the Stick-Breaking process. The motivation is to tackle the component collapsing and have a representation with stochastic dimensionality. To demonstrate the merit of their approach, the authors test this model on MNIST and SVHN in an unsupervised and semi-supervised fashion.\nAfter reading the paper in more detail, I find that the claim that the dimensionality of the latent variable is stochastic does not seem quite correct: all latent variables are \"used\" (which actually enable backpropagation) but the latent variables are parametrized differently (into $\\pi$) and the decoding process is altered as to give the impression of sparsity. The way all these latent variables are used does not involve any marginalization but is very similar to the common soft-gating mechanism already used in LSTM or attentional model.\nWith respect to the Figure 5b showing the decoder input weights: component collapsing probably does not have the same effect as Gaussian prior. $\\pi$ is positive therefore having a very small average value might mean that its value is close to zero most of the time, not requiring any update on the weight. For the standard Gaussian prior, component collapsing means having a very noisy input with no signal involved, which forces the decoder to shut down this channel, i.e. have small incoming weights from this collapsed variable.\nAdding a histogram of the latent variables in addition to that might help decide if the associated weights are relatively large because they are actually used or if it's because the inputs are zero anyway.\nThe semi-supervised results are better than a weaker version of the model used in (Kingma et al., 2014), but as to have a fairer comparison, the results should be compared with the M1+M2 model in that paper, even if that requires also using two VAEs."
  },
  {
    "people": [
      "Kumaraswamy",
      "Blei"
    ],
    "review": "This paper presents an approach which modifies the variational auto-encoder (VAE) framework so as to use stochastic latent dimensionality. This is achieved by using an inherently infinite prior, the stick-breaking process. This is coupled with inference tailored to this model, specifically the Kumaraswamy distribution as an approximate variational posterior. The resulting model is named the SB-VAE which also has a semi-supervised extension, in similar vein to the original VAE paper.\n\nThere's a lot of interest in VAEs these days; many lines of work seek to achieve automatic \"black-box\" inference in these models. For example, the authors themselves mention parallel work by Blei's lab (also others) towards this direction. However, there's a lot of merit in investigating more bespoke solutions to new models, which is what the authors are doing in this paper. Indeed, a (useful) side-effect of providing efficient inference for the SB-VAE is drawing attention to the use of the Kumaraswamy distribution which hasn't been popular in ML.\n\nAlthough the paper is in general well structured, I found it confusing at parts. I think the major source of confusion comes from the fact that the model specification and model inference are discussed in a somehow mixed manner. The pre-review questions clarified most parts.\n\nI have two main concerns regarding the methodology and motivation of this paper. Firstly, conditioning the model directly on the stick-breaking weights seems a little odd. I initially thought that there was some mixture probabilistic model involved, but this is not the case. To be fair, the authors discuss about this issue (which became clearer to me after the pre-review questions), and explain that they're investigating the apparently challenging problem of using a base distribution G_0. The question is whether their relaxation is still useful. From the experiments it seems that the method is at least competitive, so the answer is yes. Hopefully an extension will come in the future, as the authors mention.\n\nThe second concern is about the motivation of this method. It seems that the paper fails to clearly explain in a convincing way why it is beneficial to reformulate the VAE as a SB-VAE. I understand that the non-parametric property induced by the prior might result in better capacity control, however I feel that this advantage (and potentially others which are still unclear to me) is not sufficiently explained and demonstrated. Perhaps some comparison with a dropout approach or a more thorough discussion related to dropout would make this clearer.\n\nOverall, I found this to be an interesting paper, it would be a good fit for ICLR.\n"
  },
  {
    "people": [
      "Kingma"
    ],
    "review": "The paper attempts to combine Variational Auto-Encoders with the Stick-Breaking process. The motivation is to tackle the component collapsing and have a representation with stochastic dimensionality. To demonstrate the merit of their approach, the authors test this model on MNIST and SVHN in an unsupervised and semi-supervised fashion.\nAfter reading the paper in more detail, I find that the claim that the dimensionality of the latent variable is stochastic does not seem quite correct: all latent variables are \"used\" (which actually enable backpropagation) but the latent variables are parametrized differently (into $\\pi$) and the decoding process is altered as to give the impression of sparsity. The way all these latent variables are used does not involve any marginalization but is very similar to the common soft-gating mechanism already used in LSTM or attentional model.\nWith respect to the Figure 5b showing the decoder input weights: component collapsing probably does not have the same effect as Gaussian prior. $\\pi$ is positive therefore having a very small average value might mean that its value is close to zero most of the time, not requiring any update on the weight. For the standard Gaussian prior, component collapsing means having a very noisy input with no signal involved, which forces the decoder to shut down this channel, i.e. have small incoming weights from this collapsed variable.\nAdding a histogram of the latent variables in addition to that might help decide if the associated weights are relatively large because they are actually used or if it's because the inputs are zero anyway.\nThe semi-supervised results are better than a weaker version of the model used in (Kingma et al., 2014), but as to have a fairer comparison, the results should be compared with the M1+M2 model in that paper, even if that requires also using two VAEs."
  },
  {
    "people": [
      "Xu"
    ],
    "review": "The authors apply the image captioning architecture of Xu et al. 2015 to video captioning. The model is extended to have attention over multiple layers of the ConvNet instead of just a single layer. Experiments on YouTube2Text, M-VAD and MSR-VTT show that this works better than only using one of the layers at a time.\n\nI think this is solid work on the level of a well-executed course project or a workshop paper. The model makes sense, it is adequately described, and the experiments show that attending over multiple layers works better than attending over any one layer in isolation. Unfortunately, I don't think there is enough to get excited about here from a technical perspective and it's not clear what value the paper brings to the community. Other aspects of the paper, such as including the hard attention component, don't seem to add to the paper but take up space. \n\nIf the authors want to contribute a detailed, focused exploration of multi-level features this could become a more valuable paper, but in that case I would expect a much more thorough exploration of the choices and tradeoffs of different schemes without too many spurious aspects such as video features, hard attention, etc."
  },
  {
    "people": [
      "Yao"
    ],
    "review": "We thank all the reviewers for the critical comments.\n\nAll the reviewers agree that our experimental results convincingly support our hypothesis. The disagreement is that whether our paper is suitable for ICLR and whether our contribution is important to know by the representation learning community.\n\nWe made the following changes in the pdf file revision addressing the reviewers\u2019 concerns:\n\n1. We updated the abstract, emphasizing that the goal of our paper is not only about presenting another architecture for video captioning, but also about new video representations that are suitable for the video captioning task. Attention mechanisms are just technical means to achieve this goal.\n\n2. We updated the introduction, adding three technical challenges to overcome to use adaptive spatiotemporal feature representations with dynamic feature abstraction for the captioning task. We also explained why na\u00efve approaches such as MLP/max(average)-pooling did not meet our requirements.\n\n3. We updated the related work including the hypercolumn representation.\n\n4. We put additional experimental results comparing our approach ASTAR to hypercolumns, MLP, and max/average-pooling in Table 1 on Page 8. Our results clearly demonstrate the significance of dynamically selecting a specific level. The hypercolumn representation without level selection has much worse performance than our method.\n\n5. We updated Figure 1 and moved Figure 2 below it to emphasize our contributions following the review comments.\n\n6. We added the reference of Yao et al., ICCV 2015.\n\nIn summary, we believe that our contributions are clear and important to know by the representation learning community. This line of thinking might influence other researchers to perform additional research on classification and other tasks. It also inspires us to design new deep architectures to efficiently learn and effectively utilize different levels of feature representations in a dynamic fashion."
  },
  {
    "people": [
      "Xu"
    ],
    "review": "The authors apply the image captioning architecture of Xu et al. 2015 to video captioning. The model is extended to have attention over multiple layers of the ConvNet instead of just a single layer. Experiments on YouTube2Text, M-VAD and MSR-VTT show that this works better than only using one of the layers at a time.\n\nI think this is solid work on the level of a well-executed course project or a workshop paper. The model makes sense, it is adequately described, and the experiments show that attending over multiple layers works better than attending over any one layer in isolation. Unfortunately, I don't think there is enough to get excited about here from a technical perspective and it's not clear what value the paper brings to the community. Other aspects of the paper, such as including the hard attention component, don't seem to add to the paper but take up space. \n\nIf the authors want to contribute a detailed, focused exploration of multi-level features this could become a more valuable paper, but in that case I would expect a much more thorough exploration of the choices and tradeoffs of different schemes without too many spurious aspects such as video features, hard attention, etc.\n"
  },
  {
    "people": [
      "Xu"
    ],
    "review": "The authors apply the image captioning architecture of Xu et al. 2015 to video captioning. The model is extended to have attention over multiple layers of the ConvNet instead of just a single layer. Experiments on YouTube2Text, M-VAD and MSR-VTT show that this works better than only using one of the layers at a time.\n\nI think this is solid work on the level of a well-executed course project or a workshop paper. The model makes sense, it is adequately described, and the experiments show that attending over multiple layers works better than attending over any one layer in isolation. Unfortunately, I don't think there is enough to get excited about here from a technical perspective and it's not clear what value the paper brings to the community. Other aspects of the paper, such as including the hard attention component, don't seem to add to the paper but take up space. \n\nIf the authors want to contribute a detailed, focused exploration of multi-level features this could become a more valuable paper, but in that case I would expect a much more thorough exploration of the choices and tradeoffs of different schemes without too many spurious aspects such as video features, hard attention, etc."
  },
  {
    "people": [
      "Yao"
    ],
    "review": "We thank all the reviewers for the critical comments.\n\nAll the reviewers agree that our experimental results convincingly support our hypothesis. The disagreement is that whether our paper is suitable for ICLR and whether our contribution is important to know by the representation learning community.\n\nWe made the following changes in the pdf file revision addressing the reviewers\u2019 concerns:\n\n1. We updated the abstract, emphasizing that the goal of our paper is not only about presenting another architecture for video captioning, but also about new video representations that are suitable for the video captioning task. Attention mechanisms are just technical means to achieve this goal.\n\n2. We updated the introduction, adding three technical challenges to overcome to use adaptive spatiotemporal feature representations with dynamic feature abstraction for the captioning task. We also explained why na\u00efve approaches such as MLP/max(average)-pooling did not meet our requirements.\n\n3. We updated the related work including the hypercolumn representation.\n\n4. We put additional experimental results comparing our approach ASTAR to hypercolumns, MLP, and max/average-pooling in Table 1 on Page 8. Our results clearly demonstrate the significance of dynamically selecting a specific level. The hypercolumn representation without level selection has much worse performance than our method.\n\n5. We updated Figure 1 and moved Figure 2 below it to emphasize our contributions following the review comments.\n\n6. We added the reference of Yao et al., ICCV 2015.\n\nIn summary, we believe that our contributions are clear and important to know by the representation learning community. This line of thinking might influence other researchers to perform additional research on classification and other tasks. It also inspires us to design new deep architectures to efficiently learn and effectively utilize different levels of feature representations in a dynamic fashion."
  },
  {
    "people": [
      "Xu"
    ],
    "review": "The authors apply the image captioning architecture of Xu et al. 2015 to video captioning. The model is extended to have attention over multiple layers of the ConvNet instead of just a single layer. Experiments on YouTube2Text, M-VAD and MSR-VTT show that this works better than only using one of the layers at a time.\n\nI think this is solid work on the level of a well-executed course project or a workshop paper. The model makes sense, it is adequately described, and the experiments show that attending over multiple layers works better than attending over any one layer in isolation. Unfortunately, I don't think there is enough to get excited about here from a technical perspective and it's not clear what value the paper brings to the community. Other aspects of the paper, such as including the hard attention component, don't seem to add to the paper but take up space. \n\nIf the authors want to contribute a detailed, focused exploration of multi-level features this could become a more valuable paper, but in that case I would expect a much more thorough exploration of the choices and tradeoffs of different schemes without too many spurious aspects such as video features, hard attention, etc.\n"
  },
  {
    "people": [
      "Robnik-\u0160ikonja & Kononenko",
      "Zeiler",
      "Zeiler"
    ],
    "review": "The paper presents a theoretically well motivated for visualizing what parts of the input feature map are responsible for the output decision. The key insight is that features that maximally change the output and are simultaneously more unpredictable from other features are the most important ones. Most previous work has focused on finding features that maximally change the output without accounting for their predictability from other features. Authors build upon ideas presented in the work of Robnik-\u0160ikonja & Kononenko (2008).\n\nThe results indicate that the proposed visualization mechanism based on modeling conditional distribution identifies more salient regions as compared to a mechanism based on modeling marginal distribution. I like that authors have presented visualization results for a single image across multiple networks and multiple classes. There results show that the proposed method indeed picks up on class-discriminative features. Authors have provided a link to visualizations for a random sample of images in a comment \u2013 I encourage the authors to include this in the appendix of the paper. \n\nMy one concern with the paper is \u2013 Zeiler et al., proposed a visualization method by greying small square regions in the image. This is similar to computing the visualization using the marginal distribution. Authors compute the marginal visualization using 10 samples, however in the limit of infinite samples the image region would be gray. The conditional distribution is computed using a normal distribution that provides some regularization and therefore estimating the conditional and marginal distributions using 10 samples each is not justified. \nI would like to see the comparison when grey image patches (akin to Zeiler et al.) are used for visualization against the approach based on the conditional distribution. \n\n"
  },
  {
    "people": [
      "Robnik-Sikonja",
      "Konononko"
    ],
    "review": "The authors propose a way to visualize which areas of an image provide mostly influence a certain DNN response mostly. They apply some very elegant and convincing improvements to the basic method by Robnik-Sikonja and Konononko from 2008 to DNNs, thus improving it's analysis and making it usable for images and DNNs.\n\nThe authors provide a very thorough analysis of their methods and show very convincing examples (which they however handpicked. It would be very nice to have maybe at least one figure showing the analysis on e.g. 24 random picks from ImageNet).\nOne thing I would like to see is how their method compares to some other methods they mention in the introduction (like gradient-based ones or deconvolution based ones). \n\nThey paper is very clearly written, all necessary details are given and the paper is very nice to read.\n\nAlltogether: The problem of understanding how DNNs function and how they draw their conclusions is discussed a lot. The author's method provides a clear contribution that can lead to further progress in this field (E.g. I like figure 8 showing how AlexNet, GoogLeNet and VGG differ in where they collect evidence from). I can think of several potential applications of the method and therefore consider it of high significance.\n\nUpdate: The authors did a great job of adopting all of my suggestions. Therefore I improve the rating from 8 to 9."
  },
  {
    "people": [
      "Robnik-\u0160ikonja & Kononenko",
      "Zeiler",
      "Zeiler"
    ],
    "review": "The paper presents a theoretically well motivated for visualizing what parts of the input feature map are responsible for the output decision. The key insight is that features that maximally change the output and are simultaneously more unpredictable from other features are the most important ones. Most previous work has focused on finding features that maximally change the output without accounting for their predictability from other features. Authors build upon ideas presented in the work of Robnik-\u0160ikonja & Kononenko (2008).\n\nThe results indicate that the proposed visualization mechanism based on modeling conditional distribution identifies more salient regions as compared to a mechanism based on modeling marginal distribution. I like that authors have presented visualization results for a single image across multiple networks and multiple classes. There results show that the proposed method indeed picks up on class-discriminative features. Authors have provided a link to visualizations for a random sample of images in a comment \u2013 I encourage the authors to include this in the appendix of the paper. \n\nMy one concern with the paper is \u2013 Zeiler et al., proposed a visualization method by greying small square regions in the image. This is similar to computing the visualization using the marginal distribution. Authors compute the marginal visualization using 10 samples, however in the limit of infinite samples the image region would be gray. The conditional distribution is computed using a normal distribution that provides some regularization and therefore estimating the conditional and marginal distributions using 10 samples each is not justified. \nI would like to see the comparison when grey image patches (akin to Zeiler et al.) are used for visualization against the approach based on the conditional distribution. \n\n"
  },
  {
    "people": [
      "Robnik-Sikonja",
      "Konononko"
    ],
    "review": "The authors propose a way to visualize which areas of an image provide mostly influence a certain DNN response mostly. They apply some very elegant and convincing improvements to the basic method by Robnik-Sikonja and Konononko from 2008 to DNNs, thus improving it's analysis and making it usable for images and DNNs.\n\nThe authors provide a very thorough analysis of their methods and show very convincing examples (which they however handpicked. It would be very nice to have maybe at least one figure showing the analysis on e.g. 24 random picks from ImageNet).\nOne thing I would like to see is how their method compares to some other methods they mention in the introduction (like gradient-based ones or deconvolution based ones). \n\nThey paper is very clearly written, all necessary details are given and the paper is very nice to read.\n\nAlltogether: The problem of understanding how DNNs function and how they draw their conclusions is discussed a lot. The author's method provides a clear contribution that can lead to further progress in this field (E.g. I like figure 8 showing how AlexNet, GoogLeNet and VGG differ in where they collect evidence from). I can think of several potential applications of the method and therefore consider it of high significance.\n\nUpdate: The authors did a great job of adopting all of my suggestions. Therefore I improve the rating from 8 to 9."
  },
  {
    "people": [
      "Hinton"
    ],
    "review": "This paper essentially presents a new inductive bias in the architecture of (convolutional) neural networks (CNN). The mathematical motivations/derivations of the proposed architecture are detailed and rigorous. The proposed architecture promises to produce equivariant representations with steerable features using fewer parameters than traditional CNNs, which is particularly useful in small data regimes. Interesting and novel connections are presented between steerable filters and so called \u201csteerable fibers\u201d. The architecture is strongly inspired by the author\u2019s previous work, as well as that of \u201ccapsules\u201d (Hinton, 2011). The proposed architecture is compared on CIFAR10 against state-of-the-art inspired architectures (ResNets), and is shown to be superior particularly in the small data regime. The lack of empirical comparison on large scale dataset, such as ImageNet or COCO makes this largely a theoretical contribution. I would have also liked to see more empirical evaluation of the equivariance properties. It is not intuitively clear exactly why this architecture performs better on CIFAR10 as it is not clear that capturing equivariances helps to classify different instances of object categories. Wouldn\u2019t action-recognition in videos, for example, not be a better illustrative dataset?"
  },
  {
    "people": [
      "Simoncelli",
      "Freeman",
      "Adelson"
    ],
    "review": "This paper presents a theoretical treatment of transformation groups applied to convnets, and presents some empirical results showing more efficient usage of network parameters.\n\nThe basic idea of steerability makes huge sense and seems like a very important idea to develop.  It is also a very old idea in image processing and goes back to Simoncelli, Freeman, Adelson, as well as Perona/Greenspan and others in the early 1990s.  This paper approaches it through a formal treatment of group theory.  But at the end of the day the idea seems pretty simple - the feature representation of a transformed image should be equivalent to a transformed feature representation of the original image.  Given that the authors are limiting their analysis to discrete groups - for example rotations of 0, 90, 180, and 270 deg. - the formalities brought in from the group theoretic analysis seem a bit overkill.  I'm not sure what this buys us in the end.  it seems the real challenge lies in implementing continuous transformations, so if the theory could guide us in that direction it would be immensely helpful.\n\nAlso the description of the experiments is fairly opaque.  I would have a hard time replicating what exactly the authors did here in terms of implementing capsules or transformation groups.  \n\n"
  },
  {
    "people": [
      "Reisert",
      "Balduzzi",
      "Vedaldi",
      "Reisert"
    ],
    "review": "The authors propose a parameterization of CNNs that guarantees equivariance wrt a large family of geometric transformations.\n\nThe mathematical analysis is rigorous and the material is very interesting and novel. The paper overall reads well; there is a real effort to explain the math accessibly, though some small improvements could be made.\n\nThe theory is general enough to include continuous transformations, although the experiments are restricted to discrete ones. While this could be seen as a negative point, it is justified by the experiments, which show that this set of transformations is powerful enough to yield very good results on CIFAR.\n\nAnother form of intertwiner has been studied recently by Lenc & Vedaldi [1]; they have studied equivariance empirically in CNNs, which offers an orthogonal view.\n\nIn addition to the recent references on scale/rotation deep networks suggested below, geometric equivariance has been studied extensively in the 2000's; mentioning at least one work would be appropriate. The one that probably comes closest to the proposed method is the work by Reisert [2], who studied steerable filters for invariance and equivariance, using Lie group theory. The difference, of course, is that the focus at the time was on kernel machines rather than CNNs, but many of the tools and theorems are relatable.\n\n\nSome of the notation could be simplified, to make the formulas easier to grasp on a first read:\n\nWorking over a lattice Z^d is unnecessarily abstract -- since the inputs are always images, Z^2 would make much of the later math easier to parse. Generalization is straightforward, so I don't think the results lose anything by it; and the authors go back to 2D latices later anyway.\n\nIt could be more natural to do away with the layer index l which appears throughout the paper, and have notation for current/next layer instead (e.g. pi and pi'; K and D instead of K_{l+1} and K_l).\n\nIn any case I leave it up to the authors to decide whether to include these suggestions on notation, but I urge them to consider them (or other ways to unburden notation).\n\n\nA few minor issues: Some statements would be better supported with an accompanying reference (e.g. \"Explicit formulas exist\" on page 5, the introduction of intertwiners on page 3). Finally, there is a tiny mistake in the Balduzzi & Ghifary reference (some extra information was included as an author name).\n\n[1] Lenc & Vedaldi, \"Understanding image representations by measuring their equivariance and equivalence\", 2015\n[2] Reisert, \"Group integration techniques in pattern analysis: a kernel view\", 2008\n\n"
  },
  {
    "people": [
      "Hinton"
    ],
    "review": "This paper essentially presents a new inductive bias in the architecture of (convolutional) neural networks (CNN). The mathematical motivations/derivations of the proposed architecture are detailed and rigorous. The proposed architecture promises to produce equivariant representations with steerable features using fewer parameters than traditional CNNs, which is particularly useful in small data regimes. Interesting and novel connections are presented between steerable filters and so called \u201csteerable fibers\u201d. The architecture is strongly inspired by the author\u2019s previous work, as well as that of \u201ccapsules\u201d (Hinton, 2011). The proposed architecture is compared on CIFAR10 against state-of-the-art inspired architectures (ResNets), and is shown to be superior particularly in the small data regime. The lack of empirical comparison on large scale dataset, such as ImageNet or COCO makes this largely a theoretical contribution. I would have also liked to see more empirical evaluation of the equivariance properties. It is not intuitively clear exactly why this architecture performs better on CIFAR10 as it is not clear that capturing equivariances helps to classify different instances of object categories. Wouldn\u2019t action-recognition in videos, for example, not be a better illustrative dataset? \n"
  },
  {
    "people": [
      "Taco"
    ],
    "review": "Hi, Taco,\n  Great paper, I have a few comments/questions:\n- is there code available for this work?\n- how difficult (time-consuming) to train steerable CNNs on dataset such as ImageNet?\n- How does steerable CNN compare against common CNNs such as ResNet or Inception in well-benchmarked tasks such as image classification? I feel the experiment section can be strengthened by add such results.\n- Is there any reason besides mathematical simplicity to choose p4m? what about other group in the wallpaper group set?\n- What are possible huddles to extend it to continuous group? What if we want to just consider finite group such as D_n instead of continuous group such as SO(2)?\n\nThanks.\n\nPS: In the sentence right after equation (6) there is an equation for \\pi_{l+1}, there is g on the LHS, I assume g should also appear on the RHS? "
  },
  {
    "people": [
      "Shuang"
    ],
    "review": "- In the sentence right about equation (1), isn't it better to write \"... a linear operator \\pi_0(g): F_0 \\rightarrow F_0.. \" since \\pi_0 is clearly defined for the case l = 0 as indicated by its subscript.\n- right before equation (3), isn't it better to write \" ... we need the filter bank \\Psi: F_l \\rightarrow R^{K_{l+1}} ...\" since F here is for layer l specifically.\n\nI am not sure I am reading the most recent revision, so forgive me if these things have already been fixed.\n\nGreat paper, by the way.\n\nShuang"
  },
  {
    "people": [
      "Kenta Oono"
    ],
    "review": "In response to the following comment from Kenta Oono, we have uploaded a new version of the paper.\n\n\"I saw your paper submitted to ICLR 2017, Steerable CNNs.\n\nIt is wonderful as it is theoretically well founded with the representation theory and also takes engineering easiness into consideration. Further, it achieves state of the art result in CIFAR datasets. So I would like to understand the paper in detail.\n\nAs I read the paper, I encountered the problem that I could not derive some of equations (specifically, (5) in the paper). [...]\"\n\nThe problem he identified was due to a small mistake in eq. 4 (a missing inverse). In the new version we have fixed this and added an appendix where eq. 5 (the induced representation) is derived."
  },
  {
    "people": [
      "Hinton"
    ],
    "review": "This paper essentially presents a new inductive bias in the architecture of (convolutional) neural networks (CNN). The mathematical motivations/derivations of the proposed architecture are detailed and rigorous. The proposed architecture promises to produce equivariant representations with steerable features using fewer parameters than traditional CNNs, which is particularly useful in small data regimes. Interesting and novel connections are presented between steerable filters and so called \u201csteerable fibers\u201d. The architecture is strongly inspired by the author\u2019s previous work, as well as that of \u201ccapsules\u201d (Hinton, 2011). The proposed architecture is compared on CIFAR10 against state-of-the-art inspired architectures (ResNets), and is shown to be superior particularly in the small data regime. The lack of empirical comparison on large scale dataset, such as ImageNet or COCO makes this largely a theoretical contribution. I would have also liked to see more empirical evaluation of the equivariance properties. It is not intuitively clear exactly why this architecture performs better on CIFAR10 as it is not clear that capturing equivariances helps to classify different instances of object categories. Wouldn\u2019t action-recognition in videos, for example, not be a better illustrative dataset?"
  },
  {
    "people": [
      "Simoncelli",
      "Freeman",
      "Adelson"
    ],
    "review": "This paper presents a theoretical treatment of transformation groups applied to convnets, and presents some empirical results showing more efficient usage of network parameters.\n\nThe basic idea of steerability makes huge sense and seems like a very important idea to develop.  It is also a very old idea in image processing and goes back to Simoncelli, Freeman, Adelson, as well as Perona/Greenspan and others in the early 1990s.  This paper approaches it through a formal treatment of group theory.  But at the end of the day the idea seems pretty simple - the feature representation of a transformed image should be equivalent to a transformed feature representation of the original image.  Given that the authors are limiting their analysis to discrete groups - for example rotations of 0, 90, 180, and 270 deg. - the formalities brought in from the group theoretic analysis seem a bit overkill.  I'm not sure what this buys us in the end.  it seems the real challenge lies in implementing continuous transformations, so if the theory could guide us in that direction it would be immensely helpful.\n\nAlso the description of the experiments is fairly opaque.  I would have a hard time replicating what exactly the authors did here in terms of implementing capsules or transformation groups.  \n\n"
  },
  {
    "people": [
      "Reisert",
      "Balduzzi",
      "Vedaldi",
      "Reisert"
    ],
    "review": "The authors propose a parameterization of CNNs that guarantees equivariance wrt a large family of geometric transformations.\n\nThe mathematical analysis is rigorous and the material is very interesting and novel. The paper overall reads well; there is a real effort to explain the math accessibly, though some small improvements could be made.\n\nThe theory is general enough to include continuous transformations, although the experiments are restricted to discrete ones. While this could be seen as a negative point, it is justified by the experiments, which show that this set of transformations is powerful enough to yield very good results on CIFAR.\n\nAnother form of intertwiner has been studied recently by Lenc & Vedaldi [1]; they have studied equivariance empirically in CNNs, which offers an orthogonal view.\n\nIn addition to the recent references on scale/rotation deep networks suggested below, geometric equivariance has been studied extensively in the 2000's; mentioning at least one work would be appropriate. The one that probably comes closest to the proposed method is the work by Reisert [2], who studied steerable filters for invariance and equivariance, using Lie group theory. The difference, of course, is that the focus at the time was on kernel machines rather than CNNs, but many of the tools and theorems are relatable.\n\n\nSome of the notation could be simplified, to make the formulas easier to grasp on a first read:\n\nWorking over a lattice Z^d is unnecessarily abstract -- since the inputs are always images, Z^2 would make much of the later math easier to parse. Generalization is straightforward, so I don't think the results lose anything by it; and the authors go back to 2D latices later anyway.\n\nIt could be more natural to do away with the layer index l which appears throughout the paper, and have notation for current/next layer instead (e.g. pi and pi'; K and D instead of K_{l+1} and K_l).\n\nIn any case I leave it up to the authors to decide whether to include these suggestions on notation, but I urge them to consider them (or other ways to unburden notation).\n\n\nA few minor issues: Some statements would be better supported with an accompanying reference (e.g. \"Explicit formulas exist\" on page 5, the introduction of intertwiners on page 3). Finally, there is a tiny mistake in the Balduzzi & Ghifary reference (some extra information was included as an author name).\n\n[1] Lenc & Vedaldi, \"Understanding image representations by measuring their equivariance and equivalence\", 2015\n[2] Reisert, \"Group integration techniques in pattern analysis: a kernel view\", 2008\n\n"
  },
  {
    "people": [
      "Hinton"
    ],
    "review": "This paper essentially presents a new inductive bias in the architecture of (convolutional) neural networks (CNN). The mathematical motivations/derivations of the proposed architecture are detailed and rigorous. The proposed architecture promises to produce equivariant representations with steerable features using fewer parameters than traditional CNNs, which is particularly useful in small data regimes. Interesting and novel connections are presented between steerable filters and so called \u201csteerable fibers\u201d. The architecture is strongly inspired by the author\u2019s previous work, as well as that of \u201ccapsules\u201d (Hinton, 2011). The proposed architecture is compared on CIFAR10 against state-of-the-art inspired architectures (ResNets), and is shown to be superior particularly in the small data regime. The lack of empirical comparison on large scale dataset, such as ImageNet or COCO makes this largely a theoretical contribution. I would have also liked to see more empirical evaluation of the equivariance properties. It is not intuitively clear exactly why this architecture performs better on CIFAR10 as it is not clear that capturing equivariances helps to classify different instances of object categories. Wouldn\u2019t action-recognition in videos, for example, not be a better illustrative dataset? \n"
  },
  {
    "people": [
      "Taco"
    ],
    "review": "Hi, Taco,\n  Great paper, I have a few comments/questions:\n- is there code available for this work?\n- how difficult (time-consuming) to train steerable CNNs on dataset such as ImageNet?\n- How does steerable CNN compare against common CNNs such as ResNet or Inception in well-benchmarked tasks such as image classification? I feel the experiment section can be strengthened by add such results.\n- Is there any reason besides mathematical simplicity to choose p4m? what about other group in the wallpaper group set?\n- What are possible huddles to extend it to continuous group? What if we want to just consider finite group such as D_n instead of continuous group such as SO(2)?\n\nThanks.\n\nPS: In the sentence right after equation (6) there is an equation for \\pi_{l+1}, there is g on the LHS, I assume g should also appear on the RHS? "
  },
  {
    "people": [
      "Shuang"
    ],
    "review": "- In the sentence right about equation (1), isn't it better to write \"... a linear operator \\pi_0(g): F_0 \\rightarrow F_0.. \" since \\pi_0 is clearly defined for the case l = 0 as indicated by its subscript.\n- right before equation (3), isn't it better to write \" ... we need the filter bank \\Psi: F_l \\rightarrow R^{K_{l+1}} ...\" since F here is for layer l specifically.\n\nI am not sure I am reading the most recent revision, so forgive me if these things have already been fixed.\n\nGreat paper, by the way.\n\nShuang"
  },
  {
    "people": [
      "Kenta Oono"
    ],
    "review": "In response to the following comment from Kenta Oono, we have uploaded a new version of the paper.\n\n\"I saw your paper submitted to ICLR 2017, Steerable CNNs.\n\nIt is wonderful as it is theoretically well founded with the representation theory and also takes engineering easiness into consideration. Further, it achieves state of the art result in CIFAR datasets. So I would like to understand the paper in detail.\n\nAs I read the paper, I encountered the problem that I could not derive some of equations (specifically, (5) in the paper). [...]\"\n\nThe problem he identified was due to a small mistake in eq. 4 (a missing inverse). In the new version we have fixed this and added an appendix where eq. 5 (the induced representation) is derived."
  },
  {
    "people": [
      "Levy",
      "Goldberg",
      "Dagan",
      "Hoeffding",
      "Sutskever",
      "Martens",
      "Hinton"
    ],
    "review": "\nThe authors present a methodology for analyzing sentence embedding techniques by checking how much the embeddings preserve information about sentence length, word content, and word order. They examine several popular embedding methods including autoencoding LSTMs, averaged word vectors, and skip-thought vectors. The experiments are thorough and provide interesting insights into the representational power of common sentence embedding strategies, such as the fact that word ordering is surprisingly low-entropy conditioned on word content.\n\nExploring what sort of information is encoded in representation learning methods for NLP is an important and under-researched area. For example, the tide of word-embeddings research was mostly stemmed after a thread of careful experimental results showing most embeddings to be essentially equivalent, culminating in \"Improving Distributional Similarity with Lessons Learned from Word Embeddings\" by Levy, Goldberg, and Dagan. As representation learning becomes even more important in NLP this sort of research will be even more important.\n\nWhile this paper makes a valuable contribution in setting out and exploring a methodology for evaluating sentence embeddings, the evaluations themselves are quite simple and do not necessarily correlate with real-world desiderata for sentence embeddings (as the authors note in other comments, performance on these tasks is not a normative measure of embedding quality). For example, as the authors note, the ability of the averaged vector to encode sentence length is trivially to be expected given the central limit theorem (or more accurately, concentration inequalities like Hoeffding's inequality).\n\nThe word-order experiments were interesting. A relevant citation for this sort of conditional ordering procedure is \"Generating Text with Recurrent Neural Networks\" by Sutskever, Martens, and Hinton, who refer to the conversion of a bag of words into a sentence as \"debagging.\"\n\nAlthough this is just a first step in better understanding of sentence embeddings, it is an important one and I recommend this paper for publication."
  },
  {
    "people": [
      "J. Mu",
      "P. Viswanath"
    ],
    "review": "We (J. Mu and P. Viswanath) enjoyed the tour-de-force comparison of a vast variety of sentence representation algorithms all in one compact manuscript. Of particular interest to us were the three  \"synthetic\" tasks introduced here:  (a) to what extent the sentence representation encodes its length; (b) to what extent the sentence representation encodes the identities of words within it and (c) to what extent the sentence representation encodes word order.  \n\nThe best part of these tasks is that they are very well defined and labeling does not need any (expert) supervision at all and can be done over the entire corpus too. We have a reasonable intuition on why tasks (a) and (b) might be interesting/relevant for downstream tasks: the length of a sentence could be a proxy for the amount of content in the sentence; testing of a word within a sentence could be a type of test for the topic embedded in the sentence. \n\nBut we aren't so clear as to what might be the sense in which task (c) could be useful for downstream applications. One instance where this matters seems to be cause-effect relationships. For example,  the order of 'Mary' and 'John' is critical in 'Mary stole an apple from John.' Such a pair of words tend to be  named entities, however. \n\nThe tests presented in this manuscript worked with a random pair of words (and not just named entities or scenarios where cause-effect relationship mattered). We would love to hear what the authors think about the use cases of task (c) and our conjecture that they are particularly relevant in cause-effect scenarios. "
  },
  {
    "people": [
      "Levy",
      "Goldberg",
      "Dagan",
      "Hoeffding",
      "Sutskever",
      "Martens",
      "Hinton"
    ],
    "review": "\nThe authors present a methodology for analyzing sentence embedding techniques by checking how much the embeddings preserve information about sentence length, word content, and word order. They examine several popular embedding methods including autoencoding LSTMs, averaged word vectors, and skip-thought vectors. The experiments are thorough and provide interesting insights into the representational power of common sentence embedding strategies, such as the fact that word ordering is surprisingly low-entropy conditioned on word content.\n\nExploring what sort of information is encoded in representation learning methods for NLP is an important and under-researched area. For example, the tide of word-embeddings research was mostly stemmed after a thread of careful experimental results showing most embeddings to be essentially equivalent, culminating in \"Improving Distributional Similarity with Lessons Learned from Word Embeddings\" by Levy, Goldberg, and Dagan. As representation learning becomes even more important in NLP this sort of research will be even more important.\n\nWhile this paper makes a valuable contribution in setting out and exploring a methodology for evaluating sentence embeddings, the evaluations themselves are quite simple and do not necessarily correlate with real-world desiderata for sentence embeddings (as the authors note in other comments, performance on these tasks is not a normative measure of embedding quality). For example, as the authors note, the ability of the averaged vector to encode sentence length is trivially to be expected given the central limit theorem (or more accurately, concentration inequalities like Hoeffding's inequality).\n\nThe word-order experiments were interesting. A relevant citation for this sort of conditional ordering procedure is \"Generating Text with Recurrent Neural Networks\" by Sutskever, Martens, and Hinton, who refer to the conversion of a bag of words into a sentence as \"debagging.\"\n\nAlthough this is just a first step in better understanding of sentence embeddings, it is an important one and I recommend this paper for publication."
  },
  {
    "people": [
      "J. Mu",
      "P. Viswanath"
    ],
    "review": "We (J. Mu and P. Viswanath) enjoyed the tour-de-force comparison of a vast variety of sentence representation algorithms all in one compact manuscript. Of particular interest to us were the three  \"synthetic\" tasks introduced here:  (a) to what extent the sentence representation encodes its length; (b) to what extent the sentence representation encodes the identities of words within it and (c) to what extent the sentence representation encodes word order.  \n\nThe best part of these tasks is that they are very well defined and labeling does not need any (expert) supervision at all and can be done over the entire corpus too. We have a reasonable intuition on why tasks (a) and (b) might be interesting/relevant for downstream tasks: the length of a sentence could be a proxy for the amount of content in the sentence; testing of a word within a sentence could be a type of test for the topic embedded in the sentence. \n\nBut we aren't so clear as to what might be the sense in which task (c) could be useful for downstream applications. One instance where this matters seems to be cause-effect relationships. For example,  the order of 'Mary' and 'John' is critical in 'Mary stole an apple from John.' Such a pair of words tend to be  named entities, however. \n\nThe tests presented in this manuscript worked with a random pair of words (and not just named entities or scenarios where cause-effect relationship mattered). We would love to hear what the authors think about the use cases of task (c) and our conjecture that they are particularly relevant in cause-effect scenarios. "
  },
  {
    "people": [
      "Zweig"
    ],
    "review": "This is a solidly executed paper that received good reviews. However, the originality is a bit lacking. In addition, the paper would have been stronger with a comparison to the method proposed in Zweig et al. (2013). We recommend this paper for the workshop."
  },
  {
    "people": [
      "Zweig",
      "Sundermeyer",
      "Sundermeyer"
    ],
    "review": "he authors provide an interesting, computational-complexity-driven approach for efficient softmax computation for language modeling based on GPUs. An adaptive softmax approach is proposed based on a hierarchical model. Dynamic programming is applied to optimize the structure of the hierarchical approach chosen here w.r.t. computational complexity based on GPUs. \n\nHowever, it remains unclear, how robust the specific configuration obtained from dynamic programming is w.r.t. performance/perplexity. Corresponding comparative results with perplexity-based clustering would be desirable. Especially, in Sec. 5, Paragraph Baselines, and Table 1, respectively, it would be interesting to see a result on HSM(PPL) (cf. Zweig et al. 2013).\n\nAFAIK, the first successful application of an LSTM-based language model for large vocabulary was published by Sundermeyer et al. 2012 (see below), which is missing in the sumary of prior work on the bottom of p. 3.\n\nMainly, the paper is well written and accessible, though notation in some cases should be improved, see detailed comments below.\n\nPrior work on LSTM language modeling: \n - Sundermeyer et al.: LSTM Neural Networks for Language Modeling, Interspeech, pp. 194-197, 2012.\n\nNotation:\n - use of g(k) vs. g(k,B,d): g(k) should be clearly defined (constant B and d?)\n - notation should not be reused (B is matrix in Eq. (3), and batch size in Sec. 4.1).\n - notation p_{i+j} (Eq. (10) and before) is kind of misleading, as p_{i+j} is not the same as p_{(i+j)}\n\nMinor comments:\n - p. 1, item list at bottom, first item: take -> takes\n - p. 5, second paragraph: will then contained -> will then contain\n - p. 5, third paragaph: to associated -> to associate\n - Sec. 4.3, first paragraph: At the time being -> For the time being\n - below Eq. (9): most-right -> right-most\n - below Eq. (10): the second term of this equation -> the second term of the right-hand side of this equation\n - p. 6, second to last line: smaller that the -> smaller than the\n - p. 7, Sec. 5, itemize, first item: 100 millions -> 100 million\n - p. 8, last sentence: we are the -> ours is the\n"
  },
  {
    "people": [
      "Zweig"
    ],
    "review": "This is a solidly executed paper that received good reviews. However, the originality is a bit lacking. In addition, the paper would have been stronger with a comparison to the method proposed in Zweig et al. (2013). We recommend this paper for the workshop."
  },
  {
    "people": [
      "Zweig",
      "Sundermeyer",
      "Sundermeyer"
    ],
    "review": "he authors provide an interesting, computational-complexity-driven approach for efficient softmax computation for language modeling based on GPUs. An adaptive softmax approach is proposed based on a hierarchical model. Dynamic programming is applied to optimize the structure of the hierarchical approach chosen here w.r.t. computational complexity based on GPUs. \n\nHowever, it remains unclear, how robust the specific configuration obtained from dynamic programming is w.r.t. performance/perplexity. Corresponding comparative results with perplexity-based clustering would be desirable. Especially, in Sec. 5, Paragraph Baselines, and Table 1, respectively, it would be interesting to see a result on HSM(PPL) (cf. Zweig et al. 2013).\n\nAFAIK, the first successful application of an LSTM-based language model for large vocabulary was published by Sundermeyer et al. 2012 (see below), which is missing in the sumary of prior work on the bottom of p. 3.\n\nMainly, the paper is well written and accessible, though notation in some cases should be improved, see detailed comments below.\n\nPrior work on LSTM language modeling: \n - Sundermeyer et al.: LSTM Neural Networks for Language Modeling, Interspeech, pp. 194-197, 2012.\n\nNotation:\n - use of g(k) vs. g(k,B,d): g(k) should be clearly defined (constant B and d?)\n - notation should not be reused (B is matrix in Eq. (3), and batch size in Sec. 4.1).\n - notation p_{i+j} (Eq. (10) and before) is kind of misleading, as p_{i+j} is not the same as p_{(i+j)}\n\nMinor comments:\n - p. 1, item list at bottom, first item: take -> takes\n - p. 5, second paragraph: will then contained -> will then contain\n - p. 5, third paragaph: to associated -> to associate\n - Sec. 4.3, first paragraph: At the time being -> For the time being\n - below Eq. (9): most-right -> right-most\n - below Eq. (10): the second term of this equation -> the second term of the right-hand side of this equation\n - p. 6, second to last line: smaller that the -> smaller than the\n - p. 7, Sec. 5, itemize, first item: 100 millions -> 100 million\n - p. 8, last sentence: we are the -> ours is the\n"
  },
  {
    "people": [
      "Dyer"
    ],
    "review": "The paper proposes a new neural architecture, called DRAGNN, for the transition-based framework. A DRAGNN uses TBRUs which are neural units to compute hidden activations for the current state of a transition-based system. The paper proves that DRAGNNs can cover a wide range of transition-based methods in the literature. In addition, one can easily implement multitask learning systems with DRAGNNs. The experimental results shows that using DRAGNNs the authors built (near) state-of-the-art systems for 2 tasks: parsing and summarization. \n\nThe paper contains two major parts: DRAGNN and demonstrations of its usages. \n\nRegarding to the first part, the proposed DRAGNN is a neat tool for building any transition-based systems. However, it is difficult to say whether the DRAGNN is novel. Transition-based framework is already well defined and there's a huge trend in NLP using neural networks to implement transition-based systems. In my opinion, the difference between the Stack-LSTM (Dyer et al., 2015) and DRAGNN is slight. Of course, the DRAGNN is a powerful architecture but the contribution here should be considered mainly in terms of software engineering.\n\nIn the second part, the authors used DRAGNN to implement new transition-based systems for different (multi-)tasks. The implementations are neat, confirming that DRAGNN is a powerful architecture, especially for multitask learning. However, we should bear in mind that the solutions employed are already there in the literature, thus making difficult to judge the novelty of this part w.r.t. the theme of the conference.  "
  },
  {
    "people": [
      "Dyer"
    ],
    "review": "The paper proposes a new neural architecture, called DRAGNN, for the transition-based framework. A DRAGNN uses TBRUs which are neural units to compute hidden activations for the current state of a transition-based system. The paper proves that DRAGNNs can cover a wide range of transition-based methods in the literature. In addition, one can easily implement multitask learning systems with DRAGNNs. The experimental results shows that using DRAGNNs the authors built (near) state-of-the-art systems for 2 tasks: parsing and summarization. \n\nThe paper contains two major parts: DRAGNN and demonstrations of its usages. \n\nRegarding to the first part, the proposed DRAGNN is a neat tool for building any transition-based systems. However, it is difficult to say whether the DRAGNN is novel. Transition-based framework is already well defined and there's a huge trend in NLP using neural networks to implement transition-based systems. In my opinion, the difference between the Stack-LSTM (Dyer et al., 2015) and DRAGNN is slight. Of course, the DRAGNN is a powerful architecture but the contribution here should be considered mainly in terms of software engineering.\n\nIn the second part, the authors used DRAGNN to implement new transition-based systems for different (multi-)tasks. The implementations are neat, confirming that DRAGNN is a powerful architecture, especially for multitask learning. However, we should bear in mind that the solutions employed are already there in the literature, thus making difficult to judge the novelty of this part w.r.t. the theme of the conference.  "
  },
  {
    "people": [
      "Bellemare",
      "Bellemare"
    ],
    "review": "This paper introduces a new way of extending the count based exploration approach to domains where counts are not readily available. The way in which the authors do it is through hash functions. Experiments are conducted on several domains including control and Atari. \n\nIt is nice that the authors confirmed the results of Bellemare in that given the right \"density\" estimator, count based exploration can be effective. It is also great the observe that given the right features, we can crack games like Montezuma's revenge to some extend.\n\nI, however, have several complaints:\n\nFirst, by using hashing, the authors did not seem to be able to achieve significant improvements over past approaches. Without \"feature engineering\", the authors achieved only a fraction of the performance achieved in Bellemare et al. on Montezuma's Revenge. The proposed approaches In the control domains, the authors also does not outperform VIME. So experimentally, it is very hard to justify the approach. \n\nSecond, hashing, although could be effective in the domains that the authors tested on, it may not be the best way of estimating densities going forward. As the environments get more complicated, some learning methods, are required for the understanding of the environments instead of blind hashing. The authors claim that the advantage of the proposed method over Bellemare et al. is that one does not have to design density estimators. But I would argue that density estimators have become readily available (PixelCNN, VAEs, Real NVP, GANs) that they can be as easily applied as can hashing. Training the density estimators is not difficult problem as more.\n\n"
  },
  {
    "people": [
      "Strehl",
      "Littman",
      "Montezuma",
      "Bellemare",
      "Mnih"
    ],
    "review": "The paper proposes a new exploration scheme for reinforcement learning using locality-sensitive hashing states to build a table of visit counts which are then used to encourage exploration in the style of MBIE-EB of Strehl and Littman.\n\nSeveral points are appealing about this approach: first, it is quite simple compared to the current alternatives (e.g. VIME, density estimation and pseudo-counts). Second, the paper presents results across several domains, including classic benchmarks, continuous control domains, and Atari 2600 games. In addition, there are results for comparison from several other algorithms (DQN variants), many of which are quite recent. The results indicate that the approach clearly improves over the baseline. The results against other exploration algorithms are not as clear (more dependent on the individual domain/game), but I think this is fine as the appeal of the technique is its simplicity. Third, the paper presents results on the sensitivity to the granularity of the abstraction.\n\nI have only one main complaint, which is it seems there was some engineering involved to get this to work, and I do not have much confidence in the robustness of the conclusions. I am left uncertain as to how the story changes given slight perturbations over hyper-parameter values or enabling/disabling of certain choices. For example, how critical was using PixelCNN (or tying the weights?) or noisifying the output in the autoencoder, or what happens if you remove the custom additions to BASS? The granularity results show that the choice of resolution is sensitive, and even across games the story is not consistent.\n\nThe authors decide to use state-based counts instead of state-action based counts, deviating from the theory, which is odd because the reason to used LSH in the first place is to get closer to what MBIE-EB would advise via tabular counts. There are several explanations as to why state-based versus state-action based counts perform similarly in Atari; the authors do not offer any. Why?\n\nIt seems like the technique could be easily used in DQN as well, and many of the variants the authors compare to are DQN-based, so omitting DQN here again seems strange. The authors justify their choice of TRPO by saying it ensures safe policy improvement, though it is not clear that this is still true when adding these exploration bonuses.\n\nThe case study on Montezuma's revenge, while interesting, involves using domain knowledge and so does not really fit well with the rest of the paper.\n\nSo, in the end, simple and elegant idea to help with exploration tested in many domains, though I am not certain which of the many pieces are critical for the story to hold versus just slightly helpful, which could hurt the long-term impact of the paper.\n\n--- After response:\n\nThank you for the thorough response, and again my apologies for the late reply.\n\nI appreciate the follow-up version on the robustness of SimHash and state counting vs. state-action counting.\n\nThe paper addresses an important problem (exploration), suggesting a \"simple\" (compared to density estimation) counting method via hashing. It is a nice alternative approach to the one offered by Bellemare et al. If discussion among reviewers were possible, I would now try to assemble an argument to accept the paper. Specifically, I am not as concerned about beating the state of the art in Montezuma's as Reviewer3 as the merit of the current paper is one the simplicity of the hashing and on the wide comparison of domains vs. the baseline TRPO. This paper shows that we should not give up on simple hashing. There still seems to be a bunch of fiddly bits to get this to work, and I am still not confident that these results are easily reproducible. Nonetheless, it is an interesting new contrasting approach to exploration which deserves attention.\n\nNot important for the decision: The argument in the rebuttal concerning DQN & A3C is a bit of a straw man. I did not mention anything at all about A3C, I strictly referred to DQN, which is less sensitive to parameter-tuning than A3C. Also, Bellemare 2016 main result on Montezuma used DQN. Hence the omission of these techniques applied to DQN still seems a bit strange (for the Atari experiments). The figure S9 from Mnih et al. points to instances of asynchronous one-step Sarsa with varied thread counts.. of course this will be sensitive to parameters: it is both asynchronous online algorithms *and* the parameter varied is the thread count! This is hardly indicative of DQN's sensitivity to parameters, since DQN is (a) single-threaded (b) uses experience replay, leading to slower policy changes. Another source of stability, DQN uses a target network that changes infrequently. Perhaps the authors made a mistake in the reference graph in the figure? (I see no Figure 9 in "
  },
  {
    "people": [
      "Bellemare",
      "Bellemare"
    ],
    "review": "This paper introduces a new way of extending the count based exploration approach to domains where counts are not readily available. The way in which the authors do it is through hash functions. Experiments are conducted on several domains including control and Atari. \n\nIt is nice that the authors confirmed the results of Bellemare in that given the right \"density\" estimator, count based exploration can be effective. It is also great the observe that given the right features, we can crack games like Montezuma's revenge to some extend.\n\nI, however, have several complaints:\n\nFirst, by using hashing, the authors did not seem to be able to achieve significant improvements over past approaches. Without \"feature engineering\", the authors achieved only a fraction of the performance achieved in Bellemare et al. on Montezuma's Revenge. The proposed approaches In the control domains, the authors also does not outperform VIME. So experimentally, it is very hard to justify the approach. \n\nSecond, hashing, although could be effective in the domains that the authors tested on, it may not be the best way of estimating densities going forward. As the environments get more complicated, some learning methods, are required for the understanding of the environments instead of blind hashing. The authors claim that the advantage of the proposed method over Bellemare et al. is that one does not have to design density estimators. But I would argue that density estimators have become readily available (PixelCNN, VAEs, Real NVP, GANs) that they can be as easily applied as can hashing. Training the density estimators is not difficult problem as more.\n\n"
  },
  {
    "people": [
      "Strehl",
      "Littman",
      "Montezuma",
      "Bellemare",
      "Mnih"
    ],
    "review": "The paper proposes a new exploration scheme for reinforcement learning using locality-sensitive hashing states to build a table of visit counts which are then used to encourage exploration in the style of MBIE-EB of Strehl and Littman.\n\nSeveral points are appealing about this approach: first, it is quite simple compared to the current alternatives (e.g. VIME, density estimation and pseudo-counts). Second, the paper presents results across several domains, including classic benchmarks, continuous control domains, and Atari 2600 games. In addition, there are results for comparison from several other algorithms (DQN variants), many of which are quite recent. The results indicate that the approach clearly improves over the baseline. The results against other exploration algorithms are not as clear (more dependent on the individual domain/game), but I think this is fine as the appeal of the technique is its simplicity. Third, the paper presents results on the sensitivity to the granularity of the abstraction.\n\nI have only one main complaint, which is it seems there was some engineering involved to get this to work, and I do not have much confidence in the robustness of the conclusions. I am left uncertain as to how the story changes given slight perturbations over hyper-parameter values or enabling/disabling of certain choices. For example, how critical was using PixelCNN (or tying the weights?) or noisifying the output in the autoencoder, or what happens if you remove the custom additions to BASS? The granularity results show that the choice of resolution is sensitive, and even across games the story is not consistent.\n\nThe authors decide to use state-based counts instead of state-action based counts, deviating from the theory, which is odd because the reason to used LSH in the first place is to get closer to what MBIE-EB would advise via tabular counts. There are several explanations as to why state-based versus state-action based counts perform similarly in Atari; the authors do not offer any. Why?\n\nIt seems like the technique could be easily used in DQN as well, and many of the variants the authors compare to are DQN-based, so omitting DQN here again seems strange. The authors justify their choice of TRPO by saying it ensures safe policy improvement, though it is not clear that this is still true when adding these exploration bonuses.\n\nThe case study on Montezuma's revenge, while interesting, involves using domain knowledge and so does not really fit well with the rest of the paper.\n\nSo, in the end, simple and elegant idea to help with exploration tested in many domains, though I am not certain which of the many pieces are critical for the story to hold versus just slightly helpful, which could hurt the long-term impact of the paper.\n\n--- After response:\n\nThank you for the thorough response, and again my apologies for the late reply.\n\nI appreciate the follow-up version on the robustness of SimHash and state counting vs. state-action counting.\n\nThe paper addresses an important problem (exploration), suggesting a \"simple\" (compared to density estimation) counting method via hashing. It is a nice alternative approach to the one offered by Bellemare et al. If discussion among reviewers were possible, I would now try to assemble an argument to accept the paper. Specifically, I am not as concerned about beating the state of the art in Montezuma's as Reviewer3 as the merit of the current paper is one the simplicity of the hashing and on the wide comparison of domains vs. the baseline TRPO. This paper shows that we should not give up on simple hashing. There still seems to be a bunch of fiddly bits to get this to work, and I am still not confident that these results are easily reproducible. Nonetheless, it is an interesting new contrasting approach to exploration which deserves attention.\n\nNot important for the decision: The argument in the rebuttal concerning DQN & A3C is a bit of a straw man. I did not mention anything at all about A3C, I strictly referred to DQN, which is less sensitive to parameter-tuning than A3C. Also, Bellemare 2016 main result on Montezuma used DQN. Hence the omission of these techniques applied to DQN still seems a bit strange (for the Atari experiments). The figure S9 from Mnih et al. points to instances of asynchronous one-step Sarsa with varied thread counts.. of course this will be sensitive to parameters: it is both asynchronous online algorithms *and* the parameter varied is the thread count! This is hardly indicative of DQN's sensitivity to parameters, since DQN is (a) single-threaded (b) uses experience replay, leading to slower policy changes. Another source of stability, DQN uses a target network that changes infrequently. Perhaps the authors made a mistake in the reference graph in the figure? (I see no Figure 9 in "
  },
  {
    "people": [
      "Krishnan",
      "Johnson"
    ],
    "review": "Because the authors did not respond to reviewer feedback, I am maintaining my original review score.\n\n-----\n\nThis paper proposes to model relational (i.e., correlated) time series using a deep learning-inspired latent variable approach: they design a flexible parametric (but not generative) model with Gaussian latent factors and fit it using a rich training objective including terms for reconstruction (of observed time series) error, smoothness in the latent state space (via a KL divergence term encouraging neighbor states to be similarly distributed), and a final regularizer that encourages related time series to have similar latent state trajectories. Relations between trajectories are hard coded based on pre-existing knowledge, i.e., latent state trajectories for neighboring (wind speed) base stations should be similar. The model appears to be fit using gradient simple descent. The authors propose several elaborations, including a nonlinear transition function (based on an MLP) and a reconstruction error term that takes variance into account. However, the model is restricted to using a linear decoder. Experimental results are positive but not convincing.\n\nStrengths:\n- The authors target a worthwhile and challenging problem: incorporating the modeling of uncertainty over hidden states with the power of flexible neural net-like models.\n- The idea of representing relationships between hidden states using KL divergence between their (distributions over) corresponding hidden states is clever. Combined with the Gaussian distribution over hidden states, the resulting regularization term is simple and differentiable.\n- This general approach -- focusing on writing down the problem as a neural network-like loss function -- seems robust and flexible and could be combined with other approaches, including variants of variational autoencoders.\n\nWeaknesses:\n- The presentation is a muddled, especially the model definition in Sec. 3.3. The authors introduce four variants of their model with different combinations of decoder (with and without variance term) and linear vs. MLP transition function. It appears that the 2,2 variant is generally better but not on all metrics and often by small margins. This makes drawing a solid conclusions difficult: what each component of the loss contributes, whether and how the nonlinear transition function helps and how much, how in practice the model should be applied, etc. I would suggest two improvements to the manuscript: (1) focus on the main 2,2 variant in Sec. 3.3 (with the hypothesis that it should perform best) and make the simpler variants additional \"baselines\" described in a paragraph in Sec. 4.1; (2) perform more thorough experiments with larger data sets to make a stronger case for the superiority of this approach.\n- The authors only allude to learning (with references to gradient descent and ADAM during model description) in this framework. Inference gets its one subsection but only one sentence that ends in an ellipsis (?).\n- It's unclear what is the purpose of introducing the inequality in Eq. 9.\n- Experimental results are not convincing: given the size of the data, the differences vs. the RNN and KF baselines is probably not significant, and these aren't particularly strong baselines (especially if it is in fact an RNN and not an LSTM or GRU).\n- The position of this paper is unclear with respect to variational autoencoders and related models. Recurrent variants of VAEs (e.g., Krishnan, et al., 2015) seem to achieve most of the same goals as far as uncertainty modeling is concerned. It seems like those could easily be extended to model relationships between time series using the simple regularization strategy used here. Same goes for Johnson, et al., 2016 (mentioned in separate question).\n\nThis is a valuable research direction with some intriguing ideas and interesting preliminary results. I would suggest that the authors restructure this manuscript a bit, striving for clarity of model description similar to the papers cited above and providing greater detail about learning and inference. They also need to perform more thorough experiments and present results that tell a clear story about the strengths and weaknesses of this approach."
  },
  {
    "people": [
      "Krishnan",
      "Johnson"
    ],
    "review": "Because the authors did not respond to reviewer feedback, I am maintaining my original review score.\n\n-----\n\nThis paper proposes to model relational (i.e., correlated) time series using a deep learning-inspired latent variable approach: they design a flexible parametric (but not generative) model with Gaussian latent factors and fit it using a rich training objective including terms for reconstruction (of observed time series) error, smoothness in the latent state space (via a KL divergence term encouraging neighbor states to be similarly distributed), and a final regularizer that encourages related time series to have similar latent state trajectories. Relations between trajectories are hard coded based on pre-existing knowledge, i.e., latent state trajectories for neighboring (wind speed) base stations should be similar. The model appears to be fit using gradient simple descent. The authors propose several elaborations, including a nonlinear transition function (based on an MLP) and a reconstruction error term that takes variance into account. However, the model is restricted to using a linear decoder. Experimental results are positive but not convincing.\n\nStrengths:\n- The authors target a worthwhile and challenging problem: incorporating the modeling of uncertainty over hidden states with the power of flexible neural net-like models.\n- The idea of representing relationships between hidden states using KL divergence between their (distributions over) corresponding hidden states is clever. Combined with the Gaussian distribution over hidden states, the resulting regularization term is simple and differentiable.\n- This general approach -- focusing on writing down the problem as a neural network-like loss function -- seems robust and flexible and could be combined with other approaches, including variants of variational autoencoders.\n\nWeaknesses:\n- The presentation is a muddled, especially the model definition in Sec. 3.3. The authors introduce four variants of their model with different combinations of decoder (with and without variance term) and linear vs. MLP transition function. It appears that the 2,2 variant is generally better but not on all metrics and often by small margins. This makes drawing a solid conclusions difficult: what each component of the loss contributes, whether and how the nonlinear transition function helps and how much, how in practice the model should be applied, etc. I would suggest two improvements to the manuscript: (1) focus on the main 2,2 variant in Sec. 3.3 (with the hypothesis that it should perform best) and make the simpler variants additional \"baselines\" described in a paragraph in Sec. 4.1; (2) perform more thorough experiments with larger data sets to make a stronger case for the superiority of this approach.\n- The authors only allude to learning (with references to gradient descent and ADAM during model description) in this framework. Inference gets its one subsection but only one sentence that ends in an ellipsis (?).\n- It's unclear what is the purpose of introducing the inequality in Eq. 9.\n- Experimental results are not convincing: given the size of the data, the differences vs. the RNN and KF baselines is probably not significant, and these aren't particularly strong baselines (especially if it is in fact an RNN and not an LSTM or GRU).\n- The position of this paper is unclear with respect to variational autoencoders and related models. Recurrent variants of VAEs (e.g., Krishnan, et al., 2015) seem to achieve most of the same goals as far as uncertainty modeling is concerned. It seems like those could easily be extended to model relationships between time series using the simple regularization strategy used here. Same goes for Johnson, et al., 2016 (mentioned in separate question).\n\nThis is a valuable research direction with some intriguing ideas and interesting preliminary results. I would suggest that the authors restructure this manuscript a bit, striving for clarity of model description similar to the papers cited above and providing greater detail about learning and inference. They also need to perform more thorough experiments and present results that tell a clear story about the strengths and weaknesses of this approach."
  },
  {
    "people": [
      "Ziat",
      "Frigola",
      "Frigola"
    ],
    "review": "This manuscript proposes an approach for modeling correlated timeseries through a combination of loss functions which depend on neural networks. The loss functions correspond to: data fit term, autoregressive latent state term, and a term which captures relations between pairs of timeseries (relations have to be given as prior information).\n\nModeling relational timeseries is a well-researched problem, however little attention has been given to it in the neural network community. Perhaps the reason for this is the importance of having uncertainty in the representation. The authors correctly identify this need and consider an approach which considers distributions in the state space.\n\nThe formulation is quite straightforward by combining loss functions. The model adds to Ziat et al. 2016 in certain aspects which are well motivated, but unfortunately implemented in an unconvincing way. To start with, uncertainty is not treated in a very principled way, since the inference in the model is rather naive; I'd expect employing a VAE framework [1] for better uncertainty handling. Furthermore, the Gaussian co-variance collapses into a variance, which is the opposite of what one would want for modelling correlated time-series. There are approaches which take these correlations into account in the states, e.g. [2].\n\nMoreover, the treatment of uncertainty only allows for linear decoding function f. This significantly reduces the power of the model. State of the art methods in timeseries modeling have moved beyond this constraint, especially in the Gaussian process community e.g. [2,3,4,5]. Comparing to a few of these methods, or at least discussing them would be useful.\n\n\nReferences:\n[1] Kingma and Welling. Auto-encoding Variational Bayes. arXiv:1312.6114\n[2] Damianou et al. Variational Gaussian process dynamical systems. NIPS 2011.\n[3] Mattos et al. Recurrent Gaussian processes. ICLR 2016.\n[4] Frigola. Bayesian Time Series Learning with Gaussian Processes, University of Cambridge, PhD Thesis, 2015. \n[5] Frigola et al. Variational Gaussian Process State-Space Models. NIPS 2014\n\n\nOne innovation is that the prior structure of the correlation needs to be given. This is a potentially useful and also original structural component. However, it also constitutes a limitation in some sense, since it is unrealistic in many scenarios to have this prior information. Moreover, the particular regularizer that makes \"similar\" timeseries to have closeness in the state space seems problematic. Some timeseries groups might be more \"similar\" than others, and also the similarity might be of different nature across groups. These variations cannot be well captured/distilled by a simple indicator variable e_ij. Furthermore, these variables are in practice taken to be binary (by looking at the experiments), which would make it even harder to model rich correlations. \n\nThe experiments show that the proposed method works, but they are not entirely convincing. Importantly, they do not shed enough light into the different properties of the model w.r.t its different parts. For example, the effect and sensitivity of the different regularizers. The authors state in a pre-review answer that they amended with some more results, but I can't see a revision in openreview (please let me know if I've missed it). From the performance point of view, the results are not particularly exciting, especially given the fact that it's not clear which loss is better (making it difficult to use the method in practice). \n\nIt would also be very interesting to report the optimized values of the parameters \\lambda, to get an idea of how the different losses behave.\n\nTimeseries analysis is a very well-researched area. Given the above, it's not clear to me why one would prefer to use this model over other approaches. Methodology wise, there are no novel components that offer a proven advantage with respect to past methods. The uncertainty in the states and the correlation of the time-series are the aspects which could add an advantage, but are not adequately researched in this paper.\n"
  },
  {
    "people": [
      "Krishnan",
      "Johnson"
    ],
    "review": "Because the authors did not respond to reviewer feedback, I am maintaining my original review score.\n\n-----\n\nThis paper proposes to model relational (i.e., correlated) time series using a deep learning-inspired latent variable approach: they design a flexible parametric (but not generative) model with Gaussian latent factors and fit it using a rich training objective including terms for reconstruction (of observed time series) error, smoothness in the latent state space (via a KL divergence term encouraging neighbor states to be similarly distributed), and a final regularizer that encourages related time series to have similar latent state trajectories. Relations between trajectories are hard coded based on pre-existing knowledge, i.e., latent state trajectories for neighboring (wind speed) base stations should be similar. The model appears to be fit using gradient simple descent. The authors propose several elaborations, including a nonlinear transition function (based on an MLP) and a reconstruction error term that takes variance into account. However, the model is restricted to using a linear decoder. Experimental results are positive but not convincing.\n\nStrengths:\n- The authors target a worthwhile and challenging problem: incorporating the modeling of uncertainty over hidden states with the power of flexible neural net-like models.\n- The idea of representing relationships between hidden states using KL divergence between their (distributions over) corresponding hidden states is clever. Combined with the Gaussian distribution over hidden states, the resulting regularization term is simple and differentiable.\n- This general approach -- focusing on writing down the problem as a neural network-like loss function -- seems robust and flexible and could be combined with other approaches, including variants of variational autoencoders.\n\nWeaknesses:\n- The presentation is a muddled, especially the model definition in Sec. 3.3. The authors introduce four variants of their model with different combinations of decoder (with and without variance term) and linear vs. MLP transition function. It appears that the 2,2 variant is generally better but not on all metrics and often by small margins. This makes drawing a solid conclusions difficult: what each component of the loss contributes, whether and how the nonlinear transition function helps and how much, how in practice the model should be applied, etc. I would suggest two improvements to the manuscript: (1) focus on the main 2,2 variant in Sec. 3.3 (with the hypothesis that it should perform best) and make the simpler variants additional \"baselines\" described in a paragraph in Sec. 4.1; (2) perform more thorough experiments with larger data sets to make a stronger case for the superiority of this approach.\n- The authors only allude to learning (with references to gradient descent and ADAM during model description) in this framework. Inference gets its one subsection but only one sentence that ends in an ellipsis (?).\n- It's unclear what is the purpose of introducing the inequality in Eq. 9.\n- Experimental results are not convincing: given the size of the data, the differences vs. the RNN and KF baselines is probably not significant, and these aren't particularly strong baselines (especially if it is in fact an RNN and not an LSTM or GRU).\n- The position of this paper is unclear with respect to variational autoencoders and related models. Recurrent variants of VAEs (e.g., Krishnan, et al., 2015) seem to achieve most of the same goals as far as uncertainty modeling is concerned. It seems like those could easily be extended to model relationships between time series using the simple regularization strategy used here. Same goes for Johnson, et al., 2016 (mentioned in separate question).\n\nThis is a valuable research direction with some intriguing ideas and interesting preliminary results. I would suggest that the authors restructure this manuscript a bit, striving for clarity of model description similar to the papers cited above and providing greater detail about learning and inference. They also need to perform more thorough experiments and present results that tell a clear story about the strengths and weaknesses of this approach."
  },
  {
    "people": [
      "Krishnan",
      "Johnson"
    ],
    "review": "Because the authors did not respond to reviewer feedback, I am maintaining my original review score.\n\n-----\n\nThis paper proposes to model relational (i.e., correlated) time series using a deep learning-inspired latent variable approach: they design a flexible parametric (but not generative) model with Gaussian latent factors and fit it using a rich training objective including terms for reconstruction (of observed time series) error, smoothness in the latent state space (via a KL divergence term encouraging neighbor states to be similarly distributed), and a final regularizer that encourages related time series to have similar latent state trajectories. Relations between trajectories are hard coded based on pre-existing knowledge, i.e., latent state trajectories for neighboring (wind speed) base stations should be similar. The model appears to be fit using gradient simple descent. The authors propose several elaborations, including a nonlinear transition function (based on an MLP) and a reconstruction error term that takes variance into account. However, the model is restricted to using a linear decoder. Experimental results are positive but not convincing.\n\nStrengths:\n- The authors target a worthwhile and challenging problem: incorporating the modeling of uncertainty over hidden states with the power of flexible neural net-like models.\n- The idea of representing relationships between hidden states using KL divergence between their (distributions over) corresponding hidden states is clever. Combined with the Gaussian distribution over hidden states, the resulting regularization term is simple and differentiable.\n- This general approach -- focusing on writing down the problem as a neural network-like loss function -- seems robust and flexible and could be combined with other approaches, including variants of variational autoencoders.\n\nWeaknesses:\n- The presentation is a muddled, especially the model definition in Sec. 3.3. The authors introduce four variants of their model with different combinations of decoder (with and without variance term) and linear vs. MLP transition function. It appears that the 2,2 variant is generally better but not on all metrics and often by small margins. This makes drawing a solid conclusions difficult: what each component of the loss contributes, whether and how the nonlinear transition function helps and how much, how in practice the model should be applied, etc. I would suggest two improvements to the manuscript: (1) focus on the main 2,2 variant in Sec. 3.3 (with the hypothesis that it should perform best) and make the simpler variants additional \"baselines\" described in a paragraph in Sec. 4.1; (2) perform more thorough experiments with larger data sets to make a stronger case for the superiority of this approach.\n- The authors only allude to learning (with references to gradient descent and ADAM during model description) in this framework. Inference gets its one subsection but only one sentence that ends in an ellipsis (?).\n- It's unclear what is the purpose of introducing the inequality in Eq. 9.\n- Experimental results are not convincing: given the size of the data, the differences vs. the RNN and KF baselines is probably not significant, and these aren't particularly strong baselines (especially if it is in fact an RNN and not an LSTM or GRU).\n- The position of this paper is unclear with respect to variational autoencoders and related models. Recurrent variants of VAEs (e.g., Krishnan, et al., 2015) seem to achieve most of the same goals as far as uncertainty modeling is concerned. It seems like those could easily be extended to model relationships between time series using the simple regularization strategy used here. Same goes for Johnson, et al., 2016 (mentioned in separate question).\n\nThis is a valuable research direction with some intriguing ideas and interesting preliminary results. I would suggest that the authors restructure this manuscript a bit, striving for clarity of model description similar to the papers cited above and providing greater detail about learning and inference. They also need to perform more thorough experiments and present results that tell a clear story about the strengths and weaknesses of this approach."
  },
  {
    "people": [
      "Ziat",
      "Frigola",
      "Frigola"
    ],
    "review": "This manuscript proposes an approach for modeling correlated timeseries through a combination of loss functions which depend on neural networks. The loss functions correspond to: data fit term, autoregressive latent state term, and a term which captures relations between pairs of timeseries (relations have to be given as prior information).\n\nModeling relational timeseries is a well-researched problem, however little attention has been given to it in the neural network community. Perhaps the reason for this is the importance of having uncertainty in the representation. The authors correctly identify this need and consider an approach which considers distributions in the state space.\n\nThe formulation is quite straightforward by combining loss functions. The model adds to Ziat et al. 2016 in certain aspects which are well motivated, but unfortunately implemented in an unconvincing way. To start with, uncertainty is not treated in a very principled way, since the inference in the model is rather naive; I'd expect employing a VAE framework [1] for better uncertainty handling. Furthermore, the Gaussian co-variance collapses into a variance, which is the opposite of what one would want for modelling correlated time-series. There are approaches which take these correlations into account in the states, e.g. [2].\n\nMoreover, the treatment of uncertainty only allows for linear decoding function f. This significantly reduces the power of the model. State of the art methods in timeseries modeling have moved beyond this constraint, especially in the Gaussian process community e.g. [2,3,4,5]. Comparing to a few of these methods, or at least discussing them would be useful.\n\n\nReferences:\n[1] Kingma and Welling. Auto-encoding Variational Bayes. arXiv:1312.6114\n[2] Damianou et al. Variational Gaussian process dynamical systems. NIPS 2011.\n[3] Mattos et al. Recurrent Gaussian processes. ICLR 2016.\n[4] Frigola. Bayesian Time Series Learning with Gaussian Processes, University of Cambridge, PhD Thesis, 2015. \n[5] Frigola et al. Variational Gaussian Process State-Space Models. NIPS 2014\n\n\nOne innovation is that the prior structure of the correlation needs to be given. This is a potentially useful and also original structural component. However, it also constitutes a limitation in some sense, since it is unrealistic in many scenarios to have this prior information. Moreover, the particular regularizer that makes \"similar\" timeseries to have closeness in the state space seems problematic. Some timeseries groups might be more \"similar\" than others, and also the similarity might be of different nature across groups. These variations cannot be well captured/distilled by a simple indicator variable e_ij. Furthermore, these variables are in practice taken to be binary (by looking at the experiments), which would make it even harder to model rich correlations. \n\nThe experiments show that the proposed method works, but they are not entirely convincing. Importantly, they do not shed enough light into the different properties of the model w.r.t its different parts. For example, the effect and sensitivity of the different regularizers. The authors state in a pre-review answer that they amended with some more results, but I can't see a revision in openreview (please let me know if I've missed it). From the performance point of view, the results are not particularly exciting, especially given the fact that it's not clear which loss is better (making it difficult to use the method in practice). \n\nIt would also be very interesting to report the optimized values of the parameters \\lambda, to get an idea of how the different losses behave.\n\nTimeseries analysis is a very well-researched area. Given the above, it's not clear to me why one would prefer to use this model over other approaches. Methodology wise, there are no novel components that offer a proven advantage with respect to past methods. The uncertainty in the states and the correlation of the time-series are the aspects which could add an advantage, but are not adequately researched in this paper.\n"
  },
  {
    "people": [
      "Cho"
    ],
    "review": "I have not read the revised version in detail yet. \n\nSUMMARY \nThis paper studies the preimages of outputs of a feedforward neural network with ReLUs. \n\nPROS \nThe paper presents a neat idea for changes of coordinates at the individual layers. \n\nCONS \nQuite unpolished / not enough contributions for a finished paper. \n\nCOMMENTS \n- In the first version the paper contains many typos and appears to be still quite unpolished. \n\n- The paper contains nice ideas but in my opinion it does not contribute sufficiently many results for a Conference paper. \nI would be happy to recommend for the Workshop track. \n\n- Irreversibly mixed and several other notions from the present paper are closely related to the concepts discussed in [Montufar, Pascanu, Cho, Bengio, NIPS 2014]. I feel that that paper should be cited here and the connections should be discussed. In particular, that paper also contains a discussion on the local linear maps of ReLU networks. \n\n- I am curious about the practical considerations when computing the pre-images. The definition should be rather straight forward really, but the implementation / computation could be troublesome. \n\n\nDETAILED COMMENTS \n- On page 1 ``can easily be shown to be many to one'' in general. \n\n- On page 2 ``For each point x^{l+1}'' The parentheses in the superscript are missing. \n\n- After eq. 6 ``the mapping is unique''  is missing `when w1 and w2 are linearly independent' \n\n- Eq. 1 should be a vector. \n\n- Above eq. 3. ``collected the weights a_i into the vector w'' and bias b. Period is missing. \n\n- On page 2 ``... illustrate the preimage for the case of points on the lines ... respectively'' \nPlease indicate which is which.  \n\n- In Figure 1. Is this a sketch, or the actual illustration of a network. In the latter case, please state the specific value of x and the weights that are depicted. Also define and explain the arrows precisely. \nWhat are the arrows in the gray part? \n\n- On page 3 `` This means that the preimage is just the point x^{(l)}''  the points that W maps to x^{(l+1)}. \n\n- On page 3 the first display equation. There is an index i on the left but not on the right hand side. \nThe quantifier in the right hand side is not clear. \n\n- ``generated by the mapping ... w^i '' subscript\n\n- ``get mapped to this hyperplane'' to zero \n\n- ``remaining'' remaining from what? \n\n- ``using e.g. Grassmann-Cayley algebra'' \nHow about using elementary linear algebra?!\n\n- ``gives rise to a linear manifold with dimension one lower at each intersection'' \nThis holds if the hyperplanes are in general position. \n\n- ``is complete in the input space'' forms a basis \n\n- ``remaining kernel'' remaining from what? \n\n- ``kernel'' Here kernel is referring to nullspace or to a matrix of orthonormal basis vectors of the nullspace, or to what specifically? \n\n- Figure 3. Nullspaces of linear maps should pass through the origin. \n\n- `` from pairwise intersections'' \\cap \n\n- ``indicated as arrows or the shaded area'' this description is far from clear. \n\n- typos: peieces, diminsions, netork, me, \n \n\n"
  },
  {
    "people": [
      "Cho"
    ],
    "review": "I have not read the revised version in detail yet. \n\nSUMMARY \nThis paper studies the preimages of outputs of a feedforward neural network with ReLUs. \n\nPROS \nThe paper presents a neat idea for changes of coordinates at the individual layers. \n\nCONS \nQuite unpolished / not enough contributions for a finished paper. \n\nCOMMENTS \n- In the first version the paper contains many typos and appears to be still quite unpolished. \n\n- The paper contains nice ideas but in my opinion it does not contribute sufficiently many results for a Conference paper. \nI would be happy to recommend for the Workshop track. \n\n- Irreversibly mixed and several other notions from the present paper are closely related to the concepts discussed in [Montufar, Pascanu, Cho, Bengio, NIPS 2014]. I feel that that paper should be cited here and the connections should be discussed. In particular, that paper also contains a discussion on the local linear maps of ReLU networks. \n\n- I am curious about the practical considerations when computing the pre-images. The definition should be rather straight forward really, but the implementation / computation could be troublesome. \n\n\nDETAILED COMMENTS \n- On page 1 ``can easily be shown to be many to one'' in general. \n\n- On page 2 ``For each point x^{l+1}'' The parentheses in the superscript are missing. \n\n- After eq. 6 ``the mapping is unique''  is missing `when w1 and w2 are linearly independent' \n\n- Eq. 1 should be a vector. \n\n- Above eq. 3. ``collected the weights a_i into the vector w'' and bias b. Period is missing. \n\n- On page 2 ``... illustrate the preimage for the case of points on the lines ... respectively'' \nPlease indicate which is which.  \n\n- In Figure 1. Is this a sketch, or the actual illustration of a network. In the latter case, please state the specific value of x and the weights that are depicted. Also define and explain the arrows precisely. \nWhat are the arrows in the gray part? \n\n- On page 3 `` This means that the preimage is just the point x^{(l)}''  the points that W maps to x^{(l+1)}. \n\n- On page 3 the first display equation. There is an index i on the left but not on the right hand side. \nThe quantifier in the right hand side is not clear. \n\n- ``generated by the mapping ... w^i '' subscript\n\n- ``get mapped to this hyperplane'' to zero \n\n- ``remaining'' remaining from what? \n\n- ``using e.g. Grassmann-Cayley algebra'' \nHow about using elementary linear algebra?!\n\n- ``gives rise to a linear manifold with dimension one lower at each intersection'' \nThis holds if the hyperplanes are in general position. \n\n- ``is complete in the input space'' forms a basis \n\n- ``remaining kernel'' remaining from what? \n\n- ``kernel'' Here kernel is referring to nullspace or to a matrix of orthonormal basis vectors of the nullspace, or to what specifically? \n\n- Figure 3. Nullspaces of linear maps should pass through the origin. \n\n- `` from pairwise intersections'' \\cap \n\n- ``indicated as arrows or the shaded area'' this description is far from clear. \n\n- typos: peieces, diminsions, netork, me, \n \n\n"
  },
  {
    "people": [
      "Ilya Sutskever",
      "Gulcehre",
      "Caglar"
    ],
    "review": "Summary: This paper proposes a read-again attention-based representation of the document with the copy mechanism for the summarization task. The model reads each sentence in the input document twice and creates a hierarchical representation of it instead of a bidirectional RNN. During the decoding, it uses the representation of the document obtained via the read-again mechanism and points the words that are OOV in the source document. The model does abstractive summarization. The authors show improvements on DUC 2004 dataset and provide an analysis of their model with different configurations.\n\nContributions:\nThe main contribution of this paper is the read-again attention mechanism where the model reads the same sentence twice and obtains a better representation of the document.\n\nWriting:\nThe text of this paper needs more work. There are several typos and the explanations of the model/architecture are not really clear, some parts of the paper feel somewhat bloated. \n\nPros:\n- The proposed model is a simple extension to the model to the model proposed in [2] for summarization.\n- The results are better than the baselines.\n\nCons:\n- The improvements are not that large.\n- Justifications are not strong enough.\n- The paper needs a better writeup. Several parts of the text are not using a clear/precise language and the paper needs a better reorganization. Some parts of the text is somewhat informal.\n- The paper is very application oriented.\n\nQuestion:\n- How does the training speed when compared to the regular LSTM?\n\nSome Criticisms:\n\nA similar approach to the read again mechanism which is proposed in this paper has already been explored in [1] in the context of algorithmic learning and I wouldn\u2019t consider the application of that on the summarization task a significant contribution.  The justification behind the read-again mechanism proposed in this paper is very weak. It is not really clear why additional gating alpha_i is needed for the read again stage.\nAs authors also suggest, pointer mechanism for the unknown/rare words [2] and it is adopted for the read-again attention mechanism. However, in the paper, it is not clear where the real is the gain coming from, whether from \u201cread-again\u201d mechanism or the use of \u201cpointing\u201d. \nThe paper is very application focused, the contributions of the paper in terms of ML point of view is very weak.\nIt is possible to try this read-again mechanism on more tasks other than summarization, such as NMT, in order to see whether if those improvements are \nThe writing of this paper needs more work. In general, it is not very well-written. \n\nMinor comments:\n\nSome of the corrections that I would recommend fixing,\n\nOn page 4: \u201c\u2026 better than a single value \u2026 \u201d \u2014> \u201c\u2026 scalar gating \u2026\u201d\nOn page 4: \u201c\u2026 single value lacks the ability to model the variances among these dimensions.\u201d \u2014> \u201c\u2026 scalar gating couldn\u2019t capture the \u2026.\u201d\nOn page 6: \u201c \u2026 where h_0^2 and h_0^'2 are initial zero vectors \u2026 \u201c \u2014> \u201c\u2026 h_0^2 and h_0^'2 are initialized to a zero vector in the beginning of each sequence \u2026\"\n\nThere are some inconsistencies for example parts of the paper refer to Tab. 1 and some parts of the paper refer to Table 2.\n\nBetter naming of the models in Table 1 is needed.\nThe location of Table 1 is a bit off.\n\n[1] Zaremba, Wojciech, and Ilya Sutskever. \"Reinforcement learning neural Turing machines.\" arXiv preprint arXiv:1505.00521 362 (2015). \n[2] Gulcehre, Caglar, et al. \"Pointing the Unknown Words.\" arXiv preprint arXiv:1603.08148 (2016).\n"
  },
  {
    "people": [
      "Parikh"
    ],
    "review": "This paper proposed two incremental ideas to extend the current state-of-the-art summarization work based on seq2seq models with attention and copy/pointer mechanisms.\n\n1. This paper introduces 2-pass reading where the representations from the 1st-pass is used to  re-wight the contribution of each word to the sequential representation of the 2nd-pass. The authors described how such a so-called read-again process applies to both GRU and LSTM.\n \n2. On the decoder side, the authors also use the softmax to choose between generating from decoder vocabulary and copying a source position, with a new twist of representing the previous decoded word Y_{t-1} differently. This allows the author to explore a smaller decoder vocabulary hence led to faster inference time without losing summarization performance.\n \nThis paper claims the new state-of-the-art on DUC2004 but the comparison on Gigaword seems to be incomplete (missing more recent results after Rush 2015 etc). While the overall work is solid, there are also other things missing scientifically. For example, \n- how much computational costs does the 2nd pass reading add to the end-to-end system? \n- How does the decoder small vocabulary trick work without 2nd-pass reading on the encoder side for both summarization performance and runtime speed?\n- There are other ways to improve the embedding of a sentence. How does the 2nd-pass reading compare to recent work from multiple authors on self-attention and/or LSTMN? For example, Cheng et al. 2016, Long Short-Term Memory-Networks for Machine Reading; Parikh et al. 2016, A Decomposable Attention Model for Natural Language Inference?"
  },
  {
    "people": [
      "Ilya Sutskever",
      "Gulcehre",
      "Caglar"
    ],
    "review": "Summary: This paper proposes a read-again attention-based representation of the document with the copy mechanism for the summarization task. The model reads each sentence in the input document twice and creates a hierarchical representation of it instead of a bidirectional RNN. During the decoding, it uses the representation of the document obtained via the read-again mechanism and points the words that are OOV in the source document. The model does abstractive summarization. The authors show improvements on DUC 2004 dataset and provide an analysis of their model with different configurations.\n\nContributions:\nThe main contribution of this paper is the read-again attention mechanism where the model reads the same sentence twice and obtains a better representation of the document.\n\nWriting:\nThe text of this paper needs more work. There are several typos and the explanations of the model/architecture are not really clear, some parts of the paper feel somewhat bloated. \n\nPros:\n- The proposed model is a simple extension to the model to the model proposed in [2] for summarization.\n- The results are better than the baselines.\n\nCons:\n- The improvements are not that large.\n- Justifications are not strong enough.\n- The paper needs a better writeup. Several parts of the text are not using a clear/precise language and the paper needs a better reorganization. Some parts of the text is somewhat informal.\n- The paper is very application oriented.\n\nQuestion:\n- How does the training speed when compared to the regular LSTM?\n\nSome Criticisms:\n\nA similar approach to the read again mechanism which is proposed in this paper has already been explored in [1] in the context of algorithmic learning and I wouldn\u2019t consider the application of that on the summarization task a significant contribution.  The justification behind the read-again mechanism proposed in this paper is very weak. It is not really clear why additional gating alpha_i is needed for the read again stage.\nAs authors also suggest, pointer mechanism for the unknown/rare words [2] and it is adopted for the read-again attention mechanism. However, in the paper, it is not clear where the real is the gain coming from, whether from \u201cread-again\u201d mechanism or the use of \u201cpointing\u201d. \nThe paper is very application focused, the contributions of the paper in terms of ML point of view is very weak.\nIt is possible to try this read-again mechanism on more tasks other than summarization, such as NMT, in order to see whether if those improvements are \nThe writing of this paper needs more work. In general, it is not very well-written. \n\nMinor comments:\n\nSome of the corrections that I would recommend fixing,\n\nOn page 4: \u201c\u2026 better than a single value \u2026 \u201d \u2014> \u201c\u2026 scalar gating \u2026\u201d\nOn page 4: \u201c\u2026 single value lacks the ability to model the variances among these dimensions.\u201d \u2014> \u201c\u2026 scalar gating couldn\u2019t capture the \u2026.\u201d\nOn page 6: \u201c \u2026 where h_0^2 and h_0^'2 are initial zero vectors \u2026 \u201c \u2014> \u201c\u2026 h_0^2 and h_0^'2 are initialized to a zero vector in the beginning of each sequence \u2026\"\n\nThere are some inconsistencies for example parts of the paper refer to Tab. 1 and some parts of the paper refer to Table 2.\n\nBetter naming of the models in Table 1 is needed.\nThe location of Table 1 is a bit off.\n\n[1] Zaremba, Wojciech, and Ilya Sutskever. \"Reinforcement learning neural Turing machines.\" arXiv preprint arXiv:1505.00521 362 (2015). \n[2] Gulcehre, Caglar, et al. \"Pointing the Unknown Words.\" arXiv preprint arXiv:1603.08148 (2016).\n"
  },
  {
    "people": [
      "Parikh"
    ],
    "review": "This paper proposed two incremental ideas to extend the current state-of-the-art summarization work based on seq2seq models with attention and copy/pointer mechanisms.\n\n1. This paper introduces 2-pass reading where the representations from the 1st-pass is used to  re-wight the contribution of each word to the sequential representation of the 2nd-pass. The authors described how such a so-called read-again process applies to both GRU and LSTM.\n \n2. On the decoder side, the authors also use the softmax to choose between generating from decoder vocabulary and copying a source position, with a new twist of representing the previous decoded word Y_{t-1} differently. This allows the author to explore a smaller decoder vocabulary hence led to faster inference time without losing summarization performance.\n \nThis paper claims the new state-of-the-art on DUC2004 but the comparison on Gigaword seems to be incomplete (missing more recent results after Rush 2015 etc). While the overall work is solid, there are also other things missing scientifically. For example, \n- how much computational costs does the 2nd pass reading add to the end-to-end system? \n- How does the decoder small vocabulary trick work without 2nd-pass reading on the encoder side for both summarization performance and runtime speed?\n- There are other ways to improve the embedding of a sentence. How does the 2nd-pass reading compare to recent work from multiple authors on self-attention and/or LSTMN? For example, Cheng et al. 2016, Long Short-Term Memory-Networks for Machine Reading; Parikh et al. 2016, A Decomposable Attention Model for Natural Language Inference?"
  },
  {
    "people": [
      "Newton",
      "Kober"
    ],
    "review": "This paper presents iterative PoWER, an off-policy variation on PoWER, a policy gradient algorithm in the reward-weighted family.\n\nI'm not familiar enough with this type lower bound scheme to comment on it. It looks like the end result is less conservative step sizes in policy parameter space. All expectation-based algorithms (and their KL-regularized cousins a-la TRPO) take smallish steps, and this might be a sensible way to accelerate them.\n\nThe description of the experiments in Section VI is insufficient for reproducibility. Is \"The cart moved right\" supposed to be \"a positive force is applied to the cart\"? How is negative force applied? What is the representation of the state? What is the distribution of initial states? A linear policy is insufficient for swing up and balance of a cart-pole. Are you only doing balancing? What is the noise magnitude of the policy? How was it chosen? How long were the episodes?\n\nThe footnote at the bottom of page 8 threw me off. If you're using Newton's method, where is the discussion of gradients and Hessians? I thought the argmax_theta operator was a stand-in for an EM-style step, which I how I read Eq (8) in the Kober paper."
  },
  {
    "people": [
      "Newton",
      "Kober"
    ],
    "review": "This paper presents iterative PoWER, an off-policy variation on PoWER, a policy gradient algorithm in the reward-weighted family.\n\nI'm not familiar enough with this type lower bound scheme to comment on it. It looks like the end result is less conservative step sizes in policy parameter space. All expectation-based algorithms (and their KL-regularized cousins a-la TRPO) take smallish steps, and this might be a sensible way to accelerate them.\n\nThe description of the experiments in Section VI is insufficient for reproducibility. Is \"The cart moved right\" supposed to be \"a positive force is applied to the cart\"? How is negative force applied? What is the representation of the state? What is the distribution of initial states? A linear policy is insufficient for swing up and balance of a cart-pole. Are you only doing balancing? What is the noise magnitude of the policy? How was it chosen? How long were the episodes?\n\nThe footnote at the bottom of page 8 threw me off. If you're using Newton's method, where is the discussion of gradients and Hessians? I thought the argmax_theta operator was a stand-in for an EM-style step, which I how I read Eq (8) in the Kober paper. \n"
  },
  {
    "people": [
      "Newton",
      "Kober"
    ],
    "review": "This paper presents iterative PoWER, an off-policy variation on PoWER, a policy gradient algorithm in the reward-weighted family.\n\nI'm not familiar enough with this type lower bound scheme to comment on it. It looks like the end result is less conservative step sizes in policy parameter space. All expectation-based algorithms (and their KL-regularized cousins a-la TRPO) take smallish steps, and this might be a sensible way to accelerate them.\n\nThe description of the experiments in Section VI is insufficient for reproducibility. Is \"The cart moved right\" supposed to be \"a positive force is applied to the cart\"? How is negative force applied? What is the representation of the state? What is the distribution of initial states? A linear policy is insufficient for swing up and balance of a cart-pole. Are you only doing balancing? What is the noise magnitude of the policy? How was it chosen? How long were the episodes?\n\nThe footnote at the bottom of page 8 threw me off. If you're using Newton's method, where is the discussion of gradients and Hessians? I thought the argmax_theta operator was a stand-in for an EM-style step, which I how I read Eq (8) in the Kober paper."
  },
  {
    "people": [
      "Newton",
      "Kober"
    ],
    "review": "This paper presents iterative PoWER, an off-policy variation on PoWER, a policy gradient algorithm in the reward-weighted family.\n\nI'm not familiar enough with this type lower bound scheme to comment on it. It looks like the end result is less conservative step sizes in policy parameter space. All expectation-based algorithms (and their KL-regularized cousins a-la TRPO) take smallish steps, and this might be a sensible way to accelerate them.\n\nThe description of the experiments in Section VI is insufficient for reproducibility. Is \"The cart moved right\" supposed to be \"a positive force is applied to the cart\"? How is negative force applied? What is the representation of the state? What is the distribution of initial states? A linear policy is insufficient for swing up and balance of a cart-pole. Are you only doing balancing? What is the noise magnitude of the policy? How was it chosen? How long were the episodes?\n\nThe footnote at the bottom of page 8 threw me off. If you're using Newton's method, where is the discussion of gradients and Hessians? I thought the argmax_theta operator was a stand-in for an EM-style step, which I how I read Eq (8) in the Kober paper. \n"
  },
  {
    "people": [
      "Lake"
    ],
    "review": "The primary contribution of this paper is showing that k-nearest-neighbor method based memory can be usefully incorporated in a variety of architectures and supervised learning tasks. The presentation is clear, and results are good. I like the synthetic task and analysis. For the Omniglot task, running and reporting results on the original splits used by Lake would be good, as the splits used in matching nets are considerably easier and result in ceiling effects."
  },
  {
    "people": [
      "Rae",
      "Chandar",
      "Charles Blundell",
      "Benigno Uria",
      "Yazhe Li",
      "Avraham Ruderman",
      "Joel Z. Leibo",
      "Jack Rae"
    ],
    "review": "This paper proposes a new memory module for large scale life-long and one-shot learning. The module is general enough that the authors apply the module to several neural network architectures and show improvements in performance.\n\nUsing k-nearest neighbors for memory access is not completely new. This has been recently explored in Rae et al., 2016 and Chandar et al., 2016. K-nearest neighbors based memory for one-shot learning has also been explored in [R1]. This paper provides experimental evidence that such an approach can be applied to a variety of architectures.\n\nAuthors have addressed all my pre-review questions and I am ok with their response.\n\nAre the authors willing to release the source code to reproduce the results? At least for omniglot experiments and synthetic task experiments?\n\nReferences:\n\n[R1] Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z. Leibo, Jack Rae, Daan Wierstra, Demis Hassabis: Model-Free Episodic Control. CoRR abs/1606.04460 (2016)\n"
  },
  {
    "people": [
      "Lake"
    ],
    "review": "The primary contribution of this paper is showing that k-nearest-neighbor method based memory can be usefully incorporated in a variety of architectures and supervised learning tasks. The presentation is clear, and results are good. I like the synthetic task and analysis. For the Omniglot task, running and reporting results on the original splits used by Lake would be good, as the splits used in matching nets are considerably easier and result in ceiling effects."
  },
  {
    "people": [
      "Rae",
      "Chandar",
      "Charles Blundell",
      "Benigno Uria",
      "Yazhe Li",
      "Avraham Ruderman",
      "Joel Z. Leibo",
      "Jack Rae"
    ],
    "review": "This paper proposes a new memory module for large scale life-long and one-shot learning. The module is general enough that the authors apply the module to several neural network architectures and show improvements in performance.\n\nUsing k-nearest neighbors for memory access is not completely new. This has been recently explored in Rae et al., 2016 and Chandar et al., 2016. K-nearest neighbors based memory for one-shot learning has also been explored in [R1]. This paper provides experimental evidence that such an approach can be applied to a variety of architectures.\n\nAuthors have addressed all my pre-review questions and I am ok with their response.\n\nAre the authors willing to release the source code to reproduce the results? At least for omniglot experiments and synthetic task experiments?\n\nReferences:\n\n[R1] Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z. Leibo, Jack Rae, Daan Wierstra, Demis Hassabis: Model-Free Episodic Control. CoRR abs/1606.04460 (2016)\n"
  },
  {
    "people": [
      "Wenzel",
      "Ay"
    ],
    "review": "SUMMARY \nThis paper presents a study of the number of hidden units and training examples needed to learn functions from a particular class. \nThis class is defined as those Boolean functions with an upper bound on the variability of the outputs. \n\nPROS\nThe paper promotes interesting results from the theoretical computer science community to investigate the efficiency of representation of functions with limited variability in terms of shallow feedforward networks with linear threshold units. \n\nCONS \nThe analysis is limited to shallow networks. The analysis is based on piecing together interesting results, however without contributing significant innovations. \nThe presentation of the main results and conclusions is somewhat obscure, as the therein appearing terms/constants do not express a clear relation between increased robustness and decreasing number of required hidden units. \n\nCOMMENTS \n\n- In the abstract one reads ``The universal approximation theorem for neural networks says that any reasonable function is well-approximated by a two-layer neural network with sigmoid gates but it does not provide good bounds on the number of hidden-layer nodes or the weights.'' \n\nIn page 1 the paper points the reader to a review article. It could be a good idea to include also more recent references. \n\nGiven the motivation presented in the abstract of the paper it would be a good idea to also comment of works discussing the classes of Boolean functions representable by linear threshold networks. \nFor instance the paper [Hyperplane Arrangements Separating Arbitrary Vertex Classes in n-Cubes. Wenzel, Ay, Paseman] discusses various classes of functions that can be represented by shallow linear threshold networks and provides upper and lower bounds on the number of hidden units needed for representing various types of Boolean functions. In particular that paper also provides lower bounds on the number of hidden units needed to define a universal approximator. \n\n- It certainly would be a good idea to discuss the results on the learning complexity in terms of measures such as the VC-dimension. \n\n- Thank you for the explanations regarding the constants.  \nSo if the noise sensitivity is kept constant, larger values of epsilon are associated with a smaller value of delta and of 1/epsilon. \nNonetheless, the description in Theorem 2 is in terms of poly(1/epsilon, 1/delta), which still could increase? \nAlso, in Lemma 1 reducing the sensitivity at a constant noise increases the bound on k? \n\n- The fact that the descriptions are independent of n seems to be related to the definition of the noise sensitivity as an expectation over all inputs. This certainly deserves more discussion. One good start could be to discuss examples of functions with an upper bound on the noise sensitivity (aside from the linear threshold functions discussed in Lemma 2). \nAlso, reverse statements to Lemma 1 would be interesting, describing the noise sensitivity of juntas specifically, even if only as simple examples. \n\n- On page 3 ``...variables is polynomial in the noise-sensitivity parameters'' should be inverse of?\n\nMINOR COMMENTS\n\nOn page 5 Proposition 1 should be Lemma 1? \n"
  },
  {
    "people": [
      "Bourgain"
    ],
    "review": "This work finds a connection between Bourgain's junta problem, the existing results in circuit complexity, and the approximation of a boolean function using two-layer neural net. I think that finding connections between different fields and applying the insights gained is a valid contribution. For this reason, I recommend acceptance.\n\nBut my current major concern is that this work is only constrained to the domain of boolean hypercube, which is far from what is done in practice (continuous domain). Indeed, the authors could argue that understanding the former is a first step, but if the connection is only suitable for this case and not adaptable to more general scenarios, then it probably would have limited interest."
  },
  {
    "people": [
      "Wenzel",
      "Ay"
    ],
    "review": "SUMMARY \nThis paper presents a study of the number of hidden units and training examples needed to learn functions from a particular class. \nThis class is defined as those Boolean functions with an upper bound on the variability of the outputs. \n\nPROS\nThe paper promotes interesting results from the theoretical computer science community to investigate the efficiency of representation of functions with limited variability in terms of shallow feedforward networks with linear threshold units. \n\nCONS \nThe analysis is limited to shallow networks. The analysis is based on piecing together interesting results, however without contributing significant innovations. \nThe presentation of the main results and conclusions is somewhat obscure, as the therein appearing terms/constants do not express a clear relation between increased robustness and decreasing number of required hidden units. \n\nCOMMENTS \n\n- In the abstract one reads ``The universal approximation theorem for neural networks says that any reasonable function is well-approximated by a two-layer neural network with sigmoid gates but it does not provide good bounds on the number of hidden-layer nodes or the weights.'' \n\nIn page 1 the paper points the reader to a review article. It could be a good idea to include also more recent references. \n\nGiven the motivation presented in the abstract of the paper it would be a good idea to also comment of works discussing the classes of Boolean functions representable by linear threshold networks. \nFor instance the paper [Hyperplane Arrangements Separating Arbitrary Vertex Classes in n-Cubes. Wenzel, Ay, Paseman] discusses various classes of functions that can be represented by shallow linear threshold networks and provides upper and lower bounds on the number of hidden units needed for representing various types of Boolean functions. In particular that paper also provides lower bounds on the number of hidden units needed to define a universal approximator. \n\n- It certainly would be a good idea to discuss the results on the learning complexity in terms of measures such as the VC-dimension. \n\n- Thank you for the explanations regarding the constants.  \nSo if the noise sensitivity is kept constant, larger values of epsilon are associated with a smaller value of delta and of 1/epsilon. \nNonetheless, the description in Theorem 2 is in terms of poly(1/epsilon, 1/delta), which still could increase? \nAlso, in Lemma 1 reducing the sensitivity at a constant noise increases the bound on k? \n\n- The fact that the descriptions are independent of n seems to be related to the definition of the noise sensitivity as an expectation over all inputs. This certainly deserves more discussion. One good start could be to discuss examples of functions with an upper bound on the noise sensitivity (aside from the linear threshold functions discussed in Lemma 2). \nAlso, reverse statements to Lemma 1 would be interesting, describing the noise sensitivity of juntas specifically, even if only as simple examples. \n\n- On page 3 ``...variables is polynomial in the noise-sensitivity parameters'' should be inverse of?\n\nMINOR COMMENTS\n\nOn page 5 Proposition 1 should be Lemma 1? \n"
  },
  {
    "people": [
      "Bourgain"
    ],
    "review": "This work finds a connection between Bourgain's junta problem, the existing results in circuit complexity, and the approximation of a boolean function using two-layer neural net. I think that finding connections between different fields and applying the insights gained is a valid contribution. For this reason, I recommend acceptance.\n\nBut my current major concern is that this work is only constrained to the domain of boolean hypercube, which is far from what is done in practice (continuous domain). Indeed, the authors could argue that understanding the former is a first step, but if the connection is only suitable for this case and not adaptable to more general scenarios, then it probably would have limited interest."
  },
  {
    "people": [
      "Yoshua Bengio's",
      "Bengio",
      "Ducharme",
      "Vincent",
      "Jauvin",
      "Mnih",
      "Hinton"
    ],
    "review": "I just wanted to point out that is used to be common to use the same embedding matrix for the input and target words in a neural language model. For example, the \"cycling architecture\" proposed in Yoshua Bengio's first language modelling paper [1] does that, as do all three models proposed in [2].\n\n[1] Bengio, Y., Ducharme, R., Vincent, P. and Jauvin, C. A Neural Probabilistic Language Model. NIPS 2000\n[2] Mnih, A., & Hinton, G. Three new graphical models for statistical language modelling. ICML 2007"
  },
  {
    "people": [
      "Yoshua Bengio's",
      "Bengio",
      "Ducharme",
      "Vincent",
      "Jauvin",
      "Mnih",
      "Hinton"
    ],
    "review": "I just wanted to point out that is used to be common to use the same embedding matrix for the input and target words in a neural language model. For example, the \"cycling architecture\" proposed in Yoshua Bengio's first language modelling paper [1] does that, as do all three models proposed in [2].\n\n[1] Bengio, Y., Ducharme, R., Vincent, P. and Jauvin, C. A Neural Probabilistic Language Model. NIPS 2000\n[2] Mnih, A., & Hinton, G. Three new graphical models for statistical language modelling. ICML 2007"
  },
  {
    "people": [
      "Wang",
      "Boltzmann"
    ],
    "review": "This paper shows how policy gradient and Q-Learning may be combined together, improving learning as demonstrated in particular in the Atari Learning Environment. The core idea is to note that entropy-regularized policy gradient leads to a Boltzmann policy based on Q values, thus linking pi & Q together and allowing both policy gradient and Q-Learning updates to be applied.\n\nI think this is a very interesting paper, not just for its results and the proposed algorithm (dubbed PGQ), but mostly because of the links it draws between several techniques, which I found quite insightful.\n\nThat being said, I also believe it could have done a better job at clearly exposing these links: I found it somewhat difficult to follow, and it took me a while to wrap my head around it, even though the underlying concepts are not that complex. In particular:\n- The notation \\tilde{Q}^pi is introduced in a way that is not very clear, as \"an estimate of the Q-values\" while eq. 5 is an exact equality (no estimation)\n- It is not clear to me what section 3.2 is bringing exactly, I wonder if it could just be removed to expand some other sections with more explanations.\n- The links to dueling networks (Wang et al, 2016) are in my opinion not explicit and detailed enough (in 3.3 & 4.1): as far as I can tell the proposed architecture ends up being very similar to such networks and thus it would be worth telling more about it (also in experiments my understanding is that the \"variant of asynchronous deep Q-learning\" being used is essentially such a dueling network, but it is not clearly stated). I also believe it should be mentioned that PGQ can also be seen as combining Q-Learning with n-step expected Sarsa using a dueling network: this kind of example helps better understand the links between methods\n- Overall I wish section 3.3 was clearer, as it draws some very interesting links, but it is hard to see where this is all going when reading the paper for the first time. One confusing point is w.r.t. to the relationship with section 3.2, that assumes a critic outputting Q values while in 3.3 the critic outputs V. The \"mu\" distribution also comes somewhat out of nowhere.\n\nI hope the authors can try and improve the readability of the paper in a final version, as well as clarify the points raised in pre-review questions (in particular related to experimental details, the derivation of eq. 4, and the issue of the discounted distribution of states).\n\nMinor remarks:\n- The function r(s, a) used in the Bellman equation in section 2 is not formally defined. It looks a bit weird because the expectation is on s' and b' but r(s, a) does not depend on them (so either it should be moved out of the expectation, or the expectation should also be over the reward, depending on how r is defined)\n- The definition of the Boltzmann policy at end of 2.1 is a bit confusing since there is a sum over \"a\" of a quantity that does not depend (clearly) on \"a\"\n- I believe 4.3 is for the tabular case but this is not clearly stated\n- Any idea why in Fig. 1 the 3 algorithms do not all converge to the same policy? In such a simple toy setting I would expect it to be the case.\n\nTypos:\n- \"we refer to the classic text Sutton & Barto (1998)\" => missing \"by\"?\n- \"Online policy gradient typically require an estimate of the action-values function\" => requires & value\n- \"the agent generates experience from interacting the environment\" => with the environment\n- in eq. 12 (first line) there is a comma to remove near the end, just before the dlog pi\n- \"allowing us the spread the influence of a reward\" => to spread\n- \"in the off-policy case tabular case\" => remove the first case\n- \"The green line is Q-learning where at the step an update is performed\" => at each step\n- In Fig. 2 it says A2C instead of A3C\n\nNB: I did not have time to carefully read Appendix A"
  },
  {
    "people": [
      "Wang",
      "Boltzmann"
    ],
    "review": "This paper shows how policy gradient and Q-Learning may be combined together, improving learning as demonstrated in particular in the Atari Learning Environment. The core idea is to note that entropy-regularized policy gradient leads to a Boltzmann policy based on Q values, thus linking pi & Q together and allowing both policy gradient and Q-Learning updates to be applied.\n\nI think this is a very interesting paper, not just for its results and the proposed algorithm (dubbed PGQ), but mostly because of the links it draws between several techniques, which I found quite insightful.\n\nThat being said, I also believe it could have done a better job at clearly exposing these links: I found it somewhat difficult to follow, and it took me a while to wrap my head around it, even though the underlying concepts are not that complex. In particular:\n- The notation \\tilde{Q}^pi is introduced in a way that is not very clear, as \"an estimate of the Q-values\" while eq. 5 is an exact equality (no estimation)\n- It is not clear to me what section 3.2 is bringing exactly, I wonder if it could just be removed to expand some other sections with more explanations.\n- The links to dueling networks (Wang et al, 2016) are in my opinion not explicit and detailed enough (in 3.3 & 4.1): as far as I can tell the proposed architecture ends up being very similar to such networks and thus it would be worth telling more about it (also in experiments my understanding is that the \"variant of asynchronous deep Q-learning\" being used is essentially such a dueling network, but it is not clearly stated). I also believe it should be mentioned that PGQ can also be seen as combining Q-Learning with n-step expected Sarsa using a dueling network: this kind of example helps better understand the links between methods\n- Overall I wish section 3.3 was clearer, as it draws some very interesting links, but it is hard to see where this is all going when reading the paper for the first time. One confusing point is w.r.t. to the relationship with section 3.2, that assumes a critic outputting Q values while in 3.3 the critic outputs V. The \"mu\" distribution also comes somewhat out of nowhere.\n\nI hope the authors can try and improve the readability of the paper in a final version, as well as clarify the points raised in pre-review questions (in particular related to experimental details, the derivation of eq. 4, and the issue of the discounted distribution of states).\n\nMinor remarks:\n- The function r(s, a) used in the Bellman equation in section 2 is not formally defined. It looks a bit weird because the expectation is on s' and b' but r(s, a) does not depend on them (so either it should be moved out of the expectation, or the expectation should also be over the reward, depending on how r is defined)\n- The definition of the Boltzmann policy at end of 2.1 is a bit confusing since there is a sum over \"a\" of a quantity that does not depend (clearly) on \"a\"\n- I believe 4.3 is for the tabular case but this is not clearly stated\n- Any idea why in Fig. 1 the 3 algorithms do not all converge to the same policy? In such a simple toy setting I would expect it to be the case.\n\nTypos:\n- \"we refer to the classic text Sutton & Barto (1998)\" => missing \"by\"?\n- \"Online policy gradient typically require an estimate of the action-values function\" => requires & value\n- \"the agent generates experience from interacting the environment\" => with the environment\n- in eq. 12 (first line) there is a comma to remove near the end, just before the dlog pi\n- \"allowing us the spread the influence of a reward\" => to spread\n- \"in the off-policy case tabular case\" => remove the first case\n- \"The green line is Q-learning where at the step an update is performed\" => at each step\n- In Fig. 2 it says A2C instead of A3C\n\nNB: I did not have time to carefully read Appendix A"
  },
  {
    "people": [
      "Wang",
      "Boltzmann"
    ],
    "review": "This paper shows how policy gradient and Q-Learning may be combined together, improving learning as demonstrated in particular in the Atari Learning Environment. The core idea is to note that entropy-regularized policy gradient leads to a Boltzmann policy based on Q values, thus linking pi & Q together and allowing both policy gradient and Q-Learning updates to be applied.\n\nI think this is a very interesting paper, not just for its results and the proposed algorithm (dubbed PGQ), but mostly because of the links it draws between several techniques, which I found quite insightful.\n\nThat being said, I also believe it could have done a better job at clearly exposing these links: I found it somewhat difficult to follow, and it took me a while to wrap my head around it, even though the underlying concepts are not that complex. In particular:\n- The notation \\tilde{Q}^pi is introduced in a way that is not very clear, as \"an estimate of the Q-values\" while eq. 5 is an exact equality (no estimation)\n- It is not clear to me what section 3.2 is bringing exactly, I wonder if it could just be removed to expand some other sections with more explanations.\n- The links to dueling networks (Wang et al, 2016) are in my opinion not explicit and detailed enough (in 3.3 & 4.1): as far as I can tell the proposed architecture ends up being very similar to such networks and thus it would be worth telling more about it (also in experiments my understanding is that the \"variant of asynchronous deep Q-learning\" being used is essentially such a dueling network, but it is not clearly stated). I also believe it should be mentioned that PGQ can also be seen as combining Q-Learning with n-step expected Sarsa using a dueling network: this kind of example helps better understand the links between methods\n- Overall I wish section 3.3 was clearer, as it draws some very interesting links, but it is hard to see where this is all going when reading the paper for the first time. One confusing point is w.r.t. to the relationship with section 3.2, that assumes a critic outputting Q values while in 3.3 the critic outputs V. The \"mu\" distribution also comes somewhat out of nowhere.\n\nI hope the authors can try and improve the readability of the paper in a final version, as well as clarify the points raised in pre-review questions (in particular related to experimental details, the derivation of eq. 4, and the issue of the discounted distribution of states).\n\nMinor remarks:\n- The function r(s, a) used in the Bellman equation in section 2 is not formally defined. It looks a bit weird because the expectation is on s' and b' but r(s, a) does not depend on them (so either it should be moved out of the expectation, or the expectation should also be over the reward, depending on how r is defined)\n- The definition of the Boltzmann policy at end of 2.1 is a bit confusing since there is a sum over \"a\" of a quantity that does not depend (clearly) on \"a\"\n- I believe 4.3 is for the tabular case but this is not clearly stated\n- Any idea why in Fig. 1 the 3 algorithms do not all converge to the same policy? In such a simple toy setting I would expect it to be the case.\n\nTypos:\n- \"we refer to the classic text Sutton & Barto (1998)\" => missing \"by\"?\n- \"Online policy gradient typically require an estimate of the action-values function\" => requires & value\n- \"the agent generates experience from interacting the environment\" => with the environment\n- in eq. 12 (first line) there is a comma to remove near the end, just before the dlog pi\n- \"allowing us the spread the influence of a reward\" => to spread\n- \"in the off-policy case tabular case\" => remove the first case\n- \"The green line is Q-learning where at the step an update is performed\" => at each step\n- In Fig. 2 it says A2C instead of A3C\n\nNB: I did not have time to carefully read Appendix A"
  },
  {
    "people": [
      "Wang",
      "Boltzmann"
    ],
    "review": "This paper shows how policy gradient and Q-Learning may be combined together, improving learning as demonstrated in particular in the Atari Learning Environment. The core idea is to note that entropy-regularized policy gradient leads to a Boltzmann policy based on Q values, thus linking pi & Q together and allowing both policy gradient and Q-Learning updates to be applied.\n\nI think this is a very interesting paper, not just for its results and the proposed algorithm (dubbed PGQ), but mostly because of the links it draws between several techniques, which I found quite insightful.\n\nThat being said, I also believe it could have done a better job at clearly exposing these links: I found it somewhat difficult to follow, and it took me a while to wrap my head around it, even though the underlying concepts are not that complex. In particular:\n- The notation \\tilde{Q}^pi is introduced in a way that is not very clear, as \"an estimate of the Q-values\" while eq. 5 is an exact equality (no estimation)\n- It is not clear to me what section 3.2 is bringing exactly, I wonder if it could just be removed to expand some other sections with more explanations.\n- The links to dueling networks (Wang et al, 2016) are in my opinion not explicit and detailed enough (in 3.3 & 4.1): as far as I can tell the proposed architecture ends up being very similar to such networks and thus it would be worth telling more about it (also in experiments my understanding is that the \"variant of asynchronous deep Q-learning\" being used is essentially such a dueling network, but it is not clearly stated). I also believe it should be mentioned that PGQ can also be seen as combining Q-Learning with n-step expected Sarsa using a dueling network: this kind of example helps better understand the links between methods\n- Overall I wish section 3.3 was clearer, as it draws some very interesting links, but it is hard to see where this is all going when reading the paper for the first time. One confusing point is w.r.t. to the relationship with section 3.2, that assumes a critic outputting Q values while in 3.3 the critic outputs V. The \"mu\" distribution also comes somewhat out of nowhere.\n\nI hope the authors can try and improve the readability of the paper in a final version, as well as clarify the points raised in pre-review questions (in particular related to experimental details, the derivation of eq. 4, and the issue of the discounted distribution of states).\n\nMinor remarks:\n- The function r(s, a) used in the Bellman equation in section 2 is not formally defined. It looks a bit weird because the expectation is on s' and b' but r(s, a) does not depend on them (so either it should be moved out of the expectation, or the expectation should also be over the reward, depending on how r is defined)\n- The definition of the Boltzmann policy at end of 2.1 is a bit confusing since there is a sum over \"a\" of a quantity that does not depend (clearly) on \"a\"\n- I believe 4.3 is for the tabular case but this is not clearly stated\n- Any idea why in Fig. 1 the 3 algorithms do not all converge to the same policy? In such a simple toy setting I would expect it to be the case.\n\nTypos:\n- \"we refer to the classic text Sutton & Barto (1998)\" => missing \"by\"?\n- \"Online policy gradient typically require an estimate of the action-values function\" => requires & value\n- \"the agent generates experience from interacting the environment\" => with the environment\n- in eq. 12 (first line) there is a comma to remove near the end, just before the dlog pi\n- \"allowing us the spread the influence of a reward\" => to spread\n- \"in the off-policy case tabular case\" => remove the first case\n- \"The green line is Q-learning where at the step an update is performed\" => at each step\n- In Fig. 2 it says A2C instead of A3C\n\nNB: I did not have time to carefully read Appendix A"
  },
  {
    "people": [
      "Duvenaud",
      "Lusci"
    ],
    "review": "This work proposes a convolutional architecture for any graph-like input data (where the structure is example-dependent), or more generally, any data where the input dimensions that are related by a similarity matrix. If instead each input example is associated with a transition matrix, then a random walk algorithm is used generate a similarity matrix.\n\nDeveloping convolutional or recurrent architectures for graph-like data is an important problem because we would like to develop neural networks that can handle inputs such as molecule structures or social networks. However, I don't think this work contributes anything significant to the work that has already been done in this area. \n\nThe two main proposals I see in this paper are:\n1) For data associated with a transition matrix, this paper proposes that the transition matrix be converted to a similarity matrix. This seems obvious.\n2) For data associated with a similarity matrix, the k nearest neighbors of each node are computed and supply the context information for that node. This also seems obvious.\n\nPerhaps I have misunderstood the contribution, but the presentation also lacks clarity, and I cannot recommend this paper for publication. \n\nSpecific Comments:\n1) On page 4: \"An interesting attribute of this convolution, as compared to other convolutions on graphs is that, it preserves locality while still being applicable over different graphs with different structures.\"  This is false; the other proposed architectures can be applied to inputs with different structures (e.g. Duvenaud et. al., Lusci et. al. for NN architectures on molecules specifically)."
  },
  {
    "people": [
      "Defferrand",
      "Scarselli",
      "Bronstein"
    ],
    "review": "This work studies the problem of generalizing a convolutional neural network to data lacking grid-structure. \n \n The authors consider the Random Walk Normalized Laplacian and its finite powers to define a convolutional layer in a general graph. Experiments in Merck molecular discovery and mnist are reported. \n \n The reviewers all agreed that this paper, while presenting an interesting and important problem, lacks novelty relative to existing approaches. In particular, the AC would like to point out that important references seem to be missing from the current version. \n \n The proposed approach is closely related to 'Convolutional neural networks on graphs with fast localized spectral filtering', Defferrand et al. NIPS'16 , which considers Chevyshev polynomials of the Laplacian and learns the polynomial coefficients in an efficient manner. Since the Laplacian and the Random Walk Normalized Laplacian are similar operators (have same eigenvectors), the resulting model is essentially equivalent. Another related model that precedes all the cited works and is deeply related to the current submission is the Graph Neural Network from Scarselli et al.; see 'Geometric Deep Learning: going beyond Euclidean Data', Bronstein et al, "
  },
  {
    "people": [
      "Merk",
      "Coats"
    ],
    "review": "Previous literature uses data-derived adjacency matrix A to obtain neighbors to use as foundation of graph convolution. They propose extending the set of neighbors by additionally including nodes reachable by i<=k steps in this graph. This introduces an extra tunable parameter k, so it needs some justification over the previous k=1 solution. In one experiment provided\u00a0(Merk), using k=1 worked better. They don't specify which k that used, just that it was big enough for their to be p=5 nodes obtained as neighbors. In the second experiment (MNIST), they used k=1 for their experiments, which is what previous work (Coats & Ng 2011) proposed as well. A compelling experiment would compare to k=1 and show that using k>1 gives improvement strong enough to justify an extra hyper-parameter."
  },
  {
    "people": [
      "Duvenaud",
      "Lusci"
    ],
    "review": "This work proposes a convolutional architecture for any graph-like input data (where the structure is example-dependent), or more generally, any data where the input dimensions that are related by a similarity matrix. If instead each input example is associated with a transition matrix, then a random walk algorithm is used generate a similarity matrix.\n\nDeveloping convolutional or recurrent architectures for graph-like data is an important problem because we would like to develop neural networks that can handle inputs such as molecule structures or social networks. However, I don't think this work contributes anything significant to the work that has already been done in this area. \n\nThe two main proposals I see in this paper are:\n1) For data associated with a transition matrix, this paper proposes that the transition matrix be converted to a similarity matrix. This seems obvious.\n2) For data associated with a similarity matrix, the k nearest neighbors of each node are computed and supply the context information for that node. This also seems obvious.\n\nPerhaps I have misunderstood the contribution, but the presentation also lacks clarity, and I cannot recommend this paper for publication. \n\nSpecific Comments:\n1) On page 4: \"An interesting attribute of this convolution, as compared to other convolutions on graphs is that, it preserves locality while still being applicable over different graphs with different structures.\"  This is false; the other proposed architectures can be applied to inputs with different structures (e.g. Duvenaud et. al., Lusci et. al. for NN architectures on molecules specifically). "
  },
  {
    "people": [
      "Duvenaud",
      "Lusci"
    ],
    "review": "This work proposes a convolutional architecture for any graph-like input data (where the structure is example-dependent), or more generally, any data where the input dimensions that are related by a similarity matrix. If instead each input example is associated with a transition matrix, then a random walk algorithm is used generate a similarity matrix.\n\nDeveloping convolutional or recurrent architectures for graph-like data is an important problem because we would like to develop neural networks that can handle inputs such as molecule structures or social networks. However, I don't think this work contributes anything significant to the work that has already been done in this area. \n\nThe two main proposals I see in this paper are:\n1) For data associated with a transition matrix, this paper proposes that the transition matrix be converted to a similarity matrix. This seems obvious.\n2) For data associated with a similarity matrix, the k nearest neighbors of each node are computed and supply the context information for that node. This also seems obvious.\n\nPerhaps I have misunderstood the contribution, but the presentation also lacks clarity, and I cannot recommend this paper for publication. \n\nSpecific Comments:\n1) On page 4: \"An interesting attribute of this convolution, as compared to other convolutions on graphs is that, it preserves locality while still being applicable over different graphs with different structures.\"  This is false; the other proposed architectures can be applied to inputs with different structures (e.g. Duvenaud et. al., Lusci et. al. for NN architectures on molecules specifically)."
  },
  {
    "people": [
      "Defferrand",
      "Scarselli",
      "Bronstein"
    ],
    "review": "This work studies the problem of generalizing a convolutional neural network to data lacking grid-structure. \n \n The authors consider the Random Walk Normalized Laplacian and its finite powers to define a convolutional layer in a general graph. Experiments in Merck molecular discovery and mnist are reported. \n \n The reviewers all agreed that this paper, while presenting an interesting and important problem, lacks novelty relative to existing approaches. In particular, the AC would like to point out that important references seem to be missing from the current version. \n \n The proposed approach is closely related to 'Convolutional neural networks on graphs with fast localized spectral filtering', Defferrand et al. NIPS'16 , which considers Chevyshev polynomials of the Laplacian and learns the polynomial coefficients in an efficient manner. Since the Laplacian and the Random Walk Normalized Laplacian are similar operators (have same eigenvectors), the resulting model is essentially equivalent. Another related model that precedes all the cited works and is deeply related to the current submission is the Graph Neural Network from Scarselli et al.; see 'Geometric Deep Learning: going beyond Euclidean Data', Bronstein et al, "
  },
  {
    "people": [
      "Merk",
      "Coats"
    ],
    "review": "Previous literature uses data-derived adjacency matrix A to obtain neighbors to use as foundation of graph convolution. They propose extending the set of neighbors by additionally including nodes reachable by i<=k steps in this graph. This introduces an extra tunable parameter k, so it needs some justification over the previous k=1 solution. In one experiment provided\u00a0(Merk), using k=1 worked better. They don't specify which k that used, just that it was big enough for their to be p=5 nodes obtained as neighbors. In the second experiment (MNIST), they used k=1 for their experiments, which is what previous work (Coats & Ng 2011) proposed as well. A compelling experiment would compare to k=1 and show that using k>1 gives improvement strong enough to justify an extra hyper-parameter."
  },
  {
    "people": [
      "Duvenaud",
      "Lusci"
    ],
    "review": "This work proposes a convolutional architecture for any graph-like input data (where the structure is example-dependent), or more generally, any data where the input dimensions that are related by a similarity matrix. If instead each input example is associated with a transition matrix, then a random walk algorithm is used generate a similarity matrix.\n\nDeveloping convolutional or recurrent architectures for graph-like data is an important problem because we would like to develop neural networks that can handle inputs such as molecule structures or social networks. However, I don't think this work contributes anything significant to the work that has already been done in this area. \n\nThe two main proposals I see in this paper are:\n1) For data associated with a transition matrix, this paper proposes that the transition matrix be converted to a similarity matrix. This seems obvious.\n2) For data associated with a similarity matrix, the k nearest neighbors of each node are computed and supply the context information for that node. This also seems obvious.\n\nPerhaps I have misunderstood the contribution, but the presentation also lacks clarity, and I cannot recommend this paper for publication. \n\nSpecific Comments:\n1) On page 4: \"An interesting attribute of this convolution, as compared to other convolutions on graphs is that, it preserves locality while still being applicable over different graphs with different structures.\"  This is false; the other proposed architectures can be applied to inputs with different structures (e.g. Duvenaud et. al., Lusci et. al. for NN architectures on molecules specifically). "
  },
  {
    "people": [
      "Reed",
      "Kwatra"
    ],
    "review": "\"First, it allows us to assess whether auto-regressive models are able to match the GAN results of Reed et al. (2016a).\" Does it, though? Because the resolution is so bad. And resolution limitations aren't addressed until the second-to-last paragraph of the paper. And Figure 9 only shows 3 results (picked randomly? Picked to be favorable to PixelCNN?). That hardly seems conclusive.\n\nThe segmentation masks and keypoints are pretty strong input constraints. It's hard to tell how much coherent object and scene detail is emerging because the resolution is so low. For example, the cows in figure 5 look like color blobs, basically. Any color blob that follows an exact cow segmentation mask will look cow-like.\n\nThe amount of variation is impressive, though.\n\nHow can we assess how much the model is \"replaying\" training data? Figure 8 tries to get at this, but I wonder how much each of the \"red birds\", for instance, is mostly copied from a particular training example.\n\nI'm unsatisfied with the answers to the pre-review questions. You didn't answer my questions. The paper would benefit from concrete numbers on training time / epochs and testing time. You didn't say why you can't make high resolution comparisons. Yes, it's slower at test time. Is it prohibitively slow? Or is it slightly inconvenient? There really aren't that many comparisons in the paper, anyway, so it if takes an hour to generate a result that doesn't seem prohibitive. \n\nTo be clear about my biases: I don't think PixelCNN is \"the right way\" to do deep image generation. Texture synthesis methods used these causal neighborhoods with some success, but only because there wasn't a clear way to do the optimization more globally (Kwatra et al, Texture Optimization for Example-based Synthesis being one of the first alternatives). It seems simply incorrect to make hard decisions about what pixel values should be in one part of the image before synthesizing another part of the image (Another texture synthesis strategy to help fight back against this strict causality was coarse-to-fine synthesis. And I do see some deep image synthesis methods exploring that). It seems much more correct to have a deeper network and let all output pixels be conditioned on all other pixels (this conditioning will implicitly emerge at intermediate parts of the network). That said, I could be totally wrong, and the advantages stated in the paper could outweigh the disadvantages. But this paper doesn't feel very honest about the disadvantages.\n\nOverall, I think the results are somewhat tantalizing, especially the ability to generate diverse outputs. But the resolution is extremely low, especially compared to the richness of the inputs. The network gets a lot of hand holding from rich inputs (it does at least learn to obey them). \n\nThe deep image synthesis literature is moving very quickly. The field needs to move on from \"proof of concept\" papers (the first to show a particular result is possible) to more thorough comparisons. This paper has an opportunity to be a more in depth comparison, but it's not very deep in that regard. There isn't really an apples to apples comparison between PixelCNN and GAN nor is there a conclusion statement about why that is impossible. There isn't any large scale comparison, either qualitative or quantified by user studies, about the quality of the results."
  },
  {
    "people": [
      "Reed",
      "Kwatra"
    ],
    "review": "\"First, it allows us to assess whether auto-regressive models are able to match the GAN results of Reed et al. (2016a).\" Does it, though? Because the resolution is so bad. And resolution limitations aren't addressed until the second-to-last paragraph of the paper. And Figure 9 only shows 3 results (picked randomly? Picked to be favorable to PixelCNN?). That hardly seems conclusive.\n\nThe segmentation masks and keypoints are pretty strong input constraints. It's hard to tell how much coherent object and scene detail is emerging because the resolution is so low. For example, the cows in figure 5 look like color blobs, basically. Any color blob that follows an exact cow segmentation mask will look cow-like.\n\nThe amount of variation is impressive, though.\n\nHow can we assess how much the model is \"replaying\" training data? Figure 8 tries to get at this, but I wonder how much each of the \"red birds\", for instance, is mostly copied from a particular training example.\n\nI'm unsatisfied with the answers to the pre-review questions. You didn't answer my questions. The paper would benefit from concrete numbers on training time / epochs and testing time. You didn't say why you can't make high resolution comparisons. Yes, it's slower at test time. Is it prohibitively slow? Or is it slightly inconvenient? There really aren't that many comparisons in the paper, anyway, so it if takes an hour to generate a result that doesn't seem prohibitive. \n\nTo be clear about my biases: I don't think PixelCNN is \"the right way\" to do deep image generation. Texture synthesis methods used these causal neighborhoods with some success, but only because there wasn't a clear way to do the optimization more globally (Kwatra et al, Texture Optimization for Example-based Synthesis being one of the first alternatives). It seems simply incorrect to make hard decisions about what pixel values should be in one part of the image before synthesizing another part of the image (Another texture synthesis strategy to help fight back against this strict causality was coarse-to-fine synthesis. And I do see some deep image synthesis methods exploring that). It seems much more correct to have a deeper network and let all output pixels be conditioned on all other pixels (this conditioning will implicitly emerge at intermediate parts of the network). That said, I could be totally wrong, and the advantages stated in the paper could outweigh the disadvantages. But this paper doesn't feel very honest about the disadvantages.\n\nOverall, I think the results are somewhat tantalizing, especially the ability to generate diverse outputs. But the resolution is extremely low, especially compared to the richness of the inputs. The network gets a lot of hand holding from rich inputs (it does at least learn to obey them). \n\nThe deep image synthesis literature is moving very quickly. The field needs to move on from \"proof of concept\" papers (the first to show a particular result is possible) to more thorough comparisons. This paper has an opportunity to be a more in depth comparison, but it's not very deep in that regard. There isn't really an apples to apples comparison between PixelCNN and GAN nor is there a conclusion statement about why that is impossible. There isn't any large scale comparison, either qualitative or quantified by user studies, about the quality of the results."
  },
  {
    "people": [
      "Goodfellow",
      "Goodfellow",
      "Szegedy",
      "Kurakin",
      "Papernot"
    ],
    "review": "I reviewed the manuscript as of December 7th.\n\nSummary:\nThe authors investigate the transferability of adversarial examples in deep networks. The authors confirm that transferability exists even in large models but demonstrate that it is difficult to manipulate the network to adversarially perturb an image into a specifically desired label. The authors additionally demonstrate real world attacks on a vision web service and explore the geometric properties of adversarial examples.\n\nMajor Comments:\n1. The paper contains a list of many results and it is not clear what single message this paper provides. As mentioned in the comments, this paper is effectively 15 pages and 9 page of results in the Appendix heavily discussed throughout the main body of the paper. Although there is no strict page limit for this conference, I do feel this pushes the spirit of a conference publication. I do not rule out this paper for acceptance based on the length but I do hold it as a negative because clarity of presentation is an important quality. If this paper is ultimately accepted, I would suggest that the authors make some effort to cut down the length even further beyond the 13 pages posted elsewhere. I have marked some sections to highlight areas that may be trimmed.\n\n2. The section of geometric understanding is similar to results of 'Adversarial Perturbations of Deep Neural Networks' in Warde-Farley and Goodfellow (2015). See Figure 1.2. I am not clear what the authors show above-and-beyond these results. If there are additional findings, the authors should emphasize them.\n\n3. The authors expand on observations by Goodfellow et al (2014) and Szegedy et al (2013) demonstrating that large-scale models are susceptible to adversarial perturbations (see also Kurakin et al (2016)). The authors additionally demonstrate that attempting to perform adversarial manipulation to convert an image to a particular, desired label is more difficult.\n\n4. The authors demonstrate that they can target a real-world vision API. These results are compelling but it is not clear what these results demonstrate above-and-beyond Papernot et al (2016). \n\nAs far I can understand, I think that the most interesting result from this paper not previously described in the literature is to note about the unique difficulty about performing adversarial manipulation to convert an image to a particular, desired label. The rest of the results appear to expand on other results that have already appeared in the literature and the authors need to better explain what these makes these results unique above-and-beyond previous work.\n\nAreas to Trim the Paper:\n- Table 1 is not necessary. Just cite other results or write the Top-1 numbers in the text.\n- Condense Section 2.2.1 and cite heavily.\n- Figure 2 panels may be overlaid to highlight a comparison."
  },
  {
    "people": [
      "Papernot"
    ],
    "review": "The paper is the first to demonstrate that it is possible for an adversary to change the label that a convolutional network predicts for an image to a specific value. Like Papernot et al., it presents a successful attack on Clarifai's image-recognition system. I encourage the authors to condense the paper to its key results (13 pages without / 24 pages with supplemental material is too long for a conference paper)."
  },
  {
    "people": [
      "Papernot",
      "Fawzi"
    ],
    "review": "We have updated the paper with the following changes:\n\n1) We shrink the paper in the following aspects:\n   a) Sec 2.2 is condensed\n   b) We remove the original Sec 2.3, which introduces transferability and black-box attack, since all materials have been covered in Sec 1\n   c) The original Table 1 is moved to the appendix Table 7\n   d) The original Table 2 and Table 4 are now merged as two panels of Table 1, so that only one caption needs be provided.\n   e) The alternative approach to generate non-targeted adversarial images is moved to the appendix\n   f) The paragraph discussing different models make the same mistake in original Sec 3.1 is moved to the appendix\n   g) The paragraph discussing that adversarial images may come from multiple intervals along the gradient direction is moved to the appendix\n   h) The results for random perturbation in original Sec 3.2 is moved to the appendix.\n   i) We move the original Table 9 in Section 6 about cosine values between pairs of gradients to the appendix Table 33\n\n2) We provide a contribution and organization paragraph in Section 1 to highlight the the main conclusions of this work and to facilitate readers to follow the arguments in the paper to support our conclusions.\n\n3) In related work, we highlight the difference between our work and Papernot et al (2016ab) by explaining why our black-box attack is harder. Also, we discuss Fawzi et al (2016) at the beginning of Section 6.\n\n4) We provide justification of why we choose three ResNet models in Section 2.3, and highlight the findings about the transferability between both homogeneous architectures (i.e., ResNet models) and heterogeneous architectures in various places (i.e., Section 3.1, Section 5 and Section 6).\n\nWe welcome new comments!"
  },
  {
    "people": [
      "Fawzi",
      "Fawzi"
    ],
    "review": "This paper present an experimental study of the robustness of state-of-the-art CNNs to different types of \"attacks\" in the context of image classication. Specifically, an attack aims to fool the classification system with a specially corrupted image, i.e. making it misclassify the image as (1) any wrong class (non-targeted attack) or (2) a target class, chosen in advance by the attacker (targeted attack). For instance, the attacker could corrupt an image of an ostrich in such a way that it would be classified as a megalith. Even though the attacker's agenda is not so clear in this example, it is still interesting to study the weaknesses of current systems in view of (1) improving them in general and (2) actual risks with e.g. autonomous vehicles.\n\nThe paper is mostly experimental. In short, it compares different strategies (already published in previous papers) for all popular networks  (VGG, GoogLeNet, ResNet-50/101/152) and the two aforementionned types of attacks. The experiments are well conducted and clearly exposed. A convincing point is that attacks are also conducted on \"clarifai.com\" which is a black-box classification system. Some analysis and insightful explanations are also provided to help understanding why CNNs are prone to such attacks (Section 6).\n\nTo sum up, the main findings are that non-targeted attacks are easy to perform, even on a black-box system. Non-targeted attacks are more difficult to realize with existing schemes, but the authors propose a new approach for that that vastly improves over existing attacks (even though it's still far from perfect: ~20% success rate on clarifai.com versus 2% with previous schemes).\n\nArguably, The paper still has some weaknesses:\n\n - The authors are treating the 3 ResNet-based networks as different, yet they are obviously clearly correlated. See Table 7 for instance. This is naturally expected because their architecture is similar (only their depth varies). Hence, it does not sound very fair to state that \"One interesting finding is that [...] the first misclassified label (non-targeted) is the same for all models except VGG-16 and GoogLeNet.\", i.e., the three ResNet-based networks.\n\n - A subjective measure is employed to evaluate the effectiveness of the attacks on the black box system. While this is for a good reason (clarifai.com returns image labels that are different from ImageNet), it is not certain that the reported numbers are fair (even though the qualitative results look convincing).\n\n - The novelty of the proposed approach (optimizing an ensemble of network instead of a single network) is limited. However, this was not really the point of the paper, and it is effective, so it seems ok overall.\n\n - The paper is quite long. This is expected because it is an extensive evaluation study, but still. I suggest the authors prune some near-duplicate content (e.g. Section 2.3 has a high overlap with Section 1, etc.).\n\n - The paper would benefit from additional discussions with the recent and related work of Fawzi et al (NIPS'16) in Section 6. Indeed the work of Fawzi et al. is mostly theoretical and well aligned with the experimental findings and observations (in particular in Section 6).\n\n\nTo conclude, I think that this paper is somewhat useful for the community and could help to further improve existing architectures, as well as better assess their flaws and weaknesses."
  },
  {
    "people": [
      "Goodfellow",
      "Goodfellow",
      "Szegedy",
      "Kurakin",
      "Papernot"
    ],
    "review": "I reviewed the manuscript as of December 7th.\n\nSummary:\nThe authors investigate the transferability of adversarial examples in deep networks. The authors confirm that transferability exists even in large models but demonstrate that it is difficult to manipulate the network to adversarially perturb an image into a specifically desired label. The authors additionally demonstrate real world attacks on a vision web service and explore the geometric properties of adversarial examples.\n\nMajor Comments:\n1. The paper contains a list of many results and it is not clear what single message this paper provides. As mentioned in the comments, this paper is effectively 15 pages and 9 page of results in the Appendix heavily discussed throughout the main body of the paper. Although there is no strict page limit for this conference, I do feel this pushes the spirit of a conference publication. I do not rule out this paper for acceptance based on the length but I do hold it as a negative because clarity of presentation is an important quality. If this paper is ultimately accepted, I would suggest that the authors make some effort to cut down the length even further beyond the 13 pages posted elsewhere. I have marked some sections to highlight areas that may be trimmed.\n\n2. The section of geometric understanding is similar to results of 'Adversarial Perturbations of Deep Neural Networks' in Warde-Farley and Goodfellow (2015). See Figure 1.2. I am not clear what the authors show above-and-beyond these results. If there are additional findings, the authors should emphasize them.\n\n3. The authors expand on observations by Goodfellow et al (2014) and Szegedy et al (2013) demonstrating that large-scale models are susceptible to adversarial perturbations (see also Kurakin et al (2016)). The authors additionally demonstrate that attempting to perform adversarial manipulation to convert an image to a particular, desired label is more difficult.\n\n4. The authors demonstrate that they can target a real-world vision API. These results are compelling but it is not clear what these results demonstrate above-and-beyond Papernot et al (2016). \n\nAs far I can understand, I think that the most interesting result from this paper not previously described in the literature is to note about the unique difficulty about performing adversarial manipulation to convert an image to a particular, desired label. The rest of the results appear to expand on other results that have already appeared in the literature and the authors need to better explain what these makes these results unique above-and-beyond previous work.\n\nAreas to Trim the Paper:\n- Table 1 is not necessary. Just cite other results or write the Top-1 numbers in the text.\n- Condense Section 2.2.1 and cite heavily.\n- Figure 2 panels may be overlaid to highlight a comparison.\n"
  },
  {
    "people": [
      "Goodfellow",
      "Goodfellow",
      "Szegedy",
      "Kurakin",
      "Papernot"
    ],
    "review": "I reviewed the manuscript as of December 7th.\n\nSummary:\nThe authors investigate the transferability of adversarial examples in deep networks. The authors confirm that transferability exists even in large models but demonstrate that it is difficult to manipulate the network to adversarially perturb an image into a specifically desired label. The authors additionally demonstrate real world attacks on a vision web service and explore the geometric properties of adversarial examples.\n\nMajor Comments:\n1. The paper contains a list of many results and it is not clear what single message this paper provides. As mentioned in the comments, this paper is effectively 15 pages and 9 page of results in the Appendix heavily discussed throughout the main body of the paper. Although there is no strict page limit for this conference, I do feel this pushes the spirit of a conference publication. I do not rule out this paper for acceptance based on the length but I do hold it as a negative because clarity of presentation is an important quality. If this paper is ultimately accepted, I would suggest that the authors make some effort to cut down the length even further beyond the 13 pages posted elsewhere. I have marked some sections to highlight areas that may be trimmed.\n\n2. The section of geometric understanding is similar to results of 'Adversarial Perturbations of Deep Neural Networks' in Warde-Farley and Goodfellow (2015). See Figure 1.2. I am not clear what the authors show above-and-beyond these results. If there are additional findings, the authors should emphasize them.\n\n3. The authors expand on observations by Goodfellow et al (2014) and Szegedy et al (2013) demonstrating that large-scale models are susceptible to adversarial perturbations (see also Kurakin et al (2016)). The authors additionally demonstrate that attempting to perform adversarial manipulation to convert an image to a particular, desired label is more difficult.\n\n4. The authors demonstrate that they can target a real-world vision API. These results are compelling but it is not clear what these results demonstrate above-and-beyond Papernot et al (2016). \n\nAs far I can understand, I think that the most interesting result from this paper not previously described in the literature is to note about the unique difficulty about performing adversarial manipulation to convert an image to a particular, desired label. The rest of the results appear to expand on other results that have already appeared in the literature and the authors need to better explain what these makes these results unique above-and-beyond previous work.\n\nAreas to Trim the Paper:\n- Table 1 is not necessary. Just cite other results or write the Top-1 numbers in the text.\n- Condense Section 2.2.1 and cite heavily.\n- Figure 2 panels may be overlaid to highlight a comparison."
  },
  {
    "people": [
      "Papernot"
    ],
    "review": "The paper is the first to demonstrate that it is possible for an adversary to change the label that a convolutional network predicts for an image to a specific value. Like Papernot et al., it presents a successful attack on Clarifai's image-recognition system. I encourage the authors to condense the paper to its key results (13 pages without / 24 pages with supplemental material is too long for a conference paper)."
  },
  {
    "people": [
      "Papernot",
      "Fawzi"
    ],
    "review": "We have updated the paper with the following changes:\n\n1) We shrink the paper in the following aspects:\n   a) Sec 2.2 is condensed\n   b) We remove the original Sec 2.3, which introduces transferability and black-box attack, since all materials have been covered in Sec 1\n   c) The original Table 1 is moved to the appendix Table 7\n   d) The original Table 2 and Table 4 are now merged as two panels of Table 1, so that only one caption needs be provided.\n   e) The alternative approach to generate non-targeted adversarial images is moved to the appendix\n   f) The paragraph discussing different models make the same mistake in original Sec 3.1 is moved to the appendix\n   g) The paragraph discussing that adversarial images may come from multiple intervals along the gradient direction is moved to the appendix\n   h) The results for random perturbation in original Sec 3.2 is moved to the appendix.\n   i) We move the original Table 9 in Section 6 about cosine values between pairs of gradients to the appendix Table 33\n\n2) We provide a contribution and organization paragraph in Section 1 to highlight the the main conclusions of this work and to facilitate readers to follow the arguments in the paper to support our conclusions.\n\n3) In related work, we highlight the difference between our work and Papernot et al (2016ab) by explaining why our black-box attack is harder. Also, we discuss Fawzi et al (2016) at the beginning of Section 6.\n\n4) We provide justification of why we choose three ResNet models in Section 2.3, and highlight the findings about the transferability between both homogeneous architectures (i.e., ResNet models) and heterogeneous architectures in various places (i.e., Section 3.1, Section 5 and Section 6).\n\nWe welcome new comments!"
  },
  {
    "people": [
      "Fawzi",
      "Fawzi"
    ],
    "review": "This paper present an experimental study of the robustness of state-of-the-art CNNs to different types of \"attacks\" in the context of image classication. Specifically, an attack aims to fool the classification system with a specially corrupted image, i.e. making it misclassify the image as (1) any wrong class (non-targeted attack) or (2) a target class, chosen in advance by the attacker (targeted attack). For instance, the attacker could corrupt an image of an ostrich in such a way that it would be classified as a megalith. Even though the attacker's agenda is not so clear in this example, it is still interesting to study the weaknesses of current systems in view of (1) improving them in general and (2) actual risks with e.g. autonomous vehicles.\n\nThe paper is mostly experimental. In short, it compares different strategies (already published in previous papers) for all popular networks  (VGG, GoogLeNet, ResNet-50/101/152) and the two aforementionned types of attacks. The experiments are well conducted and clearly exposed. A convincing point is that attacks are also conducted on \"clarifai.com\" which is a black-box classification system. Some analysis and insightful explanations are also provided to help understanding why CNNs are prone to such attacks (Section 6).\n\nTo sum up, the main findings are that non-targeted attacks are easy to perform, even on a black-box system. Non-targeted attacks are more difficult to realize with existing schemes, but the authors propose a new approach for that that vastly improves over existing attacks (even though it's still far from perfect: ~20% success rate on clarifai.com versus 2% with previous schemes).\n\nArguably, The paper still has some weaknesses:\n\n - The authors are treating the 3 ResNet-based networks as different, yet they are obviously clearly correlated. See Table 7 for instance. This is naturally expected because their architecture is similar (only their depth varies). Hence, it does not sound very fair to state that \"One interesting finding is that [...] the first misclassified label (non-targeted) is the same for all models except VGG-16 and GoogLeNet.\", i.e., the three ResNet-based networks.\n\n - A subjective measure is employed to evaluate the effectiveness of the attacks on the black box system. While this is for a good reason (clarifai.com returns image labels that are different from ImageNet), it is not certain that the reported numbers are fair (even though the qualitative results look convincing).\n\n - The novelty of the proposed approach (optimizing an ensemble of network instead of a single network) is limited. However, this was not really the point of the paper, and it is effective, so it seems ok overall.\n\n - The paper is quite long. This is expected because it is an extensive evaluation study, but still. I suggest the authors prune some near-duplicate content (e.g. Section 2.3 has a high overlap with Section 1, etc.).\n\n - The paper would benefit from additional discussions with the recent and related work of Fawzi et al (NIPS'16) in Section 6. Indeed the work of Fawzi et al. is mostly theoretical and well aligned with the experimental findings and observations (in particular in Section 6).\n\n\nTo conclude, I think that this paper is somewhat useful for the community and could help to further improve existing architectures, as well as better assess their flaws and weaknesses."
  },
  {
    "people": [
      "Goodfellow",
      "Goodfellow",
      "Szegedy",
      "Kurakin",
      "Papernot"
    ],
    "review": "I reviewed the manuscript as of December 7th.\n\nSummary:\nThe authors investigate the transferability of adversarial examples in deep networks. The authors confirm that transferability exists even in large models but demonstrate that it is difficult to manipulate the network to adversarially perturb an image into a specifically desired label. The authors additionally demonstrate real world attacks on a vision web service and explore the geometric properties of adversarial examples.\n\nMajor Comments:\n1. The paper contains a list of many results and it is not clear what single message this paper provides. As mentioned in the comments, this paper is effectively 15 pages and 9 page of results in the Appendix heavily discussed throughout the main body of the paper. Although there is no strict page limit for this conference, I do feel this pushes the spirit of a conference publication. I do not rule out this paper for acceptance based on the length but I do hold it as a negative because clarity of presentation is an important quality. If this paper is ultimately accepted, I would suggest that the authors make some effort to cut down the length even further beyond the 13 pages posted elsewhere. I have marked some sections to highlight areas that may be trimmed.\n\n2. The section of geometric understanding is similar to results of 'Adversarial Perturbations of Deep Neural Networks' in Warde-Farley and Goodfellow (2015). See Figure 1.2. I am not clear what the authors show above-and-beyond these results. If there are additional findings, the authors should emphasize them.\n\n3. The authors expand on observations by Goodfellow et al (2014) and Szegedy et al (2013) demonstrating that large-scale models are susceptible to adversarial perturbations (see also Kurakin et al (2016)). The authors additionally demonstrate that attempting to perform adversarial manipulation to convert an image to a particular, desired label is more difficult.\n\n4. The authors demonstrate that they can target a real-world vision API. These results are compelling but it is not clear what these results demonstrate above-and-beyond Papernot et al (2016). \n\nAs far I can understand, I think that the most interesting result from this paper not previously described in the literature is to note about the unique difficulty about performing adversarial manipulation to convert an image to a particular, desired label. The rest of the results appear to expand on other results that have already appeared in the literature and the authors need to better explain what these makes these results unique above-and-beyond previous work.\n\nAreas to Trim the Paper:\n- Table 1 is not necessary. Just cite other results or write the Top-1 numbers in the text.\n- Condense Section 2.2.1 and cite heavily.\n- Figure 2 panels may be overlaid to highlight a comparison.\n"
  },
  {
    "people": [
      "Wang",
      "Wang",
      "Jiang",
      "Jiang",
      "Wang",
      "Jiang"
    ],
    "review": "Summary: The paper proposes a novel deep neural network architecture for the task of question answering on the SQuAD dataset. The model consists of two main components -- coattention encoder and dynamic pointer decoder. The encoder produces attention over the question as well as over the document in parallel and thus learns co-dependent representations of the question and the document. The decoder predicts the starting and the end token of the answer iteratively, with the motivation that multiple iterations will help the model escape local maxima and thus will reduce the errors made by the model. The proposed model achieved state-of-art result on SQuAD dataset at the time of writing the paper. The paper reports some analyses of the results such as performance across question types, document, question, answer lengths, etc. The paper also performs some ablation studies such as performing only single round of iteration on decoder, etc.\n\nStrengths:\n\n1. The paper is well-motivated with two main motivations -- co-attending to the document and the question, and iteratively producing the answer.\n\n2. The proposed model architecture is novel and the design choices made seem reasonable.\n\n3. The experiments show that the proposed model outperforms the existing model (at the time of writing the paper) on the SQuAD dataset by significant margin.\n\n4. The analyses of the results and the ablation studies performed (as per someone's request) provide insights into the various modelling design choices made.\n\nWeaknesses/Questions/Suggestions:\n\n1. In order to gain insights into how much each additional iteration in the decoder help, I would like to see the following -- for every iteration report the mean F1 for questions that converged in that iteration along with the number of questions that converged in that iteration.\n\n2. Example of Question 3 in figure 5 is an interesting example where the model is unable to decide between multiple local maxima despite several iterations. Could authors please report how often this happens?\n\n3. In order to estimate how much modelling of attention in the encoder helps, it would be good if authors could report the performance of the model when attention is not modeled at all in the encoder (neither over question, nor over document).\n\n4. I would like to see the variation in the performance of the proposed model for questions that require different types of reasoning (table 3 in SQuAD paper). This would provide insights into what are the strengths and weaknesses of the proposed model w.r.t the type reasoning required.\n\n5. In Wang and Jiang (2016), the attention is predicted over question for each word in the document. But in table 2, when performing ablation study to make the proposed model similar to Wang and Jiang, C^D is set to C^Q. But isn\u2019t C^Q attention over document for each word in the question? So, how is this similar to Wang and Jiang\u2019s attention? I think QA^D will be similar to Wang and Jiang's attention since QA^D is attention over question for each word in the document. Please clarify.\n\n6. In section 2.1, \u201cn\u201d and \u201cm\u201d are swapped when explaining the Document and Question encoding matrix. Please fix it.\n\nReview Summary: The paper presents a novel and interesting model for the task of question answering on SQuAD dataset and shows that the model outperforms existing models. However, to gain more insights into the functioning of the model, I would like see more analyses of the results and one more ablation study (see weaknesses section above).\n"
  },
  {
    "people": [
      "Wang",
      "Wang",
      "Jiang",
      "Jiang",
      "Wang",
      "Jiang"
    ],
    "review": "Summary: The paper proposes a novel deep neural network architecture for the task of question answering on the SQuAD dataset. The model consists of two main components -- coattention encoder and dynamic pointer decoder. The encoder produces attention over the question as well as over the document in parallel and thus learns co-dependent representations of the question and the document. The decoder predicts the starting and the end token of the answer iteratively, with the motivation that multiple iterations will help the model escape local maxima and thus will reduce the errors made by the model. The proposed model achieved state-of-art result on SQuAD dataset at the time of writing the paper. The paper reports some analyses of the results such as performance across question types, document, question, answer lengths, etc. The paper also performs some ablation studies such as performing only single round of iteration on decoder, etc.\n\nStrengths:\n\n1. The paper is well-motivated with two main motivations -- co-attending to the document and the question, and iteratively producing the answer.\n\n2. The proposed model architecture is novel and the design choices made seem reasonable.\n\n3. The experiments show that the proposed model outperforms the existing model (at the time of writing the paper) on the SQuAD dataset by significant margin.\n\n4. The analyses of the results and the ablation studies performed (as per someone's request) provide insights into the various modelling design choices made.\n\nWeaknesses/Questions/Suggestions:\n\n1. In order to gain insights into how much each additional iteration in the decoder help, I would like to see the following -- for every iteration report the mean F1 for questions that converged in that iteration along with the number of questions that converged in that iteration.\n\n2. Example of Question 3 in figure 5 is an interesting example where the model is unable to decide between multiple local maxima despite several iterations. Could authors please report how often this happens?\n\n3. In order to estimate how much modelling of attention in the encoder helps, it would be good if authors could report the performance of the model when attention is not modeled at all in the encoder (neither over question, nor over document).\n\n4. I would like to see the variation in the performance of the proposed model for questions that require different types of reasoning (table 3 in SQuAD paper). This would provide insights into what are the strengths and weaknesses of the proposed model w.r.t the type reasoning required.\n\n5. In Wang and Jiang (2016), the attention is predicted over question for each word in the document. But in table 2, when performing ablation study to make the proposed model similar to Wang and Jiang, C^D is set to C^Q. But isn\u2019t C^Q attention over document for each word in the question? So, how is this similar to Wang and Jiang\u2019s attention? I think QA^D will be similar to Wang and Jiang's attention since QA^D is attention over question for each word in the document. Please clarify.\n\n6. In section 2.1, \u201cn\u201d and \u201cm\u201d are swapped when explaining the Document and Question encoding matrix. Please fix it.\n\nReview Summary: The paper presents a novel and interesting model for the task of question answering on SQuAD dataset and shows that the model outperforms existing models. However, to gain more insights into the functioning of the model, I would like see more analyses of the results and one more ablation study (see weaknesses section above).\n"
  },
  {
    "people": [
      "Kingma"
    ],
    "review": "This paper proposes an alternative to Conditional Variational Auto-Encoders and Conditional MultiModal Auto-Encoders to perform inference of missing modalities in dataset with multiple modalities. The proposed approach is a Variational Auto-Encoder jointly on all the modalities  with additional KL divergence penalties between the approximate posterior given all the modalities and the approximate posterior given a subset of the modalities. The approach is named Joint Multimodal Variational Auto-Encoder.\nThe authors make a connection between this approach and the Variation of Information. It is unclear why the authors chose the JMVAE approach instead of a more elegant Variation of Information approach.\nAnother unaddressed issue is the scalability of the method. As far as I can tell (given that no code is provided and the specification of the encoder is missing), this approach requires a new encoder per subset of missing modalities. Right now this approach seems to scale since there are only two modalities.\nThe fact that the estimating the log-likelihood log(p(x)) using multiple modalities provide a lower value than with just one in Table 1 is a bit odd. Could you explain that ?\nThe comparison with between the representation learned by JMVAE and CVAE might be unfair given that the representation of CVAE is learned conditionally, on the label in the case of MNIST, and should therefore not consider the label in this representation. Intuitively, this representation could represent \"style\" as shown in (Kingma et al., 2014) in their conditional generation figure.\nFor CelebA, comparing log-likelihood on models that use GANs is probably not significant since GAN does not optimizes log-likelihood. \nOverall this is an interesting problem and there are also interesting ideas worth exploring further, but the execution of the paper requires more work."
  },
  {
    "people": [
      "Rezende"
    ],
    "review": "The paper introduces the joint multimodal variational autoencoder, a directed graphical model for modeling multimodal data with latent variable. the model is rather straightforward extension of standard VAE where two data modalities are generated from a shared latent representation independently. In order to deal with missing input modalities or bi-directional inference between two modalities the paper introduces modality-specific encoder that is trained to minimize the KL divergence of latent variable distributions between joint and modality-specific recognition networks. The paper demonstrates its effectiveness on MNIST and CelebA datasets, both in terms of test log-likelihoods and the conditional image generation and editing.\n\nThe proposed method is rather straightforward extension of VAE and therefore the model should inherent the probabilistic inference methods of VAE. For example, for missing data modalities, the model should be able to infer joint representation as well as filling in the missing modalities via iterative sampling as introduced by Rezende et al. (2014). Given marginal improvement, I am not convinced by the contribution of modality-specific encoders in Section 3.3. In addition, the inference methods introduced for generating Figure 5 looks somewhat unprincipled; I am wondering the conditional image generation results by following more principled approach (e.g., iterative sampling). Experimental results on joint image-attribute generation is also missing."
  },
  {
    "people": [
      "Kingma"
    ],
    "review": "This paper proposes an alternative to Conditional Variational Auto-Encoders and Conditional MultiModal Auto-Encoders to perform inference of missing modalities in dataset with multiple modalities. The proposed approach is a Variational Auto-Encoder jointly on all the modalities  with additional KL divergence penalties between the approximate posterior given all the modalities and the approximate posterior given a subset of the modalities. The approach is named Joint Multimodal Variational Auto-Encoder.\nThe authors make a connection between this approach and the Variation of Information. It is unclear why the authors chose the JMVAE approach instead of a more elegant Variation of Information approach.\nAnother unaddressed issue is the scalability of the method. As far as I can tell (given that no code is provided and the specification of the encoder is missing), this approach requires a new encoder per subset of missing modalities. Right now this approach seems to scale since there are only two modalities.\nThe fact that the estimating the log-likelihood log(p(x)) using multiple modalities provide a lower value than with just one in Table 1 is a bit odd. Could you explain that ?\nThe comparison with between the representation learned by JMVAE and CVAE might be unfair given that the representation of CVAE is learned conditionally, on the label in the case of MNIST, and should therefore not consider the label in this representation. Intuitively, this representation could represent \"style\" as shown in (Kingma et al., 2014) in their conditional generation figure.\nFor CelebA, comparing log-likelihood on models that use GANs is probably not significant since GAN does not optimizes log-likelihood. \nOverall this is an interesting problem and there are also interesting ideas worth exploring further, but the execution of the paper requires more work."
  },
  {
    "people": [
      "Cadena",
      "Diane"
    ],
    "review": "Have you considered the following related work : \"Multi-modal Auto-Encoders as Joint Estimators for\nRobotics Scene Understanding\" Cadena et al. ? Could you explain how your method differs from theirs ?\n\nThanks,\n\nDiane"
  },
  {
    "people": [
      "Kingma"
    ],
    "review": "This paper proposes an alternative to Conditional Variational Auto-Encoders and Conditional MultiModal Auto-Encoders to perform inference of missing modalities in dataset with multiple modalities. The proposed approach is a Variational Auto-Encoder jointly on all the modalities  with additional KL divergence penalties between the approximate posterior given all the modalities and the approximate posterior given a subset of the modalities. The approach is named Joint Multimodal Variational Auto-Encoder.\nThe authors make a connection between this approach and the Variation of Information. It is unclear why the authors chose the JMVAE approach instead of a more elegant Variation of Information approach.\nAnother unaddressed issue is the scalability of the method. As far as I can tell (given that no code is provided and the specification of the encoder is missing), this approach requires a new encoder per subset of missing modalities. Right now this approach seems to scale since there are only two modalities.\nThe fact that the estimating the log-likelihood log(p(x)) using multiple modalities provide a lower value than with just one in Table 1 is a bit odd. Could you explain that ?\nThe comparison with between the representation learned by JMVAE and CVAE might be unfair given that the representation of CVAE is learned conditionally, on the label in the case of MNIST, and should therefore not consider the label in this representation. Intuitively, this representation could represent \"style\" as shown in (Kingma et al., 2014) in their conditional generation figure.\nFor CelebA, comparing log-likelihood on models that use GANs is probably not significant since GAN does not optimizes log-likelihood. \nOverall this is an interesting problem and there are also interesting ideas worth exploring further, but the execution of the paper requires more work."
  },
  {
    "people": [
      "Rezende"
    ],
    "review": "The paper introduces the joint multimodal variational autoencoder, a directed graphical model for modeling multimodal data with latent variable. the model is rather straightforward extension of standard VAE where two data modalities are generated from a shared latent representation independently. In order to deal with missing input modalities or bi-directional inference between two modalities the paper introduces modality-specific encoder that is trained to minimize the KL divergence of latent variable distributions between joint and modality-specific recognition networks. The paper demonstrates its effectiveness on MNIST and CelebA datasets, both in terms of test log-likelihoods and the conditional image generation and editing.\n\nThe proposed method is rather straightforward extension of VAE and therefore the model should inherent the probabilistic inference methods of VAE. For example, for missing data modalities, the model should be able to infer joint representation as well as filling in the missing modalities via iterative sampling as introduced by Rezende et al. (2014). Given marginal improvement, I am not convinced by the contribution of modality-specific encoders in Section 3.3. In addition, the inference methods introduced for generating Figure 5 looks somewhat unprincipled; I am wondering the conditional image generation results by following more principled approach (e.g., iterative sampling). Experimental results on joint image-attribute generation is also missing."
  },
  {
    "people": [
      "Kingma"
    ],
    "review": "This paper proposes an alternative to Conditional Variational Auto-Encoders and Conditional MultiModal Auto-Encoders to perform inference of missing modalities in dataset with multiple modalities. The proposed approach is a Variational Auto-Encoder jointly on all the modalities  with additional KL divergence penalties between the approximate posterior given all the modalities and the approximate posterior given a subset of the modalities. The approach is named Joint Multimodal Variational Auto-Encoder.\nThe authors make a connection between this approach and the Variation of Information. It is unclear why the authors chose the JMVAE approach instead of a more elegant Variation of Information approach.\nAnother unaddressed issue is the scalability of the method. As far as I can tell (given that no code is provided and the specification of the encoder is missing), this approach requires a new encoder per subset of missing modalities. Right now this approach seems to scale since there are only two modalities.\nThe fact that the estimating the log-likelihood log(p(x)) using multiple modalities provide a lower value than with just one in Table 1 is a bit odd. Could you explain that ?\nThe comparison with between the representation learned by JMVAE and CVAE might be unfair given that the representation of CVAE is learned conditionally, on the label in the case of MNIST, and should therefore not consider the label in this representation. Intuitively, this representation could represent \"style\" as shown in (Kingma et al., 2014) in their conditional generation figure.\nFor CelebA, comparing log-likelihood on models that use GANs is probably not significant since GAN does not optimizes log-likelihood. \nOverall this is an interesting problem and there are also interesting ideas worth exploring further, but the execution of the paper requires more work."
  },
  {
    "people": [
      "Cadena",
      "Diane"
    ],
    "review": "Have you considered the following related work : \"Multi-modal Auto-Encoders as Joint Estimators for\nRobotics Scene Understanding\" Cadena et al. ? Could you explain how your method differs from theirs ?\n\nThanks,\n\nDiane"
  },
  {
    "people": [
      "Han"
    ],
    "review": "The Squeezenet paper came out in Feb 2016, and I read it with interest. It has a series of completely reasonable engineering suggestions for how to save parameter memory for CNNs for object recognition (imagenet). The suggestions make a lot of sense, and provide an excellent compression of about 50x versus AlexNet. (Looks like ~500x if combined with Han, 2015). So, very nice results, definitely worth publishing.\n\nSince the arxiv paper came out, people have noticed and worked to extend the paper. This is already evidence that this paper will have impact --- and deserves to have a permanent published home.\n\nOn the negative side, the architecture was only tested on ImageNet -- unclear whether the ideas transfer to other tasks (e.g., audio or text recognition). And, as with many other architecture-tweaking papers, there is no real mathematical or theoretical support for the ideas: they are just sensible and empirically work.\n\nOh the whole, I think the paper deserves to appear at ICLR, being in the mainline of work on deep learning architectures."
  },
  {
    "people": [
      "Han"
    ],
    "review": "The Squeezenet paper came out in Feb 2016, and I read it with interest. It has a series of completely reasonable engineering suggestions for how to save parameter memory for CNNs for object recognition (imagenet). The suggestions make a lot of sense, and provide an excellent compression of about 50x versus AlexNet. (Looks like ~500x if combined with Han, 2015). So, very nice results, definitely worth publishing.\n\nSince the arxiv paper came out, people have noticed and worked to extend the paper. This is already evidence that this paper will have impact --- and deserves to have a permanent published home.\n\nOn the negative side, the architecture was only tested on ImageNet -- unclear whether the ideas transfer to other tasks (e.g., audio or text recognition). And, as with many other architecture-tweaking papers, there is no real mathematical or theoretical support for the ideas: they are just sensible and empirically work.\n\nOh the whole, I think the paper deserves to appear at ICLR, being in the mainline of work on deep learning architectures."
  },
  {
    "people": [
      "George Konidaris",
      "Andrew G. Barto",
      "George Konidaris",
      "Ilya Scheidwasser",
      "Andrew G. Barto",
      "Bruno Castro da Silva",
      "George Konidaris",
      "Andrew G. Barto",
      "Warwick Masson",
      "George Konidaris",
      "S. R. K. Branavan",
      "Harr Chen",
      "Luke S. Zettlemoyer",
      "Regina Barzilay",
      "Tom Schaul",
      "Daniel Horgan",
      "Karol Gregor",
      "David Silver",
      "Satinder Pal Singh",
      "Richard S Sutton",
      "Doina Precup",
      "Satinder Singh",
      "J. Lei Ba",
      "K. Swersky",
      "S. Fidler",
      "A. A. Rusu",
      "S. G. Colmenarejo",
      "C. Gulcehre",
      "G. Desjardins",
      "J. Kirkpatrick",
      "R. Pascanu",
      "V. Mnih",
      "K. Kavukcuoglu",
      "R. Hadsell",
      "J. Schulman",
      "P. Moritz",
      "S. Levine",
      "M. Jordan",
      "P. Abbeel"
    ],
    "review": "Dear reviewers,\n\nThank you for your valuable comments. \nWe have revised our paper by reflecting many comments from you.\nWe also added new results from a 3D visual domain to address concerns regarding simplicity of the 2D grid-world domain (Section 6.5).\n\nWe put a common response here as many of you raised similar questions/comments about complexity of our architecture and simplicity of the problem. We describe challenging aspects of our problem (not domain) and justify each component of our method. \nTo begin with, the complex components of our architecture are designed NOT for the domain BUT for other challenges that we describe below: (1) zero-shot generalization over unseen tasks, (2) partial observability induced by instructions, and (3) mapping from instructions to subtasks. \n\n- Challenge 1:  Zero-shot generalization over unseen tasks\nMost prior work on generalization in RL considers transfer learning where either the semantics of the task are fixed, or the agent is further trained on the target task. In contrast, our work considers \u201czero-shot\u201d generalization where the agent should solve previously unseen tasks \u201cwithout experiencing them beforehand\u201d. In this setting, unlike conventional transfer learning, the agent needs to be given a description of the task (e.g., instructions) as additional input in order to be able to generalize to unseen tasks. Generalization over task descriptions is rarely tackled except for the papers we discussed in the related work section (e.g., UVFA [6], Parameterized Skill [3]). In this type of approach, it is necessary to learn a representation of task description (i.e., subtask in our work). We used a neural network to learn a representation of the subtask, and the term \u201csubtask embedding\u201d means such a learned representation. \n\n-- Why analogy-making regularization?\nSimply learning the mapping from the subtask to the policy (or parameterized skill/option) does not guarantee that the learned mapping will generalize to unseen subtasks (i.e., \u201czero-shot\u201d subtask generalization). This is why we proposed the analogy-making regularizer that allows the agent to learn the underlying manifold of the subtask space so that the agent can successfully map even an unseen subtask to a good policy. In our main experiment, we used analogy-making to encourage the agent to learn that it should perform any actions (e.g., pick up) on any target objects (e.g., cow) in the same way. Note that this is just one way of using our analogy-making regularizer. It can also address more complex generalization scenarios (e.g., \u201cinteract with X\u201d) as discussed in Appendix B; in such cases, meaning of \u201cinteract\u201d changes depending on the target objects, and simple methods (e.g., concatenation of action and target object embeddings) will fail in this scenario.\n\n- Challenge 2: Partial observability induced by instructions\nMuch of the prior work on solving sequential RL tasks uses fully-observable environments [1, 2, 3, 5, 7] (i.e., a specific subtask to execute at each time step can be unambiguously inferred from the current observation). In contrast, our environment is \u201cpartially observable\u201d because the agent is given just a full list of instructions, but it\u2019s NOT given which instruction it has to execute at each time-step. In addition, the current observation (i.e., grid-world with objects) does not tell the agent which instruction to execute. Thus, the agent needs to \u201cremember\u201d how many instructions it has finished and decide when to move on to the next instruction. We chose this challenging setting motivated by the problem of a household robot that is required to execute a list of previously unseen instructions without human supervision that tells the robot what to do for every step. \n\n-- Why use memory?\nWe believe that our memory architecture is a much simplified version of Neural Turing Machines and has only the *minimal and necessary* components for dealing with partial observability described above. Without the memory component, there is no way for the agent to keep track of its progress on instruction execution. \n\n- Challenge 3: Mapping from instructions to subtasks \nEven though the meta controller is given a subtask controller that has pre-trained skills, the mapping from instructions to a sequence of skills (subtasks) is not trivial in our problem because of the following reasons: 1) stochastic events (i.e., randomly appearing enemy) require the agent to \u201cinterrupt\u201d the current subtask. 2) \u201cTransform/Pick up ALL\u201d instructions require the agent to repeat the same subtask for a while, and the number of repetition depends on the observation. Moreover, the reward signal is quite delayed due to this type of instruction. \n\n-- Why differentiable temporal abstraction? \nIn the meta controller, selecting a subtask at every time-step is inefficient and makes it harder to learn under delayed reward. It is known that \u201ctemporal abstraction\u201d provided by options can allow the meta controller to learn faster as the meta controller can use the temporally-extended actions in SMDP [8]. However, the meta controller cannot simply use the subtask termination signal provided by the subtask controller to define the time-scale of its actions due to the necessity of \u201cinterruption\u201d mechanism. Thus, we proposed a new way to \u201clearn\u201d the dynamic time-scale in the meta controller through neural networks. Although the agent can also deal with the challenges without such learned temporal abstraction in principle, we show empirically that the meta controller with our idea (learned temporal abstraction) performs significantly better than the baseline which updates the subtask at every time-step. \n\n- Justification of the use of other minor techniques\nThe three techniques listed below are existing methods that are applied to our problem in an appropriate way. Note that they are not designed to tackle the main challenges of our problem described above, but we used them to improve the overall results or stabilize training. We provided the reason why we used those techniques. \n\n-- Parameter prediction [9]: This approach has been shown to be effective for one-shot and zero-shot image classification problems. We used this technique because we also aim for zero-shot generalization over unseen tasks. \n-- Policy distillation [10]: This approach has been shown to be more efficient for multi-task policy learning. Although we found that policy distillation is not \u201cnecessary\u201d to train the subtask controller, it gives slightly better results than training from scratch. \n-- Generalized Advantage Estimation [11]: This is a recent state-of-the-art technique to combine bootstrapped values and Monte-Carlo return to improve the stability and efficiency of training.\n\n[References]\n[1] George Konidaris, Andrew G. Barto. Building Portable Options: Skill Transfer in Reinforcement Learning, IJCAI 2007.\n[2] George Konidaris, Ilya Scheidwasser, Andrew G. Barto. Transfer in Reinforcement Learning via Shared Features, Journal of Machine Learning Research 2012.\n[3] Bruno Castro da Silva, George Konidaris, Andrew G. Barto, Learning Parameterized Skills, ICML 2012.\n[4] Warwick Masson, George Konidaris, Reinforcement Learning with Parameterized Actions, AAAI 2016.\n[5] S. R. K. Branavan, Harr Chen, Luke S. Zettlemoyer, Regina Barzilay, Reinforcement Learning for Mapping Instructions to Actions, ACL 2009.\n[6] Tom Schaul, Daniel Horgan, Karol Gregor, David Silver. Universal Value Function Approximators, ICML 2015.\n[7] Satinder Pal Singh. Transfer of learning by composing solutions of elemental sequential tasks. Machine Learning, 8(3-4):323\u2013339, 1992.\n[8] Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1):181\u2013211, 1999.\n[9] J. Lei Ba, K. Swersky, S. Fidler, et al. Predicting deep zero-shot convolutional neural networks using textual descriptions. CVPR 2015.\n[10] A. A. Rusu, S. G. Colmenarejo, C. Gulcehre, G. Desjardins, J. Kirkpatrick, R. Pascanu, V. Mnih, K. Kavukcuoglu, and R. Hadsell. Policy distillation. ICLR 2016.\n[11] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using generalized advantage estimation. ICLR 2016."
  },
  {
    "people": [
      "Sutton",
      "da Silva",
      "Mankowitz",
      "Mann",
      "Konidaris",
      "Konidaris",
      "Branavan",
      "Branavan",
      "Branavan"
    ],
    "review": "This paper presents an architecture and corresponding algorithms for\nlearning to act across multiple tasks, described in natural language.\nThe proposed system is hierarchical and is closely related to the options\nframework. However, rather than learning a discrete set of options, it learns\na mapping from natural instructions to an embedding which implicitly (dynamically)\ndefines an option. This is a novel and interesting new perspective on options\nwhich had only slightly been explored in the linear setting (see comments below).\nI find the use of policy distillation particularly relevant for this setting.\nThis, on its own, could be a takeaway for many RL readers who might not necessarily\nbe interested about NLP applications.\n\nIn general, the paper does not describe a single, simple, end-to-end,\nrecipe for learning with this architecture. It rather relies on many recent\nadvances skillfully combined: generalized advantage estimation, analogy-making\nregularizers, L1 regularization, memory addressing, matrix factorization,\npolicy distillation. I would have liked to see some analysis but\nunderstand that it would have certainly been no easy task.\nFor example, when you say \"while the parameters of the subtask controller are\nfrozen\", this sounds to me like you're having some kind of two-timescale stochastic gradient\ndescent. I'm also unsure how you deal with the SMDP structure in your gradient\nupdates when you move to the \"temporal abstractions\" setting.\n\nI am inclined to believe that this approach has the potential to scale up to\nvery large domains, but paper currently does not demonstrate this\nempirically. Like any typical reviewer, I would be tempted to say that\nyou should perform larger experiments. However, I'm also glad that you have\nshown that your system also performs well in a \"toy\" domain. The characterization\nin figure 3 is insightful and makes a good point for the analogy regularizer\nand need for hierarchy.\n\nOverall, I think that the proposed architecture would inspire other researchers\nand would be worth being presented at ICLR. It also contains novel elements\n(subtask embeddings) which could be useful outside the deep and NLP communities\ninto the more \"traditional\" RL communities.\n\n# Parameterized Options\n\nSutton et. al (1999) did not explore the concept\nof *parameterized* options originally. It only came later, perhaps first with\n[\"Optimal policy switching algorithms for reinforcement\nlearning, Comanici & Precup, 2010\"] or\n[\"Unified Inter and Intra Options Learning Using Policy Gradient Methods\", Levy & Shimkin, 2011].\nKonidaris also has a line of work  on \"parametrized skills\":\n[\"Learning Parameterized Skills\". da Silva, Konidaris, Barto, 2012)]\nor [\"Reinforcement Learning with Parameterized Actions\". Masson, Ranchod, Konidaris, 2015].\n\nAlso, I feel that there is a very important distinction to be made with\nthe expression \"parametrized options\". In your work, \"parametrized\" comes in\ntwo flavors. In the spirit of policy gradient methods,\nwe can have options whose policies and termination functions are represented\nby function approximators (in the same way that we have function approximation\nfor value functions). Those options have parameters and we might call them\n\"parameterized\" because of that. This is the setting of Comanicy & Precup (2010),\nLevy & Shimkin (2011) Bacon & Precup (2015), Mankowitz, Mann, and\nMannor (2016) for example.\n\nNow, there a second case where options/policies/skills take parameters *as inputs*\nand act accordingly. This is what Konidaris & al. means by \"parameterized\", whose\nmeaning differs from the \"function approximation\" case above.\nIn your work, the embedding of subtasks arguments is the \"input\" to your options\nand therefore behave as \"parameters\" in the sense of Konidaris.\n\n# Related Work\n\nI CTRL-F through the PDF but couldn't find references to any of S.R.K. Branavan's\nwork. Branavan's PhD thesis had to do with using control techniques from RL\nin order to interpret natural instructions so as to achieve a goal. For example,\nin \"Reinforcement Learning for Mapping Instructions to Actions\", an RL agent\nlearns from \"Windows troubleshooting articles\" to interact with UI elements\n(environment) through a Softmax policy (over linear features) learned by policy\ngradient methods.\n\nAs you mention under \"Instruction execution\" the focus of your work in\non generalization, which is not treated explicitely (afaik) in Branavan's work.\nStill, it shares some important algorithmic and architectural similarities which\nshould be discussed explicitly or perhaps even compared to in your experiments\n(as a baseline).\n\n## Zero-shot and UVFA\n\nIt might also want to consider\n\"Learning Shared Representations for Value Functions in Multi-task\nReinforcement Learning\", Borsa, Graepel, Shawe-Taylor]\nunder the section \"zero-shot tasks generalization\". \n\n\n# Minor Issues\n\nI first read the abstract without knowing what the paper would be about\nand got confused in the second sentence. You talk about \"longer sequences of\npreviously seen instructions\", but I didn't know what clearly\nmeant by \"instructions\" until the second to last sentence where you specify\n\"instructions described by *natural language*.\" You could perhaps\nre-order the sentences to make it clear in the second sentence that you are\ninterested in NLP problems.\n\nZero-generalization: I was familiar with the term \"one-shot\" but not \"zero-shot\".\nThe way that the second sentence \"[...] to have *similar* zero-shot [...]\" follows\nfrom the first sentence might as well hold for the \"one-shot\" setting. You\ncould perhaps add a citation to \"zero-shot\", or define it more\nexplicitly from the beginning and compare it to the one-shot setting. It could\nalso be useful if you explain how zero-shot relates to just the notion of\nlearning with \"priors\".\n\nUnder section 3, you say \"cooperate with each other\" which sounds to me very much\nlike a multi-agent setting, which your work does not explore in this way.\nYou might want to choose a different terminology or explain more precisely if there\nis any connection with the multi-agent setting.\n\nThe second sentence of section 6 is way to long and difficult to parse. You could\nprobably split it in two or three sentences.\n"
  },
  {
    "people": [
      "John McCarthy",
      "Jack Mostow"
    ],
    "review": "This paper can be seen as instantiating a famous paper by the founder of AI John McCarthy on learning to take advice (which was studied in depth by other later researchers, such as Jack Mostow in the card game Hearts). The idea is that the agent is given high level instructions on how to solve a problem, and must distill from it a low level policy. This is quite related to how humans learn complex tasks in many domains (e.g., driving, where a driving instructor may provide advice such as \"keep a certain distance from the car in front\"). \n\nA fairly complex neural deep learning controller architecture is used, although the details of this system are somewhat confusing in terms of many details that are presented. A simpler approach might have been easier to follow, at least initially. The experiments unfortunately are on a rather simplistic 2D maze, and it would have been worthwhile to see how the approach scaled to more complex tasks of the sort usually seen in deep RL papers these days (e.g, Atari, physics simulators etc.). \n\nNice overall idea, somewhat confusing description of the solution, and an inadequate set of experiments on a less than satisfactory domain of 2D grid worlds. "
  },
  {
    "people": [
      "George Konidaris",
      "Andrew G. Barto",
      "George Konidaris",
      "Ilya Scheidwasser",
      "Andrew G. Barto",
      "Bruno Castro da Silva",
      "George Konidaris",
      "Andrew G. Barto",
      "Warwick Masson",
      "George Konidaris",
      "S. R. K. Branavan",
      "Harr Chen",
      "Luke S. Zettlemoyer",
      "Regina Barzilay",
      "Tom Schaul",
      "Daniel Horgan",
      "Karol Gregor",
      "David Silver",
      "Satinder Pal Singh",
      "Richard S Sutton",
      "Doina Precup",
      "Satinder Singh",
      "J. Lei Ba",
      "K. Swersky",
      "S. Fidler",
      "A. A. Rusu",
      "S. G. Colmenarejo",
      "C. Gulcehre",
      "G. Desjardins",
      "J. Kirkpatrick",
      "R. Pascanu",
      "V. Mnih",
      "K. Kavukcuoglu",
      "R. Hadsell",
      "J. Schulman",
      "P. Moritz",
      "S. Levine",
      "M. Jordan",
      "P. Abbeel"
    ],
    "review": "Dear reviewers,\n\nThank you for your valuable comments. \nWe have revised our paper by reflecting many comments from you.\nWe also added new results from a 3D visual domain to address concerns regarding simplicity of the 2D grid-world domain (Section 6.5).\n\nWe put a common response here as many of you raised similar questions/comments about complexity of our architecture and simplicity of the problem. We describe challenging aspects of our problem (not domain) and justify each component of our method. \nTo begin with, the complex components of our architecture are designed NOT for the domain BUT for other challenges that we describe below: (1) zero-shot generalization over unseen tasks, (2) partial observability induced by instructions, and (3) mapping from instructions to subtasks. \n\n- Challenge 1:  Zero-shot generalization over unseen tasks\nMost prior work on generalization in RL considers transfer learning where either the semantics of the task are fixed, or the agent is further trained on the target task. In contrast, our work considers \u201czero-shot\u201d generalization where the agent should solve previously unseen tasks \u201cwithout experiencing them beforehand\u201d. In this setting, unlike conventional transfer learning, the agent needs to be given a description of the task (e.g., instructions) as additional input in order to be able to generalize to unseen tasks. Generalization over task descriptions is rarely tackled except for the papers we discussed in the related work section (e.g., UVFA [6], Parameterized Skill [3]). In this type of approach, it is necessary to learn a representation of task description (i.e., subtask in our work). We used a neural network to learn a representation of the subtask, and the term \u201csubtask embedding\u201d means such a learned representation. \n\n-- Why analogy-making regularization?\nSimply learning the mapping from the subtask to the policy (or parameterized skill/option) does not guarantee that the learned mapping will generalize to unseen subtasks (i.e., \u201czero-shot\u201d subtask generalization). This is why we proposed the analogy-making regularizer that allows the agent to learn the underlying manifold of the subtask space so that the agent can successfully map even an unseen subtask to a good policy. In our main experiment, we used analogy-making to encourage the agent to learn that it should perform any actions (e.g., pick up) on any target objects (e.g., cow) in the same way. Note that this is just one way of using our analogy-making regularizer. It can also address more complex generalization scenarios (e.g., \u201cinteract with X\u201d) as discussed in Appendix B; in such cases, meaning of \u201cinteract\u201d changes depending on the target objects, and simple methods (e.g., concatenation of action and target object embeddings) will fail in this scenario.\n\n- Challenge 2: Partial observability induced by instructions\nMuch of the prior work on solving sequential RL tasks uses fully-observable environments [1, 2, 3, 5, 7] (i.e., a specific subtask to execute at each time step can be unambiguously inferred from the current observation). In contrast, our environment is \u201cpartially observable\u201d because the agent is given just a full list of instructions, but it\u2019s NOT given which instruction it has to execute at each time-step. In addition, the current observation (i.e., grid-world with objects) does not tell the agent which instruction to execute. Thus, the agent needs to \u201cremember\u201d how many instructions it has finished and decide when to move on to the next instruction. We chose this challenging setting motivated by the problem of a household robot that is required to execute a list of previously unseen instructions without human supervision that tells the robot what to do for every step. \n\n-- Why use memory?\nWe believe that our memory architecture is a much simplified version of Neural Turing Machines and has only the *minimal and necessary* components for dealing with partial observability described above. Without the memory component, there is no way for the agent to keep track of its progress on instruction execution. \n\n- Challenge 3: Mapping from instructions to subtasks \nEven though the meta controller is given a subtask controller that has pre-trained skills, the mapping from instructions to a sequence of skills (subtasks) is not trivial in our problem because of the following reasons: 1) stochastic events (i.e., randomly appearing enemy) require the agent to \u201cinterrupt\u201d the current subtask. 2) \u201cTransform/Pick up ALL\u201d instructions require the agent to repeat the same subtask for a while, and the number of repetition depends on the observation. Moreover, the reward signal is quite delayed due to this type of instruction. \n\n-- Why differentiable temporal abstraction? \nIn the meta controller, selecting a subtask at every time-step is inefficient and makes it harder to learn under delayed reward. It is known that \u201ctemporal abstraction\u201d provided by options can allow the meta controller to learn faster as the meta controller can use the temporally-extended actions in SMDP [8]. However, the meta controller cannot simply use the subtask termination signal provided by the subtask controller to define the time-scale of its actions due to the necessity of \u201cinterruption\u201d mechanism. Thus, we proposed a new way to \u201clearn\u201d the dynamic time-scale in the meta controller through neural networks. Although the agent can also deal with the challenges without such learned temporal abstraction in principle, we show empirically that the meta controller with our idea (learned temporal abstraction) performs significantly better than the baseline which updates the subtask at every time-step. \n\n- Justification of the use of other minor techniques\nThe three techniques listed below are existing methods that are applied to our problem in an appropriate way. Note that they are not designed to tackle the main challenges of our problem described above, but we used them to improve the overall results or stabilize training. We provided the reason why we used those techniques. \n\n-- Parameter prediction [9]: This approach has been shown to be effective for one-shot and zero-shot image classification problems. We used this technique because we also aim for zero-shot generalization over unseen tasks. \n-- Policy distillation [10]: This approach has been shown to be more efficient for multi-task policy learning. Although we found that policy distillation is not \u201cnecessary\u201d to train the subtask controller, it gives slightly better results than training from scratch. \n-- Generalized Advantage Estimation [11]: This is a recent state-of-the-art technique to combine bootstrapped values and Monte-Carlo return to improve the stability and efficiency of training.\n\n[References]\n[1] George Konidaris, Andrew G. Barto. Building Portable Options: Skill Transfer in Reinforcement Learning, IJCAI 2007.\n[2] George Konidaris, Ilya Scheidwasser, Andrew G. Barto. Transfer in Reinforcement Learning via Shared Features, Journal of Machine Learning Research 2012.\n[3] Bruno Castro da Silva, George Konidaris, Andrew G. Barto, Learning Parameterized Skills, ICML 2012.\n[4] Warwick Masson, George Konidaris, Reinforcement Learning with Parameterized Actions, AAAI 2016.\n[5] S. R. K. Branavan, Harr Chen, Luke S. Zettlemoyer, Regina Barzilay, Reinforcement Learning for Mapping Instructions to Actions, ACL 2009.\n[6] Tom Schaul, Daniel Horgan, Karol Gregor, David Silver. Universal Value Function Approximators, ICML 2015.\n[7] Satinder Pal Singh. Transfer of learning by composing solutions of elemental sequential tasks. Machine Learning, 8(3-4):323\u2013339, 1992.\n[8] Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1):181\u2013211, 1999.\n[9] J. Lei Ba, K. Swersky, S. Fidler, et al. Predicting deep zero-shot convolutional neural networks using textual descriptions. CVPR 2015.\n[10] A. A. Rusu, S. G. Colmenarejo, C. Gulcehre, G. Desjardins, J. Kirkpatrick, R. Pascanu, V. Mnih, K. Kavukcuoglu, and R. Hadsell. Policy distillation. ICLR 2016.\n[11] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using generalized advantage estimation. ICLR 2016."
  },
  {
    "people": [
      "Sutton",
      "da Silva",
      "Mankowitz",
      "Mann",
      "Konidaris",
      "Konidaris",
      "Branavan",
      "Branavan",
      "Branavan"
    ],
    "review": "This paper presents an architecture and corresponding algorithms for\nlearning to act across multiple tasks, described in natural language.\nThe proposed system is hierarchical and is closely related to the options\nframework. However, rather than learning a discrete set of options, it learns\na mapping from natural instructions to an embedding which implicitly (dynamically)\ndefines an option. This is a novel and interesting new perspective on options\nwhich had only slightly been explored in the linear setting (see comments below).\nI find the use of policy distillation particularly relevant for this setting.\nThis, on its own, could be a takeaway for many RL readers who might not necessarily\nbe interested about NLP applications.\n\nIn general, the paper does not describe a single, simple, end-to-end,\nrecipe for learning with this architecture. It rather relies on many recent\nadvances skillfully combined: generalized advantage estimation, analogy-making\nregularizers, L1 regularization, memory addressing, matrix factorization,\npolicy distillation. I would have liked to see some analysis but\nunderstand that it would have certainly been no easy task.\nFor example, when you say \"while the parameters of the subtask controller are\nfrozen\", this sounds to me like you're having some kind of two-timescale stochastic gradient\ndescent. I'm also unsure how you deal with the SMDP structure in your gradient\nupdates when you move to the \"temporal abstractions\" setting.\n\nI am inclined to believe that this approach has the potential to scale up to\nvery large domains, but paper currently does not demonstrate this\nempirically. Like any typical reviewer, I would be tempted to say that\nyou should perform larger experiments. However, I'm also glad that you have\nshown that your system also performs well in a \"toy\" domain. The characterization\nin figure 3 is insightful and makes a good point for the analogy regularizer\nand need for hierarchy.\n\nOverall, I think that the proposed architecture would inspire other researchers\nand would be worth being presented at ICLR. It also contains novel elements\n(subtask embeddings) which could be useful outside the deep and NLP communities\ninto the more \"traditional\" RL communities.\n\n# Parameterized Options\n\nSutton et. al (1999) did not explore the concept\nof *parameterized* options originally. It only came later, perhaps first with\n[\"Optimal policy switching algorithms for reinforcement\nlearning, Comanici & Precup, 2010\"] or\n[\"Unified Inter and Intra Options Learning Using Policy Gradient Methods\", Levy & Shimkin, 2011].\nKonidaris also has a line of work  on \"parametrized skills\":\n[\"Learning Parameterized Skills\". da Silva, Konidaris, Barto, 2012)]\nor [\"Reinforcement Learning with Parameterized Actions\". Masson, Ranchod, Konidaris, 2015].\n\nAlso, I feel that there is a very important distinction to be made with\nthe expression \"parametrized options\". In your work, \"parametrized\" comes in\ntwo flavors. In the spirit of policy gradient methods,\nwe can have options whose policies and termination functions are represented\nby function approximators (in the same way that we have function approximation\nfor value functions). Those options have parameters and we might call them\n\"parameterized\" because of that. This is the setting of Comanicy & Precup (2010),\nLevy & Shimkin (2011) Bacon & Precup (2015), Mankowitz, Mann, and\nMannor (2016) for example.\n\nNow, there a second case where options/policies/skills take parameters *as inputs*\nand act accordingly. This is what Konidaris & al. means by \"parameterized\", whose\nmeaning differs from the \"function approximation\" case above.\nIn your work, the embedding of subtasks arguments is the \"input\" to your options\nand therefore behave as \"parameters\" in the sense of Konidaris.\n\n# Related Work\n\nI CTRL-F through the PDF but couldn't find references to any of S.R.K. Branavan's\nwork. Branavan's PhD thesis had to do with using control techniques from RL\nin order to interpret natural instructions so as to achieve a goal. For example,\nin \"Reinforcement Learning for Mapping Instructions to Actions\", an RL agent\nlearns from \"Windows troubleshooting articles\" to interact with UI elements\n(environment) through a Softmax policy (over linear features) learned by policy\ngradient methods.\n\nAs you mention under \"Instruction execution\" the focus of your work in\non generalization, which is not treated explicitely (afaik) in Branavan's work.\nStill, it shares some important algorithmic and architectural similarities which\nshould be discussed explicitly or perhaps even compared to in your experiments\n(as a baseline).\n\n## Zero-shot and UVFA\n\nIt might also want to consider\n\"Learning Shared Representations for Value Functions in Multi-task\nReinforcement Learning\", Borsa, Graepel, Shawe-Taylor]\nunder the section \"zero-shot tasks generalization\". \n\n\n# Minor Issues\n\nI first read the abstract without knowing what the paper would be about\nand got confused in the second sentence. You talk about \"longer sequences of\npreviously seen instructions\", but I didn't know what clearly\nmeant by \"instructions\" until the second to last sentence where you specify\n\"instructions described by *natural language*.\" You could perhaps\nre-order the sentences to make it clear in the second sentence that you are\ninterested in NLP problems.\n\nZero-generalization: I was familiar with the term \"one-shot\" but not \"zero-shot\".\nThe way that the second sentence \"[...] to have *similar* zero-shot [...]\" follows\nfrom the first sentence might as well hold for the \"one-shot\" setting. You\ncould perhaps add a citation to \"zero-shot\", or define it more\nexplicitly from the beginning and compare it to the one-shot setting. It could\nalso be useful if you explain how zero-shot relates to just the notion of\nlearning with \"priors\".\n\nUnder section 3, you say \"cooperate with each other\" which sounds to me very much\nlike a multi-agent setting, which your work does not explore in this way.\nYou might want to choose a different terminology or explain more precisely if there\nis any connection with the multi-agent setting.\n\nThe second sentence of section 6 is way to long and difficult to parse. You could\nprobably split it in two or three sentences.\n"
  },
  {
    "people": [
      "John McCarthy",
      "Jack Mostow"
    ],
    "review": "This paper can be seen as instantiating a famous paper by the founder of AI John McCarthy on learning to take advice (which was studied in depth by other later researchers, such as Jack Mostow in the card game Hearts). The idea is that the agent is given high level instructions on how to solve a problem, and must distill from it a low level policy. This is quite related to how humans learn complex tasks in many domains (e.g., driving, where a driving instructor may provide advice such as \"keep a certain distance from the car in front\"). \n\nA fairly complex neural deep learning controller architecture is used, although the details of this system are somewhat confusing in terms of many details that are presented. A simpler approach might have been easier to follow, at least initially. The experiments unfortunately are on a rather simplistic 2D maze, and it would have been worthwhile to see how the approach scaled to more complex tasks of the sort usually seen in deep RL papers these days (e.g, Atari, physics simulators etc.). \n\nNice overall idea, somewhat confusing description of the solution, and an inadequate set of experiments on a less than satisfactory domain of 2D grid worlds. "
  },
  {
    "people": [
      "Sanja Fidler",
      "Leo Zhu",
      "Alan Yuille",
      "Song-Chun Zhu's"
    ],
    "review": "This paper presents an approach to learn object representations by composing a set of templates which are leaned from binary images. \nIn particular, a hierarchical model is learned by combining AND, OR and POOL operations. Learning is performed by using approximated inference with MAX-product BP follow by a heuristic to threshold activations to be binary. \n\nLearning hierarchical representations that are interpretable is a very interesting topic, and this paper brings some good intuitions in light of modern convolutional neural nets. \n\nI have however, some concerns about the paper:\n\n1) the paper fails to cite and discuss relevant literature and claims to be the first one that is able to learn interpretable parts. \nI would like to see a discussion of the proposed approach compared to a variety of papers e.g.,:\n\n- Compositional hierarchies of Sanja Fidler\n- AND-OR graphs used by Leo Zhu and Alan Yuille to model objects\n- AND-OR templates of Song-Chun Zhu's group at UCLA \n\nThe claim that this paper is the first to discover such parts should be removed. \n\n2) The experimental evaluation is limited to very toy datasets. The papers I mentioned have been applied to real images (e.g., by using contours to binarize the images). \nI'll also like to see how good/bad the proposed approach is for classification in more well known benchmarks. \nA comparison to other generative models such as VAE, GANS, etc will also be useful.\n\n3) I'll also like to see a discussion of the relation/differences/advantages of the proposed approach wrt to sum product networks and grammars.\n\nOther comments:\n\n- the paper claims that after learning inference is feed-forward, but since message passing is used, it should be a recurrent network. \n\n- the algorithm and tech discussion should be moved from the appendix to the main paper\n\n- the introduction claims that compression is a prove for understanding. I disagree with this statement, and should be removed. \n\n- I'll also like to see a discussion relating the proposed approach to the Deep Rendering model. \n\n- It is not obvious how some of the constraints are satisfied during message passing. Also constraints are well known to be difficult to optimize with max product. How do you handle this?\n\n- The learning and inference algorithms seems to be very heuristic (e.g., clipping to 1, heuristics on which messages are run). Could you analyze the choices you make?\n\n- doing multiple steps of 5) 2) is not a single backward pass \n\nI'll reconsider my score in light of the answers"
  },
  {
    "people": [
      "Sanja Fidler",
      "Leo Zhu",
      "Alan Yuille",
      "Song-Chun Zhu's"
    ],
    "review": "This paper presents an approach to learn object representations by composing a set of templates which are leaned from binary images. \nIn particular, a hierarchical model is learned by combining AND, OR and POOL operations. Learning is performed by using approximated inference with MAX-product BP follow by a heuristic to threshold activations to be binary. \n\nLearning hierarchical representations that are interpretable is a very interesting topic, and this paper brings some good intuitions in light of modern convolutional neural nets. \n\nI have however, some concerns about the paper:\n\n1) the paper fails to cite and discuss relevant literature and claims to be the first one that is able to learn interpretable parts. \nI would like to see a discussion of the proposed approach compared to a variety of papers e.g.,:\n\n- Compositional hierarchies of Sanja Fidler\n- AND-OR graphs used by Leo Zhu and Alan Yuille to model objects\n- AND-OR templates of Song-Chun Zhu's group at UCLA \n\nThe claim that this paper is the first to discover such parts should be removed. \n\n2) The experimental evaluation is limited to very toy datasets. The papers I mentioned have been applied to real images (e.g., by using contours to binarize the images). \nI'll also like to see how good/bad the proposed approach is for classification in more well known benchmarks. \nA comparison to other generative models such as VAE, GANS, etc will also be useful.\n\n3) I'll also like to see a discussion of the relation/differences/advantages of the proposed approach wrt to sum product networks and grammars.\n\nOther comments:\n\n- the paper claims that after learning inference is feed-forward, but since message passing is used, it should be a recurrent network. \n\n- the algorithm and tech discussion should be moved from the appendix to the main paper\n\n- the introduction claims that compression is a prove for understanding. I disagree with this statement, and should be removed. \n\n- I'll also like to see a discussion relating the proposed approach to the Deep Rendering model. \n\n- It is not obvious how some of the constraints are satisfied during message passing. Also constraints are well known to be difficult to optimize with max product. How do you handle this?\n\n- The learning and inference algorithms seems to be very heuristic (e.g., clipping to 1, heuristics on which messages are run). Could you analyze the choices you make?\n\n- doing multiple steps of 5) 2) is not a single backward pass \n\nI'll reconsider my score in light of the answers"
  },
  {
    "people": [
      "Christian",
      "Sergey Ioffe",
      "Vincent Vanhoucke"
    ],
    "review": "This paper proposes a design principle for computation blocks in convolutional networks based on repeated application of expand and join operations resulting in a fractal-like structure. \n\nThis paper is primarily about experimental evaluation, since the objective is to show that a residual formulation is not necessary to obtain good performance, at least on some tasks.\n\nHowever, in my opinion the evaluations in the paper are not convincing. The primary issue is lack of a proper baseline, against which the improvements can be clearly demonstrated by making isolated changes. I understand that for this paper such a baseline is hard to construct, since it is about a novel architecture principle. This is why more effort should be put into this, so that core insights from this paper can be useful even after better performing architectures are discovered.\nThe number of parameters and amount of computation should be used to indicate how fair the comparisons are between architectures. Some detailed comments:\n\n- In Table 1 comparisons to Resnets, the resnets from He et al. 2016b and Wide Resnets should be compared to FractalNet (in lieu of a proper baseline). The first outperforms FractalNet on CIFAR-100 while the second outperforms it on both. The authors compare to other results without augmentation, but did not perform additional experiments without augmentation for these architectures.\n\n- The 40 layer Fractal Net should not be compared to other models unless the parameter reduction tricks are utilized for the other models as well.\n\n- A proper comparison to Inception networks should also be performed for these networks. My guess is that the reason behind a seemingly 'ad-hoc' design of Inception modules is to reduce the computational footprint of the model (which is not a central motivation of fractal nets). Since this model is directly related to the Inception module due to use of shorter and longer paths without shortcuts, one can easily simplify the Inception design to build a strong baseline e.g. by converting the concatenation operation to a mean operation among equally sized convolution outputs. As an aside, note that Inception networks have already shown that residual networks are not necessary to obtain the best performance [1].\n\n- It should be noted that Residual/Highway architectures do have a type of anytime property, as shown by lesioning experiments in Srivastava et al and Viet et al.\n\n- The architecture specific drop-path regularization is interesting, but is used along with other regularizers such as dropout, batch norm and weight decay and its benefit on its own is not clear.\n\nOverall, it's not clear to me that the experiments clearly demonstrate the utility of the proposed architecture. \n\n[1] Szegedy, Christian, Sergey Ioffe, and Vincent Vanhoucke. \"Inception-v4, inception-resnet and the impact of residual connections on learning.\" arXiv preprint arXiv:1602.07261 (2016)."
  },
  {
    "people": [
      "Veit",
      "Huang"
    ],
    "review": "Authors claim that the reviewers reflect \"narrow view\" of on the number of parameters and fractions of accuracy and  \"complete focus on engineering\" etc . Authors claims about focusing on engineering are open to debate as only the technicality in the paper is the fractal network architecture with intuitive claims. One can either formulate the system with rigorous analysis with clear assumptions (which would require satisfactory theoretical analysis and relatively small scale empirical sanity check without heavy experiments) or propose a comprehensive empirical analysis by designing careful experiments to support intuitive claims. Therefore, as the approach clearly does not belong to the first category,  such an approach needs strong experimental evidence to support intuitive claims without a doubt.  The rebuttal lists unsatisfactory answers with somehow manipulative arguments about narrow reviewing/ dates of the baselines( which will be discussed below).. \n\n\nAuthors state in the paper that \" In experiments, fractal networks **match the state-of-the-art performance held by residual networks** on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks\" and \"Fractal architectures are arguably the simplest means of satisfying this requirement, and match or exceed residual networks in experimental performance\". Therefore the main support behind all the claims is that fractal network match (or exceed) performance of state of art residual networks, indeed the empirical study only compares accuracy to some baselines without designing empirical analysis for the claims about differences to resnets (or ensemble explanation of resnet by Veit et al(NIPS 2016) ). \nHowever even this comparison lacks many results as explained in my review.   Therefore the expectation of the fair comparison is completely reasonable and it is not about only focusing on fractions of accuracy or number of parameters. As a good example, Veit et al demonstrates systematical empirical support for their claims about analyzing residual networks (which seems to be on arxiv since May but still not cited on paper ). Table 4 and Figure 3 provide good preliminary sanity check but compares only to plain networks, do not support claims about differences between residual and fractal networks.\n\nThe paper claims that the fractal network scales to \"ultra\" deep networks, however authors can not report results on dozens of layers. Authors also claim that extra depth may slow training (not clear how much) but does not decrease accuracy but this is not clear as there are no results for dozens of layers, also in Table 3, error increases as depth increase to 160 layers.  Number of parameters of the proposed architecture is significantly greater than the state of art resnet variants as I explained in the review but the authors argue that it is slightly more parameters. \n\nAs an answer to lack of comparison to DenseNet, authors argue in rebuttal that DenseNet cites FractalNet but this can not be a reason for the lack of comparison as DenseNet was published in August and well-known as holding state of art results as a resnet variant. \n\nAuthors state in rebuttal that \"Many of the variants only reported results on CIFAR/SVHN\u2026 it is a bit difficult to have already compared to results that did not exist at submission time.\" .  However there were clearly published ones, for example two results by Huang et al, 2016b on july (arxiv) ..\n\nRegarding the \"simplifying power\" claim of authors.  It is not very convincing that it is simplifying. how can it simplify with a harder training procedure with many parameters that can not scale as good as baselines?\n\nIn essence, my evaluation did not change as rebuttal did not provide satisfactory clarification or improvement.\n"
  },
  {
    "people": [
      "Veit"
    ],
    "review": "Looking through the comment section here, I agree to a large degree with the author's standpoint on many issues discussed. Points (1) through (4) in the authors comment below are, in my opinion, a good summary of the contributions of the paper. While I don't think those contributions are groundbreaking, I believe they are significant enough to merit acceptance.\n\nThe reason I am commenting here is because, having looked at several comment sections for this ICLR, I am seeing a general trend that reviews have a strong focus on performance, i.e. reviews tend to be very short and judge papers, to a large degree, on whether they are a few percentage points better or worse than the reported baseline. E.g. see the comments \"the experimental evaluation is not convincing, e.g. no improvement on SVHN\" or \"the effect of drop-path seems to vanish with data augmentation\" below.\n\nI believe that papers should be judged more on their scientific contributions (see points (1), (2) and (4) below), especially when those papers themselves state that their focus is on those scientific contributions, not on amazing performance.\n\nFurther, I believe the trend to focus excessively on performance is problematic for a number of reasons:\n\n - The Deep Learning community has focused very heavily on a few datasets (MNIST, ImageNet, CIFAR-10, CIFAR-100, SVHN). This means that at any time, a large chunk of the deep learning literature is battling for 5 SOTA titles. Hence, expecting any new model to attain one of those titles is a very high bar.\n\n - It is an arbitrary standard. Say the SOTA on ImageNet improves by 2% a year. Then a paper that outperforms by 1% in 2014 would underperform by 1% in 2015. By the performance standard, the same paper with the same ideas and the same scientific merit would have declined drastically in value over that one year. Is that really true?\n\n - How does one even draw a \"fair comparison\" on these standard datasets at this point? The bag of tricks for neural networks includes: drop-out, l2, l1, ensembling, various forms of data augmentation, various forms of normalization and initialization, various non-linearities, various learning rate schedules, various forms of pooling, label smoothing, gradient clipping etc. etc. There are a gazillion ways to eke out fractions of percentage points of performance. And - every single paper has a unique combination of tricks that they use for their model, even though the tricks themselves are unrelated to the model. Hence, the only truly fair comparison would be to compare against every reference model with the exact trick combination that the paper presenting the reference model used, which would take an exorbitant amount of time. What's worse, many papers do not even report all of the tricks they used. One would have to get the authors code and reverse engineer the model, not to mention slight differences introduced by using e.g. TensorFlow vs. Torch vs. Caffe. In this light, the request from one of the reviewers to have a baseline \"against which the improvements can be clearly demonstrated by making isolated changes\" seems unrealistic to me.\n\n - The ML community should not make excessive fine-tuning of models mandatory for publication. By requiring models to beat SOTA, we force each author to fine-tune their model ad nauseum, which leads to an arms race. To get a publications, authors would spend ever more time fine-tuning their models. This can not only lead to \"training on the test set\", but also wastes the time of researcher that could be better spent exploring new ideas.\n\n - It gives too much power to bad research. In science, there is always a certain background rate of \"bad\" results published: either the numbers are outright fake or the experimental protocol was invalid, e.g. someone used the test set as a validation set or someone did an exorbitant number of random reruns and only published the best single result. What's worse, these \"bad\" results are far more likely to hold the SOTA title at any given time than a \"good\" result. By requiring new publications to beat SOTA, we give too much power to bad results.\n\n - It punishes authors for reporting many or strong baselines. In this paper, authors were careful to report many recent results. Table 1 is thorough. And now they are criticized for not beating all of those baselines. I have a feeling that if the authors of this paper had been more selective about which baselines they report, i.e. those that they can beat, they would have received higher scores on the paper. I have written an in-depth review for another paper at this conference that used, in my opinion, very weak baselines and ended up getting high reviewer marks. I don't think that was a coincidence. \n\nThe same arguments apply, though I think to a lesser degree, to judging models excessively on how many parameters they have or their runtime. However, I agree with reviewers that more information about how models compare in terms of those metrics would enhance this paper. I would like to see a discussion of that in the final version. In general, I think this paper would benefit from an appendix with more details on model and training procedure. I also agree with reviewers that 80 layers, which is the deepest that authors can go while improving test error (Table 3), is not ultra-deep. Hence putting \"ultra-deep\" in the paper title seems exaggerated and I would recommend scaling back the language. However, I don't think being ultra-deep (~1000 layers) is necessary, because as Veit et al showed, networks that appear ultra deep might not be ultra deep in practice. Training an 80-layer net that functions at test time without residual connections seems to be enough of an achievement.\n\nIn summary, I think if a paper makes scientific contribution (see points (1), (2) and (4) below) independent of performance, then competitive performance should be enough for publication, instead of requiring SOTA. I believe this paper achieves that mark."
  },
  {
    "people": [
      "Veit",
      "Veit",
      "Greff",
      "Szegedy",
      "Viet"
    ],
    "review": "We thank the reviewers for their time, but disagree with many points in the reviews.  We first address what we feel is the overall disconnect between the reviews and the contributions of the paper, then rebut specific points below.\n\nMachine learning research is a mix of engineering, mathematics, and science.  Complete focus on engineering can restrict one to the narrow view of judging work by counting parameters and fractions of a percent in accuracy.  The reviews unfortunately reflect this mode of thinking.  However, long-term progress also requires scientific advancement, including new ideas as well as experiments that enhance understanding by revealing simple principles underlying complex phenomena.\n\nFractalNet is a new idea with:\n\n(1) Explanatory power:\n\nAs stated in the abstract, our experiments show that for the success of very deep networks, the \"key may be the ability to transition, during training, from effectively shallow to deep\".  This is the property shared by ResNet and FractalNet.\n\nFurthermore, this observation is an important counterpoint to [Veit et al., NIPS 2016] which claims that: (a) ensemble-like behavior is key and (b) the fraction of available effectively short paths is somehow related to performance.  FractalNet provides a counterexample: we can extract a single column (plain network topology with one long path) and it alone (no ensembling) performs as well the entire network.  In FractalNet, the rest of the network is a training apparatus for transitioning from shallow to deep.  This strongly suggests that, with more careful analysis, the same may be true for ResNet.\n\nIn fact, unlike [Veit et al., NIPS 2016] our explanation is consistent with the view that very deep networks, such as ResNet and FractalNet, learn unrolled iterative estimation [Greff et al., Highway and Residual Networks learn Unrolled Iterative Estimation, arXiv:1612.07771 and ICLR 2017 submission].  In this view, the longest path is most important; the ensemble can be discarded after training.  Moreover, our Section 4.3 and Figure 3 provide insight into the dynamics of this learning process.\n\nWe believe that the research community values understanding these mechanisms and would benefit from the evidence our work injects into the debate.\n\n(2) Simplifying power:\n\nFractalNet shows how a simple design principle produces structure similar to the mishmash of hacks (hand-designed modules, manually-attached auxiliary losses at intermediate depths) required in prior architectures like Inception.  It also expands the set of simple tricks for building very deep networks to include an option other than residual connections.\n\n(3) Good performance:\n\nFractalNet matches a ResNet of equal depth in terms of performance on ImageNet.  Training either of these networks from scratch takes weeks on modern GPUs.  Yet, because FractalNet uses slightly more parameters or does not beat the absolute best ResNet variant (developed after 7 or so papers iterating on ResNet) on the smaller and less important CIFAR dataset, the reviews declare experiments unsatisfactory.  There is a line between demanding high experimental standards and strangling promising ideas; we are all for the former, but hope not to fall victim to the latter.\n\n(4) New capabilities:\n\nContrary to AnonReviewer2's claim, prior work does not demonstrate an anytime property for ResNet; see our specific response below.  FractalNet, in combination with drop-path training, provides a novel anytime output capability, which could prove useful in real-world latency sensitive applications.\n\n\n-----\nResponse to AnonReviewer1\n\nPlease see our comments about experiments above.\n\n> Therefore the empirical effectiveness of drop-path is not convincing too.\n\nDrop-path serves two purposes: (a) additional regularization in the absence of data augmentation and (b) regularization that allows the resulting network to have the anytime property.  Yes, additional data augmentation can compensate for drop-path if one only cares about (a), but drop-path is essential for and the only effective means of enabling anytime output.\n\n> DenseNets (Huang et al, 2016a) should be also included in the comparison\n\nDenseNet cites FractalNet as the original version of FractalNet (arXiv:1605.07648) was published on arXiv three months prior to the original version of DenseNet (arXiv:1608.06993).  We are happy to alert readers to subsequent work, but please keep in mind the historical sequence of development.\n\n> Table 1 has Res-Net variants as baselines however Table 2 has only ResNet.\n\nMany of the variants only reported results on CIFAR/SVHN.  For example, the Wide Residual Networks paper (arXiv:1605.07146), at the time of the ICLR deadline (November 5), did not include ImageNet results.  It was updated weeks later, on November 28, to include ImageNet results.  We can expand the table, but it is a bit difficult to have already compared to results that did not exist at submission time.\n\n> there is no improvement in SVHN dataset results and this is not discussed in the empirical analysis.\n\nSection 4.2 observes \"most methods perform similarly on SVHN\".  The errors rates on SVHN are so low as to be uninformative.  We believe SVHN is simply too easy of a dataset to be a challenge to any of the modern techniques in Table 1.  Much like MNIST, SVHN is now a useful sanity check, but not a differentiator.\n\n> Also, authors give a list of some improvements over Inception (Szegedy et al., 2015) but again these intuitive claims about effectiveness of these changes are not supported with any empirical analysis.\n\nOur point is merely to draw connections between the FractalNet architecture and some hand-designed components of Inception, suggesting a more fundamental explanation for why those particular hand-designed choices are effective.  Widespread adoption of ResNet by the community has displaced Inception, VGG, and other architectures, making ResNet the clear leader and appropriate target for experimental comparison.\n\n\n-----\nResponse to AnonReviewer2\n\nPlease see our comments about experiments and the main contributions of the paper above.\n\n> The 40 layer Fractal Net should not be compared to other models unless the parameter reduction tricks are utilized for the other models as well.\n\nWe are demonstrating feasibility of parameter reduction tricks for FractalNet.  Much of the iterative improvement in the ResNet variants themselves, developed over many papers, already has a component of optimizing the architecture to reduce parameters and/or computational load.\n\n> Since this model is directly related to the Inception module due to use of shorter and longer paths without shortcuts.\n\nThis statement is incorrect.  Our model is not directly related to Inception.  It reproduces some local connectivity reminiscent of Inception modules, but the global connectivity structure is entirely different. Even locally, FractalNet has a recursive join structure which Inception modules lack.  As consequence of this is that at global scale, FractalNet contains many paths whose length range over many orders of magnitude (all powers of 2 up to the maximum depth).  In contrast, the shortest path through an Inception network grows linearly with depth (number of modules stacked).\n\n> As an aside, note that Inception networks have already shown that residual networks are not necessary to obtain the best performance\n\nImageNet performance is a useful benchmark tool, not the end goal of network design.  Mass adoption of ResNet by the community suggests that simplicity and scalability are also legitimate concerns.  With regard to scalability in depth, the only networks previously demonstrated to easily extend to the 100-1000 layer regime rely on some form of residual connection (ResNet/Highway Networks).  We demonstrate a 160-layer FractalNet, placing it in this group.\n\nMoreover, FractalNet and ResNet/Highway all have a mechanism for evolving from being effectively shallow to effectively deep over the course of training. This shallow to deep evolution seems critical, but unfortunately the design of Inception prevents its effective depth from being less than the number of stacked Inception modules.\n\n> It should be noted that Residual/Highway architectures do have a type of anytime property, as shown by lesioning experiments in Srivastava et al and Viet et al.\n\nWe disagree.  The lesioning experiments in Veit et al. demonstrate the opposite: ResNet does not have an anytime property.  If deleting a few layers, then yes, ResNet can recover.  However, Section 4.2 and Figure 5 of Veit et al, show that deleting 10 blocks of a 54 block ResNet increases CIFAR-10 error to 0.2, deleting 20 blocks pushes error above 0.5.  So one cannot maintain a reasonable error if halving the ResNet depth.  Compare this to our Table 4: subnetwork columns of one half (20) and even one fourth (10) the layers of a full FractalNet (40) maintain low error on CIFAR-100.  This robustness across many orders of magnitude in depth (and thus time) is required for a useful anytime property and is unique to FractalNet (with drop-path training).\n\n> The architecture specific drop-path regularization is interesting, but is used along with other regularizers such as dropout, batch norm and weight decay and its benefit on its own is not clear.\n\nDrop-path is essential for and the only effective means of enabling anytime output.  The fact that it can contribute as a regularizer in other scenarios is a bonus.\n\n\n-----\nResponse to AnonReviewer3\n\n> the experimental evaluation is not convincing, e.g. no improvement on SVHN\n\nAs we replied to AnonReviewer1: Section 4.2 observes \"most methods perform similarly on SVHN\".  The errors rates on SVHN are so low as to be uninformative.  We believe SVHN is simply too easy of a dataset to be a challenge to any of the modern techniques in Table 1.  Much like MNIST, SVHN is now a useful sanity check, but not a differentiator.\n\n> number of parameters should be mentioned for all models for fair comparison\n\nWe can add this to the table, but please see our larger discussion of experiments above.\n\n> the effect of drop-path seems to vanish with data augmentation\n\nDrop-path is essential for and the only effective means of enabling anytime output.  The fact that it can contribute as a regularizer in the absence of data augmentation is a bonus.\n"
  },
  {
    "people": [
      "Huang",
      "Szegedy"
    ],
    "review": "This paper proposes a new architecture that does not explicitly use residuals but constructs an architecture that is composed of networks with fractal structure by using expand and join operations. Using the fractal architecture,  authors argue and try to demonstrate that the large nominal network depth with many short paths is the key for 'training 'ultra-deep\u201d networks while residuals are incidental.\n\nThe main bottleneck of this paper is that number of parameters needed for the FractalNet is significantly higher than the baselines which makes it hard to scale to ''ultra-deep\u201d networks.  Authors replied that Wide ResNets also require many parameters but this is not the case for ResNet and other ResNet variants. ResNet and ResNet with Stochastic depth scales to depth of 110 with 1.7M parameters and to depth of 1202 with 10.2M parameters which is much less than the number of parameters for depths of 20 and 40 in Table 1(Huang et al, 2016a).   It is not clear whether FractalNet can perform better than these depths with a reasonable computation. Authors report less parameters for 40 layers but this scaling trick is not validated for other depths including depth 20 in Table 1. On the other hand, the number of parameters for 40 layers with scaling trick is clearly still large compared to most of the baselines. Unsatisfactory comparison to these baselines makes the claims of authors unconvincing.\n\nAuthors also claim that drop-path to provide improvement compared to layer dropping procedure in Huang et al, 2016b however the results show that the empirical gain of this specific regularization disappears when well-known data augmentation techniques applied. Therefore the empirical effectiveness of drop-path is not convincing too.\n\nDenseNets (Huang et al, 2016a) should be also included in the comparison since it outperforms most of the state of art Res Nets on both CIFAR10 and ImageNet and more importantly outperforms the proposed FractalNet significantly and it requires significantly less computation. \n\nTable 1 has Res-Net variants as baselines however Table 2 has only ResNet.  Therefore ImageNet comparison only shows that one can run FractalNet on ImageNet and can perform comparably well to ResNet which is not a satisfactory result given the improvements of other baselines over ResNet.  In addition, there is no improvement in SVHN dataset results and this is not discussed in the empirical analysis.\n\nAlso, authors give a list of some improvements over Inception (Szegedy et al., 2015) but again these intuitive claims about effectiveness of these changes are not supported with any empirical analysis. \n\nAlthough the paper attempts to explore many interesting intuitive directions using the proposed architecture, the empirical results are not support the given claims and the large number of parameters makes the model restrictive in practice hence the contribution does not seem to be significant. \n\nPros:\nProvides an interesting architecture compared to ResNet and its variants and investigates the differences to residual networks which can stimulate some other promising analysis\n\ncons:\n     -    Number of parameters are very large compared to baselines that can have even much higher depths with smaller number of parameters\nThe claims are intuitive but not supported well with empirical evidence\nPath regularization does not yield improvement when the data augmentation is used\n     -     The empirical results do not show whether the method is promising for \u201cultra-deep\u201d networks "
  },
  {
    "people": [
      "Christian",
      "Sergey Ioffe",
      "Vincent Vanhoucke"
    ],
    "review": "This paper proposes a design principle for computation blocks in convolutional networks based on repeated application of expand and join operations resulting in a fractal-like structure. \n\nThis paper is primarily about experimental evaluation, since the objective is to show that a residual formulation is not necessary to obtain good performance, at least on some tasks.\n\nHowever, in my opinion the evaluations in the paper are not convincing. The primary issue is lack of a proper baseline, against which the improvements can be clearly demonstrated by making isolated changes. I understand that for this paper such a baseline is hard to construct, since it is about a novel architecture principle. This is why more effort should be put into this, so that core insights from this paper can be useful even after better performing architectures are discovered.\nThe number of parameters and amount of computation should be used to indicate how fair the comparisons are between architectures. Some detailed comments:\n\n- In Table 1 comparisons to Resnets, the resnets from He et al. 2016b and Wide Resnets should be compared to FractalNet (in lieu of a proper baseline). The first outperforms FractalNet on CIFAR-100 while the second outperforms it on both. The authors compare to other results without augmentation, but did not perform additional experiments without augmentation for these architectures.\n\n- The 40 layer Fractal Net should not be compared to other models unless the parameter reduction tricks are utilized for the other models as well.\n\n- A proper comparison to Inception networks should also be performed for these networks. My guess is that the reason behind a seemingly 'ad-hoc' design of Inception modules is to reduce the computational footprint of the model (which is not a central motivation of fractal nets). Since this model is directly related to the Inception module due to use of shorter and longer paths without shortcuts, one can easily simplify the Inception design to build a strong baseline e.g. by converting the concatenation operation to a mean operation among equally sized convolution outputs. As an aside, note that Inception networks have already shown that residual networks are not necessary to obtain the best performance [1].\n\n- It should be noted that Residual/Highway architectures do have a type of anytime property, as shown by lesioning experiments in Srivastava et al and Viet et al.\n\n- The architecture specific drop-path regularization is interesting, but is used along with other regularizers such as dropout, batch norm and weight decay and its benefit on its own is not clear.\n\nOverall, it's not clear to me that the experiments clearly demonstrate the utility of the proposed architecture. \n\n[1] Szegedy, Christian, Sergey Ioffe, and Vincent Vanhoucke. \"Inception-v4, inception-resnet and the impact of residual connections on learning.\" arXiv preprint arXiv:1602.07261 (2016).\n"
  },
  {
    "people": [
      "Christian",
      "Sergey Ioffe",
      "Vincent Vanhoucke"
    ],
    "review": "This paper proposes a design principle for computation blocks in convolutional networks based on repeated application of expand and join operations resulting in a fractal-like structure. \n\nThis paper is primarily about experimental evaluation, since the objective is to show that a residual formulation is not necessary to obtain good performance, at least on some tasks.\n\nHowever, in my opinion the evaluations in the paper are not convincing. The primary issue is lack of a proper baseline, against which the improvements can be clearly demonstrated by making isolated changes. I understand that for this paper such a baseline is hard to construct, since it is about a novel architecture principle. This is why more effort should be put into this, so that core insights from this paper can be useful even after better performing architectures are discovered.\nThe number of parameters and amount of computation should be used to indicate how fair the comparisons are between architectures. Some detailed comments:\n\n- In Table 1 comparisons to Resnets, the resnets from He et al. 2016b and Wide Resnets should be compared to FractalNet (in lieu of a proper baseline). The first outperforms FractalNet on CIFAR-100 while the second outperforms it on both. The authors compare to other results without augmentation, but did not perform additional experiments without augmentation for these architectures.\n\n- The 40 layer Fractal Net should not be compared to other models unless the parameter reduction tricks are utilized for the other models as well.\n\n- A proper comparison to Inception networks should also be performed for these networks. My guess is that the reason behind a seemingly 'ad-hoc' design of Inception modules is to reduce the computational footprint of the model (which is not a central motivation of fractal nets). Since this model is directly related to the Inception module due to use of shorter and longer paths without shortcuts, one can easily simplify the Inception design to build a strong baseline e.g. by converting the concatenation operation to a mean operation among equally sized convolution outputs. As an aside, note that Inception networks have already shown that residual networks are not necessary to obtain the best performance [1].\n\n- It should be noted that Residual/Highway architectures do have a type of anytime property, as shown by lesioning experiments in Srivastava et al and Viet et al.\n\n- The architecture specific drop-path regularization is interesting, but is used along with other regularizers such as dropout, batch norm and weight decay and its benefit on its own is not clear.\n\nOverall, it's not clear to me that the experiments clearly demonstrate the utility of the proposed architecture. \n\n[1] Szegedy, Christian, Sergey Ioffe, and Vincent Vanhoucke. \"Inception-v4, inception-resnet and the impact of residual connections on learning.\" arXiv preprint arXiv:1602.07261 (2016)."
  },
  {
    "people": [
      "Veit",
      "Huang"
    ],
    "review": "Authors claim that the reviewers reflect \"narrow view\" of on the number of parameters and fractions of accuracy and  \"complete focus on engineering\" etc . Authors claims about focusing on engineering are open to debate as only the technicality in the paper is the fractal network architecture with intuitive claims. One can either formulate the system with rigorous analysis with clear assumptions (which would require satisfactory theoretical analysis and relatively small scale empirical sanity check without heavy experiments) or propose a comprehensive empirical analysis by designing careful experiments to support intuitive claims. Therefore, as the approach clearly does not belong to the first category,  such an approach needs strong experimental evidence to support intuitive claims without a doubt.  The rebuttal lists unsatisfactory answers with somehow manipulative arguments about narrow reviewing/ dates of the baselines( which will be discussed below).. \n\n\nAuthors state in the paper that \" In experiments, fractal networks **match the state-of-the-art performance held by residual networks** on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks\" and \"Fractal architectures are arguably the simplest means of satisfying this requirement, and match or exceed residual networks in experimental performance\". Therefore the main support behind all the claims is that fractal network match (or exceed) performance of state of art residual networks, indeed the empirical study only compares accuracy to some baselines without designing empirical analysis for the claims about differences to resnets (or ensemble explanation of resnet by Veit et al(NIPS 2016) ). \nHowever even this comparison lacks many results as explained in my review.   Therefore the expectation of the fair comparison is completely reasonable and it is not about only focusing on fractions of accuracy or number of parameters. As a good example, Veit et al demonstrates systematical empirical support for their claims about analyzing residual networks (which seems to be on arxiv since May but still not cited on paper ). Table 4 and Figure 3 provide good preliminary sanity check but compares only to plain networks, do not support claims about differences between residual and fractal networks.\n\nThe paper claims that the fractal network scales to \"ultra\" deep networks, however authors can not report results on dozens of layers. Authors also claim that extra depth may slow training (not clear how much) but does not decrease accuracy but this is not clear as there are no results for dozens of layers, also in Table 3, error increases as depth increase to 160 layers.  Number of parameters of the proposed architecture is significantly greater than the state of art resnet variants as I explained in the review but the authors argue that it is slightly more parameters. \n\nAs an answer to lack of comparison to DenseNet, authors argue in rebuttal that DenseNet cites FractalNet but this can not be a reason for the lack of comparison as DenseNet was published in August and well-known as holding state of art results as a resnet variant. \n\nAuthors state in rebuttal that \"Many of the variants only reported results on CIFAR/SVHN\u2026 it is a bit difficult to have already compared to results that did not exist at submission time.\" .  However there were clearly published ones, for example two results by Huang et al, 2016b on july (arxiv) ..\n\nRegarding the \"simplifying power\" claim of authors.  It is not very convincing that it is simplifying. how can it simplify with a harder training procedure with many parameters that can not scale as good as baselines?\n\nIn essence, my evaluation did not change as rebuttal did not provide satisfactory clarification or improvement.\n"
  },
  {
    "people": [
      "Veit"
    ],
    "review": "Looking through the comment section here, I agree to a large degree with the author's standpoint on many issues discussed. Points (1) through (4) in the authors comment below are, in my opinion, a good summary of the contributions of the paper. While I don't think those contributions are groundbreaking, I believe they are significant enough to merit acceptance.\n\nThe reason I am commenting here is because, having looked at several comment sections for this ICLR, I am seeing a general trend that reviews have a strong focus on performance, i.e. reviews tend to be very short and judge papers, to a large degree, on whether they are a few percentage points better or worse than the reported baseline. E.g. see the comments \"the experimental evaluation is not convincing, e.g. no improvement on SVHN\" or \"the effect of drop-path seems to vanish with data augmentation\" below.\n\nI believe that papers should be judged more on their scientific contributions (see points (1), (2) and (4) below), especially when those papers themselves state that their focus is on those scientific contributions, not on amazing performance.\n\nFurther, I believe the trend to focus excessively on performance is problematic for a number of reasons:\n\n - The Deep Learning community has focused very heavily on a few datasets (MNIST, ImageNet, CIFAR-10, CIFAR-100, SVHN). This means that at any time, a large chunk of the deep learning literature is battling for 5 SOTA titles. Hence, expecting any new model to attain one of those titles is a very high bar.\n\n - It is an arbitrary standard. Say the SOTA on ImageNet improves by 2% a year. Then a paper that outperforms by 1% in 2014 would underperform by 1% in 2015. By the performance standard, the same paper with the same ideas and the same scientific merit would have declined drastically in value over that one year. Is that really true?\n\n - How does one even draw a \"fair comparison\" on these standard datasets at this point? The bag of tricks for neural networks includes: drop-out, l2, l1, ensembling, various forms of data augmentation, various forms of normalization and initialization, various non-linearities, various learning rate schedules, various forms of pooling, label smoothing, gradient clipping etc. etc. There are a gazillion ways to eke out fractions of percentage points of performance. And - every single paper has a unique combination of tricks that they use for their model, even though the tricks themselves are unrelated to the model. Hence, the only truly fair comparison would be to compare against every reference model with the exact trick combination that the paper presenting the reference model used, which would take an exorbitant amount of time. What's worse, many papers do not even report all of the tricks they used. One would have to get the authors code and reverse engineer the model, not to mention slight differences introduced by using e.g. TensorFlow vs. Torch vs. Caffe. In this light, the request from one of the reviewers to have a baseline \"against which the improvements can be clearly demonstrated by making isolated changes\" seems unrealistic to me.\n\n - The ML community should not make excessive fine-tuning of models mandatory for publication. By requiring models to beat SOTA, we force each author to fine-tune their model ad nauseum, which leads to an arms race. To get a publications, authors would spend ever more time fine-tuning their models. This can not only lead to \"training on the test set\", but also wastes the time of researcher that could be better spent exploring new ideas.\n\n - It gives too much power to bad research. In science, there is always a certain background rate of \"bad\" results published: either the numbers are outright fake or the experimental protocol was invalid, e.g. someone used the test set as a validation set or someone did an exorbitant number of random reruns and only published the best single result. What's worse, these \"bad\" results are far more likely to hold the SOTA title at any given time than a \"good\" result. By requiring new publications to beat SOTA, we give too much power to bad results.\n\n - It punishes authors for reporting many or strong baselines. In this paper, authors were careful to report many recent results. Table 1 is thorough. And now they are criticized for not beating all of those baselines. I have a feeling that if the authors of this paper had been more selective about which baselines they report, i.e. those that they can beat, they would have received higher scores on the paper. I have written an in-depth review for another paper at this conference that used, in my opinion, very weak baselines and ended up getting high reviewer marks. I don't think that was a coincidence. \n\nThe same arguments apply, though I think to a lesser degree, to judging models excessively on how many parameters they have or their runtime. However, I agree with reviewers that more information about how models compare in terms of those metrics would enhance this paper. I would like to see a discussion of that in the final version. In general, I think this paper would benefit from an appendix with more details on model and training procedure. I also agree with reviewers that 80 layers, which is the deepest that authors can go while improving test error (Table 3), is not ultra-deep. Hence putting \"ultra-deep\" in the paper title seems exaggerated and I would recommend scaling back the language. However, I don't think being ultra-deep (~1000 layers) is necessary, because as Veit et al showed, networks that appear ultra deep might not be ultra deep in practice. Training an 80-layer net that functions at test time without residual connections seems to be enough of an achievement.\n\nIn summary, I think if a paper makes scientific contribution (see points (1), (2) and (4) below) independent of performance, then competitive performance should be enough for publication, instead of requiring SOTA. I believe this paper achieves that mark."
  },
  {
    "people": [
      "Veit",
      "Veit",
      "Greff",
      "Szegedy",
      "Viet"
    ],
    "review": "We thank the reviewers for their time, but disagree with many points in the reviews.  We first address what we feel is the overall disconnect between the reviews and the contributions of the paper, then rebut specific points below.\n\nMachine learning research is a mix of engineering, mathematics, and science.  Complete focus on engineering can restrict one to the narrow view of judging work by counting parameters and fractions of a percent in accuracy.  The reviews unfortunately reflect this mode of thinking.  However, long-term progress also requires scientific advancement, including new ideas as well as experiments that enhance understanding by revealing simple principles underlying complex phenomena.\n\nFractalNet is a new idea with:\n\n(1) Explanatory power:\n\nAs stated in the abstract, our experiments show that for the success of very deep networks, the \"key may be the ability to transition, during training, from effectively shallow to deep\".  This is the property shared by ResNet and FractalNet.\n\nFurthermore, this observation is an important counterpoint to [Veit et al., NIPS 2016] which claims that: (a) ensemble-like behavior is key and (b) the fraction of available effectively short paths is somehow related to performance.  FractalNet provides a counterexample: we can extract a single column (plain network topology with one long path) and it alone (no ensembling) performs as well the entire network.  In FractalNet, the rest of the network is a training apparatus for transitioning from shallow to deep.  This strongly suggests that, with more careful analysis, the same may be true for ResNet.\n\nIn fact, unlike [Veit et al., NIPS 2016] our explanation is consistent with the view that very deep networks, such as ResNet and FractalNet, learn unrolled iterative estimation [Greff et al., Highway and Residual Networks learn Unrolled Iterative Estimation, arXiv:1612.07771 and ICLR 2017 submission].  In this view, the longest path is most important; the ensemble can be discarded after training.  Moreover, our Section 4.3 and Figure 3 provide insight into the dynamics of this learning process.\n\nWe believe that the research community values understanding these mechanisms and would benefit from the evidence our work injects into the debate.\n\n(2) Simplifying power:\n\nFractalNet shows how a simple design principle produces structure similar to the mishmash of hacks (hand-designed modules, manually-attached auxiliary losses at intermediate depths) required in prior architectures like Inception.  It also expands the set of simple tricks for building very deep networks to include an option other than residual connections.\n\n(3) Good performance:\n\nFractalNet matches a ResNet of equal depth in terms of performance on ImageNet.  Training either of these networks from scratch takes weeks on modern GPUs.  Yet, because FractalNet uses slightly more parameters or does not beat the absolute best ResNet variant (developed after 7 or so papers iterating on ResNet) on the smaller and less important CIFAR dataset, the reviews declare experiments unsatisfactory.  There is a line between demanding high experimental standards and strangling promising ideas; we are all for the former, but hope not to fall victim to the latter.\n\n(4) New capabilities:\n\nContrary to AnonReviewer2's claim, prior work does not demonstrate an anytime property for ResNet; see our specific response below.  FractalNet, in combination with drop-path training, provides a novel anytime output capability, which could prove useful in real-world latency sensitive applications.\n\n\n-----\nResponse to AnonReviewer1\n\nPlease see our comments about experiments above.\n\n> Therefore the empirical effectiveness of drop-path is not convincing too.\n\nDrop-path serves two purposes: (a) additional regularization in the absence of data augmentation and (b) regularization that allows the resulting network to have the anytime property.  Yes, additional data augmentation can compensate for drop-path if one only cares about (a), but drop-path is essential for and the only effective means of enabling anytime output.\n\n> DenseNets (Huang et al, 2016a) should be also included in the comparison\n\nDenseNet cites FractalNet as the original version of FractalNet (arXiv:1605.07648) was published on arXiv three months prior to the original version of DenseNet (arXiv:1608.06993).  We are happy to alert readers to subsequent work, but please keep in mind the historical sequence of development.\n\n> Table 1 has Res-Net variants as baselines however Table 2 has only ResNet.\n\nMany of the variants only reported results on CIFAR/SVHN.  For example, the Wide Residual Networks paper (arXiv:1605.07146), at the time of the ICLR deadline (November 5), did not include ImageNet results.  It was updated weeks later, on November 28, to include ImageNet results.  We can expand the table, but it is a bit difficult to have already compared to results that did not exist at submission time.\n\n> there is no improvement in SVHN dataset results and this is not discussed in the empirical analysis.\n\nSection 4.2 observes \"most methods perform similarly on SVHN\".  The errors rates on SVHN are so low as to be uninformative.  We believe SVHN is simply too easy of a dataset to be a challenge to any of the modern techniques in Table 1.  Much like MNIST, SVHN is now a useful sanity check, but not a differentiator.\n\n> Also, authors give a list of some improvements over Inception (Szegedy et al., 2015) but again these intuitive claims about effectiveness of these changes are not supported with any empirical analysis.\n\nOur point is merely to draw connections between the FractalNet architecture and some hand-designed components of Inception, suggesting a more fundamental explanation for why those particular hand-designed choices are effective.  Widespread adoption of ResNet by the community has displaced Inception, VGG, and other architectures, making ResNet the clear leader and appropriate target for experimental comparison.\n\n\n-----\nResponse to AnonReviewer2\n\nPlease see our comments about experiments and the main contributions of the paper above.\n\n> The 40 layer Fractal Net should not be compared to other models unless the parameter reduction tricks are utilized for the other models as well.\n\nWe are demonstrating feasibility of parameter reduction tricks for FractalNet.  Much of the iterative improvement in the ResNet variants themselves, developed over many papers, already has a component of optimizing the architecture to reduce parameters and/or computational load.\n\n> Since this model is directly related to the Inception module due to use of shorter and longer paths without shortcuts.\n\nThis statement is incorrect.  Our model is not directly related to Inception.  It reproduces some local connectivity reminiscent of Inception modules, but the global connectivity structure is entirely different. Even locally, FractalNet has a recursive join structure which Inception modules lack.  As consequence of this is that at global scale, FractalNet contains many paths whose length range over many orders of magnitude (all powers of 2 up to the maximum depth).  In contrast, the shortest path through an Inception network grows linearly with depth (number of modules stacked).\n\n> As an aside, note that Inception networks have already shown that residual networks are not necessary to obtain the best performance\n\nImageNet performance is a useful benchmark tool, not the end goal of network design.  Mass adoption of ResNet by the community suggests that simplicity and scalability are also legitimate concerns.  With regard to scalability in depth, the only networks previously demonstrated to easily extend to the 100-1000 layer regime rely on some form of residual connection (ResNet/Highway Networks).  We demonstrate a 160-layer FractalNet, placing it in this group.\n\nMoreover, FractalNet and ResNet/Highway all have a mechanism for evolving from being effectively shallow to effectively deep over the course of training. This shallow to deep evolution seems critical, but unfortunately the design of Inception prevents its effective depth from being less than the number of stacked Inception modules.\n\n> It should be noted that Residual/Highway architectures do have a type of anytime property, as shown by lesioning experiments in Srivastava et al and Viet et al.\n\nWe disagree.  The lesioning experiments in Veit et al. demonstrate the opposite: ResNet does not have an anytime property.  If deleting a few layers, then yes, ResNet can recover.  However, Section 4.2 and Figure 5 of Veit et al, show that deleting 10 blocks of a 54 block ResNet increases CIFAR-10 error to 0.2, deleting 20 blocks pushes error above 0.5.  So one cannot maintain a reasonable error if halving the ResNet depth.  Compare this to our Table 4: subnetwork columns of one half (20) and even one fourth (10) the layers of a full FractalNet (40) maintain low error on CIFAR-100.  This robustness across many orders of magnitude in depth (and thus time) is required for a useful anytime property and is unique to FractalNet (with drop-path training).\n\n> The architecture specific drop-path regularization is interesting, but is used along with other regularizers such as dropout, batch norm and weight decay and its benefit on its own is not clear.\n\nDrop-path is essential for and the only effective means of enabling anytime output.  The fact that it can contribute as a regularizer in other scenarios is a bonus.\n\n\n-----\nResponse to AnonReviewer3\n\n> the experimental evaluation is not convincing, e.g. no improvement on SVHN\n\nAs we replied to AnonReviewer1: Section 4.2 observes \"most methods perform similarly on SVHN\".  The errors rates on SVHN are so low as to be uninformative.  We believe SVHN is simply too easy of a dataset to be a challenge to any of the modern techniques in Table 1.  Much like MNIST, SVHN is now a useful sanity check, but not a differentiator.\n\n> number of parameters should be mentioned for all models for fair comparison\n\nWe can add this to the table, but please see our larger discussion of experiments above.\n\n> the effect of drop-path seems to vanish with data augmentation\n\nDrop-path is essential for and the only effective means of enabling anytime output.  The fact that it can contribute as a regularizer in the absence of data augmentation is a bonus.\n"
  },
  {
    "people": [
      "Huang",
      "Szegedy"
    ],
    "review": "This paper proposes a new architecture that does not explicitly use residuals but constructs an architecture that is composed of networks with fractal structure by using expand and join operations. Using the fractal architecture,  authors argue and try to demonstrate that the large nominal network depth with many short paths is the key for 'training 'ultra-deep\u201d networks while residuals are incidental.\n\nThe main bottleneck of this paper is that number of parameters needed for the FractalNet is significantly higher than the baselines which makes it hard to scale to ''ultra-deep\u201d networks.  Authors replied that Wide ResNets also require many parameters but this is not the case for ResNet and other ResNet variants. ResNet and ResNet with Stochastic depth scales to depth of 110 with 1.7M parameters and to depth of 1202 with 10.2M parameters which is much less than the number of parameters for depths of 20 and 40 in Table 1(Huang et al, 2016a).   It is not clear whether FractalNet can perform better than these depths with a reasonable computation. Authors report less parameters for 40 layers but this scaling trick is not validated for other depths including depth 20 in Table 1. On the other hand, the number of parameters for 40 layers with scaling trick is clearly still large compared to most of the baselines. Unsatisfactory comparison to these baselines makes the claims of authors unconvincing.\n\nAuthors also claim that drop-path to provide improvement compared to layer dropping procedure in Huang et al, 2016b however the results show that the empirical gain of this specific regularization disappears when well-known data augmentation techniques applied. Therefore the empirical effectiveness of drop-path is not convincing too.\n\nDenseNets (Huang et al, 2016a) should be also included in the comparison since it outperforms most of the state of art Res Nets on both CIFAR10 and ImageNet and more importantly outperforms the proposed FractalNet significantly and it requires significantly less computation. \n\nTable 1 has Res-Net variants as baselines however Table 2 has only ResNet.  Therefore ImageNet comparison only shows that one can run FractalNet on ImageNet and can perform comparably well to ResNet which is not a satisfactory result given the improvements of other baselines over ResNet.  In addition, there is no improvement in SVHN dataset results and this is not discussed in the empirical analysis.\n\nAlso, authors give a list of some improvements over Inception (Szegedy et al., 2015) but again these intuitive claims about effectiveness of these changes are not supported with any empirical analysis. \n\nAlthough the paper attempts to explore many interesting intuitive directions using the proposed architecture, the empirical results are not support the given claims and the large number of parameters makes the model restrictive in practice hence the contribution does not seem to be significant. \n\nPros:\nProvides an interesting architecture compared to ResNet and its variants and investigates the differences to residual networks which can stimulate some other promising analysis\n\ncons:\n     -    Number of parameters are very large compared to baselines that can have even much higher depths with smaller number of parameters\nThe claims are intuitive but not supported well with empirical evidence\nPath regularization does not yield improvement when the data augmentation is used\n     -     The empirical results do not show whether the method is promising for \u201cultra-deep\u201d networks "
  },
  {
    "people": [
      "Christian",
      "Sergey Ioffe",
      "Vincent Vanhoucke"
    ],
    "review": "This paper proposes a design principle for computation blocks in convolutional networks based on repeated application of expand and join operations resulting in a fractal-like structure. \n\nThis paper is primarily about experimental evaluation, since the objective is to show that a residual formulation is not necessary to obtain good performance, at least on some tasks.\n\nHowever, in my opinion the evaluations in the paper are not convincing. The primary issue is lack of a proper baseline, against which the improvements can be clearly demonstrated by making isolated changes. I understand that for this paper such a baseline is hard to construct, since it is about a novel architecture principle. This is why more effort should be put into this, so that core insights from this paper can be useful even after better performing architectures are discovered.\nThe number of parameters and amount of computation should be used to indicate how fair the comparisons are between architectures. Some detailed comments:\n\n- In Table 1 comparisons to Resnets, the resnets from He et al. 2016b and Wide Resnets should be compared to FractalNet (in lieu of a proper baseline). The first outperforms FractalNet on CIFAR-100 while the second outperforms it on both. The authors compare to other results without augmentation, but did not perform additional experiments without augmentation for these architectures.\n\n- The 40 layer Fractal Net should not be compared to other models unless the parameter reduction tricks are utilized for the other models as well.\n\n- A proper comparison to Inception networks should also be performed for these networks. My guess is that the reason behind a seemingly 'ad-hoc' design of Inception modules is to reduce the computational footprint of the model (which is not a central motivation of fractal nets). Since this model is directly related to the Inception module due to use of shorter and longer paths without shortcuts, one can easily simplify the Inception design to build a strong baseline e.g. by converting the concatenation operation to a mean operation among equally sized convolution outputs. As an aside, note that Inception networks have already shown that residual networks are not necessary to obtain the best performance [1].\n\n- It should be noted that Residual/Highway architectures do have a type of anytime property, as shown by lesioning experiments in Srivastava et al and Viet et al.\n\n- The architecture specific drop-path regularization is interesting, but is used along with other regularizers such as dropout, batch norm and weight decay and its benefit on its own is not clear.\n\nOverall, it's not clear to me that the experiments clearly demonstrate the utility of the proposed architecture. \n\n[1] Szegedy, Christian, Sergey Ioffe, and Vincent Vanhoucke. \"Inception-v4, inception-resnet and the impact of residual connections on learning.\" arXiv preprint arXiv:1602.07261 (2016).\n"
  },
  {
    "people": [
      "Welling"
    ],
    "review": "This paper proposed a new joint training scheme for two probabilistic models of signals (e.g. images) which are both deep neural network based and are termed generator and descriptor networks.  In the new scheme, termed cooperative training, the two networks train together and assist each other: the generator network provides samples that work as initial samples for the descriptor network, and the descriptor network updates those samples to help guide training of the generator network.\n\nThis is an interesting approach for coupling the training of these two models.  The paper however is quite weak on the empirical studies.  In particular:\n- The training datasets are tiny, from sets of 1 image to 5-6.  What is the reason for not using larger sets?  I think the small datasets are leading to over training and are really masking the true value of the proposed cooperative training approach.\n- For most of the experiments presented in the paper it is hard to assess the specific value brought by the proposed cooperative training approach because baseline results are missing.  There are comparisons provided for face completion experiments - but even there comparisons with descriptor or generator network trained separately or with other deep auto-encoders are missing.  Thus it is hard to conclude if and how much gain is obtained by cooperative training over say individually training the descriptor and generator networks.\n\nAnother comment is that in the \u201crelated work\u201d section, I think relation with variational auto encoders (Kingma and Welling 2013) should be included.\n\nDespite limitations mentioned above, I think the ideas presented in the paper are intuitively appealing and worth discussing at ICLR.  Paper would be considerably strengthened by adding more relevant baselines and addressing the training data size issues."
  },
  {
    "people": [
      "Ruiqi Gao",
      "Langevin"
    ],
    "review": "Dear Reviewers, \n\nThank you for reviewing our paper and thank you for your comments! \n\nWe have uploaded a revision for your consideration. \n\nBecause the reviewers questioned the small training sizes in our experiments on textures and objects, we have opted to replace these experiments by a new experiment on 14 categories from standard datasets such as ImageNet and MIT place, where each training set consists of 1000 images randomly sampled from the category. Please see the experiment section as well as the appendix for the synthesis results. These are all we have got, without cheery picking. As can be seen, our method can generate meaningful and varied images. We haven\u2019t had time to tune the code. In fact, we had to recruit a new author (Ms. Ruiqi Gao) to help us run the code due to our various time constraints. With more careful tuning (including increasing image resolution), we expect to further improve the quality of synthesis. \n\nAbout the comparison with separate training method by either Algorithm D for descriptor or Algorithm G for generator individually, the separate training methods currently cannot produce synthesis results that are comparable to those produced by the cooperative training. This illustrates the advantage of cooperative training over separate training. In fact, the main motivation for this work is to overcome the difficulty with separate training by cooperative training.  \n\nWe have added a quantitative comparison with GAN for the face completion experiment, because our method is intended as an alternative to GAN.  Our original code was written in MatConvNet. We moved to TensorFlow in order to use existing code of GAN. We then rewrote Algorithm G in TensorFlow for image completion. GAN did not do well in this experiment. We are still checking and tuning our code to improve GAN performance. \n\nWe want to emphasize that we are treating the following two issues separately:\n\n(1) Train-test split and quantitative evaluation of generalizability. \n(2) Image synthesis judged qualitatively. \n\nWhile the face completion experiment is intended to address (1), the synthesis experiment is intended to address (2).  In fact, the generator network captures people\u2019s imagination mainly because of (2) (at least this is the case with ourselves), and some GAN papers are more qualitative than quantitative. \n\nWe will continue to work on experiments, to further address the questions raised by the reviewers and to continue to strengthen the quantitative side. \n\nWe have also made some minor changes to incorporate the reviewers\u2019 suggestions on wording and additional references. \n\nAs to the energy function, in particular, f(Y; W), for the descriptor, it is defined by a bottom-up ConvNet that maps the image Y to a score (very much like a discriminator), and we give the details of this ConvNet in the experiment section. We feel we made this clear in the original version. \n\nAs to equation (8), we have expanded the derivation. Equations (16) and (17) are about finite step Langevin dynamics. \n\nFinally please allow us to make some general comments regarding our paper. Our paper addresses the core issue of this conference, i.e., learning representations in the form of probabilistic generative models. There are two types of such papers: \n\n(1) Build on the successes of GAN. \n(2) Explore new connections and new routes. \n\nWe believe that papers in these two categories should be judged differently. Our paper belongs to category (2). It explores the connection between undirected model (descriptor) and directed model (generator). It also explores the connection between MCMC sampling (descriptor) and ancestral sampling (generator). Furthermore, it explores the new ground where two models interact with each other via synthesized data. We have also tried hard to gain a theoretical understanding of our method in appendix. \n\nThere have been a lot of papers in category (1) recently. We hope that the conference will be more open to the relatively fewer papers in category (2). In fact we are heartened that all three reviewers find our work interesting, and we can continue to improve our experiments. \n\nThanks for your consideration, and thanks for your comments that have helped us improve our work. \n"
  },
  {
    "people": [
      "Welling"
    ],
    "review": "This paper proposed a new joint training scheme for two probabilistic models of signals (e.g. images) which are both deep neural network based and are termed generator and descriptor networks.  In the new scheme, termed cooperative training, the two networks train together and assist each other: the generator network provides samples that work as initial samples for the descriptor network, and the descriptor network updates those samples to help guide training of the generator network.\n\nThis is an interesting approach for coupling the training of these two models.  The paper however is quite weak on the empirical studies.  In particular:\n- The training datasets are tiny, from sets of 1 image to 5-6.  What is the reason for not using larger sets?  I think the small datasets are leading to over training and are really masking the true value of the proposed cooperative training approach.\n- For most of the experiments presented in the paper it is hard to assess the specific value brought by the proposed cooperative training approach because baseline results are missing.  There are comparisons provided for face completion experiments - but even there comparisons with descriptor or generator network trained separately or with other deep auto-encoders are missing.  Thus it is hard to conclude if and how much gain is obtained by cooperative training over say individually training the descriptor and generator networks.\n\nAnother comment is that in the \u201crelated work\u201d section, I think relation with variational auto encoders (Kingma and Welling 2013) should be included.\n\nDespite limitations mentioned above, I think the ideas presented in the paper are intuitively appealing and worth discussing at ICLR.  Paper would be considerably strengthened by adding more relevant baselines and addressing the training data size issues.\n\n"
  },
  {
    "people": [
      "Welling"
    ],
    "review": "This paper proposed a new joint training scheme for two probabilistic models of signals (e.g. images) which are both deep neural network based and are termed generator and descriptor networks.  In the new scheme, termed cooperative training, the two networks train together and assist each other: the generator network provides samples that work as initial samples for the descriptor network, and the descriptor network updates those samples to help guide training of the generator network.\n\nThis is an interesting approach for coupling the training of these two models.  The paper however is quite weak on the empirical studies.  In particular:\n- The training datasets are tiny, from sets of 1 image to 5-6.  What is the reason for not using larger sets?  I think the small datasets are leading to over training and are really masking the true value of the proposed cooperative training approach.\n- For most of the experiments presented in the paper it is hard to assess the specific value brought by the proposed cooperative training approach because baseline results are missing.  There are comparisons provided for face completion experiments - but even there comparisons with descriptor or generator network trained separately or with other deep auto-encoders are missing.  Thus it is hard to conclude if and how much gain is obtained by cooperative training over say individually training the descriptor and generator networks.\n\nAnother comment is that in the \u201crelated work\u201d section, I think relation with variational auto encoders (Kingma and Welling 2013) should be included.\n\nDespite limitations mentioned above, I think the ideas presented in the paper are intuitively appealing and worth discussing at ICLR.  Paper would be considerably strengthened by adding more relevant baselines and addressing the training data size issues."
  },
  {
    "people": [
      "Ruiqi Gao",
      "Langevin"
    ],
    "review": "Dear Reviewers, \n\nThank you for reviewing our paper and thank you for your comments! \n\nWe have uploaded a revision for your consideration. \n\nBecause the reviewers questioned the small training sizes in our experiments on textures and objects, we have opted to replace these experiments by a new experiment on 14 categories from standard datasets such as ImageNet and MIT place, where each training set consists of 1000 images randomly sampled from the category. Please see the experiment section as well as the appendix for the synthesis results. These are all we have got, without cheery picking. As can be seen, our method can generate meaningful and varied images. We haven\u2019t had time to tune the code. In fact, we had to recruit a new author (Ms. Ruiqi Gao) to help us run the code due to our various time constraints. With more careful tuning (including increasing image resolution), we expect to further improve the quality of synthesis. \n\nAbout the comparison with separate training method by either Algorithm D for descriptor or Algorithm G for generator individually, the separate training methods currently cannot produce synthesis results that are comparable to those produced by the cooperative training. This illustrates the advantage of cooperative training over separate training. In fact, the main motivation for this work is to overcome the difficulty with separate training by cooperative training.  \n\nWe have added a quantitative comparison with GAN for the face completion experiment, because our method is intended as an alternative to GAN.  Our original code was written in MatConvNet. We moved to TensorFlow in order to use existing code of GAN. We then rewrote Algorithm G in TensorFlow for image completion. GAN did not do well in this experiment. We are still checking and tuning our code to improve GAN performance. \n\nWe want to emphasize that we are treating the following two issues separately:\n\n(1) Train-test split and quantitative evaluation of generalizability. \n(2) Image synthesis judged qualitatively. \n\nWhile the face completion experiment is intended to address (1), the synthesis experiment is intended to address (2).  In fact, the generator network captures people\u2019s imagination mainly because of (2) (at least this is the case with ourselves), and some GAN papers are more qualitative than quantitative. \n\nWe will continue to work on experiments, to further address the questions raised by the reviewers and to continue to strengthen the quantitative side. \n\nWe have also made some minor changes to incorporate the reviewers\u2019 suggestions on wording and additional references. \n\nAs to the energy function, in particular, f(Y; W), for the descriptor, it is defined by a bottom-up ConvNet that maps the image Y to a score (very much like a discriminator), and we give the details of this ConvNet in the experiment section. We feel we made this clear in the original version. \n\nAs to equation (8), we have expanded the derivation. Equations (16) and (17) are about finite step Langevin dynamics. \n\nFinally please allow us to make some general comments regarding our paper. Our paper addresses the core issue of this conference, i.e., learning representations in the form of probabilistic generative models. There are two types of such papers: \n\n(1) Build on the successes of GAN. \n(2) Explore new connections and new routes. \n\nWe believe that papers in these two categories should be judged differently. Our paper belongs to category (2). It explores the connection between undirected model (descriptor) and directed model (generator). It also explores the connection between MCMC sampling (descriptor) and ancestral sampling (generator). Furthermore, it explores the new ground where two models interact with each other via synthesized data. We have also tried hard to gain a theoretical understanding of our method in appendix. \n\nThere have been a lot of papers in category (1) recently. We hope that the conference will be more open to the relatively fewer papers in category (2). In fact we are heartened that all three reviewers find our work interesting, and we can continue to improve our experiments. \n\nThanks for your consideration, and thanks for your comments that have helped us improve our work. \n"
  },
  {
    "people": [
      "Welling"
    ],
    "review": "This paper proposed a new joint training scheme for two probabilistic models of signals (e.g. images) which are both deep neural network based and are termed generator and descriptor networks.  In the new scheme, termed cooperative training, the two networks train together and assist each other: the generator network provides samples that work as initial samples for the descriptor network, and the descriptor network updates those samples to help guide training of the generator network.\n\nThis is an interesting approach for coupling the training of these two models.  The paper however is quite weak on the empirical studies.  In particular:\n- The training datasets are tiny, from sets of 1 image to 5-6.  What is the reason for not using larger sets?  I think the small datasets are leading to over training and are really masking the true value of the proposed cooperative training approach.\n- For most of the experiments presented in the paper it is hard to assess the specific value brought by the proposed cooperative training approach because baseline results are missing.  There are comparisons provided for face completion experiments - but even there comparisons with descriptor or generator network trained separately or with other deep auto-encoders are missing.  Thus it is hard to conclude if and how much gain is obtained by cooperative training over say individually training the descriptor and generator networks.\n\nAnother comment is that in the \u201crelated work\u201d section, I think relation with variational auto encoders (Kingma and Welling 2013) should be included.\n\nDespite limitations mentioned above, I think the ideas presented in the paper are intuitively appealing and worth discussing at ICLR.  Paper would be considerably strengthened by adding more relevant baselines and addressing the training data size issues.\n\n"
  },
  {
    "people": [
      "Gregor",
      "Isola",
      "Lei",
      "Arun",
      "Mauser",
      "Ha"
    ],
    "review": "This paper proposes a model for iteratively refining translation hypotheses. This has several benefits, including enabling the translation model to condition not only on \u201cleft context\u201d, but also on \u201cright context\u201d, and potentially enabling more rapid and/or accurate decoding. The motivation given is that often translators (and text generators generally) use a process of refinement in generating outputs.\n\nThis is an important idea that is not currently playing much of a role in neural net models, so this paper is a welcome contribution. However, while I think this is an important first step, I do feel that the lack of in depth analysis suggests this paper is not quite ready for a final publication version. For example, there are many possible connections to prior work in NLP, MT, and other parts of ML that could better contextualize this work (see specifics below). More substantively, the model in Section 3 could be interpreted as a globally normalized, undirected (~CRF) translation model trained using a pseudo-likelihood objective. In this analysis, the model squarely back in the context of traditional discriminative translation models which used \u201cundirected\u201d features, and the decoding algorithm then looks more like a standard greedy hill-climbing algorithm (albeit with an extra heuristic model for selecting which variable to update), which is also nothing unfamiliar.\n\nMy second criticism the limitations of the model are not well discussed. For example, the proposed editing procedure cannot obviously remove or insert a word from a translation. While I think this is a reasonable assumption than can be made for the sake of tractability, it is very unfortunate since missing or extra words (esp. function words) are a common problem in the baseline models that are being used. Second, the standard objections to absolute positional models (vs. relative positional models) seem particularly crucial to bring up in this work, especially since they might make some of the design decisions a bit more justifiable.\n\nOverall, this is an initial step in an interesting direction, but it needs more thorough analysis to demonstrate its value. A more thorough analysis will also likely suggest some important model variants (for example: is a global translation model really the goal? or is a post-editing model that fixes outputs with more complex operations more ideal?)\n\nRelated work:\nI think that more could be done to put this work in the context of what has come before and what is currently going on in other parts of ML. The idea of iterative refinement has been proposed in other problems that have complex output spaces, for example the DRAW model of Gregor et al. and the conditional adversarial network models used to refine images proposed recently by Isola et al. In NLP, there have been several (stochastic) hill climbing approaches that have been proposed, such as the work on parsing by Zhang and Lei et al. (2014) who use random initial guesses and then do greedy hill climbing using a series of local refinements, the structured prediction cascades of Weiss and Taskar (2009) (not to mention general coarse-to-fine modeling strategies). Finally, in MT, Arun et al. (2009) who use a Gibbs sampler to refine an initial guess to do decoding with a more complex model. The use of an explicit error model is rather novel in the context of correction, but I would point out that although the proposed architecture is different, the discriminative word lexicon models of Mauser et al. (2009) and the neural version of the same by Ha et al. (2014) are similar in spirit. There have also been a number of papers on \u201cautomatic post editing\u201d, including the shared task at WMT2016, and there are not only standard test sets and baselines, but also datasets that could actually be used to train a post editing model with human-generated data. Minimally using the techniques they described could be a useful foil for the models presented in this paper.\n\n\u201cthe target sentence is also embedded in distributional space via a lookup table\u201d I think \u201cdistributional space\u201d is a bit unclear. Maybe \u201cthe target sentence is represented in terms of distributed word representations via a lookup table\u201d or something like that. \u201cdistributional\u201d suggests that the representations are derived from how the words are distributed in the corpus, whereas you are learning these representations on this task which isn\u2019t modeling their distribution except only very indirectly.\n\nSection 3 Model: In Section 3, the model computes the distribution over target word types at an absolute position i in the output sentence, given the target language context and the source language context. It is introduced as the model that is used to refine an existing hypothesis, but it is not immediately clear that the training data for this model (at least in this section) are the gold standard translations- \u201ctraining set\u201d could be interpreted in variety of ways. This becomes clearer when reading later in the paper, but it\u2019s a bit less clear when reading from the beginning for the first time.\n\nThe use of a fixed sized window for representing the target word in context also seems to make something like a model 1 assumption since only the lexical features (and not any \u201calignment\u201d or \u201cpositional\u201d features) determine the attention. This should be clarified since it will make the assumptions of the model more transparent (and also suggest possible refinements to the model, e.g., including (representations) of i and j as components of S^j and T^i, which would allow model 2/3-like responses to be learned- although by leaving them out, the model might behave a bit more like a relative positional model than an absolute positional model, which is probably attractive).\n\nFinally, some discussion for why a fixed window is used to represent the target sentence is worth including (since a global context is apparently used to represent the source sentence).\n\nThe relationship between this training objective and pseudo likelihood (PL; Besag, 1975) might be worth mentioning. Since I believe this is just a PL objective for a certain global model, this suggests alternative decoding algorithms, or certainly a different analysis of the proposed decoding objective.\n\nThe section 4 model conditions on the true context of a position in the true target, the current target guess, and the source. I don\u2019t completely understand the rationale for this model since at test time only two of these variables are available, and the replacement of y_ref with y_g seems hard to justify."
  },
  {
    "people": [
      "Gregor",
      "Isola",
      "Lei",
      "Arun",
      "Mauser",
      "Ha"
    ],
    "review": "This paper proposes a model for iteratively refining translation hypotheses. This has several benefits, including enabling the translation model to condition not only on \u201cleft context\u201d, but also on \u201cright context\u201d, and potentially enabling more rapid and/or accurate decoding. The motivation given is that often translators (and text generators generally) use a process of refinement in generating outputs.\n\nThis is an important idea that is not currently playing much of a role in neural net models, so this paper is a welcome contribution. However, while I think this is an important first step, I do feel that the lack of in depth analysis suggests this paper is not quite ready for a final publication version. For example, there are many possible connections to prior work in NLP, MT, and other parts of ML that could better contextualize this work (see specifics below). More substantively, the model in Section 3 could be interpreted as a globally normalized, undirected (~CRF) translation model trained using a pseudo-likelihood objective. In this analysis, the model squarely back in the context of traditional discriminative translation models which used \u201cundirected\u201d features, and the decoding algorithm then looks more like a standard greedy hill-climbing algorithm (albeit with an extra heuristic model for selecting which variable to update), which is also nothing unfamiliar.\n\nMy second criticism the limitations of the model are not well discussed. For example, the proposed editing procedure cannot obviously remove or insert a word from a translation. While I think this is a reasonable assumption than can be made for the sake of tractability, it is very unfortunate since missing or extra words (esp. function words) are a common problem in the baseline models that are being used. Second, the standard objections to absolute positional models (vs. relative positional models) seem particularly crucial to bring up in this work, especially since they might make some of the design decisions a bit more justifiable.\n\nOverall, this is an initial step in an interesting direction, but it needs more thorough analysis to demonstrate its value. A more thorough analysis will also likely suggest some important model variants (for example: is a global translation model really the goal? or is a post-editing model that fixes outputs with more complex operations more ideal?)\n\nRelated work:\nI think that more could be done to put this work in the context of what has come before and what is currently going on in other parts of ML. The idea of iterative refinement has been proposed in other problems that have complex output spaces, for example the DRAW model of Gregor et al. and the conditional adversarial network models used to refine images proposed recently by Isola et al. In NLP, there have been several (stochastic) hill climbing approaches that have been proposed, such as the work on parsing by Zhang and Lei et al. (2014) who use random initial guesses and then do greedy hill climbing using a series of local refinements, the structured prediction cascades of Weiss and Taskar (2009) (not to mention general coarse-to-fine modeling strategies). Finally, in MT, Arun et al. (2009) who use a Gibbs sampler to refine an initial guess to do decoding with a more complex model. The use of an explicit error model is rather novel in the context of correction, but I would point out that although the proposed architecture is different, the discriminative word lexicon models of Mauser et al. (2009) and the neural version of the same by Ha et al. (2014) are similar in spirit. There have also been a number of papers on \u201cautomatic post editing\u201d, including the shared task at WMT2016, and there are not only standard test sets and baselines, but also datasets that could actually be used to train a post editing model with human-generated data. Minimally using the techniques they described could be a useful foil for the models presented in this paper.\n\n\u201cthe target sentence is also embedded in distributional space via a lookup table\u201d I think \u201cdistributional space\u201d is a bit unclear. Maybe \u201cthe target sentence is represented in terms of distributed word representations via a lookup table\u201d or something like that. \u201cdistributional\u201d suggests that the representations are derived from how the words are distributed in the corpus, whereas you are learning these representations on this task which isn\u2019t modeling their distribution except only very indirectly.\n\nSection 3 Model: In Section 3, the model computes the distribution over target word types at an absolute position i in the output sentence, given the target language context and the source language context. It is introduced as the model that is used to refine an existing hypothesis, but it is not immediately clear that the training data for this model (at least in this section) are the gold standard translations- \u201ctraining set\u201d could be interpreted in variety of ways. This becomes clearer when reading later in the paper, but it\u2019s a bit less clear when reading from the beginning for the first time.\n\nThe use of a fixed sized window for representing the target word in context also seems to make something like a model 1 assumption since only the lexical features (and not any \u201calignment\u201d or \u201cpositional\u201d features) determine the attention. This should be clarified since it will make the assumptions of the model more transparent (and also suggest possible refinements to the model, e.g., including (representations) of i and j as components of S^j and T^i, which would allow model 2/3-like responses to be learned- although by leaving them out, the model might behave a bit more like a relative positional model than an absolute positional model, which is probably attractive).\n\nFinally, some discussion for why a fixed window is used to represent the target sentence is worth including (since a global context is apparently used to represent the source sentence).\n\nThe relationship between this training objective and pseudo likelihood (PL; Besag, 1975) might be worth mentioning. Since I believe this is just a PL objective for a certain global model, this suggests alternative decoding algorithms, or certainly a different analysis of the proposed decoding objective.\n\nThe section 4 model conditions on the true context of a position in the true target, the current target guess, and the source. I don\u2019t completely understand the rationale for this model since at test time only two of these variables are available, and the replacement of y_ref with y_g seems hard to justify."
  },
  {
    "people": [
      "HUGO",
      "HUGO"
    ],
    "review": "I found this paper very original and thought-provoking, but also a bit difficult to understand. It is very exciting to see a practical use case for image-generating GANs, with potentially meaningful benchmarks aside from subjective realism.\n\nI found eq. 4 interesting because it introduces a potentially non-differentiable black-box function Stego(...) into the training of (S, G). Do you in fact backprop through the Stego function?\n\n- For the train/test split, why is the SGAN trained on all 200k images? Would it not be cleaner to use the same splits for training SGAN as for \"steganalysis purposes\"? Could this account for the sensitivity to random seed shown in table 2?\n- Sec. 5.3: \"Steganographic Generative Adversarial Networks can potentially be used as a universal tool for generating Steganography containers tuned to deceive any specific steganalysis algorithm.\". This experiment showed that SGAN can fool HUGO, but I do not see how it was \"tuned\" to deceive HUGO, or how it could be tuned in general for a particular steganalyzer.\n\nAlthough S* seems to be fooled by the proposed method, in general for image generation the discriminator D is almost never fooled. I.e. contemporary GANs never converge to actually fooling the discriminator, even if they produce samples that sometimes fool humans. What if I created an additional steganalyzer S**(x) = S*(x) * D(x)? This I think would be extremely difficult to fool reliably because it requires realistic image generation.\n\nAfter reading the paper several times, it is still a bit unclear to me how or why precisely one would use a trained SGAN. I think the paper could be greatly improved by detailing, step by step, the workflow of how a hypothetical user would use a trained SGAN. This description should be aimed at a reader who knows nothing or very little about steganography (e.g. most of ICLR attendees).\n"
  },
  {
    "people": [
      "HUGO",
      "HUGO"
    ],
    "review": "I found this paper very original and thought-provoking, but also a bit difficult to understand. It is very exciting to see a practical use case for image-generating GANs, with potentially meaningful benchmarks aside from subjective realism.\n\nI found eq. 4 interesting because it introduces a potentially non-differentiable black-box function Stego(...) into the training of (S, G). Do you in fact backprop through the Stego function?\n\n- For the train/test split, why is the SGAN trained on all 200k images? Would it not be cleaner to use the same splits for training SGAN as for \"steganalysis purposes\"? Could this account for the sensitivity to random seed shown in table 2?\n- Sec. 5.3: \"Steganographic Generative Adversarial Networks can potentially be used as a universal tool for generating Steganography containers tuned to deceive any specific steganalysis algorithm.\". This experiment showed that SGAN can fool HUGO, but I do not see how it was \"tuned\" to deceive HUGO, or how it could be tuned in general for a particular steganalyzer.\n\nAlthough S* seems to be fooled by the proposed method, in general for image generation the discriminator D is almost never fooled. I.e. contemporary GANs never converge to actually fooling the discriminator, even if they produce samples that sometimes fool humans. What if I created an additional steganalyzer S**(x) = S*(x) * D(x)? This I think would be extremely difficult to fool reliably because it requires realistic image generation.\n\nAfter reading the paper several times, it is still a bit unclear to me how or why precisely one would use a trained SGAN. I think the paper could be greatly improved by detailing, step by step, the workflow of how a hypothetical user would use a trained SGAN. This description should be aimed at a reader who knows nothing or very little about steganography (e.g. most of ICLR attendees).\n"
  },
  {
    "people": [
      "Battaglia",
      "Battaglia",
      "Battaglia",
      "Battaglia",
      "Battaglia",
      "Fragkiadaki",
      "Fragkiadaki",
      "Battaglia",
      "Battaglia",
      "Battaglia",
      "Fragkiadaki"
    ],
    "review": "- summary\n\nThe paper proposes a differntiable Neural Physics Engine (NPE). The NPE consists of an encoder and a decoder function. The NPE takes as input the state of pairs of objects (within a neighbourhood of a focus object) at two previous time-steps in a scene. The encoder function summarizes the interaction of each pair of objects. The decoder then outputs the change in velocity of the focus object at the next time step. The NPE is evaluated on various environments containing bouncing balls.\n\n- novelty\n\nThe differentiable NPE is a novel concept. However, concurrently Battaglia et al. (NIPS 2016) proposes a very similar model. Just as this work, Battaglia et al. (NIPS 2016) consider a model which consists of a encoder function (relation-centric) which encodes the interaction among a focus object and other objects in the scene and a decoder (relation-centric) function which considers the cumulative (encoded) effect of object interactions on the focus object and predicts effect of the interactions.  Aspects like only considering objects interactions within a neighbourhood (versus the complete object interaction graph in Battaglia et al.) based on euclideian distance  are novel to this work. However, the advantages (if any) of NPE versus the model of Battaglia et al. are not clear. Moreover, it is not clear how this neighbourhood thresholding scene would preform in case of n-ball systems, where gravitational forces of massive objects can be felt over large distances.\n\n- citations \n\nThis work includes all relevant citations.\n\n- clarity\n\nThe article is well written and easy to understand.\n\n- experiments \n\nBattaglia et al. evaluates on wider variety senerios compared to this work (e.g. n-bodies under gravitation, falling strings). Such experiments demonstrate the ability of the models to generalize. However, this work does include more in-depth experiments in case of bouncing balls compared to Battaglia et al. (e.g. mass estimation and varying world configurations with obstacles in the bouncing balls senerio). \n\nMoreover, an extensive comparison to Fragkiadaki et al. (2015) (in the bouncing balls senerios) is missing. The authors (referring to answer to question 4) do point out to comaprable numbers in both works, but the experimental settings are different.  Comparison in a billiard table senerio like that Fragkiadaki et al. (2015) where a initial force is applied to a ball, would have been enlightening. \n\nThe authors only evaluate the error in velocity in the bouncing balls senerios. We understand that this model predicts only the velocity (refer to answer of question 2). Error analysis also with respect to ground truth ball position would be more enlightening. As small errors in velocity can quickly lead to entirely different scene configuration.\n\n- conclusion / recommendation\n\nThe main issue with this work is the unclear novelty with respect to work of Battaglia et al. at NIPS'16. A quantitative and qualitative comparison with Battaglia et al. is lacking.  But the authors state that their work was developed independently.\n\nDifferentiable physics engines like NPE or that of Battaglia et al. (NIPS 2016) requires generation of an extensive amount of synthetic data to learn about the physics of a certain senerio. Moreover, extensive retraining is required to adapt to new sceneries (e.g. bouncing balls to n-body systems). Any practical advantage versus generating new code for a physics engine is not clear. Other \"bottom-up\" approaches like that of  Fragkiadaki et al. (2015) couple vision along with learning dynamics. However, they require very few input parameters (position, mass, current velocity, world configuration), as approximate parameter estimation can be done from the visual component.  Such approaches could be potentially more useful of a robot in \"common-sense\" everyday tasks (e.g. manipulation). Thus, overall potential applications of a differentiable physics engine like NPE is unclear."
  },
  {
    "people": [
      "Sukhbaatar"
    ],
    "review": "The paper proposes a Neural Physics Engine for predicting intuitive physics. It is able to model the objects' dynamics and pairwise object interactions in order to build predictive models. This is a very nice direction, as also noted by the reviewers. One reviewer was particularly enthusiastic, while the other two less so. The main concerns were similarities to existing work, which however can be considered as done in parallel. The reviewers also had comments wrt evaluation, which the authors addressed. This is a good paper, and the AC recommends acceptance. The authors are also encouraged to look at \"Learning Multiagent Communication with Backpropagation\" by Sukhbaatar, whose method (albeit applied in a different context) seems relevant to the proposed approach."
  },
  {
    "people": [
      "Battaglia",
      "Fragkiadaki",
      "Battaglia",
      "Fragkiadaki",
      "Fragkiadaki",
      "Fragkiadaki"
    ],
    "review": "We thank all reviewers again for the thoughtful and in-depth reviews. The pre-review period helped us improve the paper by adding in comparisons with Battaglia et al. (2016) and Fragkiadaki et al. (2015), and clarify several arguments in the paper. The review period helped us add more specific and detailed comparisons with Battaglia et al. (2016) and Fragkiadaki et al. (2015). The review period also helped us add a numerical error analysis on the predicted position, add additional experiments analyzing the neighborhood mask, and clarify further arguments in the paper. We have uploaded our revised version.\n\nMany reviewers mentioned a comparison with Fragkiadaki et al. (2015). In our review responses, we highlighted several specific advantages to our approach that are evident from comparing the videos. The link to Fragkiadaki\u2019s prediction videos is in their paper ("
  },
  {
    "people": [
      "Fragkiadaki",
      "Battaglia",
      "Battaglia"
    ],
    "review": "Summary\n===\nThis paper proposes the Neural Physics Engine (NPE), a network architecture\nwhich simulates object interactions. While NPE decides to explicitly represent\nobjects (rather than video frames), it incorporates knowledge of physics\nalmost exclusively through training data. It is tested in a toy domain with\nbouncing 2d balls.\n\nThe proposed architecture processes each object in a scene one at a time.\nPairs of objects are embedded in a common space where the effect of the\nobjects on each other can be represented. These embeddings are summed\nand combined with the focus object's state to predict the focus object's\nchange in velocity. Alternative baselines are presented which either\nforego the pairwise embedding for a single object embedding or\nencode a focus object's neighbors in a sequence of LSTM states.\n\nNPE outperforms the baselines dramatically, showing the importance of\narchitecture choices in learning to do this object based simulation.\nThe model is tested in multiple ways. Ability to predict object trajectory\nover long time spans is measured. Generalization to different numbers of objects\nis measured. Generalization to slightly altered environments (difference\nshaped walls) is measured. Finally, the NPE is also trained to predict\nobject mass using only interactions with other objects, where it also\noutperforms baselines.\n\n\nComments\n===\n\n* I have one more clarifying question. Are the inputs to the blue box in\nfigure 3 (b)/(c) the concatenation of the summed embeddings and state vector\nof object 3? Or is the input to the blue module some other combination of the\ntwo vectors?\n\n\n* Section 2.1 begins with \"First, because physics does not\nchange across inertial frames, it suffices to separately predict the future state of each object conditioned\non the past states of itself and the other objects in its neighborhood, similar to Fragkiadaki\net al. (2015).\"\n\nI think this is an argument to forego the visual representation used by previous\nwork in favor of an object only representation. This would be more clear if there\nwere contrast with a visual representation.\n\n\n* As addressed in the paper, this approach is novel, though less so after taking\ninto consideration the concurrent work of Battaglia et. al. in NIPS 2016 titled\n\"Interaction Networks for Learning about Objects, Relations and Physics.\"\nThis work offers a different network architecture and set of experiments, as\nwell as great presentation, but the use of an object based representation\nfor learning to predict physical behavior is shared.\n\n\nOverall Evaluation\n===\n\nThis paper was a pleasure to read and provided many experiments that offered\nclear and interesting conclusions. It offers a novel approach (though\nless so compared to the concurrent work of Battaglia et. al. 2016) which\nrepresents a significant step forward in the current investigation of\nintuitive physics."
  },
  {
    "people": [
      "Fragkiadaki"
    ],
    "review": "Paper proposes a neural physics engine (NPE). NPE provides a factorization of physical scene into composable object-based representations. NPE predicts a future state of the given object as a function composition of the pairwise interactions between itself and near-by objects. This has a nice physical interpretation of forces being additive. In the paper NPE is investigated in the context of 2D worlds with balls and obstacles. \n\nOverall the approach is interesting and has an interesting flavor of combining neural networks with basic properties of physics. Overall, it seems like it may lead to interesting and significant follow up work in the field. The concerns with the paper is mainly with evaluation, which in places appears to be weak (see below). \n\n> Significance & Originality:\n\nThe approach is interesting. While other methods have tried to build models that can deal with physical predictions, the idea of summing over pair-wise terms, to the best of my knowledge, is novel and much more in-line with the underlying principles of mechanics. As such, while relatively simple, it seems to be an important contribution. \n\n> Clarity:\n\nThe paper is generally well written. However, large portion of the early introduction is rather abstract and it is difficult to parse until one gets to 5th paragraph. I would suggest editing the early part of introduction to include more specifics about the approach or even examples ... to make text more tangible.\n\n> Experiments\n\nGenerally there are two issues with experiments in my opinion: (1) the added indirect comparison with Fragkiadaki et al (2015) does not appears to be quantitatively flattering with respect to the proposed approach, and (2) quantitative experiments on the role the size of the mask has on performance should really be added. Authors mention that they observe that mask is helpful, but it is not clear how helpful or how sensitive the overall performance is to this parameter. This experiment should really be added.\n\nI do feel that despite few mentioned shortcomings that would make the paper stronger, this is an interesting paper and should be published."
  },
  {
    "people": [
      "Battaglia",
      "Battaglia",
      "Battaglia",
      "Battaglia",
      "Battaglia",
      "Fragkiadaki",
      "Fragkiadaki",
      "Battaglia",
      "Battaglia",
      "Battaglia",
      "Fragkiadaki"
    ],
    "review": "- summary\n\nThe paper proposes a differntiable Neural Physics Engine (NPE). The NPE consists of an encoder and a decoder function. The NPE takes as input the state of pairs of objects (within a neighbourhood of a focus object) at two previous time-steps in a scene. The encoder function summarizes the interaction of each pair of objects. The decoder then outputs the change in velocity of the focus object at the next time step. The NPE is evaluated on various environments containing bouncing balls.\n\n- novelty\n\nThe differentiable NPE is a novel concept. However, concurrently Battaglia et al. (NIPS 2016) proposes a very similar model. Just as this work, Battaglia et al. (NIPS 2016) consider a model which consists of a encoder function (relation-centric) which encodes the interaction among a focus object and other objects in the scene and a decoder (relation-centric) function which considers the cumulative (encoded) effect of object interactions on the focus object and predicts effect of the interactions.  Aspects like only considering objects interactions within a neighbourhood (versus the complete object interaction graph in Battaglia et al.) based on euclideian distance  are novel to this work. However, the advantages (if any) of NPE versus the model of Battaglia et al. are not clear. Moreover, it is not clear how this neighbourhood thresholding scene would preform in case of n-ball systems, where gravitational forces of massive objects can be felt over large distances.\n\n- citations \n\nThis work includes all relevant citations.\n\n- clarity\n\nThe article is well written and easy to understand.\n\n- experiments \n\nBattaglia et al. evaluates on wider variety senerios compared to this work (e.g. n-bodies under gravitation, falling strings). Such experiments demonstrate the ability of the models to generalize. However, this work does include more in-depth experiments in case of bouncing balls compared to Battaglia et al. (e.g. mass estimation and varying world configurations with obstacles in the bouncing balls senerio). \n\nMoreover, an extensive comparison to Fragkiadaki et al. (2015) (in the bouncing balls senerios) is missing. The authors (referring to answer to question 4) do point out to comaprable numbers in both works, but the experimental settings are different.  Comparison in a billiard table senerio like that Fragkiadaki et al. (2015) where a initial force is applied to a ball, would have been enlightening. \n\nThe authors only evaluate the error in velocity in the bouncing balls senerios. We understand that this model predicts only the velocity (refer to answer of question 2). Error analysis also with respect to ground truth ball position would be more enlightening. As small errors in velocity can quickly lead to entirely different scene configuration.\n\n- conclusion / recommendation\n\nThe main issue with this work is the unclear novelty with respect to work of Battaglia et al. at NIPS'16. A quantitative and qualitative comparison with Battaglia et al. is lacking.  But the authors state that their work was developed independently.\n\nDifferentiable physics engines like NPE or that of Battaglia et al. (NIPS 2016) requires generation of an extensive amount of synthetic data to learn about the physics of a certain senerio. Moreover, extensive retraining is required to adapt to new sceneries (e.g. bouncing balls to n-body systems). Any practical advantage versus generating new code for a physics engine is not clear. Other \"bottom-up\" approaches like that of  Fragkiadaki et al. (2015) couple vision along with learning dynamics. However, they require very few input parameters (position, mass, current velocity, world configuration), as approximate parameter estimation can be done from the visual component.  Such approaches could be potentially more useful of a robot in \"common-sense\" everyday tasks (e.g. manipulation). Thus, overall potential applications of a differentiable physics engine like NPE is unclear."
  },
  {
    "people": [
      "Battaglia",
      "Battaglia",
      "Battaglia",
      "Battaglia",
      "Battaglia",
      "Fragkiadaki",
      "Fragkiadaki",
      "Battaglia",
      "Battaglia",
      "Battaglia",
      "Fragkiadaki"
    ],
    "review": "- summary\n\nThe paper proposes a differntiable Neural Physics Engine (NPE). The NPE consists of an encoder and a decoder function. The NPE takes as input the state of pairs of objects (within a neighbourhood of a focus object) at two previous time-steps in a scene. The encoder function summarizes the interaction of each pair of objects. The decoder then outputs the change in velocity of the focus object at the next time step. The NPE is evaluated on various environments containing bouncing balls.\n\n- novelty\n\nThe differentiable NPE is a novel concept. However, concurrently Battaglia et al. (NIPS 2016) proposes a very similar model. Just as this work, Battaglia et al. (NIPS 2016) consider a model which consists of a encoder function (relation-centric) which encodes the interaction among a focus object and other objects in the scene and a decoder (relation-centric) function which considers the cumulative (encoded) effect of object interactions on the focus object and predicts effect of the interactions.  Aspects like only considering objects interactions within a neighbourhood (versus the complete object interaction graph in Battaglia et al.) based on euclideian distance  are novel to this work. However, the advantages (if any) of NPE versus the model of Battaglia et al. are not clear. Moreover, it is not clear how this neighbourhood thresholding scene would preform in case of n-ball systems, where gravitational forces of massive objects can be felt over large distances.\n\n- citations \n\nThis work includes all relevant citations.\n\n- clarity\n\nThe article is well written and easy to understand.\n\n- experiments \n\nBattaglia et al. evaluates on wider variety senerios compared to this work (e.g. n-bodies under gravitation, falling strings). Such experiments demonstrate the ability of the models to generalize. However, this work does include more in-depth experiments in case of bouncing balls compared to Battaglia et al. (e.g. mass estimation and varying world configurations with obstacles in the bouncing balls senerio). \n\nMoreover, an extensive comparison to Fragkiadaki et al. (2015) (in the bouncing balls senerios) is missing. The authors (referring to answer to question 4) do point out to comaprable numbers in both works, but the experimental settings are different.  Comparison in a billiard table senerio like that Fragkiadaki et al. (2015) where a initial force is applied to a ball, would have been enlightening. \n\nThe authors only evaluate the error in velocity in the bouncing balls senerios. We understand that this model predicts only the velocity (refer to answer of question 2). Error analysis also with respect to ground truth ball position would be more enlightening. As small errors in velocity can quickly lead to entirely different scene configuration.\n\n- conclusion / recommendation\n\nThe main issue with this work is the unclear novelty with respect to work of Battaglia et al. at NIPS'16. A quantitative and qualitative comparison with Battaglia et al. is lacking.  But the authors state that their work was developed independently.\n\nDifferentiable physics engines like NPE or that of Battaglia et al. (NIPS 2016) requires generation of an extensive amount of synthetic data to learn about the physics of a certain senerio. Moreover, extensive retraining is required to adapt to new sceneries (e.g. bouncing balls to n-body systems). Any practical advantage versus generating new code for a physics engine is not clear. Other \"bottom-up\" approaches like that of  Fragkiadaki et al. (2015) couple vision along with learning dynamics. However, they require very few input parameters (position, mass, current velocity, world configuration), as approximate parameter estimation can be done from the visual component.  Such approaches could be potentially more useful of a robot in \"common-sense\" everyday tasks (e.g. manipulation). Thus, overall potential applications of a differentiable physics engine like NPE is unclear."
  },
  {
    "people": [
      "Sukhbaatar"
    ],
    "review": "The paper proposes a Neural Physics Engine for predicting intuitive physics. It is able to model the objects' dynamics and pairwise object interactions in order to build predictive models. This is a very nice direction, as also noted by the reviewers. One reviewer was particularly enthusiastic, while the other two less so. The main concerns were similarities to existing work, which however can be considered as done in parallel. The reviewers also had comments wrt evaluation, which the authors addressed. This is a good paper, and the AC recommends acceptance. The authors are also encouraged to look at \"Learning Multiagent Communication with Backpropagation\" by Sukhbaatar, whose method (albeit applied in a different context) seems relevant to the proposed approach."
  },
  {
    "people": [
      "Battaglia",
      "Fragkiadaki",
      "Battaglia",
      "Fragkiadaki",
      "Fragkiadaki",
      "Fragkiadaki"
    ],
    "review": "We thank all reviewers again for the thoughtful and in-depth reviews. The pre-review period helped us improve the paper by adding in comparisons with Battaglia et al. (2016) and Fragkiadaki et al. (2015), and clarify several arguments in the paper. The review period helped us add more specific and detailed comparisons with Battaglia et al. (2016) and Fragkiadaki et al. (2015). The review period also helped us add a numerical error analysis on the predicted position, add additional experiments analyzing the neighborhood mask, and clarify further arguments in the paper. We have uploaded our revised version.\n\nMany reviewers mentioned a comparison with Fragkiadaki et al. (2015). In our review responses, we highlighted several specific advantages to our approach that are evident from comparing the videos. The link to Fragkiadaki\u2019s prediction videos is in their paper ("
  },
  {
    "people": [
      "Fragkiadaki",
      "Battaglia",
      "Battaglia"
    ],
    "review": "Summary\n===\nThis paper proposes the Neural Physics Engine (NPE), a network architecture\nwhich simulates object interactions. While NPE decides to explicitly represent\nobjects (rather than video frames), it incorporates knowledge of physics\nalmost exclusively through training data. It is tested in a toy domain with\nbouncing 2d balls.\n\nThe proposed architecture processes each object in a scene one at a time.\nPairs of objects are embedded in a common space where the effect of the\nobjects on each other can be represented. These embeddings are summed\nand combined with the focus object's state to predict the focus object's\nchange in velocity. Alternative baselines are presented which either\nforego the pairwise embedding for a single object embedding or\nencode a focus object's neighbors in a sequence of LSTM states.\n\nNPE outperforms the baselines dramatically, showing the importance of\narchitecture choices in learning to do this object based simulation.\nThe model is tested in multiple ways. Ability to predict object trajectory\nover long time spans is measured. Generalization to different numbers of objects\nis measured. Generalization to slightly altered environments (difference\nshaped walls) is measured. Finally, the NPE is also trained to predict\nobject mass using only interactions with other objects, where it also\noutperforms baselines.\n\n\nComments\n===\n\n* I have one more clarifying question. Are the inputs to the blue box in\nfigure 3 (b)/(c) the concatenation of the summed embeddings and state vector\nof object 3? Or is the input to the blue module some other combination of the\ntwo vectors?\n\n\n* Section 2.1 begins with \"First, because physics does not\nchange across inertial frames, it suffices to separately predict the future state of each object conditioned\non the past states of itself and the other objects in its neighborhood, similar to Fragkiadaki\net al. (2015).\"\n\nI think this is an argument to forego the visual representation used by previous\nwork in favor of an object only representation. This would be more clear if there\nwere contrast with a visual representation.\n\n\n* As addressed in the paper, this approach is novel, though less so after taking\ninto consideration the concurrent work of Battaglia et. al. in NIPS 2016 titled\n\"Interaction Networks for Learning about Objects, Relations and Physics.\"\nThis work offers a different network architecture and set of experiments, as\nwell as great presentation, but the use of an object based representation\nfor learning to predict physical behavior is shared.\n\n\nOverall Evaluation\n===\n\nThis paper was a pleasure to read and provided many experiments that offered\nclear and interesting conclusions. It offers a novel approach (though\nless so compared to the concurrent work of Battaglia et. al. 2016) which\nrepresents a significant step forward in the current investigation of\nintuitive physics."
  },
  {
    "people": [
      "Fragkiadaki"
    ],
    "review": "Paper proposes a neural physics engine (NPE). NPE provides a factorization of physical scene into composable object-based representations. NPE predicts a future state of the given object as a function composition of the pairwise interactions between itself and near-by objects. This has a nice physical interpretation of forces being additive. In the paper NPE is investigated in the context of 2D worlds with balls and obstacles. \n\nOverall the approach is interesting and has an interesting flavor of combining neural networks with basic properties of physics. Overall, it seems like it may lead to interesting and significant follow up work in the field. The concerns with the paper is mainly with evaluation, which in places appears to be weak (see below). \n\n> Significance & Originality:\n\nThe approach is interesting. While other methods have tried to build models that can deal with physical predictions, the idea of summing over pair-wise terms, to the best of my knowledge, is novel and much more in-line with the underlying principles of mechanics. As such, while relatively simple, it seems to be an important contribution. \n\n> Clarity:\n\nThe paper is generally well written. However, large portion of the early introduction is rather abstract and it is difficult to parse until one gets to 5th paragraph. I would suggest editing the early part of introduction to include more specifics about the approach or even examples ... to make text more tangible.\n\n> Experiments\n\nGenerally there are two issues with experiments in my opinion: (1) the added indirect comparison with Fragkiadaki et al (2015) does not appears to be quantitatively flattering with respect to the proposed approach, and (2) quantitative experiments on the role the size of the mask has on performance should really be added. Authors mention that they observe that mask is helpful, but it is not clear how helpful or how sensitive the overall performance is to this parameter. This experiment should really be added.\n\nI do feel that despite few mentioned shortcomings that would make the paper stronger, this is an interesting paper and should be published."
  },
  {
    "people": [
      "Battaglia",
      "Battaglia",
      "Battaglia",
      "Battaglia",
      "Battaglia",
      "Fragkiadaki",
      "Fragkiadaki",
      "Battaglia",
      "Battaglia",
      "Battaglia",
      "Fragkiadaki"
    ],
    "review": "- summary\n\nThe paper proposes a differntiable Neural Physics Engine (NPE). The NPE consists of an encoder and a decoder function. The NPE takes as input the state of pairs of objects (within a neighbourhood of a focus object) at two previous time-steps in a scene. The encoder function summarizes the interaction of each pair of objects. The decoder then outputs the change in velocity of the focus object at the next time step. The NPE is evaluated on various environments containing bouncing balls.\n\n- novelty\n\nThe differentiable NPE is a novel concept. However, concurrently Battaglia et al. (NIPS 2016) proposes a very similar model. Just as this work, Battaglia et al. (NIPS 2016) consider a model which consists of a encoder function (relation-centric) which encodes the interaction among a focus object and other objects in the scene and a decoder (relation-centric) function which considers the cumulative (encoded) effect of object interactions on the focus object and predicts effect of the interactions.  Aspects like only considering objects interactions within a neighbourhood (versus the complete object interaction graph in Battaglia et al.) based on euclideian distance  are novel to this work. However, the advantages (if any) of NPE versus the model of Battaglia et al. are not clear. Moreover, it is not clear how this neighbourhood thresholding scene would preform in case of n-ball systems, where gravitational forces of massive objects can be felt over large distances.\n\n- citations \n\nThis work includes all relevant citations.\n\n- clarity\n\nThe article is well written and easy to understand.\n\n- experiments \n\nBattaglia et al. evaluates on wider variety senerios compared to this work (e.g. n-bodies under gravitation, falling strings). Such experiments demonstrate the ability of the models to generalize. However, this work does include more in-depth experiments in case of bouncing balls compared to Battaglia et al. (e.g. mass estimation and varying world configurations with obstacles in the bouncing balls senerio). \n\nMoreover, an extensive comparison to Fragkiadaki et al. (2015) (in the bouncing balls senerios) is missing. The authors (referring to answer to question 4) do point out to comaprable numbers in both works, but the experimental settings are different.  Comparison in a billiard table senerio like that Fragkiadaki et al. (2015) where a initial force is applied to a ball, would have been enlightening. \n\nThe authors only evaluate the error in velocity in the bouncing balls senerios. We understand that this model predicts only the velocity (refer to answer of question 2). Error analysis also with respect to ground truth ball position would be more enlightening. As small errors in velocity can quickly lead to entirely different scene configuration.\n\n- conclusion / recommendation\n\nThe main issue with this work is the unclear novelty with respect to work of Battaglia et al. at NIPS'16. A quantitative and qualitative comparison with Battaglia et al. is lacking.  But the authors state that their work was developed independently.\n\nDifferentiable physics engines like NPE or that of Battaglia et al. (NIPS 2016) requires generation of an extensive amount of synthetic data to learn about the physics of a certain senerio. Moreover, extensive retraining is required to adapt to new sceneries (e.g. bouncing balls to n-body systems). Any practical advantage versus generating new code for a physics engine is not clear. Other \"bottom-up\" approaches like that of  Fragkiadaki et al. (2015) couple vision along with learning dynamics. However, they require very few input parameters (position, mass, current velocity, world configuration), as approximate parameter estimation can be done from the visual component.  Such approaches could be potentially more useful of a robot in \"common-sense\" everyday tasks (e.g. manipulation). Thus, overall potential applications of a differentiable physics engine like NPE is unclear."
  },
  {
    "people": [
      "nit"
    ],
    "review": "This paper trains a generative model of image patches, where dictionary elements undergo gated linear transformations before being combined. The transformations are motivated in terms of Lie group operators, though in practice they are a set of fixed linear transformations. This is motivated strongly in terms of learning a hierarchy of transformations, though only one layer is used in the experiments (except for a toy case in the appendix).\n\nI like the motivation for this algorithm. The realization seems very similar to a group or block sparse coding implementation. I was disappointed by the restriction to linear transformations. The experiments were all toy cases, demonstrating that the algorithm can learn groups of Gabor- or center surround-like features. They would have been somewhat underpowered five years ago, and seemed extremely small by today's standards.\n\nSpecific comments:\n\nBased on common practices in ML literature, I have a strong bias to think of $x$ as inputs and $w$ as network weights. Latent variables are often $z$ or $a$. Depending on your target audience, I would suggest permuting your choice of symbols so the reader can more quickly interpret your model.\n\nnit: number all equations for easier reference\n\nsec 2.2 -- It's weird that the transformation is fixed, but is still written as a function of x.\n\nsec 2.3 -- The updated text here confuses me actually. I had thought that you were using a fixed set of linear transformations, and were motivating in terms of Lie groups, but were not actually taking matrix exponentials in your algorithm. The equations in the second half of this section suggest you are working with matrix exponentials though. I'm not sure which direction I'm confused in, but probably good to clarify the text either way.\n\nBTW -- there's another possible solution to the local minima difficulty, which is the one used in Sohl-Dickstein, 2010. There, they introduce blurring operators matched to each transformation operator, and gradient descent can escape local minima by detouring through coarser (more blurred) scales.\n\nsec 3.2 -- I believe by degrees of freedom you mean the number of model parameters, not the number of latent coefficients that must be inferred? Should make this more clear. Is it more appropriate to compare reconstruction error while matching number of model parameters, or number of latent variables?\n\nI wonder if a convolutional version of this algorithm would be practical / would make it more suited as a generative model of whole images.\n\n====\npost rebuttal update\n\nThank you for taking the time to write the rebuttal! I have read it, but it did not significantly effect my rating."
  },
  {
    "people": [
      "nit"
    ],
    "review": "This paper trains a generative model of image patches, where dictionary elements undergo gated linear transformations before being combined. The transformations are motivated in terms of Lie group operators, though in practice they are a set of fixed linear transformations. This is motivated strongly in terms of learning a hierarchy of transformations, though only one layer is used in the experiments (except for a toy case in the appendix).\n\nI like the motivation for this algorithm. The realization seems very similar to a group or block sparse coding implementation. I was disappointed by the restriction to linear transformations. The experiments were all toy cases, demonstrating that the algorithm can learn groups of Gabor- or center surround-like features. They would have been somewhat underpowered five years ago, and seemed extremely small by today's standards.\n\nSpecific comments:\n\nBased on common practices in ML literature, I have a strong bias to think of $x$ as inputs and $w$ as network weights. Latent variables are often $z$ or $a$. Depending on your target audience, I would suggest permuting your choice of symbols so the reader can more quickly interpret your model.\n\nnit: number all equations for easier reference\n\nsec 2.2 -- It's weird that the transformation is fixed, but is still written as a function of x.\n\nsec 2.3 -- The updated text here confuses me actually. I had thought that you were using a fixed set of linear transformations, and were motivating in terms of Lie groups, but were not actually taking matrix exponentials in your algorithm. The equations in the second half of this section suggest you are working with matrix exponentials though. I'm not sure which direction I'm confused in, but probably good to clarify the text either way.\n\nBTW -- there's another possible solution to the local minima difficulty, which is the one used in Sohl-Dickstein, 2010. There, they introduce blurring operators matched to each transformation operator, and gradient descent can escape local minima by detouring through coarser (more blurred) scales.\n\nsec 3.2 -- I believe by degrees of freedom you mean the number of model parameters, not the number of latent coefficients that must be inferred? Should make this more clear. Is it more appropriate to compare reconstruction error while matching number of model parameters, or number of latent variables?\n\nI wonder if a convolutional version of this algorithm would be practical / would make it more suited as a generative model of whole images.\n\n====\npost rebuttal update\n\nThank you for taking the time to write the rebuttal! I have read it, but it did not significantly effect my rating."
  },
  {
    "people": [
      "Levin",
      "Levin",
      "Levin"
    ],
    "review": "Thank you for your helpful comments and suggestions.  We have uploaded a revised version of the paper addressing some of the comments as well as fixing some typos etc.  Below are our replies to specific comments.\n\n*Regarding the data set and task (Reviewers 1, 4):  \nThe data set is indeed on the small size, and the assumption of known word boundaries is a strong one.  This paper focuses on improving the current state of research on learning acoustic embeddings, so we are comparing to the most relevant prior work, which largely uses this data set and the word discrimination task.  Now that we have achieved very high average precisions on this task, we believe that future work should indeed focus on (and standardize) larger data sets and tasks.\n\nWe would like to point out, however, some prior work suggesting that improvements on this data set/task can transfer to other data/tasks.  Specifically, Levin et al. took embeddings optimized on this data set/task (Levin et al., ASRU 2013) to improve a query-by-example system without known word boundaries (Levin et al., ICASSP 2015), by simply applying their embedding approach to non-word segments as well.  This is encouraging.  On the other hand, it would also be straightforward, and more principled, to extend our approach to directly train on both word and non-word segments.  We mention this in the revised paper.\n\n*Regarding experimenting with phone sequences rather than character sequences (Reviewer 1):  \nAlthough working with phone sequences requires a bit more supervision, we agree that this is an interesting and straightforward experiment and we are currently working on it (though it is not complete).  In the meantime, in our revised paper we have included the rank correlation between our embedding distances (trained with character supervision) and phonetic edit distances, which are not too different from the correlations with orthographic distance (Table 3).   This is nice since it suggests we might not be losing too much by using orthographic supervision vs. phonetic supervision.\n\n*Regarding homophones (Reviewer 1):  \nWe expect our approach to be unable to distinguish homophones, since they can only be distinguished in context.  Our data set does not include a sufficient number of homophones to confirm this, but future work should look at this problem in the context of more data and longer contexts.\n\n*Regarding ASR baselines (Reviewer 4):  \nThe revised paper includes an ASR-based baseline from prior work in Table 2, using DTW on phone posteriors, which is worse than ours despite training on vastly more data.  While it is an older result, it shows that it is not trivial to get our numbers.  We believe that this is because there is a benefit to embedding the entire sequence and training with a loss that explicitly optimizes a measure related to the discrimination task at hand.\n\n*Regarding additional analysis (Reviewer 4):  \nWe have added (in the appendix) a precision-recall curve and scatterplot of embedding vs. orthographic distances.\n\n*Regarding open-sourcing the code (Reviewer 3):\nWe agree.   We are in the process of updating our code and releasing it online.\n\nThank you also for pointing out the issue with Fig. 1 (it has been fixed in the revised paper).\n\n"
  },
  {
    "people": [
      "Levin",
      "Levin",
      "Levin"
    ],
    "review": "Thank you for your helpful comments and suggestions.  We have uploaded a revised version of the paper addressing some of the comments as well as fixing some typos etc.  Below are our replies to specific comments.\n\n*Regarding the data set and task (Reviewers 1, 4):  \nThe data set is indeed on the small size, and the assumption of known word boundaries is a strong one.  This paper focuses on improving the current state of research on learning acoustic embeddings, so we are comparing to the most relevant prior work, which largely uses this data set and the word discrimination task.  Now that we have achieved very high average precisions on this task, we believe that future work should indeed focus on (and standardize) larger data sets and tasks.\n\nWe would like to point out, however, some prior work suggesting that improvements on this data set/task can transfer to other data/tasks.  Specifically, Levin et al. took embeddings optimized on this data set/task (Levin et al., ASRU 2013) to improve a query-by-example system without known word boundaries (Levin et al., ICASSP 2015), by simply applying their embedding approach to non-word segments as well.  This is encouraging.  On the other hand, it would also be straightforward, and more principled, to extend our approach to directly train on both word and non-word segments.  We mention this in the revised paper.\n\n*Regarding experimenting with phone sequences rather than character sequences (Reviewer 1):  \nAlthough working with phone sequences requires a bit more supervision, we agree that this is an interesting and straightforward experiment and we are currently working on it (though it is not complete).  In the meantime, in our revised paper we have included the rank correlation between our embedding distances (trained with character supervision) and phonetic edit distances, which are not too different from the correlations with orthographic distance (Table 3).   This is nice since it suggests we might not be losing too much by using orthographic supervision vs. phonetic supervision.\n\n*Regarding homophones (Reviewer 1):  \nWe expect our approach to be unable to distinguish homophones, since they can only be distinguished in context.  Our data set does not include a sufficient number of homophones to confirm this, but future work should look at this problem in the context of more data and longer contexts.\n\n*Regarding ASR baselines (Reviewer 4):  \nThe revised paper includes an ASR-based baseline from prior work in Table 2, using DTW on phone posteriors, which is worse than ours despite training on vastly more data.  While it is an older result, it shows that it is not trivial to get our numbers.  We believe that this is because there is a benefit to embedding the entire sequence and training with a loss that explicitly optimizes a measure related to the discrimination task at hand.\n\n*Regarding additional analysis (Reviewer 4):  \nWe have added (in the appendix) a precision-recall curve and scatterplot of embedding vs. orthographic distances.\n\n*Regarding open-sourcing the code (Reviewer 3):\nWe agree.   We are in the process of updating our code and releasing it online.\n\nThank you also for pointing out the issue with Fig. 1 (it has been fixed in the revised paper).\n\n"
  },
  {
    "people": [
      "Gulcehre",
      "Chelba"
    ],
    "review": "The reviewers liked this paper quite a bit, and so for this reason it is a perfectly fine paper to accept. However, it should be noted that the area chair was less enthusiastic. The area chairs mentions that the model appears to be an extension of Gulcehre et al. and the Penn Treebank perplexity experiments are too small scale to be taken seriously in 2017. Instead of experimenting on other known large-scale language modeling setups, the authors introduce their own new dataset (which is 1 order of magnitude smaller than the 1-Billion LM dataset by Chelba et al). The new dataset might be a good idea, but the area chair doesn't understand why the authors do not run public available systems as baselines. This should have been fairly easy to do and would have significantly strengthen the result of this work. The PCs thus encourage to authors to take into account this feedback and consider updating their paper accordingly."
  },
  {
    "people": [
      "Gulcehre",
      "Gulcehre",
      "Gulcehre",
      "Gulcehre",
      "Gulcehre",
      "Gulcehre"
    ],
    "review": "This paper proposes augmenting RNN-based language models with a pointer network in order to deal better with rare words. The pointer network can point to words in the recent context, and hence the prediction for each time step is a mixture between the usual softmax output and the pointer distribution over the recent words. The paper also introduces a new language modelling dataset, which overcomes some of the shortcomings of previous datasets.\n\nThe reason for the score I gave for this paper is that I find the proposed model a direct application of the previous work Gulcehre et al., which follows a similar approach but for machine translation and summarization. The main differences I find is that Gulcehre et al. use an encoder-decoder architecture, and use the attention weights of the encoder to point to locations of words in the input, while here an RNN is used and a pointer network produces a distribution over the full vocabulary (by summing the softmax probabilities of words in the recent context). The context (query) vector for the pointing network is also different, but this is also a direct consequence of having a different application.\n\nWhile the paper describes the differences between the proposed approach and Gulcehre et al.\u2019s approach, I find some of the claims either wrong or not that significant. For example, quoting from Section 1:\n\u201cRather than relying on the RNN hidden state to decide when to use the pointer, as in the recent work of Gulcehre et al. (2016), we allow the pointer component itself to decide when to use the softmax vocabulary through a sentinel.\u201d\nAs far as I can tell, your model also uses the recent hidden state to form a query vector,  which is matched by the pointer network to previous words. Can you please clarify what you mean here?\n\nIn addition, quoting from section 3 which describes the model of Gulcehre et al.:\n\u201cRather than constructing a mixture model as in our work, they use a switching network to decide which component to use\u201d\nThis is not correct. The model of Gulcehre is also a mixture model, where an MLP with sigmoid output (switching network) is used to form a mixture between softmax prediction and locations of the input text.\n\nFinally, in the following quote, also from section 3: \n\u201cThe pointer network is not used as a source of information for the switching network as in our model.\u201d \nIt is not clear what the authors mean by \u201csource of information\u201d here. Is it the fact that the switching probability is part of the pointer softmax? I am wondering how significant this difference is.\n\nWith regards to the proposed dataset, there are also other datasets typically used for language modelling, including The Hutter Prize Wikipedia (enwik8) dataset (Hutter, 2012) and e Text8 dataset (Mahoney, 2009). Can you please comment on the differences between your dataset and those as well?\n\nI would be happy to discuss with the authors the points I raised, and I am open to changing my vote if there is any misunderstanding on my part.\n"
  },
  {
    "people": [
      "Gulcehre",
      "Chelba"
    ],
    "review": "The reviewers liked this paper quite a bit, and so for this reason it is a perfectly fine paper to accept. However, it should be noted that the area chair was less enthusiastic. The area chairs mentions that the model appears to be an extension of Gulcehre et al. and the Penn Treebank perplexity experiments are too small scale to be taken seriously in 2017. Instead of experimenting on other known large-scale language modeling setups, the authors introduce their own new dataset (which is 1 order of magnitude smaller than the 1-Billion LM dataset by Chelba et al). The new dataset might be a good idea, but the area chair doesn't understand why the authors do not run public available systems as baselines. This should have been fairly easy to do and would have significantly strengthen the result of this work. The PCs thus encourage to authors to take into account this feedback and consider updating their paper accordingly."
  },
  {
    "people": [
      "Gulcehre",
      "Gulcehre",
      "Gulcehre",
      "Gulcehre",
      "Gulcehre",
      "Gulcehre"
    ],
    "review": "This paper proposes augmenting RNN-based language models with a pointer network in order to deal better with rare words. The pointer network can point to words in the recent context, and hence the prediction for each time step is a mixture between the usual softmax output and the pointer distribution over the recent words. The paper also introduces a new language modelling dataset, which overcomes some of the shortcomings of previous datasets.\n\nThe reason for the score I gave for this paper is that I find the proposed model a direct application of the previous work Gulcehre et al., which follows a similar approach but for machine translation and summarization. The main differences I find is that Gulcehre et al. use an encoder-decoder architecture, and use the attention weights of the encoder to point to locations of words in the input, while here an RNN is used and a pointer network produces a distribution over the full vocabulary (by summing the softmax probabilities of words in the recent context). The context (query) vector for the pointing network is also different, but this is also a direct consequence of having a different application.\n\nWhile the paper describes the differences between the proposed approach and Gulcehre et al.\u2019s approach, I find some of the claims either wrong or not that significant. For example, quoting from Section 1:\n\u201cRather than relying on the RNN hidden state to decide when to use the pointer, as in the recent work of Gulcehre et al. (2016), we allow the pointer component itself to decide when to use the softmax vocabulary through a sentinel.\u201d\nAs far as I can tell, your model also uses the recent hidden state to form a query vector,  which is matched by the pointer network to previous words. Can you please clarify what you mean here?\n\nIn addition, quoting from section 3 which describes the model of Gulcehre et al.:\n\u201cRather than constructing a mixture model as in our work, they use a switching network to decide which component to use\u201d\nThis is not correct. The model of Gulcehre is also a mixture model, where an MLP with sigmoid output (switching network) is used to form a mixture between softmax prediction and locations of the input text.\n\nFinally, in the following quote, also from section 3: \n\u201cThe pointer network is not used as a source of information for the switching network as in our model.\u201d \nIt is not clear what the authors mean by \u201csource of information\u201d here. Is it the fact that the switching probability is part of the pointer softmax? I am wondering how significant this difference is.\n\nWith regards to the proposed dataset, there are also other datasets typically used for language modelling, including The Hutter Prize Wikipedia (enwik8) dataset (Hutter, 2012) and e Text8 dataset (Mahoney, 2009). Can you please comment on the differences between your dataset and those as well?\n\nI would be happy to discuss with the authors the points I raised, and I am open to changing my vote if there is any misunderstanding on my part.\n"
  },
  {
    "people": [
      "Vinyals"
    ],
    "review": "In light of the authors' responsiveness and the updates to the manuscript -- in particular to clarify the meta-learning task -- I am updating my score to an 8.\n\n-----\n\nThis manuscript proposes to tackle few-shot learning with neural networks by leveraging meta-learning, a classic idea that has seen a renaissance in the last 12 months. The authors formulate few-shot learning as a sequential meta-learning problem: each \"example\" includes a sequence of batches of \"training\" pairs, followed by a final \"test\" batch. The inputs at each \"step\" include the outputs of a \"base learner\" (e.g., training loss and gradients), as well as the base learner's current state (parameters). The paper applies an LSTM to this meta-learning problem, using the inner memory cells in the *second* layer to directly model the updated parameters of the base learner. In doing this, they note similarities between the respective update rules of LSTM memory cells and gradient descent. Updates to the LSTM meta-learner are computed based on the base learner's prediction loss for the final \"test\" batch. The authors make several simplifying assumptions, such as sharing weights across all second layer cells (analogous to using the same learning rate for all parameters). The paper recreates the Mini-ImageNet data set proposed in Vinyals et al 2016, and shows that the meta-learner LSTM is competitive with the current state-of-the-art (Matchin Networks, Vinyals 2016) on 1- and 5-shot learning.\n\nStrengths:\n- It is intriguing -- and in hindsight, natural -- to cast the few-shot learning problem as a sequential (meta-)learning problem. While the authors did not originate the general idea of persisting learning across a series of learning problems, I think it is fair to say that they have advanced the state of the art, though I cannot confidently assert its novelty as I am not deeply familiar with recent work on meta-learning.\n- The proposed approach is competitive with and outperforms Vinyals 2016 in 1-shot and 5-shot Mini-ImageNet experiments.\n- The base learner in this setting (simple ConvNet classifier) is quite different from the nearest-neighbor-on-top-of-learned-embedding approach used in Vinyals 2016. It is always exciting when state-of-the-art results can be reported using very different approaches, rather than incremental follow-up work.\n- As far as I know, the insight about the relationship between the memory cell and gradient descent updates is novel here. It is interesting regardless.\n- The paper offers several practical insights about how to design and train an LSTM meta-learner, which should make it easier for others to replicate this work and apply these ideas to new problems. These include proper initialization, weight sharing across coordinates, and the importance of normalizing/rescaling the loss, gradient, and parameter inputs. Some of the insights have been previously described (the importance of simulating test conditions during meta-training; assuming independence between meta-learner and base learner parameters when taking gradients with respect to the meta-learner parameters), but the discussion here is useful nonetheless.\n\nWeaknesses:\n- The writing is at times quite opaque. While it describes very interesting work, I would not call the paper an enjoyable read. It took me multiple passes (as well as consulting related work) to understand the general learning problem. The task description in Section 2 (Page 2) is very abstract and uses notation and language that is not common outside of this sub-area. The paper could benefit from a brief concrete example (based on MNIST is fine), perhaps paired with a diagram illustrating a sequence of few-shot learning tasks. This would definitely make it accessible to a wider audience.\n- Following up on that note, the precise nature of the N-class, few-shot learning problem here is unclear to me. Specifically, the Mini-ImageNet data set has 100 labels, of which 64/16/20 are used during meta-training/validation/testing. Does this mean that only 64/100 classes are observed through meta-training? Or does it mean that only 64/100 are observed in each batch, but on average all 100 are observed during meta-training? If it's the former, how many outputs does the softmax layer of the ConvNet base learner have during meta-training? 64 (only those observed in training) or 100 (of which 36 are never observed)? Many other details like these are unclear (see question).\n- The plots in Figure 2 are pretty uninformative in and of themselves, and the discussion section offers very little insight around them.\n\nThis is an interesting paper with convincing results. It seems like a fairly clear accept, but the presentation of the ideas and work therein could be improved. I will definitely raise my score if the writing is improved."
  },
  {
    "people": [
      "Andrychowicz",
      "Samy Bengio",
      "S. Bengio",
      "Y. Bengio",
      "J. Cloutier",
      "Schmidhuber"
    ],
    "review": "This work presents an LSTM based meta-learning framework to learn the optimization algorithm of a another learning algorithm (here a NN).\nThe paper is globally well written and the presentation of the main material is clear. The crux of the paper: drawing the parallel between Robbins Monroe update rule and the LSTM update rule and exploit it to satisfy the two main desiderata of few shot learning (1- quick acquisition of new knowledge, 2- slower extraction of general transferable knowledge) is intriguing. \n\nSeveral tricks re-used from (Andrychowicz et al. 2016)  such as parameter sharing and normalization, and novel design choices (specific implementation of batch normalization) are well  motivated. \nThe experiments are convincing. This is a strong paper. My only concerns/questions are the following:\n\n1. Can it be redundant to use the loss, gradient and parameters as input to the meta-learner? Did you do ablative studies to make sure simpler combinations are not enough.\n2. It would be great if other architectural components of the network can be learned in a similar fashion (number of neurons, type of units, etc.). Do you have an opinion about this?\n3. The related work section (mainly focused on meta learning) is a bit shallow. Meta-learning is a rather old topic and similar approaches have been tried to solve the same problem even if they were not using LSTMs:\n     - Samy Bengio PhD thesis (1989) is all about this ;-)\n     - Use of genetic programming for the search of a new learning rule for neural networks (S. Bengio, Y. Bengio, and J. Cloutier. 1994)\n     - I am convince Schmidhuber has done something, make sure you find it and update related work section.  \n\nOverall, I like the paper. I believe the discussed material is relevant to a wide audience at ICLR.  \n"
  },
  {
    "people": [
      "Vinyals"
    ],
    "review": "In light of the authors' responsiveness and the updates to the manuscript -- in particular to clarify the meta-learning task -- I am updating my score to an 8.\n\n-----\n\nThis manuscript proposes to tackle few-shot learning with neural networks by leveraging meta-learning, a classic idea that has seen a renaissance in the last 12 months. The authors formulate few-shot learning as a sequential meta-learning problem: each \"example\" includes a sequence of batches of \"training\" pairs, followed by a final \"test\" batch. The inputs at each \"step\" include the outputs of a \"base learner\" (e.g., training loss and gradients), as well as the base learner's current state (parameters). The paper applies an LSTM to this meta-learning problem, using the inner memory cells in the *second* layer to directly model the updated parameters of the base learner. In doing this, they note similarities between the respective update rules of LSTM memory cells and gradient descent. Updates to the LSTM meta-learner are computed based on the base learner's prediction loss for the final \"test\" batch. The authors make several simplifying assumptions, such as sharing weights across all second layer cells (analogous to using the same learning rate for all parameters). The paper recreates the Mini-ImageNet data set proposed in Vinyals et al 2016, and shows that the meta-learner LSTM is competitive with the current state-of-the-art (Matchin Networks, Vinyals 2016) on 1- and 5-shot learning.\n\nStrengths:\n- It is intriguing -- and in hindsight, natural -- to cast the few-shot learning problem as a sequential (meta-)learning problem. While the authors did not originate the general idea of persisting learning across a series of learning problems, I think it is fair to say that they have advanced the state of the art, though I cannot confidently assert its novelty as I am not deeply familiar with recent work on meta-learning.\n- The proposed approach is competitive with and outperforms Vinyals 2016 in 1-shot and 5-shot Mini-ImageNet experiments.\n- The base learner in this setting (simple ConvNet classifier) is quite different from the nearest-neighbor-on-top-of-learned-embedding approach used in Vinyals 2016. It is always exciting when state-of-the-art results can be reported using very different approaches, rather than incremental follow-up work.\n- As far as I know, the insight about the relationship between the memory cell and gradient descent updates is novel here. It is interesting regardless.\n- The paper offers several practical insights about how to design and train an LSTM meta-learner, which should make it easier for others to replicate this work and apply these ideas to new problems. These include proper initialization, weight sharing across coordinates, and the importance of normalizing/rescaling the loss, gradient, and parameter inputs. Some of the insights have been previously described (the importance of simulating test conditions during meta-training; assuming independence between meta-learner and base learner parameters when taking gradients with respect to the meta-learner parameters), but the discussion here is useful nonetheless.\n\nWeaknesses:\n- The writing is at times quite opaque. While it describes very interesting work, I would not call the paper an enjoyable read. It took me multiple passes (as well as consulting related work) to understand the general learning problem. The task description in Section 2 (Page 2) is very abstract and uses notation and language that is not common outside of this sub-area. The paper could benefit from a brief concrete example (based on MNIST is fine), perhaps paired with a diagram illustrating a sequence of few-shot learning tasks. This would definitely make it accessible to a wider audience.\n- Following up on that note, the precise nature of the N-class, few-shot learning problem here is unclear to me. Specifically, the Mini-ImageNet data set has 100 labels, of which 64/16/20 are used during meta-training/validation/testing. Does this mean that only 64/100 classes are observed through meta-training? Or does it mean that only 64/100 are observed in each batch, but on average all 100 are observed during meta-training? If it's the former, how many outputs does the softmax layer of the ConvNet base learner have during meta-training? 64 (only those observed in training) or 100 (of which 36 are never observed)? Many other details like these are unclear (see question).\n- The plots in Figure 2 are pretty uninformative in and of themselves, and the discussion section offers very little insight around them.\n\nThis is an interesting paper with convincing results. It seems like a fairly clear accept, but the presentation of the ideas and work therein could be improved. I will definitely raise my score if the writing is improved."
  },
  {
    "people": [
      "Vinyals"
    ],
    "review": "In light of the authors' responsiveness and the updates to the manuscript -- in particular to clarify the meta-learning task -- I am updating my score to an 8.\n\n-----\n\nThis manuscript proposes to tackle few-shot learning with neural networks by leveraging meta-learning, a classic idea that has seen a renaissance in the last 12 months. The authors formulate few-shot learning as a sequential meta-learning problem: each \"example\" includes a sequence of batches of \"training\" pairs, followed by a final \"test\" batch. The inputs at each \"step\" include the outputs of a \"base learner\" (e.g., training loss and gradients), as well as the base learner's current state (parameters). The paper applies an LSTM to this meta-learning problem, using the inner memory cells in the *second* layer to directly model the updated parameters of the base learner. In doing this, they note similarities between the respective update rules of LSTM memory cells and gradient descent. Updates to the LSTM meta-learner are computed based on the base learner's prediction loss for the final \"test\" batch. The authors make several simplifying assumptions, such as sharing weights across all second layer cells (analogous to using the same learning rate for all parameters). The paper recreates the Mini-ImageNet data set proposed in Vinyals et al 2016, and shows that the meta-learner LSTM is competitive with the current state-of-the-art (Matchin Networks, Vinyals 2016) on 1- and 5-shot learning.\n\nStrengths:\n- It is intriguing -- and in hindsight, natural -- to cast the few-shot learning problem as a sequential (meta-)learning problem. While the authors did not originate the general idea of persisting learning across a series of learning problems, I think it is fair to say that they have advanced the state of the art, though I cannot confidently assert its novelty as I am not deeply familiar with recent work on meta-learning.\n- The proposed approach is competitive with and outperforms Vinyals 2016 in 1-shot and 5-shot Mini-ImageNet experiments.\n- The base learner in this setting (simple ConvNet classifier) is quite different from the nearest-neighbor-on-top-of-learned-embedding approach used in Vinyals 2016. It is always exciting when state-of-the-art results can be reported using very different approaches, rather than incremental follow-up work.\n- As far as I know, the insight about the relationship between the memory cell and gradient descent updates is novel here. It is interesting regardless.\n- The paper offers several practical insights about how to design and train an LSTM meta-learner, which should make it easier for others to replicate this work and apply these ideas to new problems. These include proper initialization, weight sharing across coordinates, and the importance of normalizing/rescaling the loss, gradient, and parameter inputs. Some of the insights have been previously described (the importance of simulating test conditions during meta-training; assuming independence between meta-learner and base learner parameters when taking gradients with respect to the meta-learner parameters), but the discussion here is useful nonetheless.\n\nWeaknesses:\n- The writing is at times quite opaque. While it describes very interesting work, I would not call the paper an enjoyable read. It took me multiple passes (as well as consulting related work) to understand the general learning problem. The task description in Section 2 (Page 2) is very abstract and uses notation and language that is not common outside of this sub-area. The paper could benefit from a brief concrete example (based on MNIST is fine), perhaps paired with a diagram illustrating a sequence of few-shot learning tasks. This would definitely make it accessible to a wider audience.\n- Following up on that note, the precise nature of the N-class, few-shot learning problem here is unclear to me. Specifically, the Mini-ImageNet data set has 100 labels, of which 64/16/20 are used during meta-training/validation/testing. Does this mean that only 64/100 classes are observed through meta-training? Or does it mean that only 64/100 are observed in each batch, but on average all 100 are observed during meta-training? If it's the former, how many outputs does the softmax layer of the ConvNet base learner have during meta-training? 64 (only those observed in training) or 100 (of which 36 are never observed)? Many other details like these are unclear (see question).\n- The plots in Figure 2 are pretty uninformative in and of themselves, and the discussion section offers very little insight around them.\n\nThis is an interesting paper with convincing results. It seems like a fairly clear accept, but the presentation of the ideas and work therein could be improved. I will definitely raise my score if the writing is improved."
  },
  {
    "people": [
      "Andrychowicz",
      "Samy Bengio",
      "S. Bengio",
      "Y. Bengio",
      "J. Cloutier",
      "Schmidhuber"
    ],
    "review": "This work presents an LSTM based meta-learning framework to learn the optimization algorithm of a another learning algorithm (here a NN).\nThe paper is globally well written and the presentation of the main material is clear. The crux of the paper: drawing the parallel between Robbins Monroe update rule and the LSTM update rule and exploit it to satisfy the two main desiderata of few shot learning (1- quick acquisition of new knowledge, 2- slower extraction of general transferable knowledge) is intriguing. \n\nSeveral tricks re-used from (Andrychowicz et al. 2016)  such as parameter sharing and normalization, and novel design choices (specific implementation of batch normalization) are well  motivated. \nThe experiments are convincing. This is a strong paper. My only concerns/questions are the following:\n\n1. Can it be redundant to use the loss, gradient and parameters as input to the meta-learner? Did you do ablative studies to make sure simpler combinations are not enough.\n2. It would be great if other architectural components of the network can be learned in a similar fashion (number of neurons, type of units, etc.). Do you have an opinion about this?\n3. The related work section (mainly focused on meta learning) is a bit shallow. Meta-learning is a rather old topic and similar approaches have been tried to solve the same problem even if they were not using LSTMs:\n     - Samy Bengio PhD thesis (1989) is all about this ;-)\n     - Use of genetic programming for the search of a new learning rule for neural networks (S. Bengio, Y. Bengio, and J. Cloutier. 1994)\n     - I am convince Schmidhuber has done something, make sure you find it and update related work section.  \n\nOverall, I like the paper. I believe the discussed material is relevant to a wide audience at ICLR.  \n"
  },
  {
    "people": [
      "Vinyals"
    ],
    "review": "In light of the authors' responsiveness and the updates to the manuscript -- in particular to clarify the meta-learning task -- I am updating my score to an 8.\n\n-----\n\nThis manuscript proposes to tackle few-shot learning with neural networks by leveraging meta-learning, a classic idea that has seen a renaissance in the last 12 months. The authors formulate few-shot learning as a sequential meta-learning problem: each \"example\" includes a sequence of batches of \"training\" pairs, followed by a final \"test\" batch. The inputs at each \"step\" include the outputs of a \"base learner\" (e.g., training loss and gradients), as well as the base learner's current state (parameters). The paper applies an LSTM to this meta-learning problem, using the inner memory cells in the *second* layer to directly model the updated parameters of the base learner. In doing this, they note similarities between the respective update rules of LSTM memory cells and gradient descent. Updates to the LSTM meta-learner are computed based on the base learner's prediction loss for the final \"test\" batch. The authors make several simplifying assumptions, such as sharing weights across all second layer cells (analogous to using the same learning rate for all parameters). The paper recreates the Mini-ImageNet data set proposed in Vinyals et al 2016, and shows that the meta-learner LSTM is competitive with the current state-of-the-art (Matchin Networks, Vinyals 2016) on 1- and 5-shot learning.\n\nStrengths:\n- It is intriguing -- and in hindsight, natural -- to cast the few-shot learning problem as a sequential (meta-)learning problem. While the authors did not originate the general idea of persisting learning across a series of learning problems, I think it is fair to say that they have advanced the state of the art, though I cannot confidently assert its novelty as I am not deeply familiar with recent work on meta-learning.\n- The proposed approach is competitive with and outperforms Vinyals 2016 in 1-shot and 5-shot Mini-ImageNet experiments.\n- The base learner in this setting (simple ConvNet classifier) is quite different from the nearest-neighbor-on-top-of-learned-embedding approach used in Vinyals 2016. It is always exciting when state-of-the-art results can be reported using very different approaches, rather than incremental follow-up work.\n- As far as I know, the insight about the relationship between the memory cell and gradient descent updates is novel here. It is interesting regardless.\n- The paper offers several practical insights about how to design and train an LSTM meta-learner, which should make it easier for others to replicate this work and apply these ideas to new problems. These include proper initialization, weight sharing across coordinates, and the importance of normalizing/rescaling the loss, gradient, and parameter inputs. Some of the insights have been previously described (the importance of simulating test conditions during meta-training; assuming independence between meta-learner and base learner parameters when taking gradients with respect to the meta-learner parameters), but the discussion here is useful nonetheless.\n\nWeaknesses:\n- The writing is at times quite opaque. While it describes very interesting work, I would not call the paper an enjoyable read. It took me multiple passes (as well as consulting related work) to understand the general learning problem. The task description in Section 2 (Page 2) is very abstract and uses notation and language that is not common outside of this sub-area. The paper could benefit from a brief concrete example (based on MNIST is fine), perhaps paired with a diagram illustrating a sequence of few-shot learning tasks. This would definitely make it accessible to a wider audience.\n- Following up on that note, the precise nature of the N-class, few-shot learning problem here is unclear to me. Specifically, the Mini-ImageNet data set has 100 labels, of which 64/16/20 are used during meta-training/validation/testing. Does this mean that only 64/100 classes are observed through meta-training? Or does it mean that only 64/100 are observed in each batch, but on average all 100 are observed during meta-training? If it's the former, how many outputs does the softmax layer of the ConvNet base learner have during meta-training? 64 (only those observed in training) or 100 (of which 36 are never observed)? Many other details like these are unclear (see question).\n- The plots in Figure 2 are pretty uninformative in and of themselves, and the discussion section offers very little insight around them.\n\nThis is an interesting paper with convincing results. It seems like a fairly clear accept, but the presentation of the ideas and work therein could be improved. I will definitely raise my score if the writing is improved."
  },
  {
    "people": [
      "Lopez-Paz",
      "Lopez"
    ],
    "review": "The present submission discusses a \"causal regularizer\", which promotes the use of causal dependencies (X -> Y, where X is a feature of the learning problem, and Y is the target variable) in predictive models. Similarly, such causal regularizer penalizes the use of non-causal dependencies, which can arise due to reverse causation (Y -> X) or confounding (X <- Z -> Y, where Z is a hidden confounder).\n\n+ Overall, this submission tackles one of the most important problems in machine learning, which is to build causal models. The paper discusses and addresses this issue effectively when applied to a dataset in heart disease. In their experiments, the authors correctly identify some of the common causes of heart disease by virtue of their causal regularizer.\n\n- The authors do not discuss the robustness of their approach with respect to choice of hyper-parameters (both describing the neural network architecture and the generative model that synthesizes artificial causal data). This seems like a crucial issue, in particular when dealing with medical data.\n\n- The conclusions of the experimental evaluation should be discussed in greater length. On the one hand, Figure 4.a shows that there are no differences between L1 and causal regularization in terms of predictive performance, but it is difficult to conclude if this result is statistically significant without access to error-bars. On the other hand, Table 3 describes the qualitative differences between L1 and causal regularization. However, this table is hard to read: How were the 30 rows selected? What does the red highlighting mean? Are these red rows some true causal features that were missed? If so, this is related to precision. What about recall? Did the causal regularization pick up many non-causal features as causal?\n\n- Regarding causal classifiers, this paper should do a much better job at reviewing previous work. For instance, the paper \"Towards a Learning Theory of Cause-Effect Inference\" from Lopez-Paz et al. is missing from the references. However, this prior work studies many of the aspects that are hinted as novel in this submission. In particular, the prior work of Lopez-Paz 1) introduces the concept of Mother distribution (referred as Nature hyper-prior in this submission) which explicitly factorizes the distribution over causes and mechanisms, 2) circumvented intractable likelihoods by synthesizing and training on causal data, 3) tackled the confounding case (compare Figure 1 of this submission and Appendix C of Lopez-Paz), and 4) dealt with discrete data seamlessly (such as the ChaLearn data from Section 5.3 in Lopez-Paz).\n\nOn a positive note, this is a well-written paper that addresses the important, under-appreciated problem of incorporating causal reasoning into machine learning. On a negative note, the novelty of the technical contributions is modest and the qualitative evaluation of the results could be greatly extended. In short, I am leaning slightly towards acceptance."
  },
  {
    "people": [
      "Robert Chen",
      "David Lopez-Paz"
    ],
    "review": "We made a major revision to the paper.  Here is the summary of the main changes:\n\nExperiments:\n-- Providing **ground truth causality** evaluation (by hiring Robert Chen, a clinical expert). (Figure 1, Table 2) The results show a significant gain in causality discovery using the proposed method.\n-- Including a new **publicly available dataset** (MIMIC III) to ensure reproducibility of the results. (Table 1 and Figure 4)\n-- Adding standard deviation to the results.\n\nNovel methodology:\n-- Providing a neural network architecture that allows multivariate causal hypothesis generation, see Section 2.3 and Figure 2.b.\n-- Providing the ground truth causality evaluation for the causal hypotheses generated by our algorithm. (Figure 5b)\n\nWriteup:\n-- Adding more discussion on the related works (Sections 2.1 and 2.2). \n---- In the introduction and Section 2.2 we clearly describe the novelty of this paper with respect to the papers listed by David Lopez-Paz and the reviewers.\n-- Revising the background section 2.1 and making it both more accessible and more mathematically precise.\n-- Adding new visualizations in Figure 2 and removing diagrams that didn\u2019t help delivering the concepts.\n-- Restructuring the methodology section. Now Section 2.2 only discusses the proposed causal regularizer and Section 2.3 describes its combination with neural networks.\n-- Moving the experimental results on the causality detection (and other extra results) to Appendix A.1, to allow space for evaluation of the main results of the paper.\n-- Finally, we revised the introduction accordingly.\n"
  },
  {
    "people": [
      "Chalupka"
    ],
    "review": "The authors extend their method of causal discovery (Chalupka et al 2016) to include assumptions about sparsity via regularization.  They apply this extension to an interesting private dataset from Sutter Health.  While an interesting direction, I found the presentation somewhat confused, the methodological novelty smaller than the bulk of ICLR works, and the central results (or perhaps data; see below) inadequate to address questions of causality.\n\nFirst, I found the presentation somewhat unclear.  The paper at some points seems to be entirely focused on healthcare data, at other points it uses it as a motivating example, and at other points it is neglected.  Also, algorithm 1 seems unreferenced, and I'm not entirely sure why it is needed.  Figure 2 is not needed for this community.  The key methodological advance in this work appears in section 2.1 (Causal regularizer), but it is introduced amidst toy examples and without clear terminology or standard methodological assumptions/build-up.  In Section 3.1 (bottom of first paragraph), key data and results seem to be relegated to the appendices.  Thus overall the paper read rather haphazardly.  Finally, there seems to be an assumption throughout of fairly intimate familiarity with the Cholupka preprint, which i think should be avoided.  This paper should stand alone.\n\nSecond, while the technical contributions/novelty are not a focus of the paper's presentation, I am concerned by the lack of methodological advance.  Essentially a regularization objective is added to the previous method, which of itself is not a bad idea, but I can't point to a technical novelty in the paper that the community can not do without.\n\nThird, fundamentally i don't see how the experiments address the central question of causality; they show regularization behaving as expected (or rather, influencing weights as expected), but I don't think we really have any meaningful quantitative evidence that causality has been learned.  This was briefly discussed (see \"ground truth causality?\" and the response below).  I appreciate the technical challenges/impossibility of having such a dataset, but if that's the case, then I think this work is premature, since there is no way to really validate.\n\nOverall it's clearly a sincere effort, but I found it wanting in terms of a few critical areas."
  },
  {
    "people": [
      "Lopez-Paz",
      "Lopez"
    ],
    "review": "The present submission discusses a \"causal regularizer\", which promotes the use of causal dependencies (X -> Y, where X is a feature of the learning problem, and Y is the target variable) in predictive models. Similarly, such causal regularizer penalizes the use of non-causal dependencies, which can arise due to reverse causation (Y -> X) or confounding (X <- Z -> Y, where Z is a hidden confounder).\n\n+ Overall, this submission tackles one of the most important problems in machine learning, which is to build causal models. The paper discusses and addresses this issue effectively when applied to a dataset in heart disease. In their experiments, the authors correctly identify some of the common causes of heart disease by virtue of their causal regularizer.\n\n- The authors do not discuss the robustness of their approach with respect to choice of hyper-parameters (both describing the neural network architecture and the generative model that synthesizes artificial causal data). This seems like a crucial issue, in particular when dealing with medical data.\n\n- The conclusions of the experimental evaluation should be discussed in greater length. On the one hand, Figure 4.a shows that there are no differences between L1 and causal regularization in terms of predictive performance, but it is difficult to conclude if this result is statistically significant without access to error-bars. On the other hand, Table 3 describes the qualitative differences between L1 and causal regularization. However, this table is hard to read: How were the 30 rows selected? What does the red highlighting mean? Are these red rows some true causal features that were missed? If so, this is related to precision. What about recall? Did the causal regularization pick up many non-causal features as causal?\n\n- Regarding causal classifiers, this paper should do a much better job at reviewing previous work. For instance, the paper \"Towards a Learning Theory of Cause-Effect Inference\" from Lopez-Paz et al. is missing from the references. However, this prior work studies many of the aspects that are hinted as novel in this submission. In particular, the prior work of Lopez-Paz 1) introduces the concept of Mother distribution (referred as Nature hyper-prior in this submission) which explicitly factorizes the distribution over causes and mechanisms, 2) circumvented intractable likelihoods by synthesizing and training on causal data, 3) tackled the confounding case (compare Figure 1 of this submission and Appendix C of Lopez-Paz), and 4) dealt with discrete data seamlessly (such as the ChaLearn data from Section 5.3 in Lopez-Paz).\n\nOn a positive note, this is a well-written paper that addresses the important, under-appreciated problem of incorporating causal reasoning into machine learning. On a negative note, the novelty of the technical contributions is modest and the qualitative evaluation of the results could be greatly extended. In short, I am leaning slightly towards acceptance."
  },
  {
    "people": [
      "Lopez-Paz",
      "Lopez"
    ],
    "review": "The present submission discusses a \"causal regularizer\", which promotes the use of causal dependencies (X -> Y, where X is a feature of the learning problem, and Y is the target variable) in predictive models. Similarly, such causal regularizer penalizes the use of non-causal dependencies, which can arise due to reverse causation (Y -> X) or confounding (X <- Z -> Y, where Z is a hidden confounder).\n\n+ Overall, this submission tackles one of the most important problems in machine learning, which is to build causal models. The paper discusses and addresses this issue effectively when applied to a dataset in heart disease. In their experiments, the authors correctly identify some of the common causes of heart disease by virtue of their causal regularizer.\n\n- The authors do not discuss the robustness of their approach with respect to choice of hyper-parameters (both describing the neural network architecture and the generative model that synthesizes artificial causal data). This seems like a crucial issue, in particular when dealing with medical data.\n\n- The conclusions of the experimental evaluation should be discussed in greater length. On the one hand, Figure 4.a shows that there are no differences between L1 and causal regularization in terms of predictive performance, but it is difficult to conclude if this result is statistically significant without access to error-bars. On the other hand, Table 3 describes the qualitative differences between L1 and causal regularization. However, this table is hard to read: How were the 30 rows selected? What does the red highlighting mean? Are these red rows some true causal features that were missed? If so, this is related to precision. What about recall? Did the causal regularization pick up many non-causal features as causal?\n\n- Regarding causal classifiers, this paper should do a much better job at reviewing previous work. For instance, the paper \"Towards a Learning Theory of Cause-Effect Inference\" from Lopez-Paz et al. is missing from the references. However, this prior work studies many of the aspects that are hinted as novel in this submission. In particular, the prior work of Lopez-Paz 1) introduces the concept of Mother distribution (referred as Nature hyper-prior in this submission) which explicitly factorizes the distribution over causes and mechanisms, 2) circumvented intractable likelihoods by synthesizing and training on causal data, 3) tackled the confounding case (compare Figure 1 of this submission and Appendix C of Lopez-Paz), and 4) dealt with discrete data seamlessly (such as the ChaLearn data from Section 5.3 in Lopez-Paz).\n\nOn a positive note, this is a well-written paper that addresses the important, under-appreciated problem of incorporating causal reasoning into machine learning. On a negative note, the novelty of the technical contributions is modest and the qualitative evaluation of the results could be greatly extended. In short, I am leaning slightly towards acceptance."
  },
  {
    "people": [
      "Robert Chen",
      "David Lopez-Paz"
    ],
    "review": "We made a major revision to the paper.  Here is the summary of the main changes:\n\nExperiments:\n-- Providing **ground truth causality** evaluation (by hiring Robert Chen, a clinical expert). (Figure 1, Table 2) The results show a significant gain in causality discovery using the proposed method.\n-- Including a new **publicly available dataset** (MIMIC III) to ensure reproducibility of the results. (Table 1 and Figure 4)\n-- Adding standard deviation to the results.\n\nNovel methodology:\n-- Providing a neural network architecture that allows multivariate causal hypothesis generation, see Section 2.3 and Figure 2.b.\n-- Providing the ground truth causality evaluation for the causal hypotheses generated by our algorithm. (Figure 5b)\n\nWriteup:\n-- Adding more discussion on the related works (Sections 2.1 and 2.2). \n---- In the introduction and Section 2.2 we clearly describe the novelty of this paper with respect to the papers listed by David Lopez-Paz and the reviewers.\n-- Revising the background section 2.1 and making it both more accessible and more mathematically precise.\n-- Adding new visualizations in Figure 2 and removing diagrams that didn\u2019t help delivering the concepts.\n-- Restructuring the methodology section. Now Section 2.2 only discusses the proposed causal regularizer and Section 2.3 describes its combination with neural networks.\n-- Moving the experimental results on the causality detection (and other extra results) to Appendix A.1, to allow space for evaluation of the main results of the paper.\n-- Finally, we revised the introduction accordingly.\n"
  },
  {
    "people": [
      "Chalupka"
    ],
    "review": "The authors extend their method of causal discovery (Chalupka et al 2016) to include assumptions about sparsity via regularization.  They apply this extension to an interesting private dataset from Sutter Health.  While an interesting direction, I found the presentation somewhat confused, the methodological novelty smaller than the bulk of ICLR works, and the central results (or perhaps data; see below) inadequate to address questions of causality.\n\nFirst, I found the presentation somewhat unclear.  The paper at some points seems to be entirely focused on healthcare data, at other points it uses it as a motivating example, and at other points it is neglected.  Also, algorithm 1 seems unreferenced, and I'm not entirely sure why it is needed.  Figure 2 is not needed for this community.  The key methodological advance in this work appears in section 2.1 (Causal regularizer), but it is introduced amidst toy examples and without clear terminology or standard methodological assumptions/build-up.  In Section 3.1 (bottom of first paragraph), key data and results seem to be relegated to the appendices.  Thus overall the paper read rather haphazardly.  Finally, there seems to be an assumption throughout of fairly intimate familiarity with the Cholupka preprint, which i think should be avoided.  This paper should stand alone.\n\nSecond, while the technical contributions/novelty are not a focus of the paper's presentation, I am concerned by the lack of methodological advance.  Essentially a regularization objective is added to the previous method, which of itself is not a bad idea, but I can't point to a technical novelty in the paper that the community can not do without.\n\nThird, fundamentally i don't see how the experiments address the central question of causality; they show regularization behaving as expected (or rather, influencing weights as expected), but I don't think we really have any meaningful quantitative evidence that causality has been learned.  This was briefly discussed (see \"ground truth causality?\" and the response below).  I appreciate the technical challenges/impossibility of having such a dataset, but if that's the case, then I think this work is premature, since there is no way to really validate.\n\nOverall it's clearly a sincere effort, but I found it wanting in terms of a few critical areas."
  },
  {
    "people": [
      "Lopez-Paz",
      "Lopez"
    ],
    "review": "The present submission discusses a \"causal regularizer\", which promotes the use of causal dependencies (X -> Y, where X is a feature of the learning problem, and Y is the target variable) in predictive models. Similarly, such causal regularizer penalizes the use of non-causal dependencies, which can arise due to reverse causation (Y -> X) or confounding (X <- Z -> Y, where Z is a hidden confounder).\n\n+ Overall, this submission tackles one of the most important problems in machine learning, which is to build causal models. The paper discusses and addresses this issue effectively when applied to a dataset in heart disease. In their experiments, the authors correctly identify some of the common causes of heart disease by virtue of their causal regularizer.\n\n- The authors do not discuss the robustness of their approach with respect to choice of hyper-parameters (both describing the neural network architecture and the generative model that synthesizes artificial causal data). This seems like a crucial issue, in particular when dealing with medical data.\n\n- The conclusions of the experimental evaluation should be discussed in greater length. On the one hand, Figure 4.a shows that there are no differences between L1 and causal regularization in terms of predictive performance, but it is difficult to conclude if this result is statistically significant without access to error-bars. On the other hand, Table 3 describes the qualitative differences between L1 and causal regularization. However, this table is hard to read: How were the 30 rows selected? What does the red highlighting mean? Are these red rows some true causal features that were missed? If so, this is related to precision. What about recall? Did the causal regularization pick up many non-causal features as causal?\n\n- Regarding causal classifiers, this paper should do a much better job at reviewing previous work. For instance, the paper \"Towards a Learning Theory of Cause-Effect Inference\" from Lopez-Paz et al. is missing from the references. However, this prior work studies many of the aspects that are hinted as novel in this submission. In particular, the prior work of Lopez-Paz 1) introduces the concept of Mother distribution (referred as Nature hyper-prior in this submission) which explicitly factorizes the distribution over causes and mechanisms, 2) circumvented intractable likelihoods by synthesizing and training on causal data, 3) tackled the confounding case (compare Figure 1 of this submission and Appendix C of Lopez-Paz), and 4) dealt with discrete data seamlessly (such as the ChaLearn data from Section 5.3 in Lopez-Paz).\n\nOn a positive note, this is a well-written paper that addresses the important, under-appreciated problem of incorporating causal reasoning into machine learning. On a negative note, the novelty of the technical contributions is modest and the qualitative evaluation of the results could be greatly extended. In short, I am leaning slightly towards acceptance."
  },
  {
    "people": [
      "Elena",
      "Flavian",
      "Thomas",
      "McAuley",
      "McAuley"
    ],
    "review": "Dear reviewers, \n \nthanks for your detailed reviews. We will use them to improve our paper. We tried to gather them in different categories in oder to address them properly. \nFeel free to come back to us if you have some remaining questions.\n \nElena, Flavian & Thomas\n \nCommon themes:\n     \u2022 We do not do end-to-end learning (I.1. & III.1)\n        \u25e6 TL;DR: A modular architecture is the easiest way to put in production such a complex model - efficient way of computational resources (as also ack by the first reviewer)\n           \u25aa the fact that we specialize each modality-specific network to the final task independently allows us to scale better by reducing the parameter space and the resulting computation time, gives us better interpretability and insight into the relative value of each modality (as raised by reviewer #2) and allows us to better control any convergence issues in a production environment.\n           \u25aa the fact that during specialization we do not backprop through the full ImageCNN network is based on previous findings (see 'Learning visual clothing style with heterogeneous dyadic co-occurences\u2019 for instance)  that show that the fc7 layer provides a good cross-task representation for images. For the other two networks we do backprop all the way down.\n         \u25e6 Using pretrained models for each modality allows us to leverage external source of data and do transfer learning, whose value was repeatedly confirmed in the literature such as 'How transferable are features in deep NN?' for instance. \n         \u25e6 We agree that in the limit given enough data and computational resources the models can be learnt from scratch and in a joint manner, but our concern is on delivering a simple solution that can be delivered in production setting and at very large scale.\n     \u2022 ResNet motivation (I.2. & II.2)\n         \u25e6 We decided to use \u201cresidual\u201d in describing our new pairwise embedding layer based on the similarity in motivation with the original residual unit that was introduced to help the system approximate the identity and allows the new layers to focus on the remaining unexplained error. We agree that in our case the layer does not serve the same practical purpose as in the original ResNets architectures that use the unit mostly for training very deep networks.\n     \u2022 Missing baselines\n         \u25e6 the state of the art on this dataset (I.3.b & III.3.)\n            \u25aa ImageCNN is the McAuley et al solution that introduced the dataset (He used GoogLeNet and we used AlexNet)\n            \u25aa VBPR is another baseline introduced by He & McAuley. It uses reviews as implicit feedback and try to predict the interest of a user based on products images and the sequence of products he was interested in. Our approach focuses more on a item to item similarity matrix and that is why we have not implemented this approach.   \n         \u25e6 jointly fine-tuned model (I.1)\n            \u25aa We agree that we could respecialize the networks jointly and its on our list of remaining experiments to try \n         \u25e6 additional fully-connected layer that mixes the modality specific embeddings to form a final embedding (I.2)\n            \u25aa We agree that having an additional full connected layer that compress into a lower dimension vector could be a good idea. However we did not manage to make it work. We\u2019ll try different initialisations in the next version of the paper.\n \nOther concerns:\n   \u2022 TextCNN setup details (I.3.a.)\n         \u25e6 As per not providing more info on training the TextCNN, we choose to not add more info as our paper was running long as it was. For more specific info, please let us know what details are you interested in.\n         \u25e6 Concerning the choice of keeping only the first 10 words, we agree that we could have kept more, however a lot of the descriptions were empty so we did not feel we need to make them longer. However, the architecture allows to choose an arbitrary length for the text input. \n    \u2022 The paper should clearly motivate and show how different modalities contribute to the final task. (II.1.)\n         \u25e6 We agree and this is why we reported separately the performance of each modality network in Tables 1 & 2\n         \u25e6 Something we did not mention is that in Content2Vec-linear model the image and text modalities have very similar coefficients \n    \u2022 The choice of the term embedding for the dot product of two items is not usual.  Here it refers to the final output, and renders the output layer in Figure 2 pointless. (II.3.a.)\n         \u25e6 In the Content2Vec-crossfeat and Content2Vec-embedpairs we do have a pair embedding layer (one using polynomial features, one using a real embedding representation) so this is why we differentiate  between the Output layer (the softmax) and the Pairwise layer in Figure 2.\n    \u2022 This architecture makes it hard to say how easily the model would generalize to arbitrary feature types, say e.g. if I had audio or video features describing the item. (III.2.)\n         \u25e6 The modular design assumes that the input modalities do have extra modeling value for the pairwise similarity prediction. To your point, if interested in song similarities, if shown that text and audio signals do separately predict video similarities (as shown in "
  },
  {
    "people": [
      "Julian"
    ],
    "review": "The problem of utilizing all available information (across modalities) about a product to learn a meaningful \"joint\" embedding is an interesting one, and certainly seems like it a promising direction for improving recommender systems, especially in the \"cold start\" scenario. I'm unaware of approaches combining as many modalities as proposed in this paper, so an effective solution could indeed be significant. However, there are many aspects of the proposed architecture that seem sub-optimal to me:\n\n1. A major benefit of neural-network based systems is that the entire system can be trained end-to-end, jointly. The proposed approach sticks together largely pre-trained modules for different modalities... this can be justifiable when there is very little training data available on which to train jointly. With 10M product pairs, however, this doesn't seem to be the case for the Amazon dataset (although I haven't worked with this dataset myself so perhaps I'm missing something... either way it's not discussed at all in the paper). I consider the lack of a jointly fine-tuned model a major shortcoming of the proposed approach.\n\n2. The discussion of \"pairwise residual units\" is confusing and not well-motivated. The residual formulation (if I understand it correctly) applies a ReLU layer to the concatenation of the modality specific embeddings, giving a new similarity (after dot products) that can be added to the similarity obtained from the concatenation directly. Why not just have an additional fully-connected layer that mixes the modality specific embeddings to form a final embedding (perhaps of lower dimensionality)? This should at least be presented as a baseline, if the pairwise residual unit is claimed as a contribution... I don't find the provided explanation convincing (in what way does the residual approach reduce parameter count?).\n\n3. More minor: The choice of TextCNN for the text embedding vectors seems fine (although I wonder how an LSTM-based approach would perform)... However the details surrounding how it is used are obscured in the paper. In response to a question, the authors mention that it runs on the concatenation of the first 10 words of the title and product description. Especially for the description, this seems insufficiently long to contain a lot of information to me.\n\nMore care could be given to motivating the choices made in the paper. Finally, I'm not familiar with state of the art on this dataset... do the comparisons accurately reflect it? It seems only one competing technique is presented, with none on the more challenging cold-start scenarios.\n\nMinor detail: In the second paragraph of page 3, there is a reference that just says (cite Julian)."
  },
  {
    "people": [
      "Furthemore",
      "Elena",
      "Flavian",
      "Thomas"
    ],
    "review": "Thanks for your questions. We shall try to adress them in the following:\n\nQuestion 1: There are many modules in the system that are trained separately. Furthemore, fine-tuning for e.g. the image model appears to only take place on the FC7 layer. Jointly training all the components of the model would presumably improve performance, perhaps considerably. Did you try this? If not, why not?\n\nNo we did not. We were primarily interested in knowing the additional performance one can gain by merging pretrained models and re-specializing them to the task. We agree that for the paper it would have been a worthy experiment but we did not get to it due to time constraints. We will try to add it in the next paper update.\n\nQuestion 2: For the joint product embedding, it was stated that a fully connected hidden layer that mixes the individual embeddings wasn't used in order to have a smaller number of parameters and increase comprehensibility. But presumably it could also perform better than the chosen approach. Did you try it?\n\nWe made some attempts but the convergence speed was bad. It is also on the to-do list for the next paper update.\n\nQuestion 3: Could you provide some more details on the way TextCNN was used? Was it used on full product descriptions, even when they consist of multiple sentences? Were titles and descriptions embedded separately?\n\nSure! We decided to not embed the titles and descriptions separately, but concatenate the two and keep the first ten words without stop words (with empty word padding for shorter sequences). This method can definitely be extended to the full description since text CNN are quite robust to the length of the sequence.\n\nQuestion 4:  In the table of results at the end, I didn't see any results for a model trained on both the books and movies data and tested on both in the cold start scenario. Did you evaluate this case?\n\nYes, we did try and as expected the results were not great, since the current version of the model does not support category-specific embeddings. Therefore, the resulting overall distance between products is a compromise between the two category-specific distances. If interested, we can provide you the results we achieved.\n\nQuestion 5: Earlier work on content-based recommendation focused on not just performance but also on the interpretability of the resulting models. Can you say anything about whether effective interpretations of the models latent dimensions can be gleaned when learning in this way?\n\nWe only explored the text interpretability by looking at the top activating word embeddings  for each of the text convolutions. What we saw is that most of the 100 unigram filters used in the convolution layer corresponds to a topic (could be clusters corresponding to a genre, a date or an author). It looks like while learning the right text representation for book pairwise similarity prediction the model discovers naturally occuring book topics. The paper was already long so we decide not to publish them, but if you are interested, we can provide them.\n\nQuestion 6: Is Prod2Vec realistic in cold-start scenarios? Presumably co-purchase information wouldn't be available for cold-start products and thus an embedding couldn't be estimated.\n\nThe reason why we keep Prod2vec only in Content2vec + is because prod2vec only works when you have available collaborative filtering signal on at least some of the products in test. That is why prod2vec and content2vec+ algorithm results are only reported in the two datasets with some CF signal.\n\nQuestion 7: I didn't understand why different sets of baselines seem to appear across the three tables, but maybe I missed some detail here.\n\nWe used the hard cold start dataset to do the extensive analysis of our architecture VS other available methods. We dropped crossfeat because we made the case in the hard cold start dataset.\n\nFeel free to comment if you have any other questions.\n\nElena, Flavian & Thomas"
  },
  {
    "people": [
      "Elena",
      "Flavian",
      "Thomas",
      "McAuley",
      "McAuley"
    ],
    "review": "Dear reviewers, \n \nthanks for your detailed reviews. We will use them to improve our paper. We tried to gather them in different categories in oder to address them properly. \nFeel free to come back to us if you have some remaining questions.\n \nElena, Flavian & Thomas\n \nCommon themes:\n     \u2022 We do not do end-to-end learning (I.1. & III.1)\n        \u25e6 TL;DR: A modular architecture is the easiest way to put in production such a complex model - efficient way of computational resources (as also ack by the first reviewer)\n           \u25aa the fact that we specialize each modality-specific network to the final task independently allows us to scale better by reducing the parameter space and the resulting computation time, gives us better interpretability and insight into the relative value of each modality (as raised by reviewer #2) and allows us to better control any convergence issues in a production environment.\n           \u25aa the fact that during specialization we do not backprop through the full ImageCNN network is based on previous findings (see 'Learning visual clothing style with heterogeneous dyadic co-occurences\u2019 for instance)  that show that the fc7 layer provides a good cross-task representation for images. For the other two networks we do backprop all the way down.\n         \u25e6 Using pretrained models for each modality allows us to leverage external source of data and do transfer learning, whose value was repeatedly confirmed in the literature such as 'How transferable are features in deep NN?' for instance. \n         \u25e6 We agree that in the limit given enough data and computational resources the models can be learnt from scratch and in a joint manner, but our concern is on delivering a simple solution that can be delivered in production setting and at very large scale.\n     \u2022 ResNet motivation (I.2. & II.2)\n         \u25e6 We decided to use \u201cresidual\u201d in describing our new pairwise embedding layer based on the similarity in motivation with the original residual unit that was introduced to help the system approximate the identity and allows the new layers to focus on the remaining unexplained error. We agree that in our case the layer does not serve the same practical purpose as in the original ResNets architectures that use the unit mostly for training very deep networks.\n     \u2022 Missing baselines\n         \u25e6 the state of the art on this dataset (I.3.b & III.3.)\n            \u25aa ImageCNN is the McAuley et al solution that introduced the dataset (He used GoogLeNet and we used AlexNet)\n            \u25aa VBPR is another baseline introduced by He & McAuley. It uses reviews as implicit feedback and try to predict the interest of a user based on products images and the sequence of products he was interested in. Our approach focuses more on a item to item similarity matrix and that is why we have not implemented this approach.   \n         \u25e6 jointly fine-tuned model (I.1)\n            \u25aa We agree that we could respecialize the networks jointly and its on our list of remaining experiments to try \n         \u25e6 additional fully-connected layer that mixes the modality specific embeddings to form a final embedding (I.2)\n            \u25aa We agree that having an additional full connected layer that compress into a lower dimension vector could be a good idea. However we did not manage to make it work. We\u2019ll try different initialisations in the next version of the paper.\n \nOther concerns:\n   \u2022 TextCNN setup details (I.3.a.)\n         \u25e6 As per not providing more info on training the TextCNN, we choose to not add more info as our paper was running long as it was. For more specific info, please let us know what details are you interested in.\n         \u25e6 Concerning the choice of keeping only the first 10 words, we agree that we could have kept more, however a lot of the descriptions were empty so we did not feel we need to make them longer. However, the architecture allows to choose an arbitrary length for the text input. \n    \u2022 The paper should clearly motivate and show how different modalities contribute to the final task. (II.1.)\n         \u25e6 We agree and this is why we reported separately the performance of each modality network in Tables 1 & 2\n         \u25e6 Something we did not mention is that in Content2Vec-linear model the image and text modalities have very similar coefficients \n    \u2022 The choice of the term embedding for the dot product of two items is not usual.  Here it refers to the final output, and renders the output layer in Figure 2 pointless. (II.3.a.)\n         \u25e6 In the Content2Vec-crossfeat and Content2Vec-embedpairs we do have a pair embedding layer (one using polynomial features, one using a real embedding representation) so this is why we differentiate  between the Output layer (the softmax) and the Pairwise layer in Figure 2.\n    \u2022 This architecture makes it hard to say how easily the model would generalize to arbitrary feature types, say e.g. if I had audio or video features describing the item. (III.2.)\n         \u25e6 The modular design assumes that the input modalities do have extra modeling value for the pairwise similarity prediction. To your point, if interested in song similarities, if shown that text and audio signals do separately predict video similarities (as shown in "
  },
  {
    "people": [
      "Julian"
    ],
    "review": "The problem of utilizing all available information (across modalities) about a product to learn a meaningful \"joint\" embedding is an interesting one, and certainly seems like it a promising direction for improving recommender systems, especially in the \"cold start\" scenario. I'm unaware of approaches combining as many modalities as proposed in this paper, so an effective solution could indeed be significant. However, there are many aspects of the proposed architecture that seem sub-optimal to me:\n\n1. A major benefit of neural-network based systems is that the entire system can be trained end-to-end, jointly. The proposed approach sticks together largely pre-trained modules for different modalities... this can be justifiable when there is very little training data available on which to train jointly. With 10M product pairs, however, this doesn't seem to be the case for the Amazon dataset (although I haven't worked with this dataset myself so perhaps I'm missing something... either way it's not discussed at all in the paper). I consider the lack of a jointly fine-tuned model a major shortcoming of the proposed approach.\n\n2. The discussion of \"pairwise residual units\" is confusing and not well-motivated. The residual formulation (if I understand it correctly) applies a ReLU layer to the concatenation of the modality specific embeddings, giving a new similarity (after dot products) that can be added to the similarity obtained from the concatenation directly. Why not just have an additional fully-connected layer that mixes the modality specific embeddings to form a final embedding (perhaps of lower dimensionality)? This should at least be presented as a baseline, if the pairwise residual unit is claimed as a contribution... I don't find the provided explanation convincing (in what way does the residual approach reduce parameter count?).\n\n3. More minor: The choice of TextCNN for the text embedding vectors seems fine (although I wonder how an LSTM-based approach would perform)... However the details surrounding how it is used are obscured in the paper. In response to a question, the authors mention that it runs on the concatenation of the first 10 words of the title and product description. Especially for the description, this seems insufficiently long to contain a lot of information to me.\n\nMore care could be given to motivating the choices made in the paper. Finally, I'm not familiar with state of the art on this dataset... do the comparisons accurately reflect it? It seems only one competing technique is presented, with none on the more challenging cold-start scenarios.\n\nMinor detail: In the second paragraph of page 3, there is a reference that just says (cite Julian)."
  },
  {
    "people": [
      "Furthemore",
      "Elena",
      "Flavian",
      "Thomas"
    ],
    "review": "Thanks for your questions. We shall try to adress them in the following:\n\nQuestion 1: There are many modules in the system that are trained separately. Furthemore, fine-tuning for e.g. the image model appears to only take place on the FC7 layer. Jointly training all the components of the model would presumably improve performance, perhaps considerably. Did you try this? If not, why not?\n\nNo we did not. We were primarily interested in knowing the additional performance one can gain by merging pretrained models and re-specializing them to the task. We agree that for the paper it would have been a worthy experiment but we did not get to it due to time constraints. We will try to add it in the next paper update.\n\nQuestion 2: For the joint product embedding, it was stated that a fully connected hidden layer that mixes the individual embeddings wasn't used in order to have a smaller number of parameters and increase comprehensibility. But presumably it could also perform better than the chosen approach. Did you try it?\n\nWe made some attempts but the convergence speed was bad. It is also on the to-do list for the next paper update.\n\nQuestion 3: Could you provide some more details on the way TextCNN was used? Was it used on full product descriptions, even when they consist of multiple sentences? Were titles and descriptions embedded separately?\n\nSure! We decided to not embed the titles and descriptions separately, but concatenate the two and keep the first ten words without stop words (with empty word padding for shorter sequences). This method can definitely be extended to the full description since text CNN are quite robust to the length of the sequence.\n\nQuestion 4:  In the table of results at the end, I didn't see any results for a model trained on both the books and movies data and tested on both in the cold start scenario. Did you evaluate this case?\n\nYes, we did try and as expected the results were not great, since the current version of the model does not support category-specific embeddings. Therefore, the resulting overall distance between products is a compromise between the two category-specific distances. If interested, we can provide you the results we achieved.\n\nQuestion 5: Earlier work on content-based recommendation focused on not just performance but also on the interpretability of the resulting models. Can you say anything about whether effective interpretations of the models latent dimensions can be gleaned when learning in this way?\n\nWe only explored the text interpretability by looking at the top activating word embeddings  for each of the text convolutions. What we saw is that most of the 100 unigram filters used in the convolution layer corresponds to a topic (could be clusters corresponding to a genre, a date or an author). It looks like while learning the right text representation for book pairwise similarity prediction the model discovers naturally occuring book topics. The paper was already long so we decide not to publish them, but if you are interested, we can provide them.\n\nQuestion 6: Is Prod2Vec realistic in cold-start scenarios? Presumably co-purchase information wouldn't be available for cold-start products and thus an embedding couldn't be estimated.\n\nThe reason why we keep Prod2vec only in Content2vec + is because prod2vec only works when you have available collaborative filtering signal on at least some of the products in test. That is why prod2vec and content2vec+ algorithm results are only reported in the two datasets with some CF signal.\n\nQuestion 7: I didn't understand why different sets of baselines seem to appear across the three tables, but maybe I missed some detail here.\n\nWe used the hard cold start dataset to do the extensive analysis of our architecture VS other available methods. We dropped crossfeat because we made the case in the hard cold start dataset.\n\nFeel free to comment if you have any other questions.\n\nElena, Flavian & Thomas"
  },
  {
    "people": [
      "Rusu"
    ],
    "review": "This paper presents a new technique for adapting a neural network to a new task for which there is not a lot of training data. The most widely used current technique is that of fine-tuning. The idea in this paper is to instead learn a network that learns features that are complementary to the fixed network. Additionally, the authors consider the setting where the new network/features are \u201cstitched\u201d to the old one at various levels in the hieararchy, rather that it just being a parallel \u201ctower\u201d. \n\nThis work is similar in spirit (if not in some details) to the Progressive Nets paper by Rusu et al, as already discussed. The motivations and experiments are certainly different so this submission has merit on its own.\n\nThe idea of learning a \u201cresidual\u201d with the stitched connnections is very similar in spirit to the ResNet work. It would be nice to compare and contrast those approaches.\n\nI\u2019ve never seen a batch being used 5 times in a row during training, does this work better than just regular SGD?\n\nIn Figure 5 it\u2019d be nice to label the y-axis. That Figure would also benefit from not being a bar chart, but simply emulating Figure 4, which is much more readable!\n\nFigure 5 again: what is an untrained model? It\u2019s not immediately obvious why this is a good idea at all. Is TFT-1 simply fine-tuning one more layer than \u201cRetrain Softmax\u201d?\n\nI think that the results at the end of section 3 are a bit weak because of usage of a big network. I would definitely like to see how the results change if using a smaller net.\n\nThe authors claim throughout the paper that the purpose of the added connections and layers is to learn *complementary* features and they show this with some figures. The latter are a convinving evidence, but not proof or guarantee that this is what is actually happening. I suggest the authors consider adding an explicit constraint in their loss that encourages that, e.g. by having a soft orthogonality constraing (assuming one can project intermediate features to some common feature dimensionality). The usage of very small L2 regularization maybe achieves the same thing, but there\u2019s no evidence for that in the paper (in that we don\u2019t have any visualizations of what happens if there\u2019s no L2 reg.).\n\nOne of the big questions for me while reading the paper was how would an ensemble of 2 pre-trained nets would do on the tasks that the authors consider. This is especially relevant in the cars classification example, where I suspect that a strong baseline is that of fine-tuning VGG on this task, fine-tuning resnet on this task, and possibly training a linear combination of the two outputs or just averaging them naively.\n\nDisappointing that there are no results in figure 4, 5 and 8 except the ones from this paper. It\u2019s really hard to situate this paper if we don\u2019t actually know how it compares to previously published results.\n\n\nIn general, this was an interesting and potentially useful piece of work. The problem of efficiently reusing the previously trained classifier for retraining on a small set is certainly interesting to the community. While I think that this paper takes a good step in the right direction, it falls a bit short in some dimensions (comparisons with more serious baselines, more understanding etc)."
  },
  {
    "people": [
      "Rusu"
    ],
    "review": "This paper presents a new technique for adapting a neural network to a new task for which there is not a lot of training data. The most widely used current technique is that of fine-tuning. The idea in this paper is to instead learn a network that learns features that are complementary to the fixed network. Additionally, the authors consider the setting where the new network/features are \u201cstitched\u201d to the old one at various levels in the hieararchy, rather that it just being a parallel \u201ctower\u201d. \n\nThis work is similar in spirit (if not in some details) to the Progressive Nets paper by Rusu et al, as already discussed. The motivations and experiments are certainly different so this submission has merit on its own.\n\nThe idea of learning a \u201cresidual\u201d with the stitched connnections is very similar in spirit to the ResNet work. It would be nice to compare and contrast those approaches.\n\nI\u2019ve never seen a batch being used 5 times in a row during training, does this work better than just regular SGD?\n\nIn Figure 5 it\u2019d be nice to label the y-axis. That Figure would also benefit from not being a bar chart, but simply emulating Figure 4, which is much more readable!\n\nFigure 5 again: what is an untrained model? It\u2019s not immediately obvious why this is a good idea at all. Is TFT-1 simply fine-tuning one more layer than \u201cRetrain Softmax\u201d?\n\nI think that the results at the end of section 3 are a bit weak because of usage of a big network. I would definitely like to see how the results change if using a smaller net.\n\nThe authors claim throughout the paper that the purpose of the added connections and layers is to learn *complementary* features and they show this with some figures. The latter are a convinving evidence, but not proof or guarantee that this is what is actually happening. I suggest the authors consider adding an explicit constraint in their loss that encourages that, e.g. by having a soft orthogonality constraing (assuming one can project intermediate features to some common feature dimensionality). The usage of very small L2 regularization maybe achieves the same thing, but there\u2019s no evidence for that in the paper (in that we don\u2019t have any visualizations of what happens if there\u2019s no L2 reg.).\n\nOne of the big questions for me while reading the paper was how would an ensemble of 2 pre-trained nets would do on the tasks that the authors consider. This is especially relevant in the cars classification example, where I suspect that a strong baseline is that of fine-tuning VGG on this task, fine-tuning resnet on this task, and possibly training a linear combination of the two outputs or just averaging them naively.\n\nDisappointing that there are no results in figure 4, 5 and 8 except the ones from this paper. It\u2019s really hard to situate this paper if we don\u2019t actually know how it compares to previously published results.\n\n\nIn general, this was an interesting and potentially useful piece of work. The problem of efficiently reusing the previously trained classifier for retraining on a small set is certainly interesting to the community. While I think that this paper takes a good step in the right direction, it falls a bit short in some dimensions (comparisons with more serious baselines, more understanding etc).\n\n\n"
  },
  {
    "people": [
      "Rusu"
    ],
    "review": "This paper presents a new technique for adapting a neural network to a new task for which there is not a lot of training data. The most widely used current technique is that of fine-tuning. The idea in this paper is to instead learn a network that learns features that are complementary to the fixed network. Additionally, the authors consider the setting where the new network/features are \u201cstitched\u201d to the old one at various levels in the hieararchy, rather that it just being a parallel \u201ctower\u201d. \n\nThis work is similar in spirit (if not in some details) to the Progressive Nets paper by Rusu et al, as already discussed. The motivations and experiments are certainly different so this submission has merit on its own.\n\nThe idea of learning a \u201cresidual\u201d with the stitched connnections is very similar in spirit to the ResNet work. It would be nice to compare and contrast those approaches.\n\nI\u2019ve never seen a batch being used 5 times in a row during training, does this work better than just regular SGD?\n\nIn Figure 5 it\u2019d be nice to label the y-axis. That Figure would also benefit from not being a bar chart, but simply emulating Figure 4, which is much more readable!\n\nFigure 5 again: what is an untrained model? It\u2019s not immediately obvious why this is a good idea at all. Is TFT-1 simply fine-tuning one more layer than \u201cRetrain Softmax\u201d?\n\nI think that the results at the end of section 3 are a bit weak because of usage of a big network. I would definitely like to see how the results change if using a smaller net.\n\nThe authors claim throughout the paper that the purpose of the added connections and layers is to learn *complementary* features and they show this with some figures. The latter are a convinving evidence, but not proof or guarantee that this is what is actually happening. I suggest the authors consider adding an explicit constraint in their loss that encourages that, e.g. by having a soft orthogonality constraing (assuming one can project intermediate features to some common feature dimensionality). The usage of very small L2 regularization maybe achieves the same thing, but there\u2019s no evidence for that in the paper (in that we don\u2019t have any visualizations of what happens if there\u2019s no L2 reg.).\n\nOne of the big questions for me while reading the paper was how would an ensemble of 2 pre-trained nets would do on the tasks that the authors consider. This is especially relevant in the cars classification example, where I suspect that a strong baseline is that of fine-tuning VGG on this task, fine-tuning resnet on this task, and possibly training a linear combination of the two outputs or just averaging them naively.\n\nDisappointing that there are no results in figure 4, 5 and 8 except the ones from this paper. It\u2019s really hard to situate this paper if we don\u2019t actually know how it compares to previously published results.\n\n\nIn general, this was an interesting and potentially useful piece of work. The problem of efficiently reusing the previously trained classifier for retraining on a small set is certainly interesting to the community. While I think that this paper takes a good step in the right direction, it falls a bit short in some dimensions (comparisons with more serious baselines, more understanding etc)."
  },
  {
    "people": [
      "Rusu"
    ],
    "review": "This paper presents a new technique for adapting a neural network to a new task for which there is not a lot of training data. The most widely used current technique is that of fine-tuning. The idea in this paper is to instead learn a network that learns features that are complementary to the fixed network. Additionally, the authors consider the setting where the new network/features are \u201cstitched\u201d to the old one at various levels in the hieararchy, rather that it just being a parallel \u201ctower\u201d. \n\nThis work is similar in spirit (if not in some details) to the Progressive Nets paper by Rusu et al, as already discussed. The motivations and experiments are certainly different so this submission has merit on its own.\n\nThe idea of learning a \u201cresidual\u201d with the stitched connnections is very similar in spirit to the ResNet work. It would be nice to compare and contrast those approaches.\n\nI\u2019ve never seen a batch being used 5 times in a row during training, does this work better than just regular SGD?\n\nIn Figure 5 it\u2019d be nice to label the y-axis. That Figure would also benefit from not being a bar chart, but simply emulating Figure 4, which is much more readable!\n\nFigure 5 again: what is an untrained model? It\u2019s not immediately obvious why this is a good idea at all. Is TFT-1 simply fine-tuning one more layer than \u201cRetrain Softmax\u201d?\n\nI think that the results at the end of section 3 are a bit weak because of usage of a big network. I would definitely like to see how the results change if using a smaller net.\n\nThe authors claim throughout the paper that the purpose of the added connections and layers is to learn *complementary* features and they show this with some figures. The latter are a convinving evidence, but not proof or guarantee that this is what is actually happening. I suggest the authors consider adding an explicit constraint in their loss that encourages that, e.g. by having a soft orthogonality constraing (assuming one can project intermediate features to some common feature dimensionality). The usage of very small L2 regularization maybe achieves the same thing, but there\u2019s no evidence for that in the paper (in that we don\u2019t have any visualizations of what happens if there\u2019s no L2 reg.).\n\nOne of the big questions for me while reading the paper was how would an ensemble of 2 pre-trained nets would do on the tasks that the authors consider. This is especially relevant in the cars classification example, where I suspect that a strong baseline is that of fine-tuning VGG on this task, fine-tuning resnet on this task, and possibly training a linear combination of the two outputs or just averaging them naively.\n\nDisappointing that there are no results in figure 4, 5 and 8 except the ones from this paper. It\u2019s really hard to situate this paper if we don\u2019t actually know how it compares to previously published results.\n\n\nIn general, this was an interesting and potentially useful piece of work. The problem of efficiently reusing the previously trained classifier for retraining on a small set is certainly interesting to the community. While I think that this paper takes a good step in the right direction, it falls a bit short in some dimensions (comparisons with more serious baselines, more understanding etc).\n\n\n"
  },
  {
    "people": [
      "Wu"
    ],
    "review": "Following suggestions from Reviewer 2, we have revised our submission to include comparison to the modified beam search objective proposed by Wu et al., 2016 (Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation ). Further details are provided in the paper and in the reply to Reviewer-2\u2019s comments. We thank the reviewer for this suggestion. \n\nWe also conduct additional experiments that study the correlation of SPICE with caption length and the variation of oracle accuracy with beam budget. While the first experiment suggests that longer sequences do not necessarily result in better scoring captions, the latter experiment shows that DBS utilizes the beam budget efficiently \u2014 obtaining higher oracle accuracies at much lower beam budgets compared to other decoding techniques. \n"
  },
  {
    "people": [
      "Wu"
    ],
    "review": "Following suggestions from Reviewer 2, we have revised our submission to include comparison to the modified beam search objective proposed by Wu et al., 2016 (Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation ). Further details are provided in the paper and in the reply to Reviewer-2\u2019s comments. We thank the reviewer for this suggestion. \n\nWe also conduct additional experiments that study the correlation of SPICE with caption length and the variation of oracle accuracy with beam budget. While the first experiment suggests that longer sequences do not necessarily result in better scoring captions, the latter experiment shows that DBS utilizes the beam budget efficiently \u2014 obtaining higher oracle accuracies at much lower beam budgets compared to other decoding techniques. \n"
  },
  {
    "people": [
      "Danihelka",
      "Henaff",
      "Salimans",
      "Danihelka",
      "Graves"
    ],
    "review": "I added additional results on the memory tasks and language modeling task.\n\nFor the memory task, as suggested by the reviewers, I compare against uRNN models with approx. the same number of parameters as the LRD-GRU. This uRNN, much larger than the one used in the original uRNN paper, converges quickly on the simple memory task but overfits (training was terminated by early stopping in the reported graph).\n\nOn the variable sequence length task (Danihelka 2016) it trains faster (with somewhat larger oscillations) than the LRD-GRU, but on the variable lag task (Henaff 2016) it fails.\n\nI've also considered a variant of the LRD-GRU which I tried only on these two subtasks. In this variant I add weight normalization (Salimans 2016) and a weight max-row-norm constraint in order to reduce the amounts of time that NaN recovery triggers. With this modification, NaN recovery is unnecessary, and the models train much faster. In fact, it performs on par with the uRNN even on the (Danihelka 2016) task.\n\nFor the language modelling task, I did experiments with a LSTM baseline and a low-rank plus diagonal LSTM, while trying to use the same setup of (Graves 2013). I still can't replicate those results (the code is online at "
  },
  {
    "people": [
      "LeCun",
      "Srivastava",
      "Misha Denil",
      "Babak Shakibi",
      "Laurent Dinh",
      "Marc' Aurelio Ranzato",
      "Nando de Freitas",
      "Max Jaderberg",
      "Andrea Vedaldi",
      "Andrew Zisserman",
      "Emily L Denton",
      "Wojciech Zaremba",
      "Joan Bruna",
      "Yann LeCun",
      "Rob Fergus"
    ],
    "review": "I would like to give some feedback on an area the reviewers did not touch on, presentational shortcomings. I think those would also have to be improved for an acceptance of the paper.\n\nComments:\n\nThe authors say \"Classical physical systems, however, consist of spatially separated parts with primarily local interactions, long-distance interactions are possible but they tend to be limited by propagation delays, bandwidth and noise. Therefore it may be beneficial to bias our model class towards models that tend to adhere to these physical constraints by using a parametrization which reduces the number of parameters required to represent them\" However, the diagonal + low-rank model assigns the majority of its parameters to the low-rank matrix, which is NOT local. Hence, one cannot say that the authors adhere to these physical constraints.\n\nIn section 3.1, the authors claim that 99.13% is the state-of-the-art on MNIST. LeCun achieved a lower error rate in 1998! Do you mean the state-of-the-art on randomly permuted MNIST?\n\nAlso, in section 3.1, you fail to compare against the same network without low-rank factorization. Also you do not mention how many parameters your model saved compared to a full-rank model. Overall, I think section 3.1 should just be omitted, as it contains too little information to be useful.\n\nI agree with reviewer three that the statement \"We presented a framework that unifies the description various types of recurrent and feed-forward\nneural networks as passthrough neural networks.\" makes it sound as if the framework is a contribution, which it isn't. (It is on page 2 of the original highway paper!) Also the framework is so general one almost cannot call it a framework. (We define a general function in terms of 3 new general functions.) Further, I don't understand what the framework has to do with the low-rank + symmetric approach in the first place. Any neural network or indeed any method that somehow involves matrices can have its matrices replaced with low-rank + symmetric form. I think the authors have the believe that passthrough networks do not alter the state as much per layer as other networks, and thus are more likely to have a wasteful parameter budget. However, this is just a belief and not experimentally validated. In summary, section 2 is more confusing than helpful. I would immediately start by mentioning the model you are investigating (low-rank + symmetric) and then briefly explain why you think it makes particular sense to apply it to GRU / Highway networks.\n\nThe graphs in Figure 2 are two large. One-curve graphs do not need one sixth of a page. While this is obviously just a presentation shortcoming, it nevertheless sends the message that you didn't have enough to say to fill the page space with words. Also, if you are going to use the entire page for the six graphs, at least make the axis labels of a font size comparable to that of the main text.\n\nIn section 3.2.1 and 3.2.2, you have to present your results in a table and not just in the text, as you do in section 3.2.3 and 3.2.4. If you make Figure 2 smaller (see previous paragraph), you will have plenty of space for this.\n\nDetails such as learning rate and mini-batch size should either be in the appendix or in a seperate section, not intermixed with the experimental results. That makes it harder to read.\n\nI can't find any experiment where diagonal without symmetric model outperformed the diagonal plus symmetric model. If there is no such experiment, the diagonal without symmetric model should be omitted from the paper or relegated to a side note as it adds virtually no value, as the vast majority of parameters are tied up in the diagonal matrix.\n\nIn the conclusion, you say that your approach is \"orthogonal\" to the convolutional parametrizations explored by He et. al. How is that? He et al. experiment with a low-rank structure (they call it bottleneck) between each skip connection. That seems very similar to what you do. In any case, if I use the low-rank layout from He et al., it would be reasonable to think that using your low-rank layout in addition would yield at most diminishing returns, hence the methods are not orthogonal. Also, you talk about the convolutional parametrization in Srivastava et al in the same sentence. As far as I can tell, that paper neither dealt with convlutional nets nor with low-rank factorizations.\n\nYou include a lot of very basic references in your papers, i.e. multiple references on \"neural networks were successful\", multiple on the vanishing gradient problem, etc. I think you don't need that many \"obvious\" references. (This is a very minor point.) On the other hand, I think you are missing some more important references that have looked at low-rank or related methods for neural nets, such as:\n\nMisha Denil, Babak Shakibi, Laurent Dinh, Marc' Aurelio Ranzato, and Nando de Freitas. Predicting\nparameters in deep learning. In Advances in Neural Information Processing Systems, pages 2148\u20132156.\n2013.\n\nMax Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with\nlow rank expansions. arXiv preprint arXiv:1405.3866, 2014.\n\nEmily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear structure\nwithin convolutional networks for efficient evaluation. In Advances in Neural Information Processing\nSystems, pages 1269\u20131277. 2014."
  },
  {
    "people": [
      "Danihelka",
      "Henaff",
      "Salimans",
      "Danihelka",
      "Graves"
    ],
    "review": "I added additional results on the memory tasks and language modeling task.\n\nFor the memory task, as suggested by the reviewers, I compare against uRNN models with approx. the same number of parameters as the LRD-GRU. This uRNN, much larger than the one used in the original uRNN paper, converges quickly on the simple memory task but overfits (training was terminated by early stopping in the reported graph).\n\nOn the variable sequence length task (Danihelka 2016) it trains faster (with somewhat larger oscillations) than the LRD-GRU, but on the variable lag task (Henaff 2016) it fails.\n\nI've also considered a variant of the LRD-GRU which I tried only on these two subtasks. In this variant I add weight normalization (Salimans 2016) and a weight max-row-norm constraint in order to reduce the amounts of time that NaN recovery triggers. With this modification, NaN recovery is unnecessary, and the models train much faster. In fact, it performs on par with the uRNN even on the (Danihelka 2016) task.\n\nFor the language modelling task, I did experiments with a LSTM baseline and a low-rank plus diagonal LSTM, while trying to use the same setup of (Graves 2013). I still can't replicate those results (the code is online at "
  },
  {
    "people": [
      "LeCun",
      "Srivastava",
      "Misha Denil",
      "Babak Shakibi",
      "Laurent Dinh",
      "Marc' Aurelio Ranzato",
      "Nando de Freitas",
      "Max Jaderberg",
      "Andrea Vedaldi",
      "Andrew Zisserman",
      "Emily L Denton",
      "Wojciech Zaremba",
      "Joan Bruna",
      "Yann LeCun",
      "Rob Fergus"
    ],
    "review": "I would like to give some feedback on an area the reviewers did not touch on, presentational shortcomings. I think those would also have to be improved for an acceptance of the paper.\n\nComments:\n\nThe authors say \"Classical physical systems, however, consist of spatially separated parts with primarily local interactions, long-distance interactions are possible but they tend to be limited by propagation delays, bandwidth and noise. Therefore it may be beneficial to bias our model class towards models that tend to adhere to these physical constraints by using a parametrization which reduces the number of parameters required to represent them\" However, the diagonal + low-rank model assigns the majority of its parameters to the low-rank matrix, which is NOT local. Hence, one cannot say that the authors adhere to these physical constraints.\n\nIn section 3.1, the authors claim that 99.13% is the state-of-the-art on MNIST. LeCun achieved a lower error rate in 1998! Do you mean the state-of-the-art on randomly permuted MNIST?\n\nAlso, in section 3.1, you fail to compare against the same network without low-rank factorization. Also you do not mention how many parameters your model saved compared to a full-rank model. Overall, I think section 3.1 should just be omitted, as it contains too little information to be useful.\n\nI agree with reviewer three that the statement \"We presented a framework that unifies the description various types of recurrent and feed-forward\nneural networks as passthrough neural networks.\" makes it sound as if the framework is a contribution, which it isn't. (It is on page 2 of the original highway paper!) Also the framework is so general one almost cannot call it a framework. (We define a general function in terms of 3 new general functions.) Further, I don't understand what the framework has to do with the low-rank + symmetric approach in the first place. Any neural network or indeed any method that somehow involves matrices can have its matrices replaced with low-rank + symmetric form. I think the authors have the believe that passthrough networks do not alter the state as much per layer as other networks, and thus are more likely to have a wasteful parameter budget. However, this is just a belief and not experimentally validated. In summary, section 2 is more confusing than helpful. I would immediately start by mentioning the model you are investigating (low-rank + symmetric) and then briefly explain why you think it makes particular sense to apply it to GRU / Highway networks.\n\nThe graphs in Figure 2 are two large. One-curve graphs do not need one sixth of a page. While this is obviously just a presentation shortcoming, it nevertheless sends the message that you didn't have enough to say to fill the page space with words. Also, if you are going to use the entire page for the six graphs, at least make the axis labels of a font size comparable to that of the main text.\n\nIn section 3.2.1 and 3.2.2, you have to present your results in a table and not just in the text, as you do in section 3.2.3 and 3.2.4. If you make Figure 2 smaller (see previous paragraph), you will have plenty of space for this.\n\nDetails such as learning rate and mini-batch size should either be in the appendix or in a seperate section, not intermixed with the experimental results. That makes it harder to read.\n\nI can't find any experiment where diagonal without symmetric model outperformed the diagonal plus symmetric model. If there is no such experiment, the diagonal without symmetric model should be omitted from the paper or relegated to a side note as it adds virtually no value, as the vast majority of parameters are tied up in the diagonal matrix.\n\nIn the conclusion, you say that your approach is \"orthogonal\" to the convolutional parametrizations explored by He et. al. How is that? He et al. experiment with a low-rank structure (they call it bottleneck) between each skip connection. That seems very similar to what you do. In any case, if I use the low-rank layout from He et al., it would be reasonable to think that using your low-rank layout in addition would yield at most diminishing returns, hence the methods are not orthogonal. Also, you talk about the convolutional parametrization in Srivastava et al in the same sentence. As far as I can tell, that paper neither dealt with convlutional nets nor with low-rank factorizations.\n\nYou include a lot of very basic references in your papers, i.e. multiple references on \"neural networks were successful\", multiple on the vanishing gradient problem, etc. I think you don't need that many \"obvious\" references. (This is a very minor point.) On the other hand, I think you are missing some more important references that have looked at low-rank or related methods for neural nets, such as:\n\nMisha Denil, Babak Shakibi, Laurent Dinh, Marc' Aurelio Ranzato, and Nando de Freitas. Predicting\nparameters in deep learning. In Advances in Neural Information Processing Systems, pages 2148\u20132156.\n2013.\n\nMax Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with\nlow rank expansions. arXiv preprint arXiv:1405.3866, 2014.\n\nEmily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear structure\nwithin convolutional networks for efficient evaluation. In Advances in Neural Information Processing\nSystems, pages 1269\u20131277. 2014."
  },
  {
    "people": [
      "Gleizes",
      "Marie-Pierre Gleizes"
    ],
    "review": "Very interesting paper. Maybe you could read paper by Gleizes about cooperative agent, self-organization and resolution through emergence \u2026 This paper could interest you : Self-adaptive complex systems, Marie-Pierre Gleizes, 2011, EUMAS."
  },
  {
    "people": [
      "Noam Chomsky"
    ],
    "review": "The paper is interesting and well-written. The topic of language evolution has fascinated researchers from so many different fields (all kinds of linguistics, anthropology, psychology, sociology, but even mathematics and computer science), and now it looks like there is a new surge of interest -- even a long-time sceptic Noam Chomsky recently published a book. \n\nIt is a pity the paper is rather weak on references and related work. Not on language evolution in general, but on modeling language evolution as a multi-agent cooperation. For example, the submission does not even mention the Lewis Signaling Game which is basically what the paper proposes ("
  },
  {
    "people": [
      "Gleizes",
      "Marie-Pierre Gleizes"
    ],
    "review": "Very interesting paper. Maybe you could read paper by Gleizes about cooperative agent, self-organization and resolution through emergence \u2026 This paper could interest you : Self-adaptive complex systems, Marie-Pierre Gleizes, 2011, EUMAS."
  },
  {
    "people": [
      "Noam Chomsky"
    ],
    "review": "The paper is interesting and well-written. The topic of language evolution has fascinated researchers from so many different fields (all kinds of linguistics, anthropology, psychology, sociology, but even mathematics and computer science), and now it looks like there is a new surge of interest -- even a long-time sceptic Noam Chomsky recently published a book. \n\nIt is a pity the paper is rather weak on references and related work. Not on language evolution in general, but on modeling language evolution as a multi-agent cooperation. For example, the submission does not even mention the Lewis Signaling Game which is basically what the paper proposes ("
  },
  {
    "people": [
      "Carreira-Perpinan",
      "Wang"
    ],
    "review": "The paper presents an architecture to parallelize the optimization of nested functions based on the method of auxiliary coordinates (MAC) (Carreira-Perpinan and Wang, 2012). This method decomposes the optimization into training individual layers and updating the auxiliary coordinates. The paper focuses on binary autoencoders and proposes to partition the data onto several machines allowing the parameters to move between machines. Relatively good speedup factors are reported especially on larger datasets and a theoretical model of performance is presented that matches with the experiments.\n\nMy main concern is that even though the method is presented as a general framework for nested functions, experiments focus on a restricted family of models (i.e. binary autoencoders with linear or kernel encoders and linear decoders) with only two components. While the speedup factors are encouraging, it is hard to get a sense of their importance as the binary autoencoder model considered is not well studied by other researchers and is not widely used. I encourage the authors to apply this framework to more generic architectures and problems.\n\nQuestions:\n1- Does this framework apply to some form of generic multi-layer neural network? If so, some experimental results are useful.\n2- What is the implication of applying this framework to more than two components (an encoder and a decoder) and non-linear components?\n3- It is desired to see a plot of performance as a function of time for different setups to demonstrate the speedup after convergence. It seems the paper only focuses on the speedup factors per iteration. For example, increasing the mini-batch size may improve the speed per iteration but may hurt the convergence speed.\n4- Did you consider a scenario where the dataset is too big that storing the data and auxiliary variables on multiple machines simultaneously is not possible?\n\nThe paper cites an ArXiv manuscript with the same title by the authors multiple times. Please make the paper self-contained and include any supplementary material in the appendix.\n\nI believe without applying this framework to a more generic architecture beyond binary autoencoders, this paper does not appeal to a wide audience at ICLR, hence weak reject.\n"
  },
  {
    "people": [
      "Carreira-Perpinan",
      "Wang"
    ],
    "review": "The paper presents an architecture to parallelize the optimization of nested functions based on the method of auxiliary coordinates (MAC) (Carreira-Perpinan and Wang, 2012). This method decomposes the optimization into training individual layers and updating the auxiliary coordinates. The paper focuses on binary autoencoders and proposes to partition the data onto several machines allowing the parameters to move between machines. Relatively good speedup factors are reported especially on larger datasets and a theoretical model of performance is presented that matches with the experiments.\n\nMy main concern is that even though the method is presented as a general framework for nested functions, experiments focus on a restricted family of models (i.e. binary autoencoders with linear or kernel encoders and linear decoders) with only two components. While the speedup factors are encouraging, it is hard to get a sense of their importance as the binary autoencoder model considered is not well studied by other researchers and is not widely used. I encourage the authors to apply this framework to more generic architectures and problems.\n\nQuestions:\n1- Does this framework apply to some form of generic multi-layer neural network? If so, some experimental results are useful.\n2- What is the implication of applying this framework to more than two components (an encoder and a decoder) and non-linear components?\n3- It is desired to see a plot of performance as a function of time for different setups to demonstrate the speedup after convergence. It seems the paper only focuses on the speedup factors per iteration. For example, increasing the mini-batch size may improve the speed per iteration but may hurt the convergence speed.\n4- Did you consider a scenario where the dataset is too big that storing the data and auxiliary variables on multiple machines simultaneously is not possible?\n\nThe paper cites an ArXiv manuscript with the same title by the authors multiple times. Please make the paper self-contained and include any supplementary material in the appendix.\n\nI believe without applying this framework to a more generic architecture beyond binary autoencoders, this paper does not appeal to a wide audience at ICLR, hence weak reject.\n"
  },
  {
    "people": [
      "Shuohang"
    ],
    "review": "Dear reviewers,\n\nThank you for your valuable comments again! We have made the corresponding revisions and updated a new pdf version. We briefly list the changes here:\n\nAnonReviewer1:\n1.We clarify the dimension of row vector $\\alpha_i$ to be $1\\times Q$.\n2.We add the visualization of the $\\alpha$ values for the question requiring world knowledge in Figure 2 and add the corresponding analysis at the end of the section \"Experiments\".\n\nAnonReviewer2:\nWe revise the description of the state-of-the-art results in the last paragraph in \"Introduction\".\nWe clarify the dimension of $G$ to be $l\\time Q$, the row vector $\\alpha_i$ to be $1\\times Q$, the column vector $w$ to be $l\\times 1$ for equation (2). So is the equation (8).\nWe clarify the statement of footnote 3 about the output gates in the pre-processing layer.\nWe clarify the description of global search on the spans in both the boundary model description part and the Table 2.\n\nAnonReviewer3:\nWe clarify the integration of match-LSTM and pointer network in the last two paragraphs of the \"Introduction\".\nWe directly cite the works of the baselines in Table 2.\n\nThanks,\nShuohang"
  },
  {
    "people": [
      "Shuohang"
    ],
    "review": "Dear reviewers,\n\nThank you for your valuable comments again! We have made the corresponding revisions and updated a new pdf version. We briefly list the changes here:\n\nAnonReviewer1:\n1.We clarify the dimension of row vector $\\alpha_i$ to be $1\\times Q$.\n2.We add the visualization of the $\\alpha$ values for the question requiring world knowledge in Figure 2 and add the corresponding analysis at the end of the section \"Experiments\".\n\nAnonReviewer2:\nWe revise the description of the state-of-the-art results in the last paragraph in \"Introduction\".\nWe clarify the dimension of $G$ to be $l\\time Q$, the row vector $\\alpha_i$ to be $1\\times Q$, the column vector $w$ to be $l\\times 1$ for equation (2). So is the equation (8).\nWe clarify the statement of footnote 3 about the output gates in the pre-processing layer.\nWe clarify the description of global search on the spans in both the boundary model description part and the Table 2.\n\nAnonReviewer3:\nWe clarify the integration of match-LSTM and pointer network in the last two paragraphs of the \"Introduction\".\nWe directly cite the works of the baselines in Table 2.\n\nThanks,\nShuohang"
  },
  {
    "people": [
      "Weston",
      "Weston"
    ],
    "review": "This paper builds on the work of Weston (2016), using End-to-end memory network models for a limited form of dialogue with teacher feedback. As the authors state in the comments, it is closely related to the question answering problem with the exception that a teacher provides a response after the model\u2019s answer, which does not always come with a positive reward. Thus, the model must learn to use the teacher\u2019s feedback to significantly improve performance.\n\nOverall, the paper is written clearly, and several interesting models are tested. It is certainly only a limited form of dialogue that is considered (closer to question answering, since the questions do not require the agent to look further back into the context), but investigating in this direction could prove fruitful once the tasks are scaled up to be more difficult.\n\nMy main concern is with the paper`s novelty. In the words of the authors, this paper has two primary differences with the work of Weston:\n\n\u201c(i) That earlier work did not use the natural reinforcement learning/online setting, but \u201ccheated\u201d with a fixed policy given in advance. It is important to address the realistic online setting and assess whether the methods, particularly FP, still work, or else what changes (e.g. exploration, balancing, see Fig 4 and Table 1) are needed. (ii) That earlier work had only simulated data, and no real-language data, so was only toy. This work uses Mechanical Turk to do real experiments, which again is important to assess if these methods, particularly FP, work on real language.\u201d\n\nPoint (ii) is very much appreciated, but adding additional human testing data is not sufficient for a conference paper. Thus, the main point of the paper is that \u201cthe model also works if we collect the data online (i.e. the agent\u2019s policy is used to collect data rather than a fixed policy beforehand)\u201d. While this is a step in the right direction, I\u2019m not sure if it\u2019s significant enough for an ICLR paper. Little model novelty is required to solve this additional requirement on these tasks beyond using epsilon greedy exploration. Thus, the paper is borderline accept/reject. \n\nEDIT: I have updated my score slightly in light of the author's response, where they make a good point that real-world implementation should be more strongly considered as part of the contribution.\n"
  },
  {
    "people": [
      "Weston",
      "Weston"
    ],
    "review": "This paper builds on the work of Weston (2016), using End-to-end memory network models for a limited form of dialogue with teacher feedback. As the authors state in the comments, it is closely related to the question answering problem with the exception that a teacher provides a response after the model\u2019s answer, which does not always come with a positive reward. Thus, the model must learn to use the teacher\u2019s feedback to significantly improve performance.\n\nOverall, the paper is written clearly, and several interesting models are tested. It is certainly only a limited form of dialogue that is considered (closer to question answering, since the questions do not require the agent to look further back into the context), but investigating in this direction could prove fruitful once the tasks are scaled up to be more difficult.\n\nMy main concern is with the paper`s novelty. In the words of the authors, this paper has two primary differences with the work of Weston:\n\n\u201c(i) That earlier work did not use the natural reinforcement learning/online setting, but \u201ccheated\u201d with a fixed policy given in advance. It is important to address the realistic online setting and assess whether the methods, particularly FP, still work, or else what changes (e.g. exploration, balancing, see Fig 4 and Table 1) are needed. (ii) That earlier work had only simulated data, and no real-language data, so was only toy. This work uses Mechanical Turk to do real experiments, which again is important to assess if these methods, particularly FP, work on real language.\u201d\n\nPoint (ii) is very much appreciated, but adding additional human testing data is not sufficient for a conference paper. Thus, the main point of the paper is that \u201cthe model also works if we collect the data online (i.e. the agent\u2019s policy is used to collect data rather than a fixed policy beforehand)\u201d. While this is a step in the right direction, I\u2019m not sure if it\u2019s significant enough for an ICLR paper. Little model novelty is required to solve this additional requirement on these tasks beyond using epsilon greedy exploration. Thus, the paper is borderline accept/reject. \n\nEDIT: I have updated my score slightly in light of the author's response, where they make a good point that real-world implementation should be more strongly considered as part of the contribution.\n"
  },
  {
    "people": [
      "Yamins",
      "Cadieu"
    ],
    "review": "The paper reports several connections between the image representations in state-of-the are object recognition networks and findings from human visual psychophysics:\n1) It shows that the mean L1 distance in the feature space of certain CNN layers is predictive of human noise-detection thresholds in natural images.\n2) It reports that for 3 different 2-AFC tasks for which there exists a condition that is hard and one that is easy for humans, the mutual information between decision label and quantised CNN activations is usually higher in the condition that is easier for humans.\n3) It reproduces the general bandpass nature of contrast/frequency detection sensitivity in humans. \n\nWhile these findings appear interesting, they are also rather anecdotal and some of them seem to be rather trivial (e.g. findings in 2). To make a convincing statement it would be important to explore what aspects of the CNN lead to the reported findings. One possible way of doing that could be to include good baseline models to compare against. As I mentioned before, one such baseline should be reasonable low-level vision model. Another interesting direction would be to compare the results for the same network at different training stages.\n\nIn that way one might be able to find out which parts of the reported results can be reproduced by simple low-level image processing systems,  which parts are due to the general deep network\u2019s architecture and which parts arise from the powerful computational properties (object recognition performance) of the CNNs.\n\nIn conclusion, I believe that establishing correspondences between state-of-the art CNNs and human vision is a potentially fruitful approach. However to make a convincing point that found correspondences are non-trivial, it is crucial to show that non-trivial aspects of the CNN lead to the reported findings, which was not done. Therefore, the contribution of the paper is limited since I cannot judge whether the findings really tell me something about a unique relation between high-performing CNNs and the human visual system.\n\nUPDATE:\n\nThank you very much for your extensive revision and inclusion of several of the suggested baselines. \nThe results of the baseline models often raise more questions and make the interpretation of the results more complex, but I feel that this reflects the complexity of the topic and makes the work rather more worthwhile. \n\nOne further suggestion: As the experiments with the snapshots of the CaffeNet shows, the direct relationship between CNN performance and prediction accuracy of biological vision known from Yamins et al. 2014 and Cadieu et al. 2014 does not necessarily hold in your experiments. I think this should be discussed somewhere in the paper.\n\nAll in all, I think that the paper now constitutes a decent contribution relating state-of-the art CNNs to human psychophysics and I would be happy for this work to be accepted.\n\nI raise the my rating for this paper to 7."
  },
  {
    "people": [
      "Gabor"
    ],
    "review": "Jan 16-17, 2017:\n- Edited the hypothesis about \"Overshoot\" and \"Undershoot\" inconsistency with perception (result 1).\n- Added the prediction quality of perceptual threshold as a function of layer for model ResNet-152 .\n\nJan 15, 2017:\n-\tAdded results for three baseline models: two linear filter banks (Gabor decomposition and steerable pyramid) and VGG-19 with scrambled weights.\n-\tAdded results for CaffeNet model at several snapshots during training.\n-\tAdded human data for contrast sensitivity (figure 3 results).\n-\tAdded configurations for context experiments (figure 2 results). Now there are 90 configurations per CNN architecture for Segmentation, Crowding, and Shape.\n-\tCosmetics and minor corrections.\n\n"
  },
  {
    "people": [
      "Yamins",
      "Cadieu"
    ],
    "review": "The paper reports several connections between the image representations in state-of-the are object recognition networks and findings from human visual psychophysics:\n1) It shows that the mean L1 distance in the feature space of certain CNN layers is predictive of human noise-detection thresholds in natural images.\n2) It reports that for 3 different 2-AFC tasks for which there exists a condition that is hard and one that is easy for humans, the mutual information between decision label and quantised CNN activations is usually higher in the condition that is easier for humans.\n3) It reproduces the general bandpass nature of contrast/frequency detection sensitivity in humans. \n\nWhile these findings appear interesting, they are also rather anecdotal and some of them seem to be rather trivial (e.g. findings in 2). To make a convincing statement it would be important to explore what aspects of the CNN lead to the reported findings. One possible way of doing that could be to include good baseline models to compare against. As I mentioned before, one such baseline should be reasonable low-level vision model. Another interesting direction would be to compare the results for the same network at different training stages.\n\nIn that way one might be able to find out which parts of the reported results can be reproduced by simple low-level image processing systems,  which parts are due to the general deep network\u2019s architecture and which parts arise from the powerful computational properties (object recognition performance) of the CNNs.\n\nIn conclusion, I believe that establishing correspondences between state-of-the art CNNs and human vision is a potentially fruitful approach. However to make a convincing point that found correspondences are non-trivial, it is crucial to show that non-trivial aspects of the CNN lead to the reported findings, which was not done. Therefore, the contribution of the paper is limited since I cannot judge whether the findings really tell me something about a unique relation between high-performing CNNs and the human visual system.\n\nUPDATE:\n\nThank you very much for your extensive revision and inclusion of several of the suggested baselines. \nThe results of the baseline models often raise more questions and make the interpretation of the results more complex, but I feel that this reflects the complexity of the topic and makes the work rather more worthwhile. \n\nOne further suggestion: As the experiments with the snapshots of the CaffeNet shows, the direct relationship between CNN performance and prediction accuracy of biological vision known from Yamins et al. 2014 and Cadieu et al. 2014 does not necessarily hold in your experiments. I think this should be discussed somewhere in the paper.\n\nAll in all, I think that the paper now constitutes a decent contribution relating state-of-the art CNNs to human psychophysics and I would be happy for this work to be accepted.\n\nI raise the my rating for this paper to 7."
  },
  {
    "people": [
      "Yamins",
      "Cadieu"
    ],
    "review": "The paper reports several connections between the image representations in state-of-the are object recognition networks and findings from human visual psychophysics:\n1) It shows that the mean L1 distance in the feature space of certain CNN layers is predictive of human noise-detection thresholds in natural images.\n2) It reports that for 3 different 2-AFC tasks for which there exists a condition that is hard and one that is easy for humans, the mutual information between decision label and quantised CNN activations is usually higher in the condition that is easier for humans.\n3) It reproduces the general bandpass nature of contrast/frequency detection sensitivity in humans. \n\nWhile these findings appear interesting, they are also rather anecdotal and some of them seem to be rather trivial (e.g. findings in 2). To make a convincing statement it would be important to explore what aspects of the CNN lead to the reported findings. One possible way of doing that could be to include good baseline models to compare against. As I mentioned before, one such baseline should be reasonable low-level vision model. Another interesting direction would be to compare the results for the same network at different training stages.\n\nIn that way one might be able to find out which parts of the reported results can be reproduced by simple low-level image processing systems,  which parts are due to the general deep network\u2019s architecture and which parts arise from the powerful computational properties (object recognition performance) of the CNNs.\n\nIn conclusion, I believe that establishing correspondences between state-of-the art CNNs and human vision is a potentially fruitful approach. However to make a convincing point that found correspondences are non-trivial, it is crucial to show that non-trivial aspects of the CNN lead to the reported findings, which was not done. Therefore, the contribution of the paper is limited since I cannot judge whether the findings really tell me something about a unique relation between high-performing CNNs and the human visual system.\n\nUPDATE:\n\nThank you very much for your extensive revision and inclusion of several of the suggested baselines. \nThe results of the baseline models often raise more questions and make the interpretation of the results more complex, but I feel that this reflects the complexity of the topic and makes the work rather more worthwhile. \n\nOne further suggestion: As the experiments with the snapshots of the CaffeNet shows, the direct relationship between CNN performance and prediction accuracy of biological vision known from Yamins et al. 2014 and Cadieu et al. 2014 does not necessarily hold in your experiments. I think this should be discussed somewhere in the paper.\n\nAll in all, I think that the paper now constitutes a decent contribution relating state-of-the art CNNs to human psychophysics and I would be happy for this work to be accepted.\n\nI raise the my rating for this paper to 7."
  },
  {
    "people": [
      "Gabor"
    ],
    "review": "Jan 16-17, 2017:\n- Edited the hypothesis about \"Overshoot\" and \"Undershoot\" inconsistency with perception (result 1).\n- Added the prediction quality of perceptual threshold as a function of layer for model ResNet-152 .\n\nJan 15, 2017:\n-\tAdded results for three baseline models: two linear filter banks (Gabor decomposition and steerable pyramid) and VGG-19 with scrambled weights.\n-\tAdded results for CaffeNet model at several snapshots during training.\n-\tAdded human data for contrast sensitivity (figure 3 results).\n-\tAdded configurations for context experiments (figure 2 results). Now there are 90 configurations per CNN architecture for Segmentation, Crowding, and Shape.\n-\tCosmetics and minor corrections.\n\n"
  },
  {
    "people": [
      "Yamins",
      "Cadieu"
    ],
    "review": "The paper reports several connections between the image representations in state-of-the are object recognition networks and findings from human visual psychophysics:\n1) It shows that the mean L1 distance in the feature space of certain CNN layers is predictive of human noise-detection thresholds in natural images.\n2) It reports that for 3 different 2-AFC tasks for which there exists a condition that is hard and one that is easy for humans, the mutual information between decision label and quantised CNN activations is usually higher in the condition that is easier for humans.\n3) It reproduces the general bandpass nature of contrast/frequency detection sensitivity in humans. \n\nWhile these findings appear interesting, they are also rather anecdotal and some of them seem to be rather trivial (e.g. findings in 2). To make a convincing statement it would be important to explore what aspects of the CNN lead to the reported findings. One possible way of doing that could be to include good baseline models to compare against. As I mentioned before, one such baseline should be reasonable low-level vision model. Another interesting direction would be to compare the results for the same network at different training stages.\n\nIn that way one might be able to find out which parts of the reported results can be reproduced by simple low-level image processing systems,  which parts are due to the general deep network\u2019s architecture and which parts arise from the powerful computational properties (object recognition performance) of the CNNs.\n\nIn conclusion, I believe that establishing correspondences between state-of-the art CNNs and human vision is a potentially fruitful approach. However to make a convincing point that found correspondences are non-trivial, it is crucial to show that non-trivial aspects of the CNN lead to the reported findings, which was not done. Therefore, the contribution of the paper is limited since I cannot judge whether the findings really tell me something about a unique relation between high-performing CNNs and the human visual system.\n\nUPDATE:\n\nThank you very much for your extensive revision and inclusion of several of the suggested baselines. \nThe results of the baseline models often raise more questions and make the interpretation of the results more complex, but I feel that this reflects the complexity of the topic and makes the work rather more worthwhile. \n\nOne further suggestion: As the experiments with the snapshots of the CaffeNet shows, the direct relationship between CNN performance and prediction accuracy of biological vision known from Yamins et al. 2014 and Cadieu et al. 2014 does not necessarily hold in your experiments. I think this should be discussed somewhere in the paper.\n\nAll in all, I think that the paper now constitutes a decent contribution relating state-of-the art CNNs to human psychophysics and I would be happy for this work to be accepted.\n\nI raise the my rating for this paper to 7."
  },
  {
    "people": [
      "Adam"
    ],
    "review": "Taking into account the loss in the binarization step through a proximal Newton algorithm is a nice idea. This is at least one approach to bringing in the missing loss in the binarization step, which has recently gone from a two step process of train and binarize to a single step simultaneous train/compress. Performance on a few small tasks show the benefit. It would be nice to see some results on substantial networks and tasks which really need compression on embedded systems (a point made in the introduction). Is it necessary to discuss exploding/vanishing gradients when the RNN experiments are carried out by an LSTM, and handled by the cell error carousel? We see the desire to tie into proposition 2, but not clear that the degradation we see in the binary connect is related. Adam is used in the LSTM optimization, was gradient clipping really needed, or is the degradation of binary connect simply related to capacity? For proposition 3.1, theorem 3.1 and proposition 3.2 put the pointers to proofs in appendix."
  },
  {
    "people": [
      "Adam"
    ],
    "review": "Taking into account the loss in the binarization step through a proximal Newton algorithm is a nice idea. This is at least one approach to bringing in the missing loss in the binarization step, which has recently gone from a two step process of train and binarize to a single step simultaneous train/compress. Performance on a few small tasks show the benefit. It would be nice to see some results on substantial networks and tasks which really need compression on embedded systems (a point made in the introduction). Is it necessary to discuss exploding/vanishing gradients when the RNN experiments are carried out by an LSTM, and handled by the cell error carousel? We see the desire to tie into proposition 2, but not clear that the degradation we see in the binary connect is related. Adam is used in the LSTM optimization, was gradient clipping really needed, or is the degradation of binary connect simply related to capacity? For proposition 3.1, theorem 3.1 and proposition 3.2 put the pointers to proofs in appendix."
  },
  {
    "people": [
      "Adam"
    ],
    "review": "Taking into account the loss in the binarization step through a proximal Newton algorithm is a nice idea. This is at least one approach to bringing in the missing loss in the binarization step, which has recently gone from a two step process of train and binarize to a single step simultaneous train/compress. Performance on a few small tasks show the benefit. It would be nice to see some results on substantial networks and tasks which really need compression on embedded systems (a point made in the introduction). Is it necessary to discuss exploding/vanishing gradients when the RNN experiments are carried out by an LSTM, and handled by the cell error carousel? We see the desire to tie into proposition 2, but not clear that the degradation we see in the binary connect is related. Adam is used in the LSTM optimization, was gradient clipping really needed, or is the degradation of binary connect simply related to capacity? For proposition 3.1, theorem 3.1 and proposition 3.2 put the pointers to proofs in appendix."
  },
  {
    "people": [
      "Adam"
    ],
    "review": "Taking into account the loss in the binarization step through a proximal Newton algorithm is a nice idea. This is at least one approach to bringing in the missing loss in the binarization step, which has recently gone from a two step process of train and binarize to a single step simultaneous train/compress. Performance on a few small tasks show the benefit. It would be nice to see some results on substantial networks and tasks which really need compression on embedded systems (a point made in the introduction). Is it necessary to discuss exploding/vanishing gradients when the RNN experiments are carried out by an LSTM, and handled by the cell error carousel? We see the desire to tie into proposition 2, but not clear that the degradation we see in the binary connect is related. Adam is used in the LSTM optimization, was gradient clipping really needed, or is the degradation of binary connect simply related to capacity? For proposition 3.1, theorem 3.1 and proposition 3.2 put the pointers to proofs in appendix."
  },
  {
    "people": [
      "Nal Kalchbrenner"
    ],
    "review": "Reviewer 1 asks, \"Can you offer any results comparing the performance of the proposed Quasi-Recurrent Neural Network (QRNN) to that of the recently published ByteNet? How does it compare from a computational perspective?\"\n\nYes. Nal Kalchbrenner released results for the ByteNet on the same IWSLT dataset we used in this paper in his slides for NIPS "
  },
  {
    "people": [
      "Balduzizi"
    ],
    "review": "This paper introduces a novel RNN architecture named QRNN.\n\nQNNs are similar to gated RNN , however their gate and state update  functions depend only on the recent input values, it does not depend on the previous hidden state. The gate and state update functions are computed through a temporal convolution applied on the input.\nConsequently, QRNN allows for more parallel computation since they have less  operations in their hidden-to-hidden transition depending on the previous hidden state compared to a GRU or LSTM. However, they possibly loose in expressiveness relatively to those models. For instance, it is not clear how such a model deals with long-term dependencies without having to stack up several QRNN layers.\n\nVarious extensions of QRNN, leveraging Zoneout, Densely-connected or seq2seq with attention, are also proposed.\n\nAuthors evaluate their approach on various tasks and datasets (sentiment classification, world-level language modelling and character level machine translation). \n\nOverall the paper is an enjoyable read and the proposed approach is interesting,\nPros:\n- Address an important problem\n- Nice empirical evaluation showing the benefit of their approach\n- Demonstrate up to 16x speed-up relatively to a LSTM\nCons:\n- Somewhat incremental novelty compared to (Balduzizi et al., 2016)\n\nFew specific questions:\n- Is densely layer necessary to obtain good result on the IMDB task. How does a simple 2-layer QRNN compare with 2-layer LSTM?  \n- How does the i-fo-ifo pooling perform comparatively? \n- How does QRNN deal with long-term time depency? Did you try on it on simple toy task such as the copy or the adding task? "
  },
  {
    "people": [
      "Oord"
    ],
    "review": "The authors describe the use of convolutional layers with intermediate pooling layers to more efficiently model long-range dependencies in sequential data compared with recurrent architectures. Whereas the use of convolutional layers is related to the PixelCNN architecture (Oord et al.), the main novelty is to combine them with gated pooling layers to integrate information from previous time steps. Additionally, the authors describe extensions based on zone-out regularization, densely connected layers, and an efficient attention mechanism for encoder-decoder models. The authors report a striking speed-up over RNNs by up to a factor of 16, while achieving similar or even higher performances.\n\n\nMajor comment\n=============\nQRNNs are closely related to PixelCNNs, which leverage masked dilated convolutional layers to speed-up computations. However, the authors cite ByteNet, which builds upon PixelCNN, only at the end of their manuscript and do not include it in the evaluation. The authors should cite PixelCNN already when introducing QRNN in the methods sections, and include it in the evaluation. At the very least, QRNN should be compared with ByteNet for language translation. How well does a fully convolutional model without intermediate pooling layers perform, i.e. what is the effect to the introduced pooling layers? Are their performance difference between f, fo, and ifo pooling? Did the authors investigate dilated convolutional layers?\n\n\nMinor comments\n==============\n1. How does a model without dense connections perform, i.e. what is the effect of dense connections? To illustrate dense connections, the authors might draw them in figure 1 and refer to it in section 2.1. \n\n2. The run-time results shown in figure 4 are very helpful, but as far as I understood, the breakdown shown on the left side was measured for language modeling (referred in 3.2), whereas the dependency on batch- and sequence size shown on the right side for sentiment classification (referred in 3.1). I suggest to consistently show the results for either sentiment classification or language modeling, or both. At the very least, the figure caption should describe the task explicitly. Labeling the left and right figure by a) and b) would further improve readability. \n\n3. Section 3.1 describes a high speed-up for long sequences and small batch sizes. I suggest motivating why this is the case. While computations can be parallelized along the sequence length, it is less obvious why smaller batch sizes speed-up computations.\n\n4. The proposed encoder-decoder attention is different from traditional attention in that attention vectors are not computed and used as input to the decoder sequentially, but on top of decoder output states in parallel. This should be described and motivated in the text.\n\nSentiment classification\n------------------------\n5. What was the size of the hold-out development set and how was it created? The text describes that data were split equally into training and test set, without describing the hold-out set.\n\n6. What was the convolutional filter size?\n\n7. What is the speed-up for the best hyper-parameters (batch size 24, sequence length 231)?\n\n8. Figure 3 would be easier to interpret by actually showing the text on the y-axis. For the sake of space, one might use a smaller text passage, plot it along the x-axis, and show the activations of fewer neurons along the y-axis. Showing more examples in the appendix would make the authors\u2019 claim that neurons are interpretable even more convincing.\n\n\nLanguage modeling\n-----------------\n9. What was the size of the training, test, and validation set?\n\n10. What was the convolutional filter size, denoted as \u2018k\u2019?\n\n11. Is it correct that a very high learning rate of 1 was used for six epochs at the beginning?\n\n12. The authors should show learning curves for a models with and without zone-out.\n \nTranslation\n-----------\n13. What was the size of the training, test, and validation set?\n\n14. How does translation performance depend on k?\n\n\n \n \n"
  },
  {
    "people": [
      "Nal Kalchbrenner"
    ],
    "review": "Reviewer 1 asks, \"Can you offer any results comparing the performance of the proposed Quasi-Recurrent Neural Network (QRNN) to that of the recently published ByteNet? How does it compare from a computational perspective?\"\n\nYes. Nal Kalchbrenner released results for the ByteNet on the same IWSLT dataset we used in this paper in his slides for NIPS "
  },
  {
    "people": [
      "Balduzizi"
    ],
    "review": "This paper introduces a novel RNN architecture named QRNN.\n\nQNNs are similar to gated RNN , however their gate and state update  functions depend only on the recent input values, it does not depend on the previous hidden state. The gate and state update functions are computed through a temporal convolution applied on the input.\nConsequently, QRNN allows for more parallel computation since they have less  operations in their hidden-to-hidden transition depending on the previous hidden state compared to a GRU or LSTM. However, they possibly loose in expressiveness relatively to those models. For instance, it is not clear how such a model deals with long-term dependencies without having to stack up several QRNN layers.\n\nVarious extensions of QRNN, leveraging Zoneout, Densely-connected or seq2seq with attention, are also proposed.\n\nAuthors evaluate their approach on various tasks and datasets (sentiment classification, world-level language modelling and character level machine translation). \n\nOverall the paper is an enjoyable read and the proposed approach is interesting,\nPros:\n- Address an important problem\n- Nice empirical evaluation showing the benefit of their approach\n- Demonstrate up to 16x speed-up relatively to a LSTM\nCons:\n- Somewhat incremental novelty compared to (Balduzizi et al., 2016)\n\nFew specific questions:\n- Is densely layer necessary to obtain good result on the IMDB task. How does a simple 2-layer QRNN compare with 2-layer LSTM?  \n- How does the i-fo-ifo pooling perform comparatively? \n- How does QRNN deal with long-term time depency? Did you try on it on simple toy task such as the copy or the adding task? "
  },
  {
    "people": [
      "Oord"
    ],
    "review": "The authors describe the use of convolutional layers with intermediate pooling layers to more efficiently model long-range dependencies in sequential data compared with recurrent architectures. Whereas the use of convolutional layers is related to the PixelCNN architecture (Oord et al.), the main novelty is to combine them with gated pooling layers to integrate information from previous time steps. Additionally, the authors describe extensions based on zone-out regularization, densely connected layers, and an efficient attention mechanism for encoder-decoder models. The authors report a striking speed-up over RNNs by up to a factor of 16, while achieving similar or even higher performances.\n\n\nMajor comment\n=============\nQRNNs are closely related to PixelCNNs, which leverage masked dilated convolutional layers to speed-up computations. However, the authors cite ByteNet, which builds upon PixelCNN, only at the end of their manuscript and do not include it in the evaluation. The authors should cite PixelCNN already when introducing QRNN in the methods sections, and include it in the evaluation. At the very least, QRNN should be compared with ByteNet for language translation. How well does a fully convolutional model without intermediate pooling layers perform, i.e. what is the effect to the introduced pooling layers? Are their performance difference between f, fo, and ifo pooling? Did the authors investigate dilated convolutional layers?\n\n\nMinor comments\n==============\n1. How does a model without dense connections perform, i.e. what is the effect of dense connections? To illustrate dense connections, the authors might draw them in figure 1 and refer to it in section 2.1. \n\n2. The run-time results shown in figure 4 are very helpful, but as far as I understood, the breakdown shown on the left side was measured for language modeling (referred in 3.2), whereas the dependency on batch- and sequence size shown on the right side for sentiment classification (referred in 3.1). I suggest to consistently show the results for either sentiment classification or language modeling, or both. At the very least, the figure caption should describe the task explicitly. Labeling the left and right figure by a) and b) would further improve readability. \n\n3. Section 3.1 describes a high speed-up for long sequences and small batch sizes. I suggest motivating why this is the case. While computations can be parallelized along the sequence length, it is less obvious why smaller batch sizes speed-up computations.\n\n4. The proposed encoder-decoder attention is different from traditional attention in that attention vectors are not computed and used as input to the decoder sequentially, but on top of decoder output states in parallel. This should be described and motivated in the text.\n\nSentiment classification\n------------------------\n5. What was the size of the hold-out development set and how was it created? The text describes that data were split equally into training and test set, without describing the hold-out set.\n\n6. What was the convolutional filter size?\n\n7. What is the speed-up for the best hyper-parameters (batch size 24, sequence length 231)?\n\n8. Figure 3 would be easier to interpret by actually showing the text on the y-axis. For the sake of space, one might use a smaller text passage, plot it along the x-axis, and show the activations of fewer neurons along the y-axis. Showing more examples in the appendix would make the authors\u2019 claim that neurons are interpretable even more convincing.\n\n\nLanguage modeling\n-----------------\n9. What was the size of the training, test, and validation set?\n\n10. What was the convolutional filter size, denoted as \u2018k\u2019?\n\n11. Is it correct that a very high learning rate of 1 was used for six epochs at the beginning?\n\n12. The authors should show learning curves for a models with and without zone-out.\n \nTranslation\n-----------\n13. What was the size of the training, test, and validation set?\n\n14. How does translation performance depend on k?\n\n\n \n \n"
  },
  {
    "people": [
      "Toderici"
    ],
    "review": "Dear reviewers, we made the following changes to our paper:\n\n\u2013 added direct comparison with VAE/quantization approximation of Balle et al. (Figure 10)\n\u2013\u00a0added another control to Figure 3 (incremental training vs fixed small learning rate)\n\u2013\u00a0added a motivation and reference for MOS tests\n\u2013\u00a0added more detail to appendix to make reimplementation easier\n\u2013 improved caption of Figure 3A\n\nMinor:\n\u2013\u00a0added number of scales used in Gaussian scale mixtures\n\u2013\u00a0fixed reference to Figure 4 (which pointed to Figure 9 before)\n\nToderici et al. kindly provided us with their results which include entropy encoding*. We will rerun MOS experiments and intend to update quantitative comparison figures before the meeting.\n\n* "
  },
  {
    "people": [
      "Toderici",
      "Balle"
    ],
    "review": "This paper proposes an autoencoder approach to lossy image compression by minimizing the weighted sum of reconstruction error and code length. The architecture consists of a convolutional encoder and a sub-pixel convolutional decoder. Experiments compare PSNR, SSIM, and MS-SSIM performance against JPEG, JPEG-2000, and a recent RNN-based compression approach. A mean opinion score test was also conducted.\n\nPros:\n+ The paper is clear and well-written.\n+ The decoder architecture takes advantage of recent advances in convolutional approaches to image super-resolution.\n+ The proposed approaches to quantization and rate estimation are sensible and well-justified.\n\nCons:\n- The experimental baselines do not appear to be entirely complete.\n\nThe task of using autoencoders to perform compression is important and has a large practical impact. Though directly optimizing the rate-distortion tradeoff is not an entirely novel enterprise, there are enough differences (e.g. the quantization approach and sub-pixel convolutional decoder) to sufficiently distinguish this from earlier work. I am not an image compression expert but the approach and results both seem compelling. The main shortcoming is that the implementation of Toderici et al. 2016b appears to be incomplete, and there is no comparison to Balle et al. 2016. Overall, I feel that the fact that this architecture achieves competitive performance with JPEG-2000 while simultaneously setting the stage for future work that varies the encoder/decoder size and data domain means the community will find this work to be of significant interest.\n\nI have no further specific comments at this time as they were answered sufficiently in the pre-review questions."
  },
  {
    "people": [
      "Jensen"
    ],
    "review": "Thank you for this important and inspiring work.\n\nI have some questions regarding the bound in eq. 8.\n\nMy understanding:\n\nThe bound in eq. 8 is used in the loss function as a proxy for the non-differentiable entropy of the codes. If the bound is minimized, a bound on the entropy is minimized.\n\nThe bound in eq. 8 comes directly from the equality in eq. 7, by applying Jensen's inequality. So, the bound only holds if the equality in eq. 7 holds.\n\nHowever, q(z+u) is a parametric approximation (the GSM model) of Q(z), not an equality.\n\nDoes the bound still hold? Under which circumstances?\n\n(I have a feeling that the missing term on the RHS of eq. 8 to make the bound an equality is a KL term between int q(z+u) du and Q(z), in which case the bound still holds as the KL is positive, but I have not been able to show it.)\n"
  },
  {
    "people": [
      "Toderici"
    ],
    "review": "Dear reviewers, we made the following changes to our paper:\n\n\u2013 added direct comparison with VAE/quantization approximation of Balle et al. (Figure 10)\n\u2013\u00a0added another control to Figure 3 (incremental training vs fixed small learning rate)\n\u2013\u00a0added a motivation and reference for MOS tests\n\u2013\u00a0added more detail to appendix to make reimplementation easier\n\u2013 improved caption of Figure 3A\n\nMinor:\n\u2013\u00a0added number of scales used in Gaussian scale mixtures\n\u2013\u00a0fixed reference to Figure 4 (which pointed to Figure 9 before)\n\nToderici et al. kindly provided us with their results which include entropy encoding*. We will rerun MOS experiments and intend to update quantitative comparison figures before the meeting.\n\n* "
  },
  {
    "people": [
      "Toderici",
      "Balle"
    ],
    "review": "This paper proposes an autoencoder approach to lossy image compression by minimizing the weighted sum of reconstruction error and code length. The architecture consists of a convolutional encoder and a sub-pixel convolutional decoder. Experiments compare PSNR, SSIM, and MS-SSIM performance against JPEG, JPEG-2000, and a recent RNN-based compression approach. A mean opinion score test was also conducted.\n\nPros:\n+ The paper is clear and well-written.\n+ The decoder architecture takes advantage of recent advances in convolutional approaches to image super-resolution.\n+ The proposed approaches to quantization and rate estimation are sensible and well-justified.\n\nCons:\n- The experimental baselines do not appear to be entirely complete.\n\nThe task of using autoencoders to perform compression is important and has a large practical impact. Though directly optimizing the rate-distortion tradeoff is not an entirely novel enterprise, there are enough differences (e.g. the quantization approach and sub-pixel convolutional decoder) to sufficiently distinguish this from earlier work. I am not an image compression expert but the approach and results both seem compelling. The main shortcoming is that the implementation of Toderici et al. 2016b appears to be incomplete, and there is no comparison to Balle et al. 2016. Overall, I feel that the fact that this architecture achieves competitive performance with JPEG-2000 while simultaneously setting the stage for future work that varies the encoder/decoder size and data domain means the community will find this work to be of significant interest.\n\nI have no further specific comments at this time as they were answered sufficiently in the pre-review questions."
  },
  {
    "people": [
      "Jensen"
    ],
    "review": "Thank you for this important and inspiring work.\n\nI have some questions regarding the bound in eq. 8.\n\nMy understanding:\n\nThe bound in eq. 8 is used in the loss function as a proxy for the non-differentiable entropy of the codes. If the bound is minimized, a bound on the entropy is minimized.\n\nThe bound in eq. 8 comes directly from the equality in eq. 7, by applying Jensen's inequality. So, the bound only holds if the equality in eq. 7 holds.\n\nHowever, q(z+u) is a parametric approximation (the GSM model) of Q(z), not an equality.\n\nDoes the bound still hold? Under which circumstances?\n\n(I have a feeling that the missing term on the RHS of eq. 8 to make the bound an equality is a KL term between int q(z+u) du and Q(z), in which case the bound still holds as the KL is positive, but I have not been able to show it.)\n"
  },
  {
    "people": [
      "Stoyanov"
    ],
    "review": "This is a solid paper that proposes to endow attention mechanisms with structure (the attention posterior probabilities becoming structured latent variables). Experiments are shown with segmental atention (as in semi-Markov models) and syntactic attention (as in projective dependency parsing), both in a synthetic task (tree transduction) and real world tasks (neural machine translation and natural language inference). There is a small gain in using structured attention over simple attention in the latter tasks. A clear accept.\n\nThe paper is very clear, the approach is novel and interesting, and the experiments seem to give a good proof of concept. However, the use of structured attention in neural MT seems doesn't seem to be fully exploited here: segmental attention could be a way of approaching neural phrase-based MT, and syntactic attention offers a way of incorporating latent syntax in MT -- these seem very promising directions. In particular it would be interesting to try to add some (semi-)supervision on these attention mechanisms (e.g. posterior marginals computed by an external parser) to see if that helps learning the attention components of the network, or at least help initializing them. \n\nThis seems to be the first interesting use of the backprop of forward-backward/inside-outside (Stoyanov et al. 2011). As stated in sec 3.3., for general probabilistic models the forward step over structured attention corresponds to the computation of first-order moments (posterior marginals) while the backprop step corresponds to second-order moments (gradients of marginals wrt log-potentials, i.e., Hessian of log-partition function). This extends the applicability of the proposed approach to arbitrary graphical models where these quantities can be computed efficiently. E.g. is there a generalized matrix-tree formula that allows to do backprop for non-projective syntax? On the negative side, I suspect the need for second-order statistics may bring some numerical instability in some problems, caused by the use of the signed log-space field. Was this seen in practice?\n\nMinor comments/typos:\n- last paragraph of sec 1: \"standard attention attention\"\n- third paragraph of sec 3.2: \"the on log-potentials\"\n- sec 4.1, Results: \"... as it has no information about the source ordering\" -- what do you mean here?"
  },
  {
    "people": [
      "Stoyanov"
    ],
    "review": "This is a solid paper that proposes to endow attention mechanisms with structure (the attention posterior probabilities becoming structured latent variables). Experiments are shown with segmental atention (as in semi-Markov models) and syntactic attention (as in projective dependency parsing), both in a synthetic task (tree transduction) and real world tasks (neural machine translation and natural language inference). There is a small gain in using structured attention over simple attention in the latter tasks. A clear accept.\n\nThe paper is very clear, the approach is novel and interesting, and the experiments seem to give a good proof of concept. However, the use of structured attention in neural MT seems doesn't seem to be fully exploited here: segmental attention could be a way of approaching neural phrase-based MT, and syntactic attention offers a way of incorporating latent syntax in MT -- these seem very promising directions. In particular it would be interesting to try to add some (semi-)supervision on these attention mechanisms (e.g. posterior marginals computed by an external parser) to see if that helps learning the attention components of the network, or at least help initializing them. \n\nThis seems to be the first interesting use of the backprop of forward-backward/inside-outside (Stoyanov et al. 2011). As stated in sec 3.3., for general probabilistic models the forward step over structured attention corresponds to the computation of first-order moments (posterior marginals) while the backprop step corresponds to second-order moments (gradients of marginals wrt log-potentials, i.e., Hessian of log-partition function). This extends the applicability of the proposed approach to arbitrary graphical models where these quantities can be computed efficiently. E.g. is there a generalized matrix-tree formula that allows to do backprop for non-projective syntax? On the negative side, I suspect the need for second-order statistics may bring some numerical instability in some problems, caused by the use of the signed log-space field. Was this seen in practice?\n\nMinor comments/typos:\n- last paragraph of sec 1: \"standard attention attention\"\n- third paragraph of sec 3.2: \"the on log-potentials\"\n- sec 4.1, Results: \"... as it has no information about the source ordering\" -- what do you mean here?"
  },
  {
    "people": [
      "Chen"
    ],
    "review": "The paper presents a method to reduce the memory footprint of a neural network at some increase in the computation cost. This paper is a generalization of HashedNets by Chen et al. (ICML'15) where parameters of a neural network are mapped into smaller memory arrays using some hash functions with possible collisions. Instead of training the original parameters, given a hash function, the elements of the compressed memory arrays are trained using back-propagation. In this paper, some new tricks are proposed including: (1) the compression space is shared among the layers of the neural network (2) multiple hash functions are used to reduce the effects of collisions (3) a small network is used to combine the elements retrieved from multiple hash tables into a single parameter. Fig 1 of the paper describes the gist of the approach vs. HashedNets.\n\nOn the positive side,\n+ The proposed ideas are novel and seem useful.\n+ Some theoretical justification is presented to describe why using multiple hash functions is a good idea.\n+ All of the experiments suggest that the proposed MFH approach outperforms HashedNets.\nOn the negative side,\n- The computation cost seems worse than HashedNets and is not discussed.\n- Immediate practical implication of the paper is not clear given that alternative pruning strategies perform better and should be faster at inference.\n\nThat said, I believe this paper benefits the deep learning community as it sheds light into ways to share parameters across layers of a neural network potentially leading to more interesting follow-ups. I recommend accept, while asking the authors to address the comments below.\n\nMore comments:\n- Please discuss the computation cost for both HashedNets and MFH for both fully connected and convolutional layers.\n- Are the experiments only run once for each configuration? Please run multiple times and report average / standard error.\n- For completeness, please add U1 results to Table 1.\n- In Table 1, U4-G3 is listed twice with two different numbers.\n- Some sentences are not grammatically correct. Please improve the writing.\n"
  },
  {
    "people": [
      "Chen"
    ],
    "review": "The paper presents a method to reduce the memory footprint of a neural network at some increase in the computation cost. This paper is a generalization of HashedNets by Chen et al. (ICML'15) where parameters of a neural network are mapped into smaller memory arrays using some hash functions with possible collisions. Instead of training the original parameters, given a hash function, the elements of the compressed memory arrays are trained using back-propagation. In this paper, some new tricks are proposed including: (1) the compression space is shared among the layers of the neural network (2) multiple hash functions are used to reduce the effects of collisions (3) a small network is used to combine the elements retrieved from multiple hash tables into a single parameter. Fig 1 of the paper describes the gist of the approach vs. HashedNets.\n\nOn the positive side,\n+ The proposed ideas are novel and seem useful.\n+ Some theoretical justification is presented to describe why using multiple hash functions is a good idea.\n+ All of the experiments suggest that the proposed MFH approach outperforms HashedNets.\nOn the negative side,\n- The computation cost seems worse than HashedNets and is not discussed.\n- Immediate practical implication of the paper is not clear given that alternative pruning strategies perform better and should be faster at inference.\n\nThat said, I believe this paper benefits the deep learning community as it sheds light into ways to share parameters across layers of a neural network potentially leading to more interesting follow-ups. I recommend accept, while asking the authors to address the comments below.\n\nMore comments:\n- Please discuss the computation cost for both HashedNets and MFH for both fully connected and convolutional layers.\n- Are the experiments only run once for each configuration? Please run multiple times and report average / standard error.\n- For completeness, please add U1 results to Table 1.\n- In Table 1, U4-G3 is listed twice with two different numbers.\n- Some sentences are not grammatically correct. Please improve the writing.\n"
  },
  {
    "people": [
      "Bengio",
      "Bengio",
      "Bengio",
      "Bengio",
      "Bengio",
      "Elman Mansimov",
      "Ruslan Salakhutdinov"
    ],
    "review": "We have thought more deeply about the fact that the results in Bengio et al. appear to be in contradiction with ours, in the sense that the Always Sampling training scheme (corresponding to our 100% Pred. Frames training scheme) seems to perform worse than mixed schemes. We also had a chat with one of the authors of the Bengio et al. paper to get a better understanding of the methods and experiments. \nWe have reached the conclusion that the difference might be due to the fact that Bengio et al. focus on discrete problems and to the fact that we were mostly reasoning about long-term prediction, whilst Bengio et al. focus on shorter-term prediction (e.g., in Image Captioning Section, the average prediction length is 11 (this is not mentioned in the paper, but has been pointed out to us by one of the authors)) -- if we look at our results for short-term prediction only, they do not look so much different anymore.\n\nIn Figures 12-16 of the latest version of our paper, in addition to the prediction error at time-step 100, we also plot the prediction error at time-steps 5 and 10 for most games. We can notice that the prediction error with the 100% and the 0%-100% Pred. Frames training schemes (called here Schemes I and II) (red and dark green lines) is almost never lower than the prediction error with the other mixed schemes for time-steps 5 and 10, and often higher (see for example Fishing Derby in Fig. 13). The situation is different at time-step 100, where Schemes I and II are always almost preferable to the other mixed schemes. Therefore, by looking only at the error up to time-step 10, we would also prefer other schemes to Schemes I and II. The error is higher at lower time-steps with Schemes I and II as the objects are less sharply represented in the frames. In other words, these two schemes capture the global dynamics (as this enables better long-term prediction) at the expense of not representing the details very accurately, as lack of the details at earlier predictions do not harm subsequent predictions. In Bengio et al., where the problem is discrete, one error at the beginning of the sequence might lead to drastic errors at subsequent time-steps.\nWe thank the reviewer for this question, as he made us thinking more carefully about this point. We need to modify the paper to include as summary of this discussion.\n\nWe also thank the reviewer for pointing out that in \u201cNitish Srivastava, Elman Mansimov, Ruslan Salakhutdinov. Unsupervised Learning with LSTMs. ICML 2016\u201d, the unconditional model can perform jumpy prediction. We briefly mention this paper in the introduction at the moment, but we need to add a discussion. Notice that, in this paper, prediction is shorter-term than in our case, namely 10-13 time-steps ahead. We are not sure whether, at training time, the real or the generated frames are fed in the conditioned model. From the sentence \u201cAt test time we feed in the generated frame from the previous step without adding any noise. At training time we feed in the ground truth.\u201d in Section 2.3, it would look like the model is trained with feeding in the real frames, i.e. with the 0% Pred. Frames training scheme. This approach seems to work better than the unconditional model -- this is surprising to us, as it is in contradiction with our results.\n"
  },
  {
    "people": [
      "Oh",
      "Oh",
      "Oh",
      "Oh",
      "Oh",
      "Bengio",
      "Oh",
      "Bengio",
      "Bengio",
      "Oh",
      "Oh"
    ],
    "review": "We tried to address the reviewer's concerns by adding a substantial amount of new results and discussion, mainly to the Appendix (notice that some experiments are still running, thus many figures will need to be updated). We highlighted the modifications in red. We did not finish making all the modifications: we mostly still need to improve the main text. However, we decided to send a first update to give the reviewers the time to digest the new material, and to make the remaining changes in the next few days.\n\nBelow we describe in some detail how we addressed the reviewer's comments.\n\n1. Modification to model architecture.\n\nTo address the reviewer's concern about lack of comparison of different ways of incorporating the action, we added a section in the Appendix (Section B.1.3, Figures 19 and 20) that compares the architecture used in the main text (combining the action a_t-1 only with h_t-1 in a multiplicative way), with 8 alternatives, namely combining a_t-1 with the frame before encoding (ConvA), combining a_t-1 only with the encoded frame s_t-1, combining a_t-1 with both h_t-1 and s_t-1 in 4 possible ways, having a multiplicative/additive interaction of a_t-1 with h_t-1, and considering the action as an additional input. \nFrom the figures and videos it looks like combining the action with the frame (dark green lines) is a bad idea, and that there is no much difference in performance among the remaining approaches, although combining a_t-1 with both h_t-1 and s_t-1 (W^h h_{t-1}xW^s s_{t-1}x W^{a} a_{t-1)) seems to be overall a bit better (notice that this architecture results in a quite different core than the traditional LSTM core).\nThe experiments are still running -- the experiments with considering the action as an additional input (light green lines) are behind the other experiments.\n\nWe also modified the section in the Appendix describing the difference between our direct action influence and the indirect action influence as in Oh et al. (now at the end of Section B.1.3). We changed Fig. 12 in the previous version with Figure 21 to show the difference in the other games in addition to Seaquest (we need to add Riverraid for which we want to use 2 subsequences of length T=15). \nDirect action influence performs similarly to indirect action influence in games with lower prediction error, such as Bowling, Freeway and Pong, but better in games with higher prediction error. The most striking difference is with Seaquest, which is most evident when looking at the videos (is is difficult to judge the difference in performance by looking at the prediction error only, that's why we added randomly selected videos from which the types of errors in the prediction can be visually understood). These results are computed using the best training scheme for Seaquest. The videos show that, with indirect action influence, the simulator models the fish quite poorly, even though using a training scheme that more strongly encourages long-term accuracy enables the simulator to learn the appearance of the fish better than with the training scheme of Oh et al. Therefore, these videos clearly show that the training scheme alone is not sufficient, our architecture plays a crucial role in substantially improving the prediction. Having indirect action influence also makes it more difficult to correctly update the score in some games such as Seaquest and Fishing Derby. Therefore, whilst the training scheme is the major responsible for improving long-term prediction, it needs to be combined with an appropriate architecture in order to work. \n\nAll together, the experiments in Section B.1.3 suggest that having a direct (as opposed to Oh at al.) and global (as opposed to ConvA) action influence on the state dynamics is much preferable. On the other hand, performance is less sensitive to different ways of imposing direct action influence. We need to improve the discussion in the paper about this point.\n\n\n2. Exploring the idea of jumpy predictions.\n\nTo address the reviewer's concerns about lack of a detailed analysis, we added a new section in the Appendix (Section B.2, Figures 25 and 26), and modified Figure 5(b) to include all games (except Breakout for which the prediction error is uninformative -- see the discussion about this game in Section B.1.1). Section B.2 compares different architectures, sequence lengths and number of subsequences (we plan to add a few more experiments in the next few weeks). \nIt is clear from the results that the jumpy simulator is much more sensitive to changes of structure, and therefore fragile, than the single-step simulator. However, we show that with the right training structure, the jumpy simulator can be accurate in many games. Having accurate and fast to use simulators is essential for model-based reinforcement learning. Thus, whilst the novelty might seems marginal, it is of extreme interest for the community to advance in this type of approaches. Furthermore, accurate jumpy approaches are very difficult to obtain -- we believe that we are the first ones showing some success for long-term prediction in large scale domains.\n\nWe do not understand the sentence 'However, it is again not clear whether the modification in the current model leads to this effect or it could be achieved by previous models like Oh et al.'. The model of Oh et al. cannot be modified to perform jumpy predictions as, when omitting intermediate frames, the corresponding actions do not have any influence (as they influence the hidden states indirectly through the frames). This is actually an important reason why a direct action influence is preferable.\n\n\n3. Exploring different training schemes.\n\nWe would like to comment on the sentence \u201cWhile this observation is interesting, this effect has been previously explored in detail in other works like schedule sampling (Bengio et al.) and to some extent in Oh et al.?\u201d\nThe schedule sampling work of Bengio et al. focuses on different applications, with discrete data and on shorter-term prediction (for example, the length of sequences in the Image Captioning example (Section 4.1) is 11 on average). In this applications, the 100% Pred. Frames training scheme (called Always Sampling) actually performs poorly. Thus, based on the results of Bengio et al., we would never use the 100% Pred. Frames. We show that, in simpler games, this is actually the scheme that performs best, and that, in more complex games, a scheme that is as close as possible to such a scheme is preferable. Our analysis (especially in the new material) shows in detail the characteristics of each scheme on both short-term and long-term prediction. Thus, we make a significant contribution in advancing the understanding of different training schemes in frame prediction.\nOh et al. used a mixed scheme without any explanation. Thus, it is totally unclear from their paper the importance of using a mixed scheme and what are the differences of different types of mixing. That's actually the reason that motivatesd us to perform a detailed analysis. \n\nTo improve the understanding of the results, we have added a section in the Appendix (Section B.1.1) that discusses the characteristics and effects of different training schemes on each game.\n\nIn conclusion, we addressed the reviewer's concerns on the analysis by adding more results and discussion. Regarding the concern on little novelty, we strongly disagree that this should be a concern. We greatly improved on the performance of previous methods (it is enough to compare the videos in Oh at al. with ours to realize that there is an enormous difference). Even more importantly, we improved the understanding of these methods with a detailed analysis. We believe that papers like our paper are as important for the community as papers that provide completely novel architectures without much analysis. \n\nFinally, the reviewer should understand that providing a detailed analysis and understanding is extremely challenging and time consuming, due to both the large scale and the complexity of the problem. Ours, whilst not fully comprehensive (as this is not possible), represents an enormous effort in trying to provide an in depth analysis and advance the understanding on the problem of high dimensional frame prediction."
  },
  {
    "people": [
      "Oh",
      "Oh",
      "Oh",
      "Srivastava",
      "Nitish Srivastava",
      "Elman Mansimov",
      "Ruslan Salakhutdinov"
    ],
    "review": "[UPDATE]\nAfter going through the response from the author and the revision, I increased my review score for two reasons.\n1. I thank the reviewers for further investigating the difference between yours and the other work (Scheduled sampling, Unsupervised learning using LSTM) and providing some insights about it.\nThis paper at least shows empirically that 100%-Pred scheme is better for high-dimensional video and for long-term predictions.\nIt would be good if the authors briefly discuss this in the final revision (either in the appendix or in the main text).\n\n2. The revised paper contains more comprehensive results than before.\nThe presented result and discussion in this paper will be quite useful to the research community as high-dimensional video prediction involves large-scale experiments that are computationally expensive.\n\n- Summary\nThis paper presents a new RNN architecture for action-conditional future prediction. The proposed architecture combines actions into the recurrent connection of the LSTM core, which performs better than the previous state-of-the-art architecture [Oh et al.]. The paper also explores and compares different architectures such as frame-dependent/independent mode and observation/prediction-dependent architectures. The experimental result shows that the proposed architecture with fully prediction-dependent training scheme achieves the state-of-the-art performance on several complex visual domains. It is also shown that the proposed prediction architecture can be used to improve exploration in a 3D environment.\n\n- Novelty\nThe novelty of the proposed architecture is not strong. The difference between [Oh et al.] and this work is that actions are combined into the LSTM in this paper, while actions are combined after LSTM in [Oh et al.]. The jumpy prediction was already introduced by [Srivastava et al.] in the deep learning area. \n\n- Experiment\nThe experiments are well-designed and thorough. Specifically, the paper evaluates different training schemes and compares different architectures using several rich domains (Atari, 3D worlds). Besides, the proposed method achieves the state-of-the-art results on many domains and presents an application for model-based exploration.\n\n- Clarity\nThe paper is well-written and easy to follow.\n\n- Overall \nAlthough the proposed architecture is not much novel, it achieves promising results on Atari games and 3D environments. In addition, the systematic evaluation of different architectures presented in the paper would be useful to the community.\n\n[Reference]\nNitish Srivastava, Elman Mansimov, Ruslan Salakhutdinov. Unsupervised Learning with LSTMs. ICML 2016."
  },
  {
    "people": [
      "Oh",
      "Oh",
      "Oh",
      "Bengio",
      "Oh",
      "Oh",
      "Oh"
    ],
    "review": "The paper presents an action-conditional recurrent network that can predict frames in video games hundreds of steps in the future. The paper claims three main contributions: \n1. modification to model architecture (used in Oh et al.) by using action at time t-1 to directly predict hidden state at t\n2. exploring the idea of jumpy predictions (predictions multiple frames in future without using intermediate frames)\n3. exploring different training schemes (trade-off between observation and prediction frames for training LSTM)\n\n1. modification to model architecture\n+ The motivation seems good that in past work (Oh et al.) the action at t-1 influences x_t, but not the state h_t of the LSTM. This could be fixed by making the LSTM state h_t dependent on a_{t-1}\n- However, this is of minor technical novelty. Also, as pointed in reviewer questions, a similar effect could be achieved by adding a_t-1 as an input to the LSTM at time t. This could be done without modifying the LSTM architecture as stated in the paper. While the authors claim that combining a_t-1 with h_t-1 and s_t-1 performs worse than the current method which combines a_t-1 only with h_t-1, I would have liked to see the empirical difference in combining a_t-1 only with s_t-1 or only with h_t-1. Also, a stronger motivation is required to support the current formulation.\n- Further, the benefits of this change in architecture is not well analyzed in experiments. Fig. 5(a) provides the difference between Oh et al. (with traditional LSTM) and current method. However, the performance difference is composed of 2 components (difference in training scheme and architecture). This contribution of the architecture to the performance is not clear from this experiment. The authors did claim in the pre-review phase that Fig. 12 (a) shows the difference in performance only due to architecture for \"Seaquest\". However, from this plot it appears that the gain at 100-steps (~15)  is only a small fraction of the overall gain in Fig. 5 (a) (~90). It is difficult to judge the significance of the architecture modification from this result for one game.\n\n2. Exploring the idea of jumpy predictions:\n+ As stated by the authors, omitting the intermediate frames while predicting future frames could significantly sppedup simulations.\n+ The results in Fig. 5(b) present some interesting observations that omitting intermediate frames does not lead to significant error-increase for at least a few games.\n- However, it is again not clear whether the modification in the current model leads to this effect or it could be achieved by previous models like Oh et al.\n- While, the observations themselves are interesting, it would have been better to provide a more detailed analysis for more games. Also, the novelty in dropping intermediate frames for speedup is marginal.\n\n3. Exploring different training schemes\n+ This is perhaps the most interesting observation presented in the paper. The authors present the difference in performance for different training schemes in Fig. 2(a). The training schemes are varied based on the fraction of training phase which only uses observation frames and the fraction that uses only prediction frames.\n+ The results show that this change in training can significantly affect prediction results and is the biggest contributor to performance improvement compared to Oh et al.\n- While this observation is interesting, this effect has been previously explored in detail in other works like schedule sampling (Bengio et al.) and to some extent in Oh et al.\n\nClarity of presentation:\n- The exact experimental setup is not clearly stated for some of the results. For instance, the paper does not say that Fig. 2(a) uses the same architecture as Oh et al. However, this is stated in the response to reviewer questions.\n- Fig. 4 is difficult to interpret. The qualitative difference between Oh et al. and current method could be highlighted explicitly. \n- Minor: The qualitative analysis section requires the reader to navigate to various video-links in order to understand the section. This leads to a discontinuity in reading and is particularly difficult while reading a printed-copy.\n\nOverall, the paper presents some interesting experimental observations. However, the technical novelty and contribution of the proposed architecture and training scheme is not clear."
  },
  {
    "people": [
      "Bengio",
      "Bengio",
      "Bengio",
      "Bengio",
      "Bengio",
      "Elman Mansimov",
      "Ruslan Salakhutdinov"
    ],
    "review": "We have thought more deeply about the fact that the results in Bengio et al. appear to be in contradiction with ours, in the sense that the Always Sampling training scheme (corresponding to our 100% Pred. Frames training scheme) seems to perform worse than mixed schemes. We also had a chat with one of the authors of the Bengio et al. paper to get a better understanding of the methods and experiments. \nWe have reached the conclusion that the difference might be due to the fact that Bengio et al. focus on discrete problems and to the fact that we were mostly reasoning about long-term prediction, whilst Bengio et al. focus on shorter-term prediction (e.g., in Image Captioning Section, the average prediction length is 11 (this is not mentioned in the paper, but has been pointed out to us by one of the authors)) -- if we look at our results for short-term prediction only, they do not look so much different anymore.\n\nIn Figures 12-16 of the latest version of our paper, in addition to the prediction error at time-step 100, we also plot the prediction error at time-steps 5 and 10 for most games. We can notice that the prediction error with the 100% and the 0%-100% Pred. Frames training schemes (called here Schemes I and II) (red and dark green lines) is almost never lower than the prediction error with the other mixed schemes for time-steps 5 and 10, and often higher (see for example Fishing Derby in Fig. 13). The situation is different at time-step 100, where Schemes I and II are always almost preferable to the other mixed schemes. Therefore, by looking only at the error up to time-step 10, we would also prefer other schemes to Schemes I and II. The error is higher at lower time-steps with Schemes I and II as the objects are less sharply represented in the frames. In other words, these two schemes capture the global dynamics (as this enables better long-term prediction) at the expense of not representing the details very accurately, as lack of the details at earlier predictions do not harm subsequent predictions. In Bengio et al., where the problem is discrete, one error at the beginning of the sequence might lead to drastic errors at subsequent time-steps.\nWe thank the reviewer for this question, as he made us thinking more carefully about this point. We need to modify the paper to include as summary of this discussion.\n\nWe also thank the reviewer for pointing out that in \u201cNitish Srivastava, Elman Mansimov, Ruslan Salakhutdinov. Unsupervised Learning with LSTMs. ICML 2016\u201d, the unconditional model can perform jumpy prediction. We briefly mention this paper in the introduction at the moment, but we need to add a discussion. Notice that, in this paper, prediction is shorter-term than in our case, namely 10-13 time-steps ahead. We are not sure whether, at training time, the real or the generated frames are fed in the conditioned model. From the sentence \u201cAt test time we feed in the generated frame from the previous step without adding any noise. At training time we feed in the ground truth.\u201d in Section 2.3, it would look like the model is trained with feeding in the real frames, i.e. with the 0% Pred. Frames training scheme. This approach seems to work better than the unconditional model -- this is surprising to us, as it is in contradiction with our results.\n"
  },
  {
    "people": [
      "Oh",
      "Oh",
      "Oh",
      "Oh",
      "Oh",
      "Bengio",
      "Oh",
      "Bengio",
      "Bengio",
      "Oh",
      "Oh"
    ],
    "review": "We tried to address the reviewer's concerns by adding a substantial amount of new results and discussion, mainly to the Appendix (notice that some experiments are still running, thus many figures will need to be updated). We highlighted the modifications in red. We did not finish making all the modifications: we mostly still need to improve the main text. However, we decided to send a first update to give the reviewers the time to digest the new material, and to make the remaining changes in the next few days.\n\nBelow we describe in some detail how we addressed the reviewer's comments.\n\n1. Modification to model architecture.\n\nTo address the reviewer's concern about lack of comparison of different ways of incorporating the action, we added a section in the Appendix (Section B.1.3, Figures 19 and 20) that compares the architecture used in the main text (combining the action a_t-1 only with h_t-1 in a multiplicative way), with 8 alternatives, namely combining a_t-1 with the frame before encoding (ConvA), combining a_t-1 only with the encoded frame s_t-1, combining a_t-1 with both h_t-1 and s_t-1 in 4 possible ways, having a multiplicative/additive interaction of a_t-1 with h_t-1, and considering the action as an additional input. \nFrom the figures and videos it looks like combining the action with the frame (dark green lines) is a bad idea, and that there is no much difference in performance among the remaining approaches, although combining a_t-1 with both h_t-1 and s_t-1 (W^h h_{t-1}xW^s s_{t-1}x W^{a} a_{t-1)) seems to be overall a bit better (notice that this architecture results in a quite different core than the traditional LSTM core).\nThe experiments are still running -- the experiments with considering the action as an additional input (light green lines) are behind the other experiments.\n\nWe also modified the section in the Appendix describing the difference between our direct action influence and the indirect action influence as in Oh et al. (now at the end of Section B.1.3). We changed Fig. 12 in the previous version with Figure 21 to show the difference in the other games in addition to Seaquest (we need to add Riverraid for which we want to use 2 subsequences of length T=15). \nDirect action influence performs similarly to indirect action influence in games with lower prediction error, such as Bowling, Freeway and Pong, but better in games with higher prediction error. The most striking difference is with Seaquest, which is most evident when looking at the videos (is is difficult to judge the difference in performance by looking at the prediction error only, that's why we added randomly selected videos from which the types of errors in the prediction can be visually understood). These results are computed using the best training scheme for Seaquest. The videos show that, with indirect action influence, the simulator models the fish quite poorly, even though using a training scheme that more strongly encourages long-term accuracy enables the simulator to learn the appearance of the fish better than with the training scheme of Oh et al. Therefore, these videos clearly show that the training scheme alone is not sufficient, our architecture plays a crucial role in substantially improving the prediction. Having indirect action influence also makes it more difficult to correctly update the score in some games such as Seaquest and Fishing Derby. Therefore, whilst the training scheme is the major responsible for improving long-term prediction, it needs to be combined with an appropriate architecture in order to work. \n\nAll together, the experiments in Section B.1.3 suggest that having a direct (as opposed to Oh at al.) and global (as opposed to ConvA) action influence on the state dynamics is much preferable. On the other hand, performance is less sensitive to different ways of imposing direct action influence. We need to improve the discussion in the paper about this point.\n\n\n2. Exploring the idea of jumpy predictions.\n\nTo address the reviewer's concerns about lack of a detailed analysis, we added a new section in the Appendix (Section B.2, Figures 25 and 26), and modified Figure 5(b) to include all games (except Breakout for which the prediction error is uninformative -- see the discussion about this game in Section B.1.1). Section B.2 compares different architectures, sequence lengths and number of subsequences (we plan to add a few more experiments in the next few weeks). \nIt is clear from the results that the jumpy simulator is much more sensitive to changes of structure, and therefore fragile, than the single-step simulator. However, we show that with the right training structure, the jumpy simulator can be accurate in many games. Having accurate and fast to use simulators is essential for model-based reinforcement learning. Thus, whilst the novelty might seems marginal, it is of extreme interest for the community to advance in this type of approaches. Furthermore, accurate jumpy approaches are very difficult to obtain -- we believe that we are the first ones showing some success for long-term prediction in large scale domains.\n\nWe do not understand the sentence 'However, it is again not clear whether the modification in the current model leads to this effect or it could be achieved by previous models like Oh et al.'. The model of Oh et al. cannot be modified to perform jumpy predictions as, when omitting intermediate frames, the corresponding actions do not have any influence (as they influence the hidden states indirectly through the frames). This is actually an important reason why a direct action influence is preferable.\n\n\n3. Exploring different training schemes.\n\nWe would like to comment on the sentence \u201cWhile this observation is interesting, this effect has been previously explored in detail in other works like schedule sampling (Bengio et al.) and to some extent in Oh et al.?\u201d\nThe schedule sampling work of Bengio et al. focuses on different applications, with discrete data and on shorter-term prediction (for example, the length of sequences in the Image Captioning example (Section 4.1) is 11 on average). In this applications, the 100% Pred. Frames training scheme (called Always Sampling) actually performs poorly. Thus, based on the results of Bengio et al., we would never use the 100% Pred. Frames. We show that, in simpler games, this is actually the scheme that performs best, and that, in more complex games, a scheme that is as close as possible to such a scheme is preferable. Our analysis (especially in the new material) shows in detail the characteristics of each scheme on both short-term and long-term prediction. Thus, we make a significant contribution in advancing the understanding of different training schemes in frame prediction.\nOh et al. used a mixed scheme without any explanation. Thus, it is totally unclear from their paper the importance of using a mixed scheme and what are the differences of different types of mixing. That's actually the reason that motivatesd us to perform a detailed analysis. \n\nTo improve the understanding of the results, we have added a section in the Appendix (Section B.1.1) that discusses the characteristics and effects of different training schemes on each game.\n\nIn conclusion, we addressed the reviewer's concerns on the analysis by adding more results and discussion. Regarding the concern on little novelty, we strongly disagree that this should be a concern. We greatly improved on the performance of previous methods (it is enough to compare the videos in Oh at al. with ours to realize that there is an enormous difference). Even more importantly, we improved the understanding of these methods with a detailed analysis. We believe that papers like our paper are as important for the community as papers that provide completely novel architectures without much analysis. \n\nFinally, the reviewer should understand that providing a detailed analysis and understanding is extremely challenging and time consuming, due to both the large scale and the complexity of the problem. Ours, whilst not fully comprehensive (as this is not possible), represents an enormous effort in trying to provide an in depth analysis and advance the understanding on the problem of high dimensional frame prediction."
  },
  {
    "people": [
      "Oh",
      "Oh",
      "Oh",
      "Srivastava",
      "Nitish Srivastava",
      "Elman Mansimov",
      "Ruslan Salakhutdinov"
    ],
    "review": "[UPDATE]\nAfter going through the response from the author and the revision, I increased my review score for two reasons.\n1. I thank the reviewers for further investigating the difference between yours and the other work (Scheduled sampling, Unsupervised learning using LSTM) and providing some insights about it.\nThis paper at least shows empirically that 100%-Pred scheme is better for high-dimensional video and for long-term predictions.\nIt would be good if the authors briefly discuss this in the final revision (either in the appendix or in the main text).\n\n2. The revised paper contains more comprehensive results than before.\nThe presented result and discussion in this paper will be quite useful to the research community as high-dimensional video prediction involves large-scale experiments that are computationally expensive.\n\n- Summary\nThis paper presents a new RNN architecture for action-conditional future prediction. The proposed architecture combines actions into the recurrent connection of the LSTM core, which performs better than the previous state-of-the-art architecture [Oh et al.]. The paper also explores and compares different architectures such as frame-dependent/independent mode and observation/prediction-dependent architectures. The experimental result shows that the proposed architecture with fully prediction-dependent training scheme achieves the state-of-the-art performance on several complex visual domains. It is also shown that the proposed prediction architecture can be used to improve exploration in a 3D environment.\n\n- Novelty\nThe novelty of the proposed architecture is not strong. The difference between [Oh et al.] and this work is that actions are combined into the LSTM in this paper, while actions are combined after LSTM in [Oh et al.]. The jumpy prediction was already introduced by [Srivastava et al.] in the deep learning area. \n\n- Experiment\nThe experiments are well-designed and thorough. Specifically, the paper evaluates different training schemes and compares different architectures using several rich domains (Atari, 3D worlds). Besides, the proposed method achieves the state-of-the-art results on many domains and presents an application for model-based exploration.\n\n- Clarity\nThe paper is well-written and easy to follow.\n\n- Overall \nAlthough the proposed architecture is not much novel, it achieves promising results on Atari games and 3D environments. In addition, the systematic evaluation of different architectures presented in the paper would be useful to the community.\n\n[Reference]\nNitish Srivastava, Elman Mansimov, Ruslan Salakhutdinov. Unsupervised Learning with LSTMs. ICML 2016."
  },
  {
    "people": [
      "Oh",
      "Oh",
      "Oh",
      "Bengio",
      "Oh",
      "Oh",
      "Oh"
    ],
    "review": "The paper presents an action-conditional recurrent network that can predict frames in video games hundreds of steps in the future. The paper claims three main contributions: \n1. modification to model architecture (used in Oh et al.) by using action at time t-1 to directly predict hidden state at t\n2. exploring the idea of jumpy predictions (predictions multiple frames in future without using intermediate frames)\n3. exploring different training schemes (trade-off between observation and prediction frames for training LSTM)\n\n1. modification to model architecture\n+ The motivation seems good that in past work (Oh et al.) the action at t-1 influences x_t, but not the state h_t of the LSTM. This could be fixed by making the LSTM state h_t dependent on a_{t-1}\n- However, this is of minor technical novelty. Also, as pointed in reviewer questions, a similar effect could be achieved by adding a_t-1 as an input to the LSTM at time t. This could be done without modifying the LSTM architecture as stated in the paper. While the authors claim that combining a_t-1 with h_t-1 and s_t-1 performs worse than the current method which combines a_t-1 only with h_t-1, I would have liked to see the empirical difference in combining a_t-1 only with s_t-1 or only with h_t-1. Also, a stronger motivation is required to support the current formulation.\n- Further, the benefits of this change in architecture is not well analyzed in experiments. Fig. 5(a) provides the difference between Oh et al. (with traditional LSTM) and current method. However, the performance difference is composed of 2 components (difference in training scheme and architecture). This contribution of the architecture to the performance is not clear from this experiment. The authors did claim in the pre-review phase that Fig. 12 (a) shows the difference in performance only due to architecture for \"Seaquest\". However, from this plot it appears that the gain at 100-steps (~15)  is only a small fraction of the overall gain in Fig. 5 (a) (~90). It is difficult to judge the significance of the architecture modification from this result for one game.\n\n2. Exploring the idea of jumpy predictions:\n+ As stated by the authors, omitting the intermediate frames while predicting future frames could significantly sppedup simulations.\n+ The results in Fig. 5(b) present some interesting observations that omitting intermediate frames does not lead to significant error-increase for at least a few games.\n- However, it is again not clear whether the modification in the current model leads to this effect or it could be achieved by previous models like Oh et al.\n- While, the observations themselves are interesting, it would have been better to provide a more detailed analysis for more games. Also, the novelty in dropping intermediate frames for speedup is marginal.\n\n3. Exploring different training schemes\n+ This is perhaps the most interesting observation presented in the paper. The authors present the difference in performance for different training schemes in Fig. 2(a). The training schemes are varied based on the fraction of training phase which only uses observation frames and the fraction that uses only prediction frames.\n+ The results show that this change in training can significantly affect prediction results and is the biggest contributor to performance improvement compared to Oh et al.\n- While this observation is interesting, this effect has been previously explored in detail in other works like schedule sampling (Bengio et al.) and to some extent in Oh et al.\n\nClarity of presentation:\n- The exact experimental setup is not clearly stated for some of the results. For instance, the paper does not say that Fig. 2(a) uses the same architecture as Oh et al. However, this is stated in the response to reviewer questions.\n- Fig. 4 is difficult to interpret. The qualitative difference between Oh et al. and current method could be highlighted explicitly. \n- Minor: The qualitative analysis section requires the reader to navigate to various video-links in order to understand the section. This leads to a discontinuity in reading and is particularly difficult while reading a printed-copy.\n\nOverall, the paper presents some interesting experimental observations. However, the technical novelty and contribution of the proposed architecture and training scheme is not clear."
  },
  {
    "people": [
      "Mnih"
    ],
    "review": "This paper provides a simple method to handle action repetitions. They make the action a tuple (a,x), where a is the action chosen, and x the number of repetitions. Overall they report some improvements over A3C/DDPG, dramatic in some games, moderate in other. The idea seems natural and there is a wealth of experiment to support it.\n\nComments:\n\n- The scores reported on A3C in this paper and in the Mnih et al. publication (table S3) differ significantly. Where does this discrepancy come from? If it's from a different training regime (fewer iterations, for instance), did the authors confirm that running  their replication to the same settings as Mnih et al provide similar results?\n\n- It is intriguing that the best results of FiGAR are reported on games where few actions repeat dominate. This seems to imply that for those, the performance overhead of FiGAR over A3C is high since A3C uses an action repeat of 4 (and therefore has 4 times fewer gradient updates). A3C could be run for a comparable computation cost with a lower action repeat, which would probably result in increased performance of A3C.  Nevertheless,  the automatic determination of the appropriate action repeat is interesting, even if the overall message seems to be to not repeat actions too often.\n\n- Slightly problematic notation, where r sometimes denotes rewards, sometimes denotes elements of the repetition set R (top of page 5)\n\n- In the equation at the bottom of page 5 - since the sum is not indexed over decision steps, not time steps, shouldn't the rewards r_k be modified to be the sum of rewards (appropriately discounted) between those time steps?\n\n- The section on DDPG is confusingly written. \"Concatenating\" loss is a strange operation; doesn't FiGAR correspond to a loss to roughly looks like Q(x,mu(x)) + R log p(x) (with separate loss for learning the critic)? It feels that REINFORCE should be applied for the repetition variable x (second term of the sum) and reparametrization for the action a (first term)? \n\n- Is the 'name_this_game' name in the tables  intentional?\n\n- A potential weakness of the method is that the agent must decide to commit to an action for a fixed number of steps, independently of what happens next. Have the authors considered a scheme in which, at each time step, the agent decides to stick with the current decision or not? (It feels like it might be a relatively simple modification of FiGAR)."
  },
  {
    "people": [
      "AnonReviewer3",
      "AnonReviewer3",
      "AnonReviewer3",
      "AnonReviewer4",
      "AnonReviewer4",
      "AnonReviewer2",
      "AnonReviewer2",
      "AnonReviewer2"
    ],
    "review": "We thank all the reviewers for asking interesting questions and pointing out important flaws in the paper. We have uploaded a revised version of the paper that we believe addresses the questions raised. Major features of the revision are:\n\n1. We have added results on 2 more Atari 2600 games: Enduro and Q-bert. FiGAR seems to improve performance rather dramatically on Enduro with the FiGAR agent being close to 100 times better than the baseline A3C agent. (Note that the baseline agent performs very poorly according to the published results as well)\n\n2. In response to AnonReviewer3\u2019s comment about skipping intermediate frames, we have added Appendix F (page 23) by conducting experiments on what happens when FiGAR does not discard any intermediate frames (during evaluation phase). The general pattern seems to be that for games wherein lower action repetition is preferred, gains are made in terms of improved gameplay performance. However, for 24 out of 33 games the performance becomes worse, which depicts the importance of the temporal abstractions learnt by the action repetition part of the policy (\\pi_{\\theta_{x}}). This does not address the reviewer\u2019s question completely since at train time we still skip all the frames, as suggested by the action repetition policy. We have added a small discussion on future works section (section 6, page 10) which could potentially address this comment.\n\n3. In response to AnonReviewer3\u2019s suggestion to turn table1 into a bar graph we have done so (Figure 3, page 8) and it indeed does look much better.\n\n4. In response to AnonReviewer3\u2019s suggestion to compare directly to STRAW we have added Table 5 (Appendix A, page 14) which contains performance of STRAW models on all games which we have also experimented with. The general conclusion seems to be that in some games STRAW does better and in some games FiGAR does better.\n\n5. In response to AnonReviewer4\u2019s comment, we conducted experiments on shared representations for the FiGAR-TRPO agent. Appendix G (page 24) contains the results of the experiments. In general we observe that FiGAR-TRPO with shared representations does marginally better than FiGAR-TRPO, but not much better. The performance goes down on some tasks and improves on others. The average action repetition rate of the best policies learnt improves.\n\n6. In response to AnonReviewer4\u2019s comment on SMDPs we have added the relevant discussion to related works section (page 3).\n\n7. In response to AnonReviewer2\u2019s comment on the confusing nature of FiGAR-DDPG section, we have rewritten the section. It is hopefully clearer now.\n \n8. In response to AnonReviewer2\u2019s comment on the confusing notation \u2018r\u2019 for action repetition we have completely changed the notation for action repetition to the letter \u2018w\u2019.\n\n9. In response to AnonReviewer2\u2019s comment on  the potential weakness of the FiGAR framework, we have added a discussion on the shortcomings of the FiGAR in section 6 (page 10).\n\n10.We have corrected several typos as pointed out by the reviewers. \n"
  },
  {
    "people": [
      "Vezhnevets"
    ],
    "review": "This paper shows that extending deep RL algorithms to decide which action to take as well as how many times to repeat it leads to improved performance on a number of domains. The evaluation is very thorough and shows that this simple idea works well in both discrete and continuous actions spaces.\n\nA few comments/questions:\n- Table 1 could be easier to interpret as a figure of histograms.\n- Figure 3 could be easier to interpret as a table.\n- How was the subset of Atari games selected?\n- The Atari evaluation does show convincing improvements over A3C on games requiring extended exploration (e.g. Freeway and Seaquest), but it would be nice to see a full evaluation on 57 games. This has become quite standard and would make it possible to compare overall performance using mean and median scores.\n- It would also be nice to see a more direct comparison to the STRAW model of Vezhnevets et al., which aims to solve some of the same problems as FiGAR.\n- FiGAR currently discards frames between action decisions. There might be a tradeoff between repeating an action more times and throwing away more information. Have you thought about separating these effects? You could train a model that does process intermediate frames. Just a thought.\n\nOverall, this is a nice simple addition to deep RL algorithms that many people will probably start using.\n\n--------------------\n\nI'm increasing my score to 8 based on the rebuttal and the revised paper."
  },
  {
    "people": [
      "Mnih"
    ],
    "review": "This paper provides a simple method to handle action repetitions. They make the action a tuple (a,x), where a is the action chosen, and x the number of repetitions. Overall they report some improvements over A3C/DDPG, dramatic in some games, moderate in other. The idea seems natural and there is a wealth of experiment to support it.\n\nComments:\n\n- The scores reported on A3C in this paper and in the Mnih et al. publication (table S3) differ significantly. Where does this discrepancy come from? If it's from a different training regime (fewer iterations, for instance), did the authors confirm that running  their replication to the same settings as Mnih et al provide similar results?\n\n- It is intriguing that the best results of FiGAR are reported on games where few actions repeat dominate. This seems to imply that for those, the performance overhead of FiGAR over A3C is high since A3C uses an action repeat of 4 (and therefore has 4 times fewer gradient updates). A3C could be run for a comparable computation cost with a lower action repeat, which would probably result in increased performance of A3C.  Nevertheless,  the automatic determination of the appropriate action repeat is interesting, even if the overall message seems to be to not repeat actions too often.\n\n- Slightly problematic notation, where r sometimes denotes rewards, sometimes denotes elements of the repetition set R (top of page 5)\n\n- In the equation at the bottom of page 5 - since the sum is not indexed over decision steps, not time steps, shouldn't the rewards r_k be modified to be the sum of rewards (appropriately discounted) between those time steps?\n\n- The section on DDPG is confusingly written. \"Concatenating\" loss is a strange operation; doesn't FiGAR correspond to a loss to roughly looks like Q(x,mu(x)) + R log p(x) (with separate loss for learning the critic)? It feels that REINFORCE should be applied for the repetition variable x (second term of the sum) and reparametrization for the action a (first term)? \n\n- Is the 'name_this_game' name in the tables  intentional?\n\n- A potential weakness of the method is that the agent must decide to commit to an action for a fixed number of steps, independently of what happens next. Have the authors considered a scheme in which, at each time step, the agent decides to stick with the current decision or not? (It feels like it might be a relatively simple modification of FiGAR)."
  },
  {
    "people": [
      "Duan",
      "Durugkar"
    ],
    "review": "This paper proposes a simple but effective extension to reinforcement learning algorithms, by adding a temporal repetition component as part of the action space, enabling the policy to select how long to repeat the chosen action for. The extension applies to all reinforcement learning algorithms, including both discrete and continuous domains, as it is primarily changing the action parametrization. The paper is well-written, and the experiments extensively evaluate the approach with 3 different RL algorithms in 3 different domains (Atari, MuJoCo, and TORCS).\n\nHere are some comments and questions, for improving the paper:\n\nThe introduction states that \"all DRL algorithms repeatedly execute a chosen action for a fixed number of time steps k\". This statement is too strong, and is actually disproved in the experiments \u2014 repeating an action is helpful in many tasks, but not in all tasks. The sentence should be rephrased to be more precise.\n\nIn the related work, a discussion of the relation to semi-MDPs would be useful to help the reader better understand the approach and how it compares and differs (e.g. the response from the pre-review questions)\n\nExperiments:\nCan you provide error bars on the experimental results? (from running multiple random seeds)\n\nIt would be useful to see experiments with parameter sharing in the TRPO experiments, to be more consistent with the other domains, especially since it seems that the improvement in the TRPO experiments is smaller than that of the other two domains. Right now, it is hard to tell if the smaller improvement is because of the nature of the task, because of the lack of parameter sharing, or something else.\n\nThe TRPO evaluation is different from the results reported in Duan et al. ICML \u201916. Why not use the same benchmark?\n\nVideos only show the policies learned with FiGAR, which are uninformative without also seeing the policies learned without FiGAR. Can you also include videos of the policies learned without FiGAR, as a comparison point?\n\nHow many laps does DDPG complete without FiGAR? The difference in reward achieved seems quite substantial (557K vs. 59K).\n\nCan the tables be visualized as histograms? This seems like it would more effectively and efficiently communicate the results.\n\nMinor comments:\n-- On the plot in Figure 2, the label for the first bar should be changed from 1000 to 3500.\n-- \u201cidea of deciding when necessary\u201d - seems like it would be better to say \u201cidea of only deciding when necessary\"\n-- \"spaces.Durugkar et al.\u201d \u2014 missing a space.\n-- \u201cR={4}\u201d \u2014 why 4? Could you use a letter to indicate a constant instead? (or a different notation)\n"
  },
  {
    "people": [
      "Mnih"
    ],
    "review": "This paper provides a simple method to handle action repetitions. They make the action a tuple (a,x), where a is the action chosen, and x the number of repetitions. Overall they report some improvements over A3C/DDPG, dramatic in some games, moderate in other. The idea seems natural and there is a wealth of experiment to support it.\n\nComments:\n\n- The scores reported on A3C in this paper and in the Mnih et al. publication (table S3) differ significantly. Where does this discrepancy come from? If it's from a different training regime (fewer iterations, for instance), did the authors confirm that running  their replication to the same settings as Mnih et al provide similar results?\n\n- It is intriguing that the best results of FiGAR are reported on games where few actions repeat dominate. This seems to imply that for those, the performance overhead of FiGAR over A3C is high since A3C uses an action repeat of 4 (and therefore has 4 times fewer gradient updates). A3C could be run for a comparable computation cost with a lower action repeat, which would probably result in increased performance of A3C.  Nevertheless,  the automatic determination of the appropriate action repeat is interesting, even if the overall message seems to be to not repeat actions too often.\n\n- Slightly problematic notation, where r sometimes denotes rewards, sometimes denotes elements of the repetition set R (top of page 5)\n\n- In the equation at the bottom of page 5 - since the sum is not indexed over decision steps, not time steps, shouldn't the rewards r_k be modified to be the sum of rewards (appropriately discounted) between those time steps?\n\n- The section on DDPG is confusingly written. \"Concatenating\" loss is a strange operation; doesn't FiGAR correspond to a loss to roughly looks like Q(x,mu(x)) + R log p(x) (with separate loss for learning the critic)? It feels that REINFORCE should be applied for the repetition variable x (second term of the sum) and reparametrization for the action a (first term)? \n\n- Is the 'name_this_game' name in the tables  intentional?\n\n- A potential weakness of the method is that the agent must decide to commit to an action for a fixed number of steps, independently of what happens next. Have the authors considered a scheme in which, at each time step, the agent decides to stick with the current decision or not? (It feels like it might be a relatively simple modification of FiGAR)."
  },
  {
    "people": [
      "AnonReviewer3",
      "AnonReviewer3",
      "AnonReviewer3",
      "AnonReviewer4",
      "AnonReviewer4",
      "AnonReviewer2",
      "AnonReviewer2",
      "AnonReviewer2"
    ],
    "review": "We thank all the reviewers for asking interesting questions and pointing out important flaws in the paper. We have uploaded a revised version of the paper that we believe addresses the questions raised. Major features of the revision are:\n\n1. We have added results on 2 more Atari 2600 games: Enduro and Q-bert. FiGAR seems to improve performance rather dramatically on Enduro with the FiGAR agent being close to 100 times better than the baseline A3C agent. (Note that the baseline agent performs very poorly according to the published results as well)\n\n2. In response to AnonReviewer3\u2019s comment about skipping intermediate frames, we have added Appendix F (page 23) by conducting experiments on what happens when FiGAR does not discard any intermediate frames (during evaluation phase). The general pattern seems to be that for games wherein lower action repetition is preferred, gains are made in terms of improved gameplay performance. However, for 24 out of 33 games the performance becomes worse, which depicts the importance of the temporal abstractions learnt by the action repetition part of the policy (\\pi_{\\theta_{x}}). This does not address the reviewer\u2019s question completely since at train time we still skip all the frames, as suggested by the action repetition policy. We have added a small discussion on future works section (section 6, page 10) which could potentially address this comment.\n\n3. In response to AnonReviewer3\u2019s suggestion to turn table1 into a bar graph we have done so (Figure 3, page 8) and it indeed does look much better.\n\n4. In response to AnonReviewer3\u2019s suggestion to compare directly to STRAW we have added Table 5 (Appendix A, page 14) which contains performance of STRAW models on all games which we have also experimented with. The general conclusion seems to be that in some games STRAW does better and in some games FiGAR does better.\n\n5. In response to AnonReviewer4\u2019s comment, we conducted experiments on shared representations for the FiGAR-TRPO agent. Appendix G (page 24) contains the results of the experiments. In general we observe that FiGAR-TRPO with shared representations does marginally better than FiGAR-TRPO, but not much better. The performance goes down on some tasks and improves on others. The average action repetition rate of the best policies learnt improves.\n\n6. In response to AnonReviewer4\u2019s comment on SMDPs we have added the relevant discussion to related works section (page 3).\n\n7. In response to AnonReviewer2\u2019s comment on the confusing nature of FiGAR-DDPG section, we have rewritten the section. It is hopefully clearer now.\n \n8. In response to AnonReviewer2\u2019s comment on the confusing notation \u2018r\u2019 for action repetition we have completely changed the notation for action repetition to the letter \u2018w\u2019.\n\n9. In response to AnonReviewer2\u2019s comment on  the potential weakness of the FiGAR framework, we have added a discussion on the shortcomings of the FiGAR in section 6 (page 10).\n\n10.We have corrected several typos as pointed out by the reviewers. \n"
  },
  {
    "people": [
      "Vezhnevets"
    ],
    "review": "This paper shows that extending deep RL algorithms to decide which action to take as well as how many times to repeat it leads to improved performance on a number of domains. The evaluation is very thorough and shows that this simple idea works well in both discrete and continuous actions spaces.\n\nA few comments/questions:\n- Table 1 could be easier to interpret as a figure of histograms.\n- Figure 3 could be easier to interpret as a table.\n- How was the subset of Atari games selected?\n- The Atari evaluation does show convincing improvements over A3C on games requiring extended exploration (e.g. Freeway and Seaquest), but it would be nice to see a full evaluation on 57 games. This has become quite standard and would make it possible to compare overall performance using mean and median scores.\n- It would also be nice to see a more direct comparison to the STRAW model of Vezhnevets et al., which aims to solve some of the same problems as FiGAR.\n- FiGAR currently discards frames between action decisions. There might be a tradeoff between repeating an action more times and throwing away more information. Have you thought about separating these effects? You could train a model that does process intermediate frames. Just a thought.\n\nOverall, this is a nice simple addition to deep RL algorithms that many people will probably start using.\n\n--------------------\n\nI'm increasing my score to 8 based on the rebuttal and the revised paper."
  },
  {
    "people": [
      "Mnih"
    ],
    "review": "This paper provides a simple method to handle action repetitions. They make the action a tuple (a,x), where a is the action chosen, and x the number of repetitions. Overall they report some improvements over A3C/DDPG, dramatic in some games, moderate in other. The idea seems natural and there is a wealth of experiment to support it.\n\nComments:\n\n- The scores reported on A3C in this paper and in the Mnih et al. publication (table S3) differ significantly. Where does this discrepancy come from? If it's from a different training regime (fewer iterations, for instance), did the authors confirm that running  their replication to the same settings as Mnih et al provide similar results?\n\n- It is intriguing that the best results of FiGAR are reported on games where few actions repeat dominate. This seems to imply that for those, the performance overhead of FiGAR over A3C is high since A3C uses an action repeat of 4 (and therefore has 4 times fewer gradient updates). A3C could be run for a comparable computation cost with a lower action repeat, which would probably result in increased performance of A3C.  Nevertheless,  the automatic determination of the appropriate action repeat is interesting, even if the overall message seems to be to not repeat actions too often.\n\n- Slightly problematic notation, where r sometimes denotes rewards, sometimes denotes elements of the repetition set R (top of page 5)\n\n- In the equation at the bottom of page 5 - since the sum is not indexed over decision steps, not time steps, shouldn't the rewards r_k be modified to be the sum of rewards (appropriately discounted) between those time steps?\n\n- The section on DDPG is confusingly written. \"Concatenating\" loss is a strange operation; doesn't FiGAR correspond to a loss to roughly looks like Q(x,mu(x)) + R log p(x) (with separate loss for learning the critic)? It feels that REINFORCE should be applied for the repetition variable x (second term of the sum) and reparametrization for the action a (first term)? \n\n- Is the 'name_this_game' name in the tables  intentional?\n\n- A potential weakness of the method is that the agent must decide to commit to an action for a fixed number of steps, independently of what happens next. Have the authors considered a scheme in which, at each time step, the agent decides to stick with the current decision or not? (It feels like it might be a relatively simple modification of FiGAR)."
  },
  {
    "people": [
      "Duan",
      "Durugkar"
    ],
    "review": "This paper proposes a simple but effective extension to reinforcement learning algorithms, by adding a temporal repetition component as part of the action space, enabling the policy to select how long to repeat the chosen action for. The extension applies to all reinforcement learning algorithms, including both discrete and continuous domains, as it is primarily changing the action parametrization. The paper is well-written, and the experiments extensively evaluate the approach with 3 different RL algorithms in 3 different domains (Atari, MuJoCo, and TORCS).\n\nHere are some comments and questions, for improving the paper:\n\nThe introduction states that \"all DRL algorithms repeatedly execute a chosen action for a fixed number of time steps k\". This statement is too strong, and is actually disproved in the experiments \u2014 repeating an action is helpful in many tasks, but not in all tasks. The sentence should be rephrased to be more precise.\n\nIn the related work, a discussion of the relation to semi-MDPs would be useful to help the reader better understand the approach and how it compares and differs (e.g. the response from the pre-review questions)\n\nExperiments:\nCan you provide error bars on the experimental results? (from running multiple random seeds)\n\nIt would be useful to see experiments with parameter sharing in the TRPO experiments, to be more consistent with the other domains, especially since it seems that the improvement in the TRPO experiments is smaller than that of the other two domains. Right now, it is hard to tell if the smaller improvement is because of the nature of the task, because of the lack of parameter sharing, or something else.\n\nThe TRPO evaluation is different from the results reported in Duan et al. ICML \u201916. Why not use the same benchmark?\n\nVideos only show the policies learned with FiGAR, which are uninformative without also seeing the policies learned without FiGAR. Can you also include videos of the policies learned without FiGAR, as a comparison point?\n\nHow many laps does DDPG complete without FiGAR? The difference in reward achieved seems quite substantial (557K vs. 59K).\n\nCan the tables be visualized as histograms? This seems like it would more effectively and efficiently communicate the results.\n\nMinor comments:\n-- On the plot in Figure 2, the label for the first bar should be changed from 1000 to 3500.\n-- \u201cidea of deciding when necessary\u201d - seems like it would be better to say \u201cidea of only deciding when necessary\"\n-- \"spaces.Durugkar et al.\u201d \u2014 missing a space.\n-- \u201cR={4}\u201d \u2014 why 4? Could you use a letter to indicate a constant instead? (or a different notation)\n"
  },
  {
    "people": [
      "Lempitsky"
    ],
    "review": "This paper develops Submodular Sum Product Networks (SSPNs) and\nan efficient inference algorithm for approximately computing the\nmost probable labeling of variables in the model. The main\napplication in the paper is on scene parsing. In this context,\nSSPNs define an energy function with a grammar component for\nrepresenting a hierarchy of labels and an MRF for encoding\nsmoothness of labels over space. To perform inference, the\nauthors develop a move-making algorithm, somewhat in the spirit\nof fusion moves (Lempitsky et al., 2010) that repeatedly improves\na solution by considering a large neighborhood of alternative segmentations\nand solving an optimization problem to choose the best neighbor.\nEmpirical results show that the proposed algorithm achieves better\nenergy that belief propagation of alpha expansion and is much faster.\n\nThis is generally a well-executed paper. The model is interesting\nand clearly defined, the algorithm is well presented with proper\nanalysis of the relevant runtimes and guarantees on the\nbehavior. Overall, the algorithm seems effective at minimizing\nthe energy of SSPN models.\n\nHaving said that, I don't think this paper is a great fit for\nICLR. The model is even somewhat to the antithesis of the idea of\nlearning representations, in that a highly structured form of\nenergy function is asserted by the human modeller, and then\ninference is performed. I don't see the connection to learning\nrepresentations. One additional issue is that while the proposed\nalgorithm is faster than alternatives, the times are still on the\norder of 1-287 seconds per image, which means that the\napplicability of this method (as is) to something like training\nConvNets is limited.\n\nFinally, there is no attempt to argue that the model produces\nbetter segmentations than alternative models. The only\nevaluations in the paper are on energy values achieved and on\ntraining data.\n\nSo overall I think this is a good paper that should be published\nat a good machine learning conference, but I don't think ICLR is\nthe right fit."
  },
  {
    "people": [
      "Poon"
    ],
    "review": "This paper is about submodular sum product networks applied to scene understanding. SPNs have shown great success in deep linear models since the work of Poon 2011.  The authors propose an extension to the initial SPNs model to be submodular, introducing submodular unary and pairwise potentials.  The authors propose a new inference algorithm. The authors evaluated their results on Stanford Background Dataset and compared against multiple baselines.\n\nPros:\n+ New formulation of SPNs \n+ New inference algorithm\n\nCons:\n- The authors did not discuss how the SSPN structure is learned and how the generative process chooses the a symbol (operation) at each level)\n- The evaluations is lacking. The authors only showed results on their own approach and baselines, leaving out every other approach. Evaluations could have been also done on BSD for regular image segmentation (hierarchical segmentation). \n\nThe idea is great, however, the paper needs more work to be published.  I would also recommend for the authors to include more details about their approach and present a full paper with extended experiments and full learning approach."
  },
  {
    "people": [
      "Lempitsky"
    ],
    "review": "This paper develops Submodular Sum Product Networks (SSPNs) and\nan efficient inference algorithm for approximately computing the\nmost probable labeling of variables in the model. The main\napplication in the paper is on scene parsing. In this context,\nSSPNs define an energy function with a grammar component for\nrepresenting a hierarchy of labels and an MRF for encoding\nsmoothness of labels over space. To perform inference, the\nauthors develop a move-making algorithm, somewhat in the spirit\nof fusion moves (Lempitsky et al., 2010) that repeatedly improves\na solution by considering a large neighborhood of alternative segmentations\nand solving an optimization problem to choose the best neighbor.\nEmpirical results show that the proposed algorithm achieves better\nenergy that belief propagation of alpha expansion and is much faster.\n\nThis is generally a well-executed paper. The model is interesting\nand clearly defined, the algorithm is well presented with proper\nanalysis of the relevant runtimes and guarantees on the\nbehavior. Overall, the algorithm seems effective at minimizing\nthe energy of SSPN models.\n\nHaving said that, I don't think this paper is a great fit for\nICLR. The model is even somewhat to the antithesis of the idea of\nlearning representations, in that a highly structured form of\nenergy function is asserted by the human modeller, and then\ninference is performed. I don't see the connection to learning\nrepresentations. One additional issue is that while the proposed\nalgorithm is faster than alternatives, the times are still on the\norder of 1-287 seconds per image, which means that the\napplicability of this method (as is) to something like training\nConvNets is limited.\n\nFinally, there is no attempt to argue that the model produces\nbetter segmentations than alternative models. The only\nevaluations in the paper are on energy values achieved and on\ntraining data.\n\nSo overall I think this is a good paper that should be published\nat a good machine learning conference, but I don't think ICLR is\nthe right fit."
  },
  {
    "people": [
      "Lempitsky"
    ],
    "review": "This paper develops Submodular Sum Product Networks (SSPNs) and\nan efficient inference algorithm for approximately computing the\nmost probable labeling of variables in the model. The main\napplication in the paper is on scene parsing. In this context,\nSSPNs define an energy function with a grammar component for\nrepresenting a hierarchy of labels and an MRF for encoding\nsmoothness of labels over space. To perform inference, the\nauthors develop a move-making algorithm, somewhat in the spirit\nof fusion moves (Lempitsky et al., 2010) that repeatedly improves\na solution by considering a large neighborhood of alternative segmentations\nand solving an optimization problem to choose the best neighbor.\nEmpirical results show that the proposed algorithm achieves better\nenergy that belief propagation of alpha expansion and is much faster.\n\nThis is generally a well-executed paper. The model is interesting\nand clearly defined, the algorithm is well presented with proper\nanalysis of the relevant runtimes and guarantees on the\nbehavior. Overall, the algorithm seems effective at minimizing\nthe energy of SSPN models.\n\nHaving said that, I don't think this paper is a great fit for\nICLR. The model is even somewhat to the antithesis of the idea of\nlearning representations, in that a highly structured form of\nenergy function is asserted by the human modeller, and then\ninference is performed. I don't see the connection to learning\nrepresentations. One additional issue is that while the proposed\nalgorithm is faster than alternatives, the times are still on the\norder of 1-287 seconds per image, which means that the\napplicability of this method (as is) to something like training\nConvNets is limited.\n\nFinally, there is no attempt to argue that the model produces\nbetter segmentations than alternative models. The only\nevaluations in the paper are on energy values achieved and on\ntraining data.\n\nSo overall I think this is a good paper that should be published\nat a good machine learning conference, but I don't think ICLR is\nthe right fit."
  },
  {
    "people": [
      "Poon"
    ],
    "review": "This paper is about submodular sum product networks applied to scene understanding. SPNs have shown great success in deep linear models since the work of Poon 2011.  The authors propose an extension to the initial SPNs model to be submodular, introducing submodular unary and pairwise potentials.  The authors propose a new inference algorithm. The authors evaluated their results on Stanford Background Dataset and compared against multiple baselines.\n\nPros:\n+ New formulation of SPNs \n+ New inference algorithm\n\nCons:\n- The authors did not discuss how the SSPN structure is learned and how the generative process chooses the a symbol (operation) at each level)\n- The evaluations is lacking. The authors only showed results on their own approach and baselines, leaving out every other approach. Evaluations could have been also done on BSD for regular image segmentation (hierarchical segmentation). \n\nThe idea is great, however, the paper needs more work to be published.  I would also recommend for the authors to include more details about their approach and present a full paper with extended experiments and full learning approach."
  },
  {
    "people": [
      "Lempitsky"
    ],
    "review": "This paper develops Submodular Sum Product Networks (SSPNs) and\nan efficient inference algorithm for approximately computing the\nmost probable labeling of variables in the model. The main\napplication in the paper is on scene parsing. In this context,\nSSPNs define an energy function with a grammar component for\nrepresenting a hierarchy of labels and an MRF for encoding\nsmoothness of labels over space. To perform inference, the\nauthors develop a move-making algorithm, somewhat in the spirit\nof fusion moves (Lempitsky et al., 2010) that repeatedly improves\na solution by considering a large neighborhood of alternative segmentations\nand solving an optimization problem to choose the best neighbor.\nEmpirical results show that the proposed algorithm achieves better\nenergy that belief propagation of alpha expansion and is much faster.\n\nThis is generally a well-executed paper. The model is interesting\nand clearly defined, the algorithm is well presented with proper\nanalysis of the relevant runtimes and guarantees on the\nbehavior. Overall, the algorithm seems effective at minimizing\nthe energy of SSPN models.\n\nHaving said that, I don't think this paper is a great fit for\nICLR. The model is even somewhat to the antithesis of the idea of\nlearning representations, in that a highly structured form of\nenergy function is asserted by the human modeller, and then\ninference is performed. I don't see the connection to learning\nrepresentations. One additional issue is that while the proposed\nalgorithm is faster than alternatives, the times are still on the\norder of 1-287 seconds per image, which means that the\napplicability of this method (as is) to something like training\nConvNets is limited.\n\nFinally, there is no attempt to argue that the model produces\nbetter segmentations than alternative models. The only\nevaluations in the paper are on energy values achieved and on\ntraining data.\n\nSo overall I think this is a good paper that should be published\nat a good machine learning conference, but I don't think ICLR is\nthe right fit."
  },
  {
    "people": [
      "Tobi Delbr\u00fcck"
    ],
    "review": "This is an interesting paper about quantized networks that work on temporal difference inputs.  The basic idea is that when a network has only to process differences then this is computational much more efficient specifically with natural video data since large parts of an image would be fairly constant so that the network only has to process the informative sections of the image (video stream). This is of course how the human visual system works, and it is hence of interest even beyond the core machine learning community. \n\nAs an aside, there is a strong community interested in event-based vision such as the group of Tobi Delbr\u00fcck, and it might be interesting to connect to this community. This might even provide a reference for your comments on page 1.\n\nI guess the biggest novel contribution is that a rounding network can be replaced by a sigma-delta network, but that the order of discretization and summation doe make some difference in the actual processing load. I think I followed the steps and \nMost of my questions have already been answers in the pre-review period. My only question remaining is on page 3, \u201cIt should be noted that when we refer to \u201ctemporal differences\u201d, we refer not to the change in the signal over time, but in the change between two inputs presented sequentially. The output of our network only depends on the value and order of inputs, not on the temporal spacing between them.\u201d\n\nThis does not make sense to me. As I understand you just take the difference between two frames regardless if you call this temporal or not it is a change in one frame. So this statement rather confuses me and maybe should be dropped unless I do miss something here, in which case some more explanation would be necessary.\n\nFigure 1 should be made bigger.\n\nAn improvement of the paper that I could think about is a better discussion of the relevance of the findings. Yes, you do show that your sigma-delta network save some operation compared to threshold, but is this difference essential for a specific task, or does your solution has relevance for neuroscience?"
  },
  {
    "people": [
      "Tobi Delbr\u00fcck"
    ],
    "review": "This is an interesting paper about quantized networks that work on temporal difference inputs.  The basic idea is that when a network has only to process differences then this is computational much more efficient specifically with natural video data since large parts of an image would be fairly constant so that the network only has to process the informative sections of the image (video stream). This is of course how the human visual system works, and it is hence of interest even beyond the core machine learning community. \n\nAs an aside, there is a strong community interested in event-based vision such as the group of Tobi Delbr\u00fcck, and it might be interesting to connect to this community. This might even provide a reference for your comments on page 1.\n\nI guess the biggest novel contribution is that a rounding network can be replaced by a sigma-delta network, but that the order of discretization and summation doe make some difference in the actual processing load. I think I followed the steps and \nMost of my questions have already been answers in the pre-review period. My only question remaining is on page 3, \u201cIt should be noted that when we refer to \u201ctemporal differences\u201d, we refer not to the change in the signal over time, but in the change between two inputs presented sequentially. The output of our network only depends on the value and order of inputs, not on the temporal spacing between them.\u201d\n\nThis does not make sense to me. As I understand you just take the difference between two frames regardless if you call this temporal or not it is a change in one frame. So this statement rather confuses me and maybe should be dropped unless I do miss something here, in which case some more explanation would be necessary.\n\nFigure 1 should be made bigger.\n\nAn improvement of the paper that I could think about is a better discussion of the relevance of the findings. Yes, you do show that your sigma-delta network save some operation compared to threshold, but is this difference essential for a specific task, or does your solution has relevance for neuroscience? "
  },
  {
    "people": [
      "Tobi Delbr\u00fcck"
    ],
    "review": "This is an interesting paper about quantized networks that work on temporal difference inputs.  The basic idea is that when a network has only to process differences then this is computational much more efficient specifically with natural video data since large parts of an image would be fairly constant so that the network only has to process the informative sections of the image (video stream). This is of course how the human visual system works, and it is hence of interest even beyond the core machine learning community. \n\nAs an aside, there is a strong community interested in event-based vision such as the group of Tobi Delbr\u00fcck, and it might be interesting to connect to this community. This might even provide a reference for your comments on page 1.\n\nI guess the biggest novel contribution is that a rounding network can be replaced by a sigma-delta network, but that the order of discretization and summation doe make some difference in the actual processing load. I think I followed the steps and \nMost of my questions have already been answers in the pre-review period. My only question remaining is on page 3, \u201cIt should be noted that when we refer to \u201ctemporal differences\u201d, we refer not to the change in the signal over time, but in the change between two inputs presented sequentially. The output of our network only depends on the value and order of inputs, not on the temporal spacing between them.\u201d\n\nThis does not make sense to me. As I understand you just take the difference between two frames regardless if you call this temporal or not it is a change in one frame. So this statement rather confuses me and maybe should be dropped unless I do miss something here, in which case some more explanation would be necessary.\n\nFigure 1 should be made bigger.\n\nAn improvement of the paper that I could think about is a better discussion of the relevance of the findings. Yes, you do show that your sigma-delta network save some operation compared to threshold, but is this difference essential for a specific task, or does your solution has relevance for neuroscience?"
  },
  {
    "people": [
      "Tobi Delbr\u00fcck"
    ],
    "review": "This is an interesting paper about quantized networks that work on temporal difference inputs.  The basic idea is that when a network has only to process differences then this is computational much more efficient specifically with natural video data since large parts of an image would be fairly constant so that the network only has to process the informative sections of the image (video stream). This is of course how the human visual system works, and it is hence of interest even beyond the core machine learning community. \n\nAs an aside, there is a strong community interested in event-based vision such as the group of Tobi Delbr\u00fcck, and it might be interesting to connect to this community. This might even provide a reference for your comments on page 1.\n\nI guess the biggest novel contribution is that a rounding network can be replaced by a sigma-delta network, but that the order of discretization and summation doe make some difference in the actual processing load. I think I followed the steps and \nMost of my questions have already been answers in the pre-review period. My only question remaining is on page 3, \u201cIt should be noted that when we refer to \u201ctemporal differences\u201d, we refer not to the change in the signal over time, but in the change between two inputs presented sequentially. The output of our network only depends on the value and order of inputs, not on the temporal spacing between them.\u201d\n\nThis does not make sense to me. As I understand you just take the difference between two frames regardless if you call this temporal or not it is a change in one frame. So this statement rather confuses me and maybe should be dropped unless I do miss something here, in which case some more explanation would be necessary.\n\nFigure 1 should be made bigger.\n\nAn improvement of the paper that I could think about is a better discussion of the relevance of the findings. Yes, you do show that your sigma-delta network save some operation compared to threshold, but is this difference essential for a specific task, or does your solution has relevance for neuroscience? "
  },
  {
    "people": [
      "Zhou",
      "Das",
      "Mnih",
      "Simonyan",
      "Zahavy",
      "Simonyan"
    ],
    "review": "Summary\n===\nThis paper presents tic-tac-toe as toy problem for investigating CNNs.\nA dataset is created containing tic-tac-toe boards where one player is one\nmove away from winning and a CNN is trained to label boards according\nto (1) the player who can win (2 choices) and (2) the position they may move\nto win (9 choices), resulting in 18 labels. The CNN evaluated in this paper\nperforms perfectly at the task and the paper's goal is to inspect how the\nCNN works.\n\nThe fundamental mechanism for this inspection is Class Activation\nMapping (CAM) (Zhou et. al. 2016), which identifies regions of implicit attention\nin the CNN. These implicit attention maps (localization heat maps) are used to\nderive actions (which square each player should move). The attention maps  \n\n(1) attend to squares in the tic-tac-toe board rather than arbitrary\nblobs, despite the fact that one square in a board has uniform color, and\n\n(2) they can be used to pick correct (winning) actions.\n\nThis experiment are used to support assertions that the network understands\n(1) chess (tic-tac-toe) boards\n(2) a rule for winning tic-tac-toe\n(3) that there are two players.\n\nSome follow up experiments indicate similar results under various renderings\nof the tic-tac-toe boards and an incomplete training regime.\n\n\nMore Clarifying Questions\n===\n\n* I am not quite sure precisely how CAM is implemented here. In the original CAM\none must identify a class of interest to visualize (e.g., cat or dog). I don't\nthink this paper identifies such a choice. How is one of the 18 possible classes\nchosen for creating the CAM visualization and through that visualization\nchoosing an action?\n\n* How was the test set for this dataset for the table 1 results created?\nHow many of the final 1029 states were used for test and was the\ndistribution of labels the same in train and test?\n\n* How is RCO computed? Is rank correlation or Pearson correlation used?\nIf Pearson correlation is used then it may be good to consider rank correlation,\nas argued in \"Human Attention in Visual Question Answering: Do Humans and\nDeep Networks Look at the Same Regions?\" by Das et. al. in EMNLP 2016.\nIn table 1, what does the 10^3 next to RCO mean?\n\n\nPros\n===\n\n* The proposed method, deriving an action to take from the result of a\nvisualization technique, is very novel.\n\n* This paper provides an experiment that clearly shows a CNN relying on context\nto make accurate predictions.\n\n* The use of a toy tic-tac-toe domain to study attention in CNNs\n(implicit or otherwise) is a potentially fruitful setting that may\nlead to better understanding of implicit and maybe explicit attention mechanisms.\n\n\nCons\n===\n\n* This work distinguishes between predictions about \"what will happen\"\n(will the white player win?) and \"what to do\" (where should the white\nplayer move to win?). The central idea is generalization from \"what will happen\"\nto \"what to do\" indicates concept learning (sec. 2.1). Why should an ability to\nact be any more indicative of a learned concept than an ability to predict\nfuture states. I see a further issue with the presentation of this approach and\na potential correctness problem:\n\n1. (correctness)\nIn the specific setting proposed I see no difference between \"what to do\"\nand \"what will happen.\"\n\nSuppose one created labels dictating \"what to do\" for each example in the\nproposed dataset. How would these differ from the labels of \"what will happen\"\nin the proposed dataset? In this case \"what will happen\" labels include\nboth player identity (who wins) and board position (which position they move\nto win). Wouldn't the \"what to do\" labels need to indicate board position?\nThey could also chosen to indicate player identity, which would make them\nidentical to the \"what will happen\" labels (both 18-way softmaxes).\n\n2. (presentation)\nI think this distinction would usually be handled by the Reinforcement Learning\nframework, but the proposed method is not presented in that framework or\nrelated to an RL based approach. In RL \"what will happen\" is the reward an\nagent will receive for making a particular action and \"what to do\" is the\naction an agent should take. From this point of view, generalization from\n\"what will happen\" to \"what to do\" is not a novel thing to study.\n\nAlternate models include:\n    * A deep Q network (Mnih. et. al. 2015) could predict the value of\n      every possible action where an action is a (player, board position) tuple.\n    * The argmax of the current model's softmax could be used as an action\n      prediction.\nThe deep Q network approach need not be implemented, but differences between\nmethods should be explained because of the uniqueness of the proposed approach.\n\n\n* Comparison to work that uses visualization to investigate deep RL networks\nis missing. In particular, other work in RL has used Simonyan et. al.\n(arXiv 2013) style saliency maps to investigate network behavior. For example, \n\"Dueling Network Architectures for Deep Reinforcement Learning\" by Wang et. al.\nin (ICML 2016) uses saliency maps to identify differences between their\nstate-value and advantage networks. In \"Graying the black box:\nUnderstanding DQNs\" by Zahavy et. al. (ICML 2016) these saliency maps are\nalso used to analyze network behavior.\n\n\n* In section 2.3, saliency maps of Simonyan et. al. are said to not be able to\nactivate on grid squares because they have constant intensity, yet no empirical\nor theoretical evidence is provided for this claim.\n\nOn a related note, what precisely is the notion of information referenced in\nsection 2.3 and why is it relevant? Is it entropy of the distribution of pixel\nintensities in a patch? To me it seems that any measure which depends only\non one patch is irrelevant because the methods discussed (e.g., saliency maps)\ndepend on context as well as the intensities within a patch.\n\n\n* The presentation in the paper would be improved if the results in section 7\nwere presented along with relevant discussion in preceding sections.\n\n\nOverall Evaluation\n===\nThe experiments presented here are novel, but I am not sure they are very\nsignificant or offer clear conclusions. The methods and goals are not presented\nclearly and lack the broader relevant context mentioned above. Furthermore, I\nfind the lines of thought mentioned in the Cons section possibly incorrect\nor incomplete. As detailed with further clarifying questions, upon closer\ninspection I do not see how some aspects of the proposed approach were\nimplemented, so my opinion may change with further details."
  },
  {
    "people": [
      "Mnih",
      "Lillicrap"
    ],
    "review": "Game of tic-tac-toe is considered. 1029 tic-tac-toe board combinations are chosen so that a single move will result into victory of either the black or the white player. There are 18 possible moves - 2 players x 9 locations. A CNN is trained from a visual rendering of the game board to these 18 possible outputs. CAM technique is used to visualize the salient regions in the inputs responsible for the prediction that CNN makes. Authors find that predictions correspond to the winning board locations. \n\nAuthors claim that this:\n1. is a very interesting finding. \n2. CNN has figured out game rules. \n3. Cross modal supervision is applicable to higher-level semantics. \n\nI don't think (2) be can be claimed because the knowledge of game rules is not tested by any experiment. There is only \"one\" stage of a game - i.e. last move that is considered. Further, the results are on the training set itself - the bare minimum requirement of any implicit or explicit representation of game rules is the ability to act in previously unseen states (i.e. generalization). Even if the CNN did generalize, I would avoid making any claims about knowledge of game rules. \n\nFor (3), author's definition of cross-modal seems to be training from images to games moves. In image-classification we go from images --> labels (i.e. between two different domains). We already know CNNs can perform such mappings. CNNs have been used to map images to actions such as in DQN my Mnih et al., or DDPG by Lillicrap et al. and a lot of other classical work such as ALVIN. It's unclear what points authors are trying to make. \n\nFor (1): how interesting is an implicit attention mechanism is a subjective matter. The authors claim a difference between the concepts of \"what do do\" and \"what will happen\". They claim by supervising for \"what will happen\", the CNN can automatically learn about \"what to do\". This is extensively studied in the model predictive control literature. Where model is \"what will happen next\", and the model is used to infer a control law - \"what to do\". However, in the experimental setup presented in the paper what will happen and what to do seem to be the exact same things. \n\nFor further analysis of what the CNN has learnt I would recommend:\n(a) Visualizing CAM with respect to incorrect classes. For eg, visualize the CAM with respect to player would lose (instead of winning).\n\n(b) Split the data into train/val and use the predictions on the val-set for visualization. These would be much more informative about what kind of \"generalizable\" features the CNN pays attention to. \n\nIn summary, understanding why CNN's make what decisions they make is a very interesting area of research. While the emergence of an implicit attention mechanism may be considered to be an interesting finding by some, many claims made by the authors are not supported by experiments (see comments above). \n\n\n \n\n\n\n"
  },
  {
    "people": [
      "Zhou",
      "Das",
      "Mnih",
      "Simonyan",
      "Zahavy",
      "Simonyan"
    ],
    "review": "Summary\n===\nThis paper presents tic-tac-toe as toy problem for investigating CNNs.\nA dataset is created containing tic-tac-toe boards where one player is one\nmove away from winning and a CNN is trained to label boards according\nto (1) the player who can win (2 choices) and (2) the position they may move\nto win (9 choices), resulting in 18 labels. The CNN evaluated in this paper\nperforms perfectly at the task and the paper's goal is to inspect how the\nCNN works.\n\nThe fundamental mechanism for this inspection is Class Activation\nMapping (CAM) (Zhou et. al. 2016), which identifies regions of implicit attention\nin the CNN. These implicit attention maps (localization heat maps) are used to\nderive actions (which square each player should move). The attention maps  \n\n(1) attend to squares in the tic-tac-toe board rather than arbitrary\nblobs, despite the fact that one square in a board has uniform color, and\n\n(2) they can be used to pick correct (winning) actions.\n\nThis experiment are used to support assertions that the network understands\n(1) chess (tic-tac-toe) boards\n(2) a rule for winning tic-tac-toe\n(3) that there are two players.\n\nSome follow up experiments indicate similar results under various renderings\nof the tic-tac-toe boards and an incomplete training regime.\n\n\nMore Clarifying Questions\n===\n\n* I am not quite sure precisely how CAM is implemented here. In the original CAM\none must identify a class of interest to visualize (e.g., cat or dog). I don't\nthink this paper identifies such a choice. How is one of the 18 possible classes\nchosen for creating the CAM visualization and through that visualization\nchoosing an action?\n\n* How was the test set for this dataset for the table 1 results created?\nHow many of the final 1029 states were used for test and was the\ndistribution of labels the same in train and test?\n\n* How is RCO computed? Is rank correlation or Pearson correlation used?\nIf Pearson correlation is used then it may be good to consider rank correlation,\nas argued in \"Human Attention in Visual Question Answering: Do Humans and\nDeep Networks Look at the Same Regions?\" by Das et. al. in EMNLP 2016.\nIn table 1, what does the 10^3 next to RCO mean?\n\n\nPros\n===\n\n* The proposed method, deriving an action to take from the result of a\nvisualization technique, is very novel.\n\n* This paper provides an experiment that clearly shows a CNN relying on context\nto make accurate predictions.\n\n* The use of a toy tic-tac-toe domain to study attention in CNNs\n(implicit or otherwise) is a potentially fruitful setting that may\nlead to better understanding of implicit and maybe explicit attention mechanisms.\n\n\nCons\n===\n\n* This work distinguishes between predictions about \"what will happen\"\n(will the white player win?) and \"what to do\" (where should the white\nplayer move to win?). The central idea is generalization from \"what will happen\"\nto \"what to do\" indicates concept learning (sec. 2.1). Why should an ability to\nact be any more indicative of a learned concept than an ability to predict\nfuture states. I see a further issue with the presentation of this approach and\na potential correctness problem:\n\n1. (correctness)\nIn the specific setting proposed I see no difference between \"what to do\"\nand \"what will happen.\"\n\nSuppose one created labels dictating \"what to do\" for each example in the\nproposed dataset. How would these differ from the labels of \"what will happen\"\nin the proposed dataset? In this case \"what will happen\" labels include\nboth player identity (who wins) and board position (which position they move\nto win). Wouldn't the \"what to do\" labels need to indicate board position?\nThey could also chosen to indicate player identity, which would make them\nidentical to the \"what will happen\" labels (both 18-way softmaxes).\n\n2. (presentation)\nI think this distinction would usually be handled by the Reinforcement Learning\nframework, but the proposed method is not presented in that framework or\nrelated to an RL based approach. In RL \"what will happen\" is the reward an\nagent will receive for making a particular action and \"what to do\" is the\naction an agent should take. From this point of view, generalization from\n\"what will happen\" to \"what to do\" is not a novel thing to study.\n\nAlternate models include:\n    * A deep Q network (Mnih. et. al. 2015) could predict the value of\n      every possible action where an action is a (player, board position) tuple.\n    * The argmax of the current model's softmax could be used as an action\n      prediction.\nThe deep Q network approach need not be implemented, but differences between\nmethods should be explained because of the uniqueness of the proposed approach.\n\n\n* Comparison to work that uses visualization to investigate deep RL networks\nis missing. In particular, other work in RL has used Simonyan et. al.\n(arXiv 2013) style saliency maps to investigate network behavior. For example, \n\"Dueling Network Architectures for Deep Reinforcement Learning\" by Wang et. al.\nin (ICML 2016) uses saliency maps to identify differences between their\nstate-value and advantage networks. In \"Graying the black box:\nUnderstanding DQNs\" by Zahavy et. al. (ICML 2016) these saliency maps are\nalso used to analyze network behavior.\n\n\n* In section 2.3, saliency maps of Simonyan et. al. are said to not be able to\nactivate on grid squares because they have constant intensity, yet no empirical\nor theoretical evidence is provided for this claim.\n\nOn a related note, what precisely is the notion of information referenced in\nsection 2.3 and why is it relevant? Is it entropy of the distribution of pixel\nintensities in a patch? To me it seems that any measure which depends only\non one patch is irrelevant because the methods discussed (e.g., saliency maps)\ndepend on context as well as the intensities within a patch.\n\n\n* The presentation in the paper would be improved if the results in section 7\nwere presented along with relevant discussion in preceding sections.\n\n\nOverall Evaluation\n===\nThe experiments presented here are novel, but I am not sure they are very\nsignificant or offer clear conclusions. The methods and goals are not presented\nclearly and lack the broader relevant context mentioned above. Furthermore, I\nfind the lines of thought mentioned in the Cons section possibly incorrect\nor incomplete. As detailed with further clarifying questions, upon closer\ninspection I do not see how some aspects of the proposed approach were\nimplemented, so my opinion may change with further details."
  },
  {
    "people": [
      "Zhou"
    ],
    "review": "1029 tic-tac-toe boards are rendered (in various ways). These 1029 boards are legal boards where the next legal play can end the game. There are 18 categories of such boards -- 9 for the different locations of the next play, and 2 for the color of the next play. The supervision is basically saying \"If you place a black square in the middle right, black will win\" or \"if you place a white square in the upper left, white will win\". A CNN is trained to predict these 18 categories and can do so with 100% accuracy.\n\nThe focus of the paper is using Zhou et al's Class Activation Mapping to show where the CNN focuses when making it's decision. As I understand it, an input to CAM is the class of interest. So let's say it is class 1 (black wins with a play to the bottom right square, if I've deciphered figure 2 correctly. Figure 2 should really be more clear about what each class is). So we ask CAM to determine the area of focus of the CNN for deciding whether class 1 is exhibited. The focus ends up being on the empty bottom right square (because certainly you can't exhibit class 1 if the bottom right square is occupied). The CNN also needs to condition its decision on other parts of the board -- it needs to know whether there will be 3 in a row from some direction. But maybe that conditioning is weaker?\n\nThat's kind of interesting but I'm not sure about the deeper statements about discovering game rules that the paper hints at. I'm also not sure about the connection of this work to weakly supervised learning or multi-modal learning.\n\nThe paper is pretty well written, overall, with some grammatical mistakes, but I simply don't see the surprising discovery of this work. \n\nI also have some concerns about how contrived this scenario is -- using a big, expressive CNN for such a simple game domain and using a particular CNN visualization method.\n\nI am not an expert in reinforcement learning (which isn't happening in this paper, but is in related works on CNN game playing), so maybe I'm not appreciating the paper appropriately."
  },
  {
    "people": [
      "Zhou",
      "Das",
      "Mnih",
      "Simonyan",
      "Zahavy",
      "Simonyan"
    ],
    "review": "Summary\n===\nThis paper presents tic-tac-toe as toy problem for investigating CNNs.\nA dataset is created containing tic-tac-toe boards where one player is one\nmove away from winning and a CNN is trained to label boards according\nto (1) the player who can win (2 choices) and (2) the position they may move\nto win (9 choices), resulting in 18 labels. The CNN evaluated in this paper\nperforms perfectly at the task and the paper's goal is to inspect how the\nCNN works.\n\nThe fundamental mechanism for this inspection is Class Activation\nMapping (CAM) (Zhou et. al. 2016), which identifies regions of implicit attention\nin the CNN. These implicit attention maps (localization heat maps) are used to\nderive actions (which square each player should move). The attention maps  \n\n(1) attend to squares in the tic-tac-toe board rather than arbitrary\nblobs, despite the fact that one square in a board has uniform color, and\n\n(2) they can be used to pick correct (winning) actions.\n\nThis experiment are used to support assertions that the network understands\n(1) chess (tic-tac-toe) boards\n(2) a rule for winning tic-tac-toe\n(3) that there are two players.\n\nSome follow up experiments indicate similar results under various renderings\nof the tic-tac-toe boards and an incomplete training regime.\n\n\nMore Clarifying Questions\n===\n\n* I am not quite sure precisely how CAM is implemented here. In the original CAM\none must identify a class of interest to visualize (e.g., cat or dog). I don't\nthink this paper identifies such a choice. How is one of the 18 possible classes\nchosen for creating the CAM visualization and through that visualization\nchoosing an action?\n\n* How was the test set for this dataset for the table 1 results created?\nHow many of the final 1029 states were used for test and was the\ndistribution of labels the same in train and test?\n\n* How is RCO computed? Is rank correlation or Pearson correlation used?\nIf Pearson correlation is used then it may be good to consider rank correlation,\nas argued in \"Human Attention in Visual Question Answering: Do Humans and\nDeep Networks Look at the Same Regions?\" by Das et. al. in EMNLP 2016.\nIn table 1, what does the 10^3 next to RCO mean?\n\n\nPros\n===\n\n* The proposed method, deriving an action to take from the result of a\nvisualization technique, is very novel.\n\n* This paper provides an experiment that clearly shows a CNN relying on context\nto make accurate predictions.\n\n* The use of a toy tic-tac-toe domain to study attention in CNNs\n(implicit or otherwise) is a potentially fruitful setting that may\nlead to better understanding of implicit and maybe explicit attention mechanisms.\n\n\nCons\n===\n\n* This work distinguishes between predictions about \"what will happen\"\n(will the white player win?) and \"what to do\" (where should the white\nplayer move to win?). The central idea is generalization from \"what will happen\"\nto \"what to do\" indicates concept learning (sec. 2.1). Why should an ability to\nact be any more indicative of a learned concept than an ability to predict\nfuture states. I see a further issue with the presentation of this approach and\na potential correctness problem:\n\n1. (correctness)\nIn the specific setting proposed I see no difference between \"what to do\"\nand \"what will happen.\"\n\nSuppose one created labels dictating \"what to do\" for each example in the\nproposed dataset. How would these differ from the labels of \"what will happen\"\nin the proposed dataset? In this case \"what will happen\" labels include\nboth player identity (who wins) and board position (which position they move\nto win). Wouldn't the \"what to do\" labels need to indicate board position?\nThey could also chosen to indicate player identity, which would make them\nidentical to the \"what will happen\" labels (both 18-way softmaxes).\n\n2. (presentation)\nI think this distinction would usually be handled by the Reinforcement Learning\nframework, but the proposed method is not presented in that framework or\nrelated to an RL based approach. In RL \"what will happen\" is the reward an\nagent will receive for making a particular action and \"what to do\" is the\naction an agent should take. From this point of view, generalization from\n\"what will happen\" to \"what to do\" is not a novel thing to study.\n\nAlternate models include:\n    * A deep Q network (Mnih. et. al. 2015) could predict the value of\n      every possible action where an action is a (player, board position) tuple.\n    * The argmax of the current model's softmax could be used as an action\n      prediction.\nThe deep Q network approach need not be implemented, but differences between\nmethods should be explained because of the uniqueness of the proposed approach.\n\n\n* Comparison to work that uses visualization to investigate deep RL networks\nis missing. In particular, other work in RL has used Simonyan et. al.\n(arXiv 2013) style saliency maps to investigate network behavior. For example, \n\"Dueling Network Architectures for Deep Reinforcement Learning\" by Wang et. al.\nin (ICML 2016) uses saliency maps to identify differences between their\nstate-value and advantage networks. In \"Graying the black box:\nUnderstanding DQNs\" by Zahavy et. al. (ICML 2016) these saliency maps are\nalso used to analyze network behavior.\n\n\n* In section 2.3, saliency maps of Simonyan et. al. are said to not be able to\nactivate on grid squares because they have constant intensity, yet no empirical\nor theoretical evidence is provided for this claim.\n\nOn a related note, what precisely is the notion of information referenced in\nsection 2.3 and why is it relevant? Is it entropy of the distribution of pixel\nintensities in a patch? To me it seems that any measure which depends only\non one patch is irrelevant because the methods discussed (e.g., saliency maps)\ndepend on context as well as the intensities within a patch.\n\n\n* The presentation in the paper would be improved if the results in section 7\nwere presented along with relevant discussion in preceding sections.\n\n\nOverall Evaluation\n===\nThe experiments presented here are novel, but I am not sure they are very\nsignificant or offer clear conclusions. The methods and goals are not presented\nclearly and lack the broader relevant context mentioned above. Furthermore, I\nfind the lines of thought mentioned in the Cons section possibly incorrect\nor incomplete. As detailed with further clarifying questions, upon closer\ninspection I do not see how some aspects of the proposed approach were\nimplemented, so my opinion may change with further details."
  },
  {
    "people": [
      "Mnih",
      "Lillicrap"
    ],
    "review": "Game of tic-tac-toe is considered. 1029 tic-tac-toe board combinations are chosen so that a single move will result into victory of either the black or the white player. There are 18 possible moves - 2 players x 9 locations. A CNN is trained from a visual rendering of the game board to these 18 possible outputs. CAM technique is used to visualize the salient regions in the inputs responsible for the prediction that CNN makes. Authors find that predictions correspond to the winning board locations. \n\nAuthors claim that this:\n1. is a very interesting finding. \n2. CNN has figured out game rules. \n3. Cross modal supervision is applicable to higher-level semantics. \n\nI don't think (2) be can be claimed because the knowledge of game rules is not tested by any experiment. There is only \"one\" stage of a game - i.e. last move that is considered. Further, the results are on the training set itself - the bare minimum requirement of any implicit or explicit representation of game rules is the ability to act in previously unseen states (i.e. generalization). Even if the CNN did generalize, I would avoid making any claims about knowledge of game rules. \n\nFor (3), author's definition of cross-modal seems to be training from images to games moves. In image-classification we go from images --> labels (i.e. between two different domains). We already know CNNs can perform such mappings. CNNs have been used to map images to actions such as in DQN my Mnih et al., or DDPG by Lillicrap et al. and a lot of other classical work such as ALVIN. It's unclear what points authors are trying to make. \n\nFor (1): how interesting is an implicit attention mechanism is a subjective matter. The authors claim a difference between the concepts of \"what do do\" and \"what will happen\". They claim by supervising for \"what will happen\", the CNN can automatically learn about \"what to do\". This is extensively studied in the model predictive control literature. Where model is \"what will happen next\", and the model is used to infer a control law - \"what to do\". However, in the experimental setup presented in the paper what will happen and what to do seem to be the exact same things. \n\nFor further analysis of what the CNN has learnt I would recommend:\n(a) Visualizing CAM with respect to incorrect classes. For eg, visualize the CAM with respect to player would lose (instead of winning).\n\n(b) Split the data into train/val and use the predictions on the val-set for visualization. These would be much more informative about what kind of \"generalizable\" features the CNN pays attention to. \n\nIn summary, understanding why CNN's make what decisions they make is a very interesting area of research. While the emergence of an implicit attention mechanism may be considered to be an interesting finding by some, many claims made by the authors are not supported by experiments (see comments above). \n\n\n \n\n\n\n"
  },
  {
    "people": [
      "Zhou",
      "Das",
      "Mnih",
      "Simonyan",
      "Zahavy",
      "Simonyan"
    ],
    "review": "Summary\n===\nThis paper presents tic-tac-toe as toy problem for investigating CNNs.\nA dataset is created containing tic-tac-toe boards where one player is one\nmove away from winning and a CNN is trained to label boards according\nto (1) the player who can win (2 choices) and (2) the position they may move\nto win (9 choices), resulting in 18 labels. The CNN evaluated in this paper\nperforms perfectly at the task and the paper's goal is to inspect how the\nCNN works.\n\nThe fundamental mechanism for this inspection is Class Activation\nMapping (CAM) (Zhou et. al. 2016), which identifies regions of implicit attention\nin the CNN. These implicit attention maps (localization heat maps) are used to\nderive actions (which square each player should move). The attention maps  \n\n(1) attend to squares in the tic-tac-toe board rather than arbitrary\nblobs, despite the fact that one square in a board has uniform color, and\n\n(2) they can be used to pick correct (winning) actions.\n\nThis experiment are used to support assertions that the network understands\n(1) chess (tic-tac-toe) boards\n(2) a rule for winning tic-tac-toe\n(3) that there are two players.\n\nSome follow up experiments indicate similar results under various renderings\nof the tic-tac-toe boards and an incomplete training regime.\n\n\nMore Clarifying Questions\n===\n\n* I am not quite sure precisely how CAM is implemented here. In the original CAM\none must identify a class of interest to visualize (e.g., cat or dog). I don't\nthink this paper identifies such a choice. How is one of the 18 possible classes\nchosen for creating the CAM visualization and through that visualization\nchoosing an action?\n\n* How was the test set for this dataset for the table 1 results created?\nHow many of the final 1029 states were used for test and was the\ndistribution of labels the same in train and test?\n\n* How is RCO computed? Is rank correlation or Pearson correlation used?\nIf Pearson correlation is used then it may be good to consider rank correlation,\nas argued in \"Human Attention in Visual Question Answering: Do Humans and\nDeep Networks Look at the Same Regions?\" by Das et. al. in EMNLP 2016.\nIn table 1, what does the 10^3 next to RCO mean?\n\n\nPros\n===\n\n* The proposed method, deriving an action to take from the result of a\nvisualization technique, is very novel.\n\n* This paper provides an experiment that clearly shows a CNN relying on context\nto make accurate predictions.\n\n* The use of a toy tic-tac-toe domain to study attention in CNNs\n(implicit or otherwise) is a potentially fruitful setting that may\nlead to better understanding of implicit and maybe explicit attention mechanisms.\n\n\nCons\n===\n\n* This work distinguishes between predictions about \"what will happen\"\n(will the white player win?) and \"what to do\" (where should the white\nplayer move to win?). The central idea is generalization from \"what will happen\"\nto \"what to do\" indicates concept learning (sec. 2.1). Why should an ability to\nact be any more indicative of a learned concept than an ability to predict\nfuture states. I see a further issue with the presentation of this approach and\na potential correctness problem:\n\n1. (correctness)\nIn the specific setting proposed I see no difference between \"what to do\"\nand \"what will happen.\"\n\nSuppose one created labels dictating \"what to do\" for each example in the\nproposed dataset. How would these differ from the labels of \"what will happen\"\nin the proposed dataset? In this case \"what will happen\" labels include\nboth player identity (who wins) and board position (which position they move\nto win). Wouldn't the \"what to do\" labels need to indicate board position?\nThey could also chosen to indicate player identity, which would make them\nidentical to the \"what will happen\" labels (both 18-way softmaxes).\n\n2. (presentation)\nI think this distinction would usually be handled by the Reinforcement Learning\nframework, but the proposed method is not presented in that framework or\nrelated to an RL based approach. In RL \"what will happen\" is the reward an\nagent will receive for making a particular action and \"what to do\" is the\naction an agent should take. From this point of view, generalization from\n\"what will happen\" to \"what to do\" is not a novel thing to study.\n\nAlternate models include:\n    * A deep Q network (Mnih. et. al. 2015) could predict the value of\n      every possible action where an action is a (player, board position) tuple.\n    * The argmax of the current model's softmax could be used as an action\n      prediction.\nThe deep Q network approach need not be implemented, but differences between\nmethods should be explained because of the uniqueness of the proposed approach.\n\n\n* Comparison to work that uses visualization to investigate deep RL networks\nis missing. In particular, other work in RL has used Simonyan et. al.\n(arXiv 2013) style saliency maps to investigate network behavior. For example, \n\"Dueling Network Architectures for Deep Reinforcement Learning\" by Wang et. al.\nin (ICML 2016) uses saliency maps to identify differences between their\nstate-value and advantage networks. In \"Graying the black box:\nUnderstanding DQNs\" by Zahavy et. al. (ICML 2016) these saliency maps are\nalso used to analyze network behavior.\n\n\n* In section 2.3, saliency maps of Simonyan et. al. are said to not be able to\nactivate on grid squares because they have constant intensity, yet no empirical\nor theoretical evidence is provided for this claim.\n\nOn a related note, what precisely is the notion of information referenced in\nsection 2.3 and why is it relevant? Is it entropy of the distribution of pixel\nintensities in a patch? To me it seems that any measure which depends only\non one patch is irrelevant because the methods discussed (e.g., saliency maps)\ndepend on context as well as the intensities within a patch.\n\n\n* The presentation in the paper would be improved if the results in section 7\nwere presented along with relevant discussion in preceding sections.\n\n\nOverall Evaluation\n===\nThe experiments presented here are novel, but I am not sure they are very\nsignificant or offer clear conclusions. The methods and goals are not presented\nclearly and lack the broader relevant context mentioned above. Furthermore, I\nfind the lines of thought mentioned in the Cons section possibly incorrect\nor incomplete. As detailed with further clarifying questions, upon closer\ninspection I do not see how some aspects of the proposed approach were\nimplemented, so my opinion may change with further details."
  },
  {
    "people": [
      "Zhou"
    ],
    "review": "1029 tic-tac-toe boards are rendered (in various ways). These 1029 boards are legal boards where the next legal play can end the game. There are 18 categories of such boards -- 9 for the different locations of the next play, and 2 for the color of the next play. The supervision is basically saying \"If you place a black square in the middle right, black will win\" or \"if you place a white square in the upper left, white will win\". A CNN is trained to predict these 18 categories and can do so with 100% accuracy.\n\nThe focus of the paper is using Zhou et al's Class Activation Mapping to show where the CNN focuses when making it's decision. As I understand it, an input to CAM is the class of interest. So let's say it is class 1 (black wins with a play to the bottom right square, if I've deciphered figure 2 correctly. Figure 2 should really be more clear about what each class is). So we ask CAM to determine the area of focus of the CNN for deciding whether class 1 is exhibited. The focus ends up being on the empty bottom right square (because certainly you can't exhibit class 1 if the bottom right square is occupied). The CNN also needs to condition its decision on other parts of the board -- it needs to know whether there will be 3 in a row from some direction. But maybe that conditioning is weaker?\n\nThat's kind of interesting but I'm not sure about the deeper statements about discovering game rules that the paper hints at. I'm also not sure about the connection of this work to weakly supervised learning or multi-modal learning.\n\nThe paper is pretty well written, overall, with some grammatical mistakes, but I simply don't see the surprising discovery of this work. \n\nI also have some concerns about how contrived this scenario is -- using a big, expressive CNN for such a simple game domain and using a particular CNN visualization method.\n\nI am not an expert in reinforcement learning (which isn't happening in this paper, but is in related works on CNN game playing), so maybe I'm not appreciating the paper appropriately."
  },
  {
    "people": [
      "Dyer"
    ],
    "review": "We would like to thank the reviewers for their comments, and specifically, reviewers #2 and #3, who acknowledge that the proposed model is interesting and works well, outperforming strong baselines.  To summarize, the main contribution of our work is to propose a novel joint model based on pointer network architecture which achieves state-of-the-art results on argumentation mining task with a large gap.  \n\nThere are three main concerns raised by the reviewers. The first and the main concern is the novelty of the model.  We believe that in part this concern is due to a misunderstanding that occurred because we mislabeled our proposed joint model as \u201cPN\u201d in the results table (see our discussion below).  We think this led some of the reviewers to believe that the paper merely pointer network model to a specific task. \n\nThe second concern is about the overall contribution of the paper to representation learning.  We argue that it is precisely the joint representation learned by our model in the encoding phase, as well as the fact that our model supports separate source and target representations for a given text span, that allows us to substantially outperform standard recurrent models.  \n\nThe third question raised by the reviewers concerns the meaningful comparison to other methods for recovering relations, such as stack-based models for syntactic parsing.  We believe that this comparison is not appropriate in our case.  As a discourse parsing task, argumentation mining requires the flexibility of recovering relations that are quite distinct from syntactic parsing, in that they allow both projective and non-projective structures, multi-root parse fragments, and components with no incoming or no outgoing links. \n\nWe give more specific responses to reviewer comments below.\n\n\u201cPointer network has been proposed before.\u201d \n\nAs is evident in Table 1, a direct application of a pointer network (PN) does not achieve state-of-the-art on link prediction.  Neither is it suitable for AC type prediction.  In fact, a direct application of PN performs substantially worse on link prediction than the joint model, which achieves state-of-the-art on both tasks in the persuasive essay corpus, as well as on link prediction in the microtext corpus. \n\n\u201cI am concerned, though, that the paper doesn't make a substantial novel contribution to representation learning.\u201d \n\nWe argue that the better performance of the joint PN model is purely due to the representations of the components learned by the model.  When the representation learned in the encoding phase is shared between the two tasks, the model can factor in the information about the types of argument components that are more likely to be linked.  And indeed, our results shows that the information encoded for type prediction is also useful for link prediction, substantially boosting the performance of the joint model.\n\nFurthermore, we also show that the sequence-to-sequence model we propose does much better than standard recurrent models, e.g. Table 1 shows that a BLSTM model can match previous state-of-the-art, which the joint PN model surpasses substantially.  We argue that the better performance of our model is due to the separate representations learned during encoding/decoding. Effectively, our model allows an argument component to have separate representations in its role as a source or a target of a link.\n\n\u201cThe proposed multi-task learning method is interesting, but the authors only verified it on one task.\u201d \n\nAlthough we do focus on a single task, we test the model on two different datasets, each with its own characteristics. For example, the Microtext corpus has only 100 training examples, and our model is still able to achieve state-of-the-art on link prediction. Furthermore, compared to the Persuasive Essay corpus, the Microtext corpus is much more standardized; all examples are trees and there is exactly one claim in each example. Therefore, even though we focus on a single task, the datasets we test on have varying characteristics, which highlights the generalizability of the model.\n\n\u201c...the stack-based method can be used for forest prediction\u2026\u201d \n\nCompared to stack-based models, the PN-based framework is more flexible.  Specifically, this framework easily handles projective and non-projective structures, multi-root parse fragments, and components with no incoming or no outgoing links. For example, a stack-based model, such as the one proposed by Dyer et al. (2016), assumes a single-root, projective tree, which is an assumption commonly violated in discourse parsing tasks such as argumentation mining. \n\n\u201c...I found that the assumption that the boundaries of AC are given is a very strong constraint, and could potentially limit the usefulness of the proposed model.\u201d \n\nWe would like to point out that previous work on AC boundary detection, specifically, the work by the creators of the persuasive essay corpus (Stab & Gurevich 2016) has already achieved near human-level performance in identifying argument components; this is in stark contrast to the previous models for link and type prediction.\n\n\u201cthe experiments right now cannot reflect the advantages of pointer network models.\u201d \n\nWe believe this is incorrect.  Specifically, The ILP Joint model uses 18 different hand-engineered features, often requiring external tools such as POS tagging. Our model outperforms it with minimal feature extraction. The MP+p model assumes a single-tree structure explicitly for links (which is unique to the Microtext corpus). Our model outperforms it for link extraction without any explicit single-tree constraint."
  },
  {
    "people": [
      "Dyer"
    ],
    "review": "We would like to thank the reviewers for their comments, and specifically, reviewers #2 and #3, who acknowledge that the proposed model is interesting and works well, outperforming strong baselines.  To summarize, the main contribution of our work is to propose a novel joint model based on pointer network architecture which achieves state-of-the-art results on argumentation mining task with a large gap.  \n\nThere are three main concerns raised by the reviewers. The first and the main concern is the novelty of the model.  We believe that in part this concern is due to a misunderstanding that occurred because we mislabeled our proposed joint model as \u201cPN\u201d in the results table (see our discussion below).  We think this led some of the reviewers to believe that the paper merely pointer network model to a specific task. \n\nThe second concern is about the overall contribution of the paper to representation learning.  We argue that it is precisely the joint representation learned by our model in the encoding phase, as well as the fact that our model supports separate source and target representations for a given text span, that allows us to substantially outperform standard recurrent models.  \n\nThe third question raised by the reviewers concerns the meaningful comparison to other methods for recovering relations, such as stack-based models for syntactic parsing.  We believe that this comparison is not appropriate in our case.  As a discourse parsing task, argumentation mining requires the flexibility of recovering relations that are quite distinct from syntactic parsing, in that they allow both projective and non-projective structures, multi-root parse fragments, and components with no incoming or no outgoing links. \n\nWe give more specific responses to reviewer comments below.\n\n\u201cPointer network has been proposed before.\u201d \n\nAs is evident in Table 1, a direct application of a pointer network (PN) does not achieve state-of-the-art on link prediction.  Neither is it suitable for AC type prediction.  In fact, a direct application of PN performs substantially worse on link prediction than the joint model, which achieves state-of-the-art on both tasks in the persuasive essay corpus, as well as on link prediction in the microtext corpus. \n\n\u201cI am concerned, though, that the paper doesn't make a substantial novel contribution to representation learning.\u201d \n\nWe argue that the better performance of the joint PN model is purely due to the representations of the components learned by the model.  When the representation learned in the encoding phase is shared between the two tasks, the model can factor in the information about the types of argument components that are more likely to be linked.  And indeed, our results shows that the information encoded for type prediction is also useful for link prediction, substantially boosting the performance of the joint model.\n\nFurthermore, we also show that the sequence-to-sequence model we propose does much better than standard recurrent models, e.g. Table 1 shows that a BLSTM model can match previous state-of-the-art, which the joint PN model surpasses substantially.  We argue that the better performance of our model is due to the separate representations learned during encoding/decoding. Effectively, our model allows an argument component to have separate representations in its role as a source or a target of a link.\n\n\u201cThe proposed multi-task learning method is interesting, but the authors only verified it on one task.\u201d \n\nAlthough we do focus on a single task, we test the model on two different datasets, each with its own characteristics. For example, the Microtext corpus has only 100 training examples, and our model is still able to achieve state-of-the-art on link prediction. Furthermore, compared to the Persuasive Essay corpus, the Microtext corpus is much more standardized; all examples are trees and there is exactly one claim in each example. Therefore, even though we focus on a single task, the datasets we test on have varying characteristics, which highlights the generalizability of the model.\n\n\u201c...the stack-based method can be used for forest prediction\u2026\u201d \n\nCompared to stack-based models, the PN-based framework is more flexible.  Specifically, this framework easily handles projective and non-projective structures, multi-root parse fragments, and components with no incoming or no outgoing links. For example, a stack-based model, such as the one proposed by Dyer et al. (2016), assumes a single-root, projective tree, which is an assumption commonly violated in discourse parsing tasks such as argumentation mining. \n\n\u201c...I found that the assumption that the boundaries of AC are given is a very strong constraint, and could potentially limit the usefulness of the proposed model.\u201d \n\nWe would like to point out that previous work on AC boundary detection, specifically, the work by the creators of the persuasive essay corpus (Stab & Gurevich 2016) has already achieved near human-level performance in identifying argument components; this is in stark contrast to the previous models for link and type prediction.\n\n\u201cthe experiments right now cannot reflect the advantages of pointer network models.\u201d \n\nWe believe this is incorrect.  Specifically, The ILP Joint model uses 18 different hand-engineered features, often requiring external tools such as POS tagging. Our model outperforms it with minimal feature extraction. The MP+p model assumes a single-tree structure explicitly for links (which is unique to the Microtext corpus). Our model outperforms it for link extraction without any explicit single-tree constraint."
  },
  {
    "people": [
      "Rumelhart"
    ],
    "review": "Experimental results look reasonable, validated on 3 tasks. \nReferences could be improved, for example I would rather see\nRumelhart's paper cited for back-propagation than the Deep Learning book.\n"
  },
  {
    "people": [
      "Yann LeCun"
    ],
    "review": "The paper proposes a sparsely connected network and an efficient hardware architecture that can save up to 90% of memory compared to the conventional implementations of fully connected neural networks. \nThe paper removes some of the connections in the fully connected layers and shows performance and computational efficiency increase in networks on three different datasets. It is also a good addition that the authors combine their method with binary and ternary connect studies and show further improvements.\nThe paper was hard for me to understand because of this misleading statement: In this paper, we propose sparsely-connected networks by reducing the number of connections of fully-connected networks using linear-feedback shift registers (LFSRs). It led me to think that LFSRs reduced the connections by keeping some of the information in the registers. However, LFSR is only used as a random binary generator. Any random generator could be used but LFSR is chosen for the convenience in VLSI implementation. \nThis explanation would be clearer to me: In this paper, we propose sparsely-connected networks by randomly removing some of the connections in fully-connected networks. Random connection masks are generated by LFSR, which is also used in the VLSI implementation to disable the connections.\nAlgorithm 1 is basically training a network with back-propogation where each layer has a binary mask that disables some of the connections. This explanation can be added to the text.\nUsing random connections is not a new idea in CNNs. It was used between CNN layers in a 1998 paper by Yann LeCun and others: "
  },
  {
    "people": [
      "Rumelhart"
    ],
    "review": "Experimental results look reasonable, validated on 3 tasks. \nReferences could be improved, for example I would rather see\nRumelhart's paper cited for back-propagation than the Deep Learning book.\n"
  },
  {
    "people": [
      "Yann LeCun"
    ],
    "review": "The paper proposes a sparsely connected network and an efficient hardware architecture that can save up to 90% of memory compared to the conventional implementations of fully connected neural networks. \nThe paper removes some of the connections in the fully connected layers and shows performance and computational efficiency increase in networks on three different datasets. It is also a good addition that the authors combine their method with binary and ternary connect studies and show further improvements.\nThe paper was hard for me to understand because of this misleading statement: In this paper, we propose sparsely-connected networks by reducing the number of connections of fully-connected networks using linear-feedback shift registers (LFSRs). It led me to think that LFSRs reduced the connections by keeping some of the information in the registers. However, LFSR is only used as a random binary generator. Any random generator could be used but LFSR is chosen for the convenience in VLSI implementation. \nThis explanation would be clearer to me: In this paper, we propose sparsely-connected networks by randomly removing some of the connections in fully-connected networks. Random connection masks are generated by LFSR, which is also used in the VLSI implementation to disable the connections.\nAlgorithm 1 is basically training a network with back-propogation where each layer has a binary mask that disables some of the connections. This explanation can be added to the text.\nUsing random connections is not a new idea in CNNs. It was used between CNN layers in a 1998 paper by Yann LeCun and others: "
  },
  {
    "people": [
      "Yoojin"
    ],
    "review": "Dear Reviewers,\n\nWe have reduced the length of the paper further and it is now 9 pages (excluding references and Appendix). Here is a list of major changes:\n\n- We removed equation (2) of the previous revision since it is redundant.\n- We removed Section 2 of the previous revision, and added Section 3.1 instead in this revision.\n- We removed Remarks 1-3 in Section 4.1 of the previous revision, and moved all of them to Appendix A.1 instead in this revision.\n- We removed the second paragraph of Section 4.5 in the previous revision.\n- We reduced the length of Remark 4 in Section 5.3 of the previous revision.\n\nWe are open to reduce it further if needed. Please let us know your opinion. Thank you.\n\nBest,\nYoojin"
  },
  {
    "people": [
      "Yoojin"
    ],
    "review": "Dear Reviewers,\n\nWe have uploaded a revised paper.\n\n1) We reduced the length of the paper to 10 pages excluding references and Appendix. To this end, \n\n- We reduced the length of Abstract.\n- We curtailed the first three sections substantially while removing Figure 1 and Figure 2.\n- We removed the detailed discussion on the alternative of Hessian in Section 4.4.\n- We removed the detailed discussion on the iterative ECSQ algorithm in Section 5.4 while moving some to Appendix A.2.\n- We removed the experiment results for LeNet.\n\nWe will try to reduce it further next week. Please let us know your opinion. We are open to reduce it further.\n\n2) We added Remark 1 in Section 4.1 to address the reviewer's comment on the effect of diagonal approximation:\n\n\"Remark 1. The diagonal approximation for Hessian simplifies the optimization problem as well as its solution for network quantization. This simplification however comes with some performance loss. We conjecture that the loss due to this approximation is small. The reason is that the contributions from off-diagonal terms are not always additive and their summation may end up with a small value. However, diagonal terms are all non-negative and therefore their contributions are always additive.\"\n\n3) We added Remark 3 in Section 4.1 to address the reviewer's comment on the generalization of Hessian-weighted quantization methods to other models:\n\n\"Remark 3. Observe that the relation of the Hessian-weighted distortion measure to the quantization loss holds for any model for which the objective function can be approximated as a quadratic function with respect to the parameters to quantize. Hence, the quantization methods proposed in this paper to minimize the Hessian-weighted distortion measure are not specific to neural networks but are generally applicable to quantization of parameters of any model whose objective function is locally quadratic with respect to its parameters approximately.\"\n\n4) We elaborated the exponential time complexity of layer-by-layer quantization in the last paragraph of Section 4.5:\n\n\"Optimizing compression ratios jointly across all layers (to maximize the overall compression ratio for all layers) requires exponential time complexity with respect to the number of layers. This is because the total number of possible combinations of compression ratios for individual layers increases exponentially as the number of layers increases.\"\n\n5) We removed the description on the \"additional bits\" part from Section 4.5 to reduce the length of the paper. We note that it is just a minor implementation issue as the reviewer pointed out.\n\n6) We clarified that [Han 2015] did not evaluate ResNet by adding a row for [Han 2015] in Table 1 and put \"N/A\" for ResNet.\n\n7) We fixed the typos found by reviewers.\n\n8) We removed Appendix A.1 and Appendix A.4 to reduce the length of the paper further.\n\nThank you again for your valuable comments and suggestions. We look forward to further comments.\n\nBest,\nYoojin"
  },
  {
    "people": [
      "Han",
      "Adam",
      "Han",
      "Han"
    ],
    "review": "This paper proposes a novel neural network compression technique.\nThe goal is to compress maximally the network specification via parameter quantisation with a minimum impact on the expected loss.\nIt assumes pruning of the network parameters has already been performed, and only considers the quantisation of the individual scalar parameters of the network.\nIn contrast to previous work (Han et al. 2015a, Gong et al. 2014) the proposed approach takes into account the effect of the weight quantisation on the loss function that is used to train the network, and also takes into account the effect on a variable-length binary encoding of the cluster centers used for the quantisation. \n\nUnfortunately, the submitted paper is 20 pages, rather than the 8 recommended. The length of the paper seems unjustified to me, since the first three sections (first five pages) are very generic and redundant can be largely compressed or skipped (including figures 1 and 2). Although not a strict requirement by the submission guidelines, I would suggest the authors to compress their paper to 8 pages, this will improve the readability of the paper.\n\nTo take into account the impact on the network\u2019s loss the authors propose to use a second order approximation of the cost function of the loss. In the case of weights that originally constitute a local minimum of the loss, this leads to a formulation of the impact of the weight quantization on the loss in terms of a weighted k-means clustering objective, where the weights are derived from the hessian of the loss function at the original weights.\nThe hessian can be computed efficiently using a back-propagation algorithm similar to that used to compute the gradient, as shown in cited work from the literature. \nThe authors also propose to alternatively use a second-order moment term used by the Adam optimisation algorithm, since it can be loosely interpreted as an approximate Hessian. \n\nIn section 4.5 the authors argue that with their approach it is more natural to quantise weights across all layers together, due to the hessian weighting which takes into account the variable impact across layers of quantisation errors on the network performance. \nThe last statement in this section, however, was not clear to me: \n\u201cIn such deep neural networks, quantising network parameters of all layers together is more efficient since optimizing layer-by-layer clustering jointly across all layers requires exponential time complexity with respect to the number of layers.\u201d\nPerhaps the authors could elaborate a bit more on this point?\n\nIn section 5 the authors develop methods to take into account the code length of the weight quantisation in the clustering process. \nThe first method described by the authors (based on previous work), is uniform quantisation of the weight space, which is then further optimised by their hessian-weighted clustering procedure from section 4. \nFor the case of nonuniform codeword lengths to encode the cluster indices, the authors develop a modification of the Hessian weighted k-means algorithm in which the code length of each cluster is also taken into account, weighted by a factor lambda. Different values of lambda give rise to different compression-accuracy trade-offs, and the authors propose to cluster weights for a variety of lambda values and then pick the most accurate solution obtained, given a certain compression budget.  \n\nIn section 6 the authors report a number of experimental results that were obtained with the proposed methods, and compare these results to those obtained by the layer-wise compression technique of Han et al 2015, and to the uncompressed models. \nFor these experiments the authors used three datasets, MNIST, CIFAR10 and ImageNet, with data-set specific architectures taken from the literature. \nThese results suggest a consistent and significant advantage of the proposed method over the work of Han et al. Comparison to the work of Gong et al 2014 is not made.\nThe results illustrate the advantage of the hessian weighted k-means clustering criterion, and the advantages of the variable bitrate cluster encoding.  \n\nIn conclusion I would say that this is quite interesting work, although the technical novelty seems limited (but I\u2019m not a quantisation expert).\nInterestingly, the proposed techniques do not seem specific to deep conv nets, but rather generically applicable to quantisation of parameters of any model with an associated cost function for which a locally quadratic approximation can be formulated. It would be useful if the authors would discuss this point in their paper.\n"
  },
  {
    "people": [
      "Yoojin"
    ],
    "review": "Dear Reviewers,\n\nWe have upload a revised paper. Here is a list of summary for our revision.\n\n1) We added a performance comparison of uniform quantization with non-weighed mean and uniform quantization with Hessian-weighted mean in Appendix A.3 in the revised paper. We also added a sentence for this in lines 2-4 of the last paragraph of Section 5.3 in the revised paper.\n\n2) We revised the second paragraph of Section 4.5 in the original paper and clarified it as follows:\n\n\"We note that quantization of all layers is proposed under the assumption that all of binary encoded quantized parameters in a network are simply stored in one single array. Under this assumption, if layer-by-layer quantization is employed, then we need to assign some (additional) bits to each binary codeword for layer information (layer index), and it hurts the compression ratio. If we quantize all parameters of a network together, then we can avoid such additional overhead for layer indication when storing binary encoded quantized parameters. Thus, in this case, quantizing all layers together is beneficial and Hessian-weighting can be used to address the different impact of the quantization errors across layers.\n\nFor layer-by-layer quantization, it is advantageous to use separate arrays and separate lookup tables for different layers since layer information can be excluded in each of binary codewords for network parameters. Hessian-weighting can still provide gain even in this case for layer-by-layer quantization since it can address the different impact of the quantization errors of network parameters within each layer as well.\"\n\n3) We added the following sentence in the abstract as your suggestion (see Abstract lines 12-15 in the revised paper).\n\n\"Hessian-weighting properly handles the different impact of quantization errors not only within layers but also across layers and thus it can be employed for quantizing all layers of a network together at once; it is beneficial since one can avoid layer-by-layer compression rate optimization.\"\n\nWe also added the following sentence in Section 7 as your suggestion (see Section 7 lines 8-11 in the revised paper).\n\n\"Hessian-weighting is beneficial in quantizing all of the network parameters together at once since it can handle the different impact of quantization errors properly not only within layers but also across layers; thus, using Hessian-weighting, we can avoid layer-by-layer compression rate optimization.\"\n\n4) We reduced the number of pages from 17 to 15 (excluding reference and appendices).\n\n- We removed the third paragraph in page 2 of the original paper.\n- We removed the last two sentences in the first paragraph of Section 2 of the original paper.\n- We removed the last three sentences in the first paragraph of Section 3 of the original paper.\n- We removed the last three sentences in the first paragraph of Section 4 of the original paper.\n- We removed the second to the fifth paragraphs in Section 5 of the original paper.\n- We revised the paragraphs right before Section 6.1 and Section 6.1 as well in order to remove any duplications.\n- We moved the second table in Table 1 of the original paper to Appendix A.4 in the revised paper since it is extra information.\n\n5) We added the following sentence at the end of Section 4.1 of the revised paper for clarification.\n\n\"We note that we do not consider the interactions between retraining and quantization in our formulation. In this paper, we analyze the expected loss due to quantization of all network parameters assuming no further retraining and focus on finding optimal network quantization schemes that minimize the performance loss while maximizing the compression ratio. After quantization, however, in our experiments, we fine-tune the quantized values (cluster centers) so that we can recover the loss due to quantization and improve the performance further.\"\n\nThank you again for your comments and suggestions. We look forward to further comments.\n\nBest,\nYoojin"
  },
  {
    "people": [
      "Yoojin"
    ],
    "review": "Dear Reviewers,\n\nWe have reduced the length of the paper further and it is now 9 pages (excluding references and Appendix). Here is a list of major changes:\n\n- We removed equation (2) of the previous revision since it is redundant.\n- We removed Section 2 of the previous revision, and added Section 3.1 instead in this revision.\n- We removed Remarks 1-3 in Section 4.1 of the previous revision, and moved all of them to Appendix A.1 instead in this revision.\n- We removed the second paragraph of Section 4.5 in the previous revision.\n- We reduced the length of Remark 4 in Section 5.3 of the previous revision.\n\nWe are open to reduce it further if needed. Please let us know your opinion. Thank you.\n\nBest,\nYoojin"
  },
  {
    "people": [
      "Yoojin"
    ],
    "review": "Dear Reviewers,\n\nWe have uploaded a revised paper.\n\n1) We reduced the length of the paper to 10 pages excluding references and Appendix. To this end, \n\n- We reduced the length of Abstract.\n- We curtailed the first three sections substantially while removing Figure 1 and Figure 2.\n- We removed the detailed discussion on the alternative of Hessian in Section 4.4.\n- We removed the detailed discussion on the iterative ECSQ algorithm in Section 5.4 while moving some to Appendix A.2.\n- We removed the experiment results for LeNet.\n\nWe will try to reduce it further next week. Please let us know your opinion. We are open to reduce it further.\n\n2) We added Remark 1 in Section 4.1 to address the reviewer's comment on the effect of diagonal approximation:\n\n\"Remark 1. The diagonal approximation for Hessian simplifies the optimization problem as well as its solution for network quantization. This simplification however comes with some performance loss. We conjecture that the loss due to this approximation is small. The reason is that the contributions from off-diagonal terms are not always additive and their summation may end up with a small value. However, diagonal terms are all non-negative and therefore their contributions are always additive.\"\n\n3) We added Remark 3 in Section 4.1 to address the reviewer's comment on the generalization of Hessian-weighted quantization methods to other models:\n\n\"Remark 3. Observe that the relation of the Hessian-weighted distortion measure to the quantization loss holds for any model for which the objective function can be approximated as a quadratic function with respect to the parameters to quantize. Hence, the quantization methods proposed in this paper to minimize the Hessian-weighted distortion measure are not specific to neural networks but are generally applicable to quantization of parameters of any model whose objective function is locally quadratic with respect to its parameters approximately.\"\n\n4) We elaborated the exponential time complexity of layer-by-layer quantization in the last paragraph of Section 4.5:\n\n\"Optimizing compression ratios jointly across all layers (to maximize the overall compression ratio for all layers) requires exponential time complexity with respect to the number of layers. This is because the total number of possible combinations of compression ratios for individual layers increases exponentially as the number of layers increases.\"\n\n5) We removed the description on the \"additional bits\" part from Section 4.5 to reduce the length of the paper. We note that it is just a minor implementation issue as the reviewer pointed out.\n\n6) We clarified that [Han 2015] did not evaluate ResNet by adding a row for [Han 2015] in Table 1 and put \"N/A\" for ResNet.\n\n7) We fixed the typos found by reviewers.\n\n8) We removed Appendix A.1 and Appendix A.4 to reduce the length of the paper further.\n\nThank you again for your valuable comments and suggestions. We look forward to further comments.\n\nBest,\nYoojin"
  },
  {
    "people": [
      "Han",
      "Adam",
      "Han",
      "Han"
    ],
    "review": "This paper proposes a novel neural network compression technique.\nThe goal is to compress maximally the network specification via parameter quantisation with a minimum impact on the expected loss.\nIt assumes pruning of the network parameters has already been performed, and only considers the quantisation of the individual scalar parameters of the network.\nIn contrast to previous work (Han et al. 2015a, Gong et al. 2014) the proposed approach takes into account the effect of the weight quantisation on the loss function that is used to train the network, and also takes into account the effect on a variable-length binary encoding of the cluster centers used for the quantisation. \n\nUnfortunately, the submitted paper is 20 pages, rather than the 8 recommended. The length of the paper seems unjustified to me, since the first three sections (first five pages) are very generic and redundant can be largely compressed or skipped (including figures 1 and 2). Although not a strict requirement by the submission guidelines, I would suggest the authors to compress their paper to 8 pages, this will improve the readability of the paper.\n\nTo take into account the impact on the network\u2019s loss the authors propose to use a second order approximation of the cost function of the loss. In the case of weights that originally constitute a local minimum of the loss, this leads to a formulation of the impact of the weight quantization on the loss in terms of a weighted k-means clustering objective, where the weights are derived from the hessian of the loss function at the original weights.\nThe hessian can be computed efficiently using a back-propagation algorithm similar to that used to compute the gradient, as shown in cited work from the literature. \nThe authors also propose to alternatively use a second-order moment term used by the Adam optimisation algorithm, since it can be loosely interpreted as an approximate Hessian. \n\nIn section 4.5 the authors argue that with their approach it is more natural to quantise weights across all layers together, due to the hessian weighting which takes into account the variable impact across layers of quantisation errors on the network performance. \nThe last statement in this section, however, was not clear to me: \n\u201cIn such deep neural networks, quantising network parameters of all layers together is more efficient since optimizing layer-by-layer clustering jointly across all layers requires exponential time complexity with respect to the number of layers.\u201d\nPerhaps the authors could elaborate a bit more on this point?\n\nIn section 5 the authors develop methods to take into account the code length of the weight quantisation in the clustering process. \nThe first method described by the authors (based on previous work), is uniform quantisation of the weight space, which is then further optimised by their hessian-weighted clustering procedure from section 4. \nFor the case of nonuniform codeword lengths to encode the cluster indices, the authors develop a modification of the Hessian weighted k-means algorithm in which the code length of each cluster is also taken into account, weighted by a factor lambda. Different values of lambda give rise to different compression-accuracy trade-offs, and the authors propose to cluster weights for a variety of lambda values and then pick the most accurate solution obtained, given a certain compression budget.  \n\nIn section 6 the authors report a number of experimental results that were obtained with the proposed methods, and compare these results to those obtained by the layer-wise compression technique of Han et al 2015, and to the uncompressed models. \nFor these experiments the authors used three datasets, MNIST, CIFAR10 and ImageNet, with data-set specific architectures taken from the literature. \nThese results suggest a consistent and significant advantage of the proposed method over the work of Han et al. Comparison to the work of Gong et al 2014 is not made.\nThe results illustrate the advantage of the hessian weighted k-means clustering criterion, and the advantages of the variable bitrate cluster encoding.  \n\nIn conclusion I would say that this is quite interesting work, although the technical novelty seems limited (but I\u2019m not a quantisation expert).\nInterestingly, the proposed techniques do not seem specific to deep conv nets, but rather generically applicable to quantisation of parameters of any model with an associated cost function for which a locally quadratic approximation can be formulated. It would be useful if the authors would discuss this point in their paper.\n"
  },
  {
    "people": [
      "Yoojin"
    ],
    "review": "Dear Reviewers,\n\nWe have upload a revised paper. Here is a list of summary for our revision.\n\n1) We added a performance comparison of uniform quantization with non-weighed mean and uniform quantization with Hessian-weighted mean in Appendix A.3 in the revised paper. We also added a sentence for this in lines 2-4 of the last paragraph of Section 5.3 in the revised paper.\n\n2) We revised the second paragraph of Section 4.5 in the original paper and clarified it as follows:\n\n\"We note that quantization of all layers is proposed under the assumption that all of binary encoded quantized parameters in a network are simply stored in one single array. Under this assumption, if layer-by-layer quantization is employed, then we need to assign some (additional) bits to each binary codeword for layer information (layer index), and it hurts the compression ratio. If we quantize all parameters of a network together, then we can avoid such additional overhead for layer indication when storing binary encoded quantized parameters. Thus, in this case, quantizing all layers together is beneficial and Hessian-weighting can be used to address the different impact of the quantization errors across layers.\n\nFor layer-by-layer quantization, it is advantageous to use separate arrays and separate lookup tables for different layers since layer information can be excluded in each of binary codewords for network parameters. Hessian-weighting can still provide gain even in this case for layer-by-layer quantization since it can address the different impact of the quantization errors of network parameters within each layer as well.\"\n\n3) We added the following sentence in the abstract as your suggestion (see Abstract lines 12-15 in the revised paper).\n\n\"Hessian-weighting properly handles the different impact of quantization errors not only within layers but also across layers and thus it can be employed for quantizing all layers of a network together at once; it is beneficial since one can avoid layer-by-layer compression rate optimization.\"\n\nWe also added the following sentence in Section 7 as your suggestion (see Section 7 lines 8-11 in the revised paper).\n\n\"Hessian-weighting is beneficial in quantizing all of the network parameters together at once since it can handle the different impact of quantization errors properly not only within layers but also across layers; thus, using Hessian-weighting, we can avoid layer-by-layer compression rate optimization.\"\n\n4) We reduced the number of pages from 17 to 15 (excluding reference and appendices).\n\n- We removed the third paragraph in page 2 of the original paper.\n- We removed the last two sentences in the first paragraph of Section 2 of the original paper.\n- We removed the last three sentences in the first paragraph of Section 3 of the original paper.\n- We removed the last three sentences in the first paragraph of Section 4 of the original paper.\n- We removed the second to the fifth paragraphs in Section 5 of the original paper.\n- We revised the paragraphs right before Section 6.1 and Section 6.1 as well in order to remove any duplications.\n- We moved the second table in Table 1 of the original paper to Appendix A.4 in the revised paper since it is extra information.\n\n5) We added the following sentence at the end of Section 4.1 of the revised paper for clarification.\n\n\"We note that we do not consider the interactions between retraining and quantization in our formulation. In this paper, we analyze the expected loss due to quantization of all network parameters assuming no further retraining and focus on finding optimal network quantization schemes that minimize the performance loss while maximizing the compression ratio. After quantization, however, in our experiments, we fine-tune the quantized values (cluster centers) so that we can recover the loss due to quantization and improve the performance further.\"\n\nThank you again for your comments and suggestions. We look forward to further comments.\n\nBest,\nYoojin"
  },
  {
    "people": [
      "Ghadimi",
      "Lan"
    ],
    "review": "The work addresses the question of whether mini-batching improves the convergence of stochastic gradient methods, in terms of the number of examples, in the general non-asymptotic/non-convex setting of Ghadimi and Lan. Similar results are already known (at least as folk theory) in simpler regimes, but this result is novel. The reviewers are mixed in their opinions of the paper, but AnonReviewer4 brings up a point that deserves further attention: the analysis assumes that the same step-size is used for different mini-batch sizes. Because of this assumption, the conclusion of the paper is misleading. One of the effects of increasing the mini-batch is to decrease the variance of the estimator, which allows a larger step-size. With this larger step-size the net effect of increasing the mini-batch can actually be positive even in terms of the number of examples. Fixing the step-size removes the chance for the mini-batch strategy to do better in theory, and there is indeed a large amount of empirical evidence that mini-batches can lead to practical improvements. I do think the authors should continue this work, but I think accepting the current version is problematic because of the current conclusion."
  },
  {
    "people": [
      "Ghadimi",
      "Lan"
    ],
    "review": "The work addresses the question of whether mini-batching improves the convergence of stochastic gradient methods, in terms of the number of examples, in the general non-asymptotic/non-convex setting of Ghadimi and Lan. Similar results are already known (at least as folk theory) in simpler regimes, but this result is novel. The reviewers are mixed in their opinions of the paper, but AnonReviewer4 brings up a point that deserves further attention: the analysis assumes that the same step-size is used for different mini-batch sizes. Because of this assumption, the conclusion of the paper is misleading. One of the effects of increasing the mini-batch is to decrease the variance of the estimator, which allows a larger step-size. With this larger step-size the net effect of increasing the mini-batch can actually be positive even in terms of the number of examples. Fixing the step-size removes the chance for the mini-batch strategy to do better in theory, and there is indeed a large amount of empirical evidence that mini-batches can lead to practical improvements. I do think the authors should continue this work, but I think accepting the current version is problematic because of the current conclusion."
  },
  {
    "people": [
      "Neelakantan",
      "Chen"
    ],
    "review": "This paper presents a small trick to improve the model quality of variational autoencoders (further optimizing the ELBO while initializing it from the predictions of the q network, instead of just using those directly) and the idea of using Jacobian vectors to replace simple embeddings when interpreting variational autoencoders.\n\nThe idea of the Jacobian as a natural replacement for embeddings is interesting, as it does seem to cleanly generalize the notion of embeddings from linear models. It'd be interesting to see comparisons with other work seeking to provide context-specific embeddings, either by clustering or by smarter techniques (like Neelakantan et al, Efficient non-parametric estimation of multiple embeddings per word in vector space, or Chen et al A Unified Model for Word Sense Representation and Disambiguation). With the evidence provided in the experimental section of the paper it's hard to be convinced that the Jacobian of VAE-generated embeddings is substantially better at being context-sensitive than prior work.\n\nSimilarly, the idea of further optimizing the ELBO is interesting but not fully explored. It's unclear, for example, what is the tradeoff between the complexity of the q network and steps further optimizing the ELBO, in terms of compute versus accuracy.\n\nOverall the ideas in this paper are good but I'd like to see them a little more fleshed out."
  },
  {
    "people": [
      "Neelakantan",
      "Chen"
    ],
    "review": "This paper presents a small trick to improve the model quality of variational autoencoders (further optimizing the ELBO while initializing it from the predictions of the q network, instead of just using those directly) and the idea of using Jacobian vectors to replace simple embeddings when interpreting variational autoencoders.\n\nThe idea of the Jacobian as a natural replacement for embeddings is interesting, as it does seem to cleanly generalize the notion of embeddings from linear models. It'd be interesting to see comparisons with other work seeking to provide context-specific embeddings, either by clustering or by smarter techniques (like Neelakantan et al, Efficient non-parametric estimation of multiple embeddings per word in vector space, or Chen et al A Unified Model for Word Sense Representation and Disambiguation). With the evidence provided in the experimental section of the paper it's hard to be convinced that the Jacobian of VAE-generated embeddings is substantially better at being context-sensitive than prior work.\n\nSimilarly, the idea of further optimizing the ELBO is interesting but not fully explored. It's unclear, for example, what is the tradeoff between the complexity of the q network and steps further optimizing the ELBO, in terms of compute versus accuracy.\n\nOverall the ideas in this paper are good but I'd like to see them a little more fleshed out."
  },
  {
    "people": [
      "Pushpendre Rastogi",
      "Benamin Van Durme",
      "Raman Arora"
    ],
    "review": "Thank you for your helpful comments.  We just uploaded a revised draft, incorporating the reviewers' suggestions, and hopefully addressing many of their concerns.  Below are the things to note:\n\n- The linear GCCA solution for G and U is included in Appendix A, along with a full gradient derivation: \"... the rows of G are the top r (orthonormal) eigenvectors of M, and $U_j = C_{jj}^{\u22121} Y_j G^T$\" (reviewer 2)\n\n- In the last paragraph of the Optimization subsection (page 4), we include big-Oh notation for the gradient update time complexity.  We leverage the GCCA solution presented in [R1] to scale DGCCA to large datasets. (reviewer 2)\n\n- We qualify the pronouncement of being \"the first nonlinear multiview learning technique\" with the adjective \"CCA-style\".  Although our work focuses on extending CCA-based multiview methods, we recognize that others have attempted to learn embeddings by merging information from multiple views. (reviewer 5)\n\n- In Section 5, \"Other Multiview Work\", we include a discussion of a non-CCA-based techniques for nonlinear representation learning from multiple views, and how they differ from DGCCA.  As reviewers 2 and 5 mention, the multiview learning literature is vast, and we are not able to address all models proposed that make learned representations from more than one view.  However, we do hope that this section will clarify how our proposed model differs from other representation learning techniques exploiting multiple views. (reviewers 2 and 5)\n\n- Appendix C includes a short discussion of how the DGCCA objective reconstruction error relates to downstream task performance for Twitter hashtag recommendation.  In short, we found that high reconstruction error is a strong signal for poor downstream performance, but there is significant variation in downstream performance between embeddings learned by models with low reconstruction error. (reviewer 4)\n\nWe also trained Bridge Correlational Neural Network embeddings for Twitter hashtag and friend recommendation in a series of preliminary experiments.  We swept over hidden layer width in the same range as the DGCCA experiments, $\\lambda \\in \\{0.0, 0.1, 1.0, 10.0\\}$ (the strength of the correlation term in the DGCCA objective), and used either the Twitter user ego text view or their friend network view as the pivot view, since these were the solely most effective views for hashtag and friend recommendtion.  Other learning parameters were left at the defaults and networks were trained for 50 epochs.  However, the performance of these embeddings was much worse than the CCA-style models we compare to (R@1000=0.06 for hashtag recommendation.)  We grant that downstream performance would be improved by tuning learning parameters, but these preliminary experiments underscore the fact that this class of models is not a panacea, and may not be appropriate for these recommendation tasks.  We explicitly note this in the text, since these models assume that all views should be correlated with a pivot view representation.  Entraining all embeddings to a single view is, thus, probably not as effective at hashtag recommendation as learning a CCA-style joint representation for all views. (reviewer 5)\n\nPlease let us know if any of these revisions are confusing, or if you have any additional suggestions.\n\n[R1] Pushpendre Rastogi, Benamin Van Durme, and Raman Arora. Multiview LSA: Representation Learning via Generalized CCA. Proceedings of NAACL. 2015."
  },
  {
    "people": [
      "A. Kumar",
      "X. Dong",
      "Ping-Yu Chen",
      "M. Bronstein"
    ],
    "review": "The authors propose a method that extends the non-linear two-view representation learning methods, and the linear multiview techniques, and combines information from multiple sources into a new non-linear representation learning techniques. \n\nIn general, the method is well described and seems to lead to benefits in different experiments of phonetic transcription of hashtag recommendation. Even if the method is mostly a extension of classical tools (the scheme learns a (deep) network for each view essentially), the combination of the different sources of information seems to be effective for the studied datasets. \n\nIt would be interesting to add or discuss the following issues:\n\n- what is the complexity of the proposed method, esp. the representation learning part?\n- would there by any alternative solution to combine the different networks/views? That could make the proposed solution more novel.\n- the experimental settings, especially in the synthetic experiments, should be more detailed. If possible, the datasets should be made available to encourage reproducibility. \n- the related work is far from complete unfortunately, especially from the perspective of the numerous multiview/multi-modal/multi-layer algorithms that have been proposed in the literature, in different applications domaines like image retrieval or classification, or bibliographic data for example (authors like A. Kumar, X. Dong, Ping-Yu Chen, M. Bronstein, and many others have proposed works in that direction in the last 5 years). No need to compare to all these works obviously, but a more complete description of the related could help appreciating better the true benefits of DGCCA.\n"
  },
  {
    "people": [
      "Pushpendre Rastogi",
      "Benamin Van Durme",
      "Raman Arora"
    ],
    "review": "Thank you for your helpful comments.  We just uploaded a revised draft, incorporating the reviewers' suggestions, and hopefully addressing many of their concerns.  Below are the things to note:\n\n- The linear GCCA solution for G and U is included in Appendix A, along with a full gradient derivation: \"... the rows of G are the top r (orthonormal) eigenvectors of M, and $U_j = C_{jj}^{\u22121} Y_j G^T$\" (reviewer 2)\n\n- In the last paragraph of the Optimization subsection (page 4), we include big-Oh notation for the gradient update time complexity.  We leverage the GCCA solution presented in [R1] to scale DGCCA to large datasets. (reviewer 2)\n\n- We qualify the pronouncement of being \"the first nonlinear multiview learning technique\" with the adjective \"CCA-style\".  Although our work focuses on extending CCA-based multiview methods, we recognize that others have attempted to learn embeddings by merging information from multiple views. (reviewer 5)\n\n- In Section 5, \"Other Multiview Work\", we include a discussion of a non-CCA-based techniques for nonlinear representation learning from multiple views, and how they differ from DGCCA.  As reviewers 2 and 5 mention, the multiview learning literature is vast, and we are not able to address all models proposed that make learned representations from more than one view.  However, we do hope that this section will clarify how our proposed model differs from other representation learning techniques exploiting multiple views. (reviewers 2 and 5)\n\n- Appendix C includes a short discussion of how the DGCCA objective reconstruction error relates to downstream task performance for Twitter hashtag recommendation.  In short, we found that high reconstruction error is a strong signal for poor downstream performance, but there is significant variation in downstream performance between embeddings learned by models with low reconstruction error. (reviewer 4)\n\nWe also trained Bridge Correlational Neural Network embeddings for Twitter hashtag and friend recommendation in a series of preliminary experiments.  We swept over hidden layer width in the same range as the DGCCA experiments, $\\lambda \\in \\{0.0, 0.1, 1.0, 10.0\\}$ (the strength of the correlation term in the DGCCA objective), and used either the Twitter user ego text view or their friend network view as the pivot view, since these were the solely most effective views for hashtag and friend recommendtion.  Other learning parameters were left at the defaults and networks were trained for 50 epochs.  However, the performance of these embeddings was much worse than the CCA-style models we compare to (R@1000=0.06 for hashtag recommendation.)  We grant that downstream performance would be improved by tuning learning parameters, but these preliminary experiments underscore the fact that this class of models is not a panacea, and may not be appropriate for these recommendation tasks.  We explicitly note this in the text, since these models assume that all views should be correlated with a pivot view representation.  Entraining all embeddings to a single view is, thus, probably not as effective at hashtag recommendation as learning a CCA-style joint representation for all views. (reviewer 5)\n\nPlease let us know if any of these revisions are confusing, or if you have any additional suggestions.\n\n[R1] Pushpendre Rastogi, Benamin Van Durme, and Raman Arora. Multiview LSA: Representation Learning via Generalized CCA. Proceedings of NAACL. 2015."
  },
  {
    "people": [
      "A. Kumar",
      "X. Dong",
      "Ping-Yu Chen",
      "M. Bronstein"
    ],
    "review": "The authors propose a method that extends the non-linear two-view representation learning methods, and the linear multiview techniques, and combines information from multiple sources into a new non-linear representation learning techniques. \n\nIn general, the method is well described and seems to lead to benefits in different experiments of phonetic transcription of hashtag recommendation. Even if the method is mostly a extension of classical tools (the scheme learns a (deep) network for each view essentially), the combination of the different sources of information seems to be effective for the studied datasets. \n\nIt would be interesting to add or discuss the following issues:\n\n- what is the complexity of the proposed method, esp. the representation learning part?\n- would there by any alternative solution to combine the different networks/views? That could make the proposed solution more novel.\n- the experimental settings, especially in the synthetic experiments, should be more detailed. If possible, the datasets should be made available to encourage reproducibility. \n- the related work is far from complete unfortunately, especially from the perspective of the numerous multiview/multi-modal/multi-layer algorithms that have been proposed in the literature, in different applications domaines like image retrieval or classification, or bibliographic data for example (authors like A. Kumar, X. Dong, Ping-Yu Chen, M. Bronstein, and many others have proposed works in that direction in the last 5 years). No need to compare to all these works obviously, but a more complete description of the related could help appreciating better the true benefits of DGCCA.\n"
  },
  {
    "people": [
      "Antoine Bordes"
    ],
    "review": "Attempts to use chatbots for every form of human-computer interaction has been a major trend in 2016, with claims that they could solve many forms of dialogs beyond simple chit-chat. This paper represents a serious reality check. While it is mostly relevant for Dialog/Natural Language venues (to educate software engineer about the limitations of current chatbots), it can also be published at Machine Learning venues (to educate researchers about the need for more realistic validation of ML applied to dialogs), so I would consider this work of  high significance.\n\nTwo important conjectures are underlying this paper and likely to open to more research. While they are not in writing, Antoine Bordes clearly stated them during a NIPS workshop presentation that covered this work. Considering the metrics chosen in this paper:\n1)\tThe performance of end2end ML approaches is still insufficient for goal oriented dialogs.\n2)\tWhen comparing algorithms, relative performance on synthetic data is a good predictor of performance on natural data. This would be quite a departure from previous observations, but the authors made a strong effort to match the synthetic and natural conditions.\n\nWhile its original algorithmic contribution consists in one rather simple addition to memory networks (match type), it is the first time these are deployed and tested on a goal-oriented dialog, and the experimental protocol is excellent. The overall paper clarity is excellent and accessible to a readership beyond ML and dialog researchers. I was in particular impressed by how the short appendix on memory networks summarized them so well, followed by the tables that explained the influence of the number of hops.\n\nWhile this paper represents the state-of-the-art in the exploration of more rigorous metrics for dialog modeling, it also reminds us how brittle and somewhat arbitrary these remain. Note this is more a recommendation for future research than  for revision.\n\nFirst they use the per-response accuracy (basically the next utterance classification among a fixed list of responses). Looking at table 3 clearly shows how absurd this can be in practice: all that matters is a correct API call and a reasonably short dialog, though this would only give us a 1/7 accuracy, as the 6 bot responses needed to reach the API call also have to be exact.\n\nWould the per-dialog accuracy, where all responses must be correct, be better? Table 2 shows how sensitive it is to the experimental protocol. I was initially puzzled that the accuracy for subtask T3 (0.0) was much lower that the accuracy for the full dialog T5 (19.7), until the authors pointed me to the tasks definitions (3.1.1) where T3 requires displaying 3 options while T5 only requires displaying one.\n\nFor the concierge data, what would happen if \u2018correct\u2019 meant being the best, not among the 5-best? \n\nWhile I cannot fault the authors for using standard dialog metrics, and coming up with new ones that are actually too pessimistic, I can think of one way to represent dialogs that could result in more meaningful metrics in goal oriented dialogs. Suppose I sell Virtual Assistants as a service, being paid upon successful completion of a dialog. What is the metric that would maximize my revenue? In this restaurant problem, the loss would probably be some weighted sum of the number of errors in the API call, the number of turns to reach that API call and the number of rejected options by the user. However, such as loss cannot be measured on canned dialogs and would either require a real human user or an realistic simulator\n\nAnother issue closely related to representation learning that this paper fails to address or explain properly is what happens if the vocabulary used by the user does not match exactly the vocabulary in the knowledge base. In particular, for the match type algorithm to code \u2018Indian\u2019 as \u2018type of cuisine\u2019, this word would have to occur exactly in the KB. I can imagine situations where the KB uses some obfuscated terminology, and we would like ML to learn the associations rather than humans to hand-describe them.\n"
  },
  {
    "people": [
      "Wen",
      "Liu",
      "Yu"
    ],
    "review": "This paper presents a new, public dataset and tasks for goal-oriented dialogue applications. The dataset and tasks are constructed artificially using rule-based programs, in such a way that different aspects of dialogue system performance can be evaluated ranging from issuing API calls to displaying options, as well as full-fledged dialogue.\n\nThis is a welcome contribution to the dialogue literature, which will help facilitate future research into developing and understanding dialogue systems. Still, there are pitfalls in taking this approach. First, it is not clear how suitable Deep Learning models are for these tasks compared to traditional methods (rule-based systems or shallow models), since Deep Learning models are known to require many training examples and therefore performance difference between different neural networks may simply boil down to regularization techniques. The tasks 1-5 are also completely deterministic, which means evaluating performance on these tasks won't measure the ability of the models to handle noisy and ambiguous interactions (e.g. inferring a distribution over user goals, or executing dialogue repair strategies), which is a very important aspect in dialogue applications. Overall, I still believe this is an interesting direction to explore.\n\nAs discussed in the comments below, the paper does not have any baseline model with word order information. I think this is a strong weakness of the paper, because it makes the neural networks appear unreasonably strong, yet simpler baselines could very likely be be competitive (or better) than the proposed neural networks. To maintain a fair evaluation and correctly assess the power of representation learning for this task, I think it's important that the authors experiment with one additional non-neural network benchmark model which takes into account word order information. This would more convincly demonstrate the utility of Deep Learning models for this task. For example, the one could experiment with a logistic regression model which takes as input 1) word embeddings (similar to the Supervised Embeddings model), 2) bi-gram features, and 3) match-type features. If such a baseline is included, I will increase my rating to 8.\n\n\n\nFinal minor comment: in the conclusion, the paper states \"the existing work has no well defined measures of performances\". This is not really true. End-to-end trainable models for task-oriented dialogue have well-defined performance measures. See, for example \"A Network-based End-to-End Trainable Task-oriented Dialogue System\" by Wen et al. On the other hand, non-goal-oriented dialogue are generally harder to evaluate, but given human subjects these can also be evaluated. In fact, this is what Liu et al (2016) do for Twitter. See also, \"Strategy and Policy Learning for Non-Task-Oriented Conversational Systems\" by Yu et al.\n\n----\n\nI've updated my score following the new results added in the paper."
  },
  {
    "people": [
      "Antoine Bordes"
    ],
    "review": "Attempts to use chatbots for every form of human-computer interaction has been a major trend in 2016, with claims that they could solve many forms of dialogs beyond simple chit-chat. This paper represents a serious reality check. While it is mostly relevant for Dialog/Natural Language venues (to educate software engineer about the limitations of current chatbots), it can also be published at Machine Learning venues (to educate researchers about the need for more realistic validation of ML applied to dialogs), so I would consider this work of  high significance.\n\nTwo important conjectures are underlying this paper and likely to open to more research. While they are not in writing, Antoine Bordes clearly stated them during a NIPS workshop presentation that covered this work. Considering the metrics chosen in this paper:\n1)\tThe performance of end2end ML approaches is still insufficient for goal oriented dialogs.\n2)\tWhen comparing algorithms, relative performance on synthetic data is a good predictor of performance on natural data. This would be quite a departure from previous observations, but the authors made a strong effort to match the synthetic and natural conditions.\n\nWhile its original algorithmic contribution consists in one rather simple addition to memory networks (match type), it is the first time these are deployed and tested on a goal-oriented dialog, and the experimental protocol is excellent. The overall paper clarity is excellent and accessible to a readership beyond ML and dialog researchers. I was in particular impressed by how the short appendix on memory networks summarized them so well, followed by the tables that explained the influence of the number of hops.\n\nWhile this paper represents the state-of-the-art in the exploration of more rigorous metrics for dialog modeling, it also reminds us how brittle and somewhat arbitrary these remain. Note this is more a recommendation for future research than  for revision.\n\nFirst they use the per-response accuracy (basically the next utterance classification among a fixed list of responses). Looking at table 3 clearly shows how absurd this can be in practice: all that matters is a correct API call and a reasonably short dialog, though this would only give us a 1/7 accuracy, as the 6 bot responses needed to reach the API call also have to be exact.\n\nWould the per-dialog accuracy, where all responses must be correct, be better? Table 2 shows how sensitive it is to the experimental protocol. I was initially puzzled that the accuracy for subtask T3 (0.0) was much lower that the accuracy for the full dialog T5 (19.7), until the authors pointed me to the tasks definitions (3.1.1) where T3 requires displaying 3 options while T5 only requires displaying one.\n\nFor the concierge data, what would happen if \u2018correct\u2019 meant being the best, not among the 5-best? \n\nWhile I cannot fault the authors for using standard dialog metrics, and coming up with new ones that are actually too pessimistic, I can think of one way to represent dialogs that could result in more meaningful metrics in goal oriented dialogs. Suppose I sell Virtual Assistants as a service, being paid upon successful completion of a dialog. What is the metric that would maximize my revenue? In this restaurant problem, the loss would probably be some weighted sum of the number of errors in the API call, the number of turns to reach that API call and the number of rejected options by the user. However, such as loss cannot be measured on canned dialogs and would either require a real human user or an realistic simulator\n\nAnother issue closely related to representation learning that this paper fails to address or explain properly is what happens if the vocabulary used by the user does not match exactly the vocabulary in the knowledge base. In particular, for the match type algorithm to code \u2018Indian\u2019 as \u2018type of cuisine\u2019, this word would have to occur exactly in the KB. I can imagine situations where the KB uses some obfuscated terminology, and we would like ML to learn the associations rather than humans to hand-describe them.\n"
  },
  {
    "people": [
      "Wen",
      "Liu",
      "Yu"
    ],
    "review": "This paper presents a new, public dataset and tasks for goal-oriented dialogue applications. The dataset and tasks are constructed artificially using rule-based programs, in such a way that different aspects of dialogue system performance can be evaluated ranging from issuing API calls to displaying options, as well as full-fledged dialogue.\n\nThis is a welcome contribution to the dialogue literature, which will help facilitate future research into developing and understanding dialogue systems. Still, there are pitfalls in taking this approach. First, it is not clear how suitable Deep Learning models are for these tasks compared to traditional methods (rule-based systems or shallow models), since Deep Learning models are known to require many training examples and therefore performance difference between different neural networks may simply boil down to regularization techniques. The tasks 1-5 are also completely deterministic, which means evaluating performance on these tasks won't measure the ability of the models to handle noisy and ambiguous interactions (e.g. inferring a distribution over user goals, or executing dialogue repair strategies), which is a very important aspect in dialogue applications. Overall, I still believe this is an interesting direction to explore.\n\nAs discussed in the comments below, the paper does not have any baseline model with word order information. I think this is a strong weakness of the paper, because it makes the neural networks appear unreasonably strong, yet simpler baselines could very likely be be competitive (or better) than the proposed neural networks. To maintain a fair evaluation and correctly assess the power of representation learning for this task, I think it's important that the authors experiment with one additional non-neural network benchmark model which takes into account word order information. This would more convincly demonstrate the utility of Deep Learning models for this task. For example, the one could experiment with a logistic regression model which takes as input 1) word embeddings (similar to the Supervised Embeddings model), 2) bi-gram features, and 3) match-type features. If such a baseline is included, I will increase my rating to 8.\n\n\n\nFinal minor comment: in the conclusion, the paper states \"the existing work has no well defined measures of performances\". This is not really true. End-to-end trainable models for task-oriented dialogue have well-defined performance measures. See, for example \"A Network-based End-to-End Trainable Task-oriented Dialogue System\" by Wen et al. On the other hand, non-goal-oriented dialogue are generally harder to evaluate, but given human subjects these can also be evaluated. In fact, this is what Liu et al (2016) do for Twitter. See also, \"Strategy and Policy Learning for Non-Task-Oriented Conversational Systems\" by Yu et al.\n\n----\n\nI've updated my score following the new results added in the paper."
  },
  {
    "people": [
      "Uehara"
    ],
    "review": "This paper describes the creation of a corpus of freely-licensed classical music recordings along with corresponding MIDI-scores aligned to the audio.  It also describes experiments in polyphonic transcription using various deep learning approaches, which show promising results.\n\nThe paper is a little disorganised and somewhat contradictory in parts. For example, I find the first sentence in section 2 (MusicNet) would better be pushed one paragraph below so that the section be allowed to begin with a survey of the tools available to researchers in music. Also, the description for Table 3 should probably appear somewhere in the Methods section. Last example: the abstract/intro says the purpose is note prediction; later (4th paragraph of intro) there's a claim that the focus is \"learning low-level features of music....\" I find this slightly disorienting.\n\nAlthough others (Uehara et al., 2016, for example) have discussed collection platforms and corpora, this work is interesting because of its size and the approach for generating features. I'm interested in what the authors will to do expand the offerings in the corpus, both in terms of volume and diversity.\n"
  },
  {
    "people": [
      "Uehara"
    ],
    "review": "This paper describes the creation of a corpus of freely-licensed classical music recordings along with corresponding MIDI-scores aligned to the audio.  It also describes experiments in polyphonic transcription using various deep learning approaches, which show promising results.\n\nThe paper is a little disorganised and somewhat contradictory in parts. For example, I find the first sentence in section 2 (MusicNet) would better be pushed one paragraph below so that the section be allowed to begin with a survey of the tools available to researchers in music. Also, the description for Table 3 should probably appear somewhere in the Methods section. Last example: the abstract/intro says the purpose is note prediction; later (4th paragraph of intro) there's a claim that the focus is \"learning low-level features of music....\" I find this slightly disorienting.\n\nAlthough others (Uehara et al., 2016, for example) have discussed collection platforms and corpora, this work is interesting because of its size and the approach for generating features. I'm interested in what the authors will to do expand the offerings in the corpus, both in terms of volume and diversity.\n"
  },
  {
    "people": [
      "Chen"
    ],
    "review": "Summary: The paper proposes a large-scale dataset for reading comprehension, with the final goal of releasing 1 million questions and answers. The authors have currently released 100,000 queries and their answers. The dataset differs from existing reading comprehension datasets mainly w.r.t queries being sampled from user queries rather than being generated by crowd-workers and answers being generated by crowd-workers rather than being spans of text from the provided passage. The paper presents some analysis of the dataset such as distribution of answer types. The paper also presents the results of some generative and some cloze-style models on the MS MARCO dataset.\n\nStrengths:\n\n1. The paper provides useful insights about the limitations of the existing reading comprehension datasets \u2013 questions asked by crowd-workers have different distribution compared to that of questions asked by actual users of intelligent agents, answers being restricted to span from the reading text rather than requiring reasoning across multiple pieces of text/passages.\n\n2. MS MARCO dataset has novel useful characteristics compared to existing reading comprehension datasets \u2013 questions are sampled from user queries, answers are generated by humans.\n\n3. The experimental evaluation of the existing baseline models on the MS MARCO dataset is satisfactory.\n\nWeaknesses/Suggestions:\n\n1. The paper does not report human performance on the dataset. Human performance should be reported to estimate the difficulty of the dataset. The degree of inter-human agreement will also reflect how well the metric (being used to compute inter-human agreement and accuracies of the baseline models) can deal with variance in the sentence structure with similar semantics.\n\n2. I would like to see the comparison between the answer type distribution in the MS MARCO dataset and that in existing reading comprehension datasets such as SQuAD. This would ground the claim made in the paper the distributions of questions asked by crowd-workers is different from that of user queries.\n\n3. The paper uses automatic metrics such as ROUGE, BLEU for evaluating natural language answers. However, it is known that such metrics poorly correlate with human judgement for tasks such as image caption evaluation (Chen et al., Microsoft COCO Captions: Data Collection and Evaluation Server, CoRR abs/1504.00325 (2015)). So, I wonder how authors justify using such metrics for evaluating open-ended natural language answers.\n\n4. The paper mentions that a classifier was used to filter answer seeking queries from all Bing queries. It would be good to mention the accuracy of this classifier. This will provide insights into what percentage of the MS MARCO questions are answer seeking queries. Similarly, what is the accuracy of the information retrieval based system used to retrieve passages for filtered queries?\n\n5. Please include the description of the best passage baseline in the paper.\n  \n6. Fix opening quotes, i.e. \u201d -> \u201c (for instance, on page 5, \u201dwhat\u201d -> \u201cwhat\u201d).\n\nReview Summary: The paper is well motivated, the use of user queries and human generated answers makes the dataset different from existing datasets. However, I would like to see the human performance on the dataset and quantitative comparison between the distribution of questions obtained from user queries and that of crowd-sourced questions. I would also like the authors to comment on the use of automatic metrics (such as ROUGE, BLEU) in the light of the fact that such metrics do not correlate well with human judgements for tasks such as image caption evaluation."
  },
  {
    "people": [
      "Chen"
    ],
    "review": "Summary: The paper proposes a large-scale dataset for reading comprehension, with the final goal of releasing 1 million questions and answers. The authors have currently released 100,000 queries and their answers. The dataset differs from existing reading comprehension datasets mainly w.r.t queries being sampled from user queries rather than being generated by crowd-workers and answers being generated by crowd-workers rather than being spans of text from the provided passage. The paper presents some analysis of the dataset such as distribution of answer types. The paper also presents the results of some generative and some cloze-style models on the MS MARCO dataset.\n\nStrengths:\n\n1. The paper provides useful insights about the limitations of the existing reading comprehension datasets \u2013 questions asked by crowd-workers have different distribution compared to that of questions asked by actual users of intelligent agents, answers being restricted to span from the reading text rather than requiring reasoning across multiple pieces of text/passages.\n\n2. MS MARCO dataset has novel useful characteristics compared to existing reading comprehension datasets \u2013 questions are sampled from user queries, answers are generated by humans.\n\n3. The experimental evaluation of the existing baseline models on the MS MARCO dataset is satisfactory.\n\nWeaknesses/Suggestions:\n\n1. The paper does not report human performance on the dataset. Human performance should be reported to estimate the difficulty of the dataset. The degree of inter-human agreement will also reflect how well the metric (being used to compute inter-human agreement and accuracies of the baseline models) can deal with variance in the sentence structure with similar semantics.\n\n2. I would like to see the comparison between the answer type distribution in the MS MARCO dataset and that in existing reading comprehension datasets such as SQuAD. This would ground the claim made in the paper the distributions of questions asked by crowd-workers is different from that of user queries.\n\n3. The paper uses automatic metrics such as ROUGE, BLEU for evaluating natural language answers. However, it is known that such metrics poorly correlate with human judgement for tasks such as image caption evaluation (Chen et al., Microsoft COCO Captions: Data Collection and Evaluation Server, CoRR abs/1504.00325 (2015)). So, I wonder how authors justify using such metrics for evaluating open-ended natural language answers.\n\n4. The paper mentions that a classifier was used to filter answer seeking queries from all Bing queries. It would be good to mention the accuracy of this classifier. This will provide insights into what percentage of the MS MARCO questions are answer seeking queries. Similarly, what is the accuracy of the information retrieval based system used to retrieve passages for filtered queries?\n\n5. Please include the description of the best passage baseline in the paper.\n  \n6. Fix opening quotes, i.e. \u201d -> \u201c (for instance, on page 5, \u201dwhat\u201d -> \u201cwhat\u201d).\n\nReview Summary: The paper is well motivated, the use of user queries and human generated answers makes the dataset different from existing datasets. However, I would like to see the human performance on the dataset and quantitative comparison between the distribution of questions obtained from user queries and that of crowd-sourced questions. I would also like the authors to comment on the use of automatic metrics (such as ROUGE, BLEU) in the light of the fact that such metrics do not correlate well with human judgements for tasks such as image caption evaluation."
  },
  {
    "people": [
      "Chen"
    ],
    "review": "Summary: The paper proposes a large-scale dataset for reading comprehension, with the final goal of releasing 1 million questions and answers. The authors have currently released 100,000 queries and their answers. The dataset differs from existing reading comprehension datasets mainly w.r.t queries being sampled from user queries rather than being generated by crowd-workers and answers being generated by crowd-workers rather than being spans of text from the provided passage. The paper presents some analysis of the dataset such as distribution of answer types. The paper also presents the results of some generative and some cloze-style models on the MS MARCO dataset.\n\nStrengths:\n\n1. The paper provides useful insights about the limitations of the existing reading comprehension datasets \u2013 questions asked by crowd-workers have different distribution compared to that of questions asked by actual users of intelligent agents, answers being restricted to span from the reading text rather than requiring reasoning across multiple pieces of text/passages.\n\n2. MS MARCO dataset has novel useful characteristics compared to existing reading comprehension datasets \u2013 questions are sampled from user queries, answers are generated by humans.\n\n3. The experimental evaluation of the existing baseline models on the MS MARCO dataset is satisfactory.\n\nWeaknesses/Suggestions:\n\n1. The paper does not report human performance on the dataset. Human performance should be reported to estimate the difficulty of the dataset. The degree of inter-human agreement will also reflect how well the metric (being used to compute inter-human agreement and accuracies of the baseline models) can deal with variance in the sentence structure with similar semantics.\n\n2. I would like to see the comparison between the answer type distribution in the MS MARCO dataset and that in existing reading comprehension datasets such as SQuAD. This would ground the claim made in the paper the distributions of questions asked by crowd-workers is different from that of user queries.\n\n3. The paper uses automatic metrics such as ROUGE, BLEU for evaluating natural language answers. However, it is known that such metrics poorly correlate with human judgement for tasks such as image caption evaluation (Chen et al., Microsoft COCO Captions: Data Collection and Evaluation Server, CoRR abs/1504.00325 (2015)). So, I wonder how authors justify using such metrics for evaluating open-ended natural language answers.\n\n4. The paper mentions that a classifier was used to filter answer seeking queries from all Bing queries. It would be good to mention the accuracy of this classifier. This will provide insights into what percentage of the MS MARCO questions are answer seeking queries. Similarly, what is the accuracy of the information retrieval based system used to retrieve passages for filtered queries?\n\n5. Please include the description of the best passage baseline in the paper.\n  \n6. Fix opening quotes, i.e. \u201d -> \u201c (for instance, on page 5, \u201dwhat\u201d -> \u201cwhat\u201d).\n\nReview Summary: The paper is well motivated, the use of user queries and human generated answers makes the dataset different from existing datasets. However, I would like to see the human performance on the dataset and quantitative comparison between the distribution of questions obtained from user queries and that of crowd-sourced questions. I would also like the authors to comment on the use of automatic metrics (such as ROUGE, BLEU) in the light of the fact that such metrics do not correlate well with human judgements for tasks such as image caption evaluation."
  },
  {
    "people": [
      "Chen"
    ],
    "review": "Summary: The paper proposes a large-scale dataset for reading comprehension, with the final goal of releasing 1 million questions and answers. The authors have currently released 100,000 queries and their answers. The dataset differs from existing reading comprehension datasets mainly w.r.t queries being sampled from user queries rather than being generated by crowd-workers and answers being generated by crowd-workers rather than being spans of text from the provided passage. The paper presents some analysis of the dataset such as distribution of answer types. The paper also presents the results of some generative and some cloze-style models on the MS MARCO dataset.\n\nStrengths:\n\n1. The paper provides useful insights about the limitations of the existing reading comprehension datasets \u2013 questions asked by crowd-workers have different distribution compared to that of questions asked by actual users of intelligent agents, answers being restricted to span from the reading text rather than requiring reasoning across multiple pieces of text/passages.\n\n2. MS MARCO dataset has novel useful characteristics compared to existing reading comprehension datasets \u2013 questions are sampled from user queries, answers are generated by humans.\n\n3. The experimental evaluation of the existing baseline models on the MS MARCO dataset is satisfactory.\n\nWeaknesses/Suggestions:\n\n1. The paper does not report human performance on the dataset. Human performance should be reported to estimate the difficulty of the dataset. The degree of inter-human agreement will also reflect how well the metric (being used to compute inter-human agreement and accuracies of the baseline models) can deal with variance in the sentence structure with similar semantics.\n\n2. I would like to see the comparison between the answer type distribution in the MS MARCO dataset and that in existing reading comprehension datasets such as SQuAD. This would ground the claim made in the paper the distributions of questions asked by crowd-workers is different from that of user queries.\n\n3. The paper uses automatic metrics such as ROUGE, BLEU for evaluating natural language answers. However, it is known that such metrics poorly correlate with human judgement for tasks such as image caption evaluation (Chen et al., Microsoft COCO Captions: Data Collection and Evaluation Server, CoRR abs/1504.00325 (2015)). So, I wonder how authors justify using such metrics for evaluating open-ended natural language answers.\n\n4. The paper mentions that a classifier was used to filter answer seeking queries from all Bing queries. It would be good to mention the accuracy of this classifier. This will provide insights into what percentage of the MS MARCO questions are answer seeking queries. Similarly, what is the accuracy of the information retrieval based system used to retrieve passages for filtered queries?\n\n5. Please include the description of the best passage baseline in the paper.\n  \n6. Fix opening quotes, i.e. \u201d -> \u201c (for instance, on page 5, \u201dwhat\u201d -> \u201cwhat\u201d).\n\nReview Summary: The paper is well motivated, the use of user queries and human generated answers makes the dataset different from existing datasets. However, I would like to see the human performance on the dataset and quantitative comparison between the distribution of questions obtained from user queries and that of crowd-sourced questions. I would also like the authors to comment on the use of automatic metrics (such as ROUGE, BLEU) in the light of the fact that such metrics do not correlate well with human judgements for tasks such as image caption evaluation."
  },
  {
    "people": [
      "wsabie"
    ],
    "review": "The paper proposes a way to learn continuous features for input data which consists of multiple categorical data. The idea is to embed each category in a learnable low dimensional continuous space, explicitly compute the pair-wise interaction among different categories in a given input sample (which is achieved by either taking a component-wise dot product or component-wise addition), perform k-max pooling to select a subset of the most informative interactions, and repeat the process some number of times, until you get the final feature vector of the given input. This feature vector is then used as input to a classifier/regressor to accomplish the final task. The embeddings of the categories are learnt in the usual way. In the experiment section, the authors show on a synthetic dataset that their procedure is indeed able to select the relevant interactions in the data. On one real world dataset (iPinYou) the model seems to outperform a couple of simple baselines. \n\nMy major concern with this paper is that their's nothing new in it. The idea of embedding the categorical data having mixed categories has already been handled in the past literature, where essentially one learns a separate lookup table for each class of categories: an input is represented by concatenation of the embeddings from these lookup table, and a non-linear function (a deep network) is plugged on top to get the features of the input. The only rather marginal contribution is the explicit modeling of the interactions among categories in equations 2/3/4/5. Other than that there's nothing else in the paper. \n\nNot only that, I feel that these interactions can (and should) automatically be learned by plugging in a deep convolutional network on top of the embeddings of the input. So I'm not sure how useful the contribution is. \n\nThe experimental section is rather weak. They authors test their method on a single real world data set against a couple of rather weak baselines. I would have much preferred for them to evaluate against numerous models proposed in the literature which handle similar problems, including wsabie. \n\nWhile the authors argued in their response that wsabie was not suited for their problem, i strongly disagree with that claim. While the original wsabie paper showed experiments using images as inputs, their training methodology can easily be extended to other types of data sets, including categorical data. For instance, I conjecture that the model i proposed above (embed all the categorical inputs, concatenate the embeddings, plug a deep conv-net on top and train using some margin loss) will perform as well if not better than the hand coded interaction model proposed in this paper. Of course I could be wrong, but it would be far more convincing if their model was tested against such baselines. "
  },
  {
    "people": [
      "wsabie"
    ],
    "review": "The paper proposes a way to learn continuous features for input data which consists of multiple categorical data. The idea is to embed each category in a learnable low dimensional continuous space, explicitly compute the pair-wise interaction among different categories in a given input sample (which is achieved by either taking a component-wise dot product or component-wise addition), perform k-max pooling to select a subset of the most informative interactions, and repeat the process some number of times, until you get the final feature vector of the given input. This feature vector is then used as input to a classifier/regressor to accomplish the final task. The embeddings of the categories are learnt in the usual way. In the experiment section, the authors show on a synthetic dataset that their procedure is indeed able to select the relevant interactions in the data. On one real world dataset (iPinYou) the model seems to outperform a couple of simple baselines. \n\nMy major concern with this paper is that their's nothing new in it. The idea of embedding the categorical data having mixed categories has already been handled in the past literature, where essentially one learns a separate lookup table for each class of categories: an input is represented by concatenation of the embeddings from these lookup table, and a non-linear function (a deep network) is plugged on top to get the features of the input. The only rather marginal contribution is the explicit modeling of the interactions among categories in equations 2/3/4/5. Other than that there's nothing else in the paper. \n\nNot only that, I feel that these interactions can (and should) automatically be learned by plugging in a deep convolutional network on top of the embeddings of the input. So I'm not sure how useful the contribution is. \n\nThe experimental section is rather weak. They authors test their method on a single real world data set against a couple of rather weak baselines. I would have much preferred for them to evaluate against numerous models proposed in the literature which handle similar problems, including wsabie. \n\nWhile the authors argued in their response that wsabie was not suited for their problem, i strongly disagree with that claim. While the original wsabie paper showed experiments using images as inputs, their training methodology can easily be extended to other types of data sets, including categorical data. For instance, I conjecture that the model i proposed above (embed all the categorical inputs, concatenate the embeddings, plug a deep conv-net on top and train using some margin loss) will perform as well if not better than the hand coded interaction model proposed in this paper. Of course I could be wrong, but it would be far more convincing if their model was tested against such baselines. "
  },
  {
    "people": [
      "Wang",
      "Manning"
    ],
    "review": "The paper introduces supervised deep learning with layer-wise reconstruction loss (in addition to the supervised loss) and class-conditional semantic additive noise for better representation learning. Total correlation measure and additional insights from auto-encoder are used to derive layer-wise reconstruction loss and is further combined with supervised loss. When combining with supervised loss the class-conditional additive noise model is proposed, which showed consistent improvement over the baseline model. Experiments on MNIST and CIFAR-10 datasets while changing the number of training examples per class are done extensively.\n\nThe derivation of Equation (3) from total correlation is hacky. Moreover, assuming graphical model between X, Y and Z, it should be more carefully derived to estimate H(X|Z) and H(Z|Y). The current proposal, encoding Z and Y from X and decoding from encoded representation is not really well justified.\n\nIs \\sigma in Equation 8 trainable parameter or hyperparameter? If it is trainable how it is trained? If it is not, how are they set? Does j correspond to one of the class? The proposed feature augmentation sounds like simply adding gaussian noise to the pre-softmax neurons. That being said, the proposed method is not different from gaussian dropout (Wang and Manning, ICML 2013) but applied on different layers. In addition, there is a missing reference (DisturbLabel: Regularizing CNN on the Loss Layer, CVPR 2016) that applied synthetic noise process on the loss layer.\n\nExperiments should be done for multiple times with different random subsets and authors should provide mean and standard error. Overall, I believe the proposed method is not very well justified and has limited novelty. "
  },
  {
    "people": [
      "Wang",
      "Manning"
    ],
    "review": "The paper introduces supervised deep learning with layer-wise reconstruction loss (in addition to the supervised loss) and class-conditional semantic additive noise for better representation learning. Total correlation measure and additional insights from auto-encoder are used to derive layer-wise reconstruction loss and is further combined with supervised loss. When combining with supervised loss the class-conditional additive noise model is proposed, which showed consistent improvement over the baseline model. Experiments on MNIST and CIFAR-10 datasets while changing the number of training examples per class are done extensively.\n\nThe derivation of Equation (3) from total correlation is hacky. Moreover, assuming graphical model between X, Y and Z, it should be more carefully derived to estimate H(X|Z) and H(Z|Y). The current proposal, encoding Z and Y from X and decoding from encoded representation is not really well justified.\n\nIs \\sigma in Equation 8 trainable parameter or hyperparameter? If it is trainable how it is trained? If it is not, how are they set? Does j correspond to one of the class? The proposed feature augmentation sounds like simply adding gaussian noise to the pre-softmax neurons. That being said, the proposed method is not different from gaussian dropout (Wang and Manning, ICML 2013) but applied on different layers. In addition, there is a missing reference (DisturbLabel: Regularizing CNN on the Loss Layer, CVPR 2016) that applied synthetic noise process on the loss layer.\n\nExperiments should be done for multiple times with different random subsets and authors should provide mean and standard error. Overall, I believe the proposed method is not very well justified and has limited novelty. "
  },
  {
    "people": [
      "Choromanska",
      "Saxe",
      "L. Hamey"
    ],
    "review": "This paper studies the error surface of deep rectifier networks, giving specific examples for which the error surface has local minima. Several experimental results show that learning can be trapped at apparent local minima by a variety of factors ranging from the nature of the dataset to the nature of the initializations. This paper develops a lot of good intuitions and useful examples of ways that training can go awry. \n\nEven though the examples constructed in this paper are contrived, this does not necessarily remove their theoretical importance. It is very useful to have simple examples where things go wrong. However the broader theoretical framing of the paper appears to be going after a strawman.\n\n\u201cThe underlying easiness of optimizing deep networks does not simply rest just in the emerging structures due to high dimensional spaces, but is rather tightly connected to the intrinsic characteristics of the data these models are run on.\u201d I believe this perspective is already contained in several of the works cited as not belonging to this perspective. Choromanska et al., for instance, analyze Gaussian inputs, and so clearly make claims based on characteristics of the data the models are run on. More broadly, the loss function is determined jointly by the dataset and the model parameters, and so no account of the error surface can be separated from dataset properties. It is not clear to me what \u2018emerging structures due to high dimensional spaces\u2019 are, or what they could be, that would make them independent of the dataset and initial model parameters. The emerging structure of the error surface is necessarily related to the dataset and model parameters.\n\nAgain, a key worry with this paper is that it is aiming at a strawman: replica methods characterize average behavior for infinite systems, so it is not surprising that specific finite sized systems might yield poor optimization landscapes. The paper seems surprised that training can be broken with a bad initialization, but initialization is known to be critical, even for linear networks: saddle points are not innocuous, with bad initializations dramatically slowing learning (e.g. Saxe et al. 2014).\n\nIt seems like the proof of proposition 5 may have an error. Suppose cdf_b(0) = 0 and cdf_W(0)=1/2. We have P(learning fails) >= 1 - 1/2^{h^2(k-1)}, meaning that the probability of failure _increases_ as the number of hidden units increases. It seems like it should rather be (ignoring the bias) p(fails) >= 1 - [ 1 - p(w<0)^h^2]^{k-1}. In this case the limit as k-> infinity depends on how h scales with k, so it is no longer necessarily true that \u201cone does not have a globally good behaviour of learning regardless of the model size.\u201d\n\nThe paper also appears to insufficiently distinguish between local minima and saddle points. Section 3.1 states it shows training being stuck in a local minimum, but this is based on training with a fixed budget of epochs. It is not possible to tell whether this result reflects a genuine local minimum or a saddle point based on simulation results. \nIt may also be the case that, while rectifiers suffer from genuine blind spots, sigmoid or soft rectifier nonlinearities may not. On the XOR problem with two hidden nodes, for instance, it was thought that were local minima but in fact there are none (e.g. L. Hamey, \u201cAnalysis of the error surface of the XOR network with two hidden nodes,\u201d 1995). \n\nIf the desire is simply to show that training does not converge for particular finite problems, much simpler counterexamples can be constructed and would suffice: set all hidden unit weights to zero, for instance. \n\nIn the response to prereview questions, the authors write \u2018If the \u201ccomplete characterization\u201d [of the error surface] was indeed universally valid, we would not be able to break the learning with the initialization\u2019 but, as mentioned previously, the basic results for even deep linear networks show that a bad initialization (at or near a saddle point) will break learning. Again, it seems this paper is attacking a straw man along the lines of \u201cnothing can possibly go wrong with neural network training.\u201d No prior theoretical result claims this. \n\nThe Figure 2 explanation seems counterintuitive to me. Simply scaling the input, if the weight matrices are initialized with zero biases, will not change the regions over which each ReLU activates. That is, this manipulation does not achieve the goal of concentrating \u201cmost of the data points in very few linear regions.\u201d A far more likely explanation is that the much weaker scaling has not been compensated by the learning algorithm, but the algorithm would converge if run longer. The response notes that training has been conducted for an order of magnitude longer than required for the unscaled input to converge, but the scaling on the data is not one but five orders of magnitude\u2014and indeed the training does converge without issue for scaling up to four orders of magnitude. The response notes that Adam should compensate for the scaling factor, but this depends on the details of the Adam implementation\u2014the epsilon factor used to protect against division by zero, for example. \n\nThis paper contains many interesting results, but a variety of small technical concerns remain.\n\n"
  },
  {
    "people": [
      "Choromanska",
      "Saxe",
      "L. Hamey"
    ],
    "review": "This paper studies the error surface of deep rectifier networks, giving specific examples for which the error surface has local minima. Several experimental results show that learning can be trapped at apparent local minima by a variety of factors ranging from the nature of the dataset to the nature of the initializations. This paper develops a lot of good intuitions and useful examples of ways that training can go awry. \n\nEven though the examples constructed in this paper are contrived, this does not necessarily remove their theoretical importance. It is very useful to have simple examples where things go wrong. However the broader theoretical framing of the paper appears to be going after a strawman.\n\n\u201cThe underlying easiness of optimizing deep networks does not simply rest just in the emerging structures due to high dimensional spaces, but is rather tightly connected to the intrinsic characteristics of the data these models are run on.\u201d I believe this perspective is already contained in several of the works cited as not belonging to this perspective. Choromanska et al., for instance, analyze Gaussian inputs, and so clearly make claims based on characteristics of the data the models are run on. More broadly, the loss function is determined jointly by the dataset and the model parameters, and so no account of the error surface can be separated from dataset properties. It is not clear to me what \u2018emerging structures due to high dimensional spaces\u2019 are, or what they could be, that would make them independent of the dataset and initial model parameters. The emerging structure of the error surface is necessarily related to the dataset and model parameters.\n\nAgain, a key worry with this paper is that it is aiming at a strawman: replica methods characterize average behavior for infinite systems, so it is not surprising that specific finite sized systems might yield poor optimization landscapes. The paper seems surprised that training can be broken with a bad initialization, but initialization is known to be critical, even for linear networks: saddle points are not innocuous, with bad initializations dramatically slowing learning (e.g. Saxe et al. 2014).\n\nIt seems like the proof of proposition 5 may have an error. Suppose cdf_b(0) = 0 and cdf_W(0)=1/2. We have P(learning fails) >= 1 - 1/2^{h^2(k-1)}, meaning that the probability of failure _increases_ as the number of hidden units increases. It seems like it should rather be (ignoring the bias) p(fails) >= 1 - [ 1 - p(w<0)^h^2]^{k-1}. In this case the limit as k-> infinity depends on how h scales with k, so it is no longer necessarily true that \u201cone does not have a globally good behaviour of learning regardless of the model size.\u201d\n\nThe paper also appears to insufficiently distinguish between local minima and saddle points. Section 3.1 states it shows training being stuck in a local minimum, but this is based on training with a fixed budget of epochs. It is not possible to tell whether this result reflects a genuine local minimum or a saddle point based on simulation results. \nIt may also be the case that, while rectifiers suffer from genuine blind spots, sigmoid or soft rectifier nonlinearities may not. On the XOR problem with two hidden nodes, for instance, it was thought that were local minima but in fact there are none (e.g. L. Hamey, \u201cAnalysis of the error surface of the XOR network with two hidden nodes,\u201d 1995). \n\nIf the desire is simply to show that training does not converge for particular finite problems, much simpler counterexamples can be constructed and would suffice: set all hidden unit weights to zero, for instance. \n\nIn the response to prereview questions, the authors write \u2018If the \u201ccomplete characterization\u201d [of the error surface] was indeed universally valid, we would not be able to break the learning with the initialization\u2019 but, as mentioned previously, the basic results for even deep linear networks show that a bad initialization (at or near a saddle point) will break learning. Again, it seems this paper is attacking a straw man along the lines of \u201cnothing can possibly go wrong with neural network training.\u201d No prior theoretical result claims this. \n\nThe Figure 2 explanation seems counterintuitive to me. Simply scaling the input, if the weight matrices are initialized with zero biases, will not change the regions over which each ReLU activates. That is, this manipulation does not achieve the goal of concentrating \u201cmost of the data points in very few linear regions.\u201d A far more likely explanation is that the much weaker scaling has not been compensated by the learning algorithm, but the algorithm would converge if run longer. The response notes that training has been conducted for an order of magnitude longer than required for the unscaled input to converge, but the scaling on the data is not one but five orders of magnitude\u2014and indeed the training does converge without issue for scaling up to four orders of magnitude. The response notes that Adam should compensate for the scaling factor, but this depends on the details of the Adam implementation\u2014the epsilon factor used to protect against division by zero, for example. \n\nThis paper contains many interesting results, but a variety of small technical concerns remain.\n\n"
  },
  {
    "people": [
      "Ammar",
      "Raimalwala"
    ],
    "review": "The paper considers the problem of transferring skills between robots with different morphologies, in the context of agents that have to perform several tasks.  A core component of the proposed approach is to use a task-invariant future space, which can be shared between tasks & between agents.\n\nCompared to previous work (Ammar et al. 2015), it seems the main contribution here is to \u201cassume that good correspondences in episodic tasks can be extracted through time alignment\u201d (Sec. 2).  This is an interesting hypothesis. There is also similarity to work by Raimalwala et al (2016), but the authors argue their method is better equipped to handle non-linear dynamics. These are two interesting hypotheses, however I don\u2019t see that they have been verified in the presented empirical results.  In particular, the question of the pairing correspondence seems crucial. What happens when the time alignment is not suitable. Is it possible to use dynamic time warping (or similar method) to achieve reasonable results?  Robustness to misspecification of the pairing correspondence P seems a major concern.\n\nIn general, more comparison to other transfer methods, including those listed in Sec.2, would be very valuable.  The addition of Sec.5.1 is definitely a right step in this direction, but represents a small portion of the recent work on transfer learning.  I appreciate that other methods transfer other pieces of information (e.g. the policy), but still if the end goal is better performance, what is worth transferring (in addition to how to do the transfer) should be a reasonable question to explore.\n\nOverall, the paper tackles an important problem, but this is a very active area of research, and further comparison to other methods would be worthwhile.  The method proposed of transferring the representation is well motivated, cleanly described, and conceptually sound.  The assumption that time alignment can be used for the state pairing seems problematic, and should be further validated."
  },
  {
    "people": [
      "Ammar",
      "Bou Ammar et al",
      "Bou Ammar"
    ],
    "review": "Dear Reviewers and Area Chair,\nThe reviewers for the paper provided a positive evaluation, with suggestions for additional experiments providing comparisons with several prior methods, some clarifications, and brought up the important problem of addressing time-based correspondence alignment. We have edited the paper to include additional experiments that address these issues, and therefore believe that the reviewer concerns about the work have been addressed. We discuss the specifics below.\n\nAnonReviewer 1 Comments: \n\u201cCompared to previous work (Ammar et al. 2015)\u201d\n- Performed this comparison and added the results into the Figures 5 and 8. We found that our method performed significantly better than this prior work. \n\u201cIs it possible to use dynamic time warping (or similar method) to achieve reasonable results?  Robustness to misspecification of the pairing correspondence P seems a major concern.\u201d\n- We introduced an EM style algorithm which removes this assumption. Description of this method are in Section 3.3.2, and results in Figures 5 and 8. \n\u201cmore comparison to other transfer methods, including those listed in Sec.2, would be very valuable\u201d\n- We have implemented several more baselines: KCCA, Unsupervised Manifold Alignment as suggested by Bou Ammar et al, Direct mappings, random projections, CCA in Figures 5 and 8.\n\nAnonReviewer3 Comments: \n\u201cpreferable to see comparisons for something more current and up to date, such as manifold alignment or kernel CCA\u201d, \u201ccomparisons to related approaches is not very up to date\u201d \n-Performed comparisons to Kernel CCA and Manifold Alignment using Unsupervised Manifold Alignment (Wang and Mahadevan, Bou Ammar et al) and added to our experiments in Figures 5 and 8. \n\nAnonReviewer4 Comments: \n\u201cA limitation of the paper is that the authors suppose that time alignment is trivial [...] could be dealt with through subsampling, dynamic time warping, or learning a matching function\u201d \n-We introduce a new EM style algorithm alternating between feature learning and dynamic time warping. Description is in Section 3.3.2 and results in Figures 5 and 8. \n\u201cAnother baseline (worse than CCA) would be to just have the random projections for \"f\" and \"g\"\u201d\n-We added this comparison in Figures 5 and 8.\n\u201cat least a much bigger sample budget should be tested [...] control for the fact that the embeddings were trained with more iterations in the case of doing transfer\u201d \n- We ran the baseline with a significantly higher sample budget as shown in Table 1. The poor performance is likely due to not enough guided exploration happening without good reward shaping for the baseline. \n\u201cproblem of learning invariant feature spaces is also linked to metric learning [...] no parallel is drawn with Multi-Task learning in ML \u201d\n-The additional references suggested have been added in the Related Work (Section 2).\n"
  },
  {
    "people": [
      "Abhishek",
      "Sergey"
    ],
    "review": "Dear reviewers,\n\nWe have addressed the concerns presented in the reviews, including comparisons to several prior methods and a method for automatically determining alignment, as detailed in the individual reviewer responses. If you have suggestions for additional comparisons, we would be happy to add them in the final version. As such, we hope you will consider adjusting your reviews accordingly. \n\nRegards\nAbhishek, Coline, Yuxuan, Pieter, Sergey"
  },
  {
    "people": [
      "Ammar"
    ],
    "review": "We thank the reviewers for their comments. Most comments asked for additional comparisons and learned state correspondences. \nIn response to reviewer comments, we have added the following to our paper:\n\n1. Learning state correspondences by using an alternating optimization to jointly assign the state pairs in P and learn the embedding functions f and g. This significantly relaxes the assumption that states can be paired by time-step and provides better performance in experiment 1. See section 3.3.2 for a description of this method and figures 5 and 7a for results.\n\n2. All of the comparisons suggested by the reviewers: kernel-CCA, unsupervised manifold alignment (Ammar et al. 2015), and random projections. Results are in Figures 5 and 7a.\n\n3. Connections to metric learning and multitask learning in Section 2.\n\nThe comparisons have been run on experiments 1 and 2 and will be added to experiment 3 for the final version."
  },
  {
    "people": [
      "Xing"
    ],
    "review": "This paper presents an approach for skills transfer from one task to another in a control setting (trained by RL) by forcing the embeddings learned on two different tasks to be close (L2 penalty). The experiments are conducted in MuJoCo, with a set of experiments being from the state of the joints/links (5.2/5.3) and a set of experiments on the pixels (5.4). They exhibit transfer from arms with different number of links, and from a torque-driven arm to a tendon-driven arm.\n\nOne limitation of the paper is that the authors suppose that time alignment is trivial, because the tasks are all episodic and in the same domain. Time alignment is one form of domain adaptation / transfer that is not dealt with in the paper, that could be dealt with through subsampling, dynamic time warping, or learning a matching function (e.g. neural network).\n\nGeneral remarks: The approach is compared to CCA, which is a relevant baseline. However, as the paper is purely experimental, another baseline (worse than CCA) would be to just have the random projections for \"f\" and \"g\" (the embedding functions on the two domains), to check that the bad performance of the \"no transfer\" version of the model is due to over-specialisation of these embeddings. I would also add (for information) that the problem of learning invariant feature spaces is also linked to metric learning (e.g. [Xing et al. 2002]). More generally, no parallel is drawn with multi-task learning in ML. In the case of knowledge transfer (4.1.1), it may make sense to anneal \\alpha.\n\nThe experiments feel a bit rushed. In particular, the performance of the baseline being always 0 (no transfer at all) is uninformative, at least a much bigger sample budget should be tested. Also, why does Figure 7.b contain no \"CCA\" nor \"direct mapping\" results? Another concern that I have with the experiments: (if/how) did the author control for the fact that the embeddings were trained with more iterations in the case of doing transfer?\n\nOverall, the study of transfer is most welcomed in RL. The experiments in this paper are interesting enough for publication, but the paper could have been more thorough."
  },
  {
    "people": [
      "Ammar",
      "Raimalwala"
    ],
    "review": "The paper considers the problem of transferring skills between robots with different morphologies, in the context of agents that have to perform several tasks.  A core component of the proposed approach is to use a task-invariant future space, which can be shared between tasks & between agents.\n\nCompared to previous work (Ammar et al. 2015), it seems the main contribution here is to \u201cassume that good correspondences in episodic tasks can be extracted through time alignment\u201d (Sec. 2).  This is an interesting hypothesis. There is also similarity to work by Raimalwala et al (2016), but the authors argue their method is better equipped to handle non-linear dynamics. These are two interesting hypotheses, however I don\u2019t see that they have been verified in the presented empirical results.  In particular, the question of the pairing correspondence seems crucial. What happens when the time alignment is not suitable. Is it possible to use dynamic time warping (or similar method) to achieve reasonable results?  Robustness to misspecification of the pairing correspondence P seems a major concern.\n\nIn general, more comparison to other transfer methods, including those listed in Sec.2, would be very valuable.  The addition of Sec.5.1 is definitely a right step in this direction, but represents a small portion of the recent work on transfer learning.  I appreciate that other methods transfer other pieces of information (e.g. the policy), but still if the end goal is better performance, what is worth transferring (in addition to how to do the transfer) should be a reasonable question to explore.\n\nOverall, the paper tackles an important problem, but this is a very active area of research, and further comparison to other methods would be worthwhile.  The method proposed of transferring the representation is well motivated, cleanly described, and conceptually sound.  The assumption that time alignment can be used for the state pairing seems problematic, and should be further validated."
  },
  {
    "people": [
      "Ammar",
      "Raimalwala"
    ],
    "review": "The paper considers the problem of transferring skills between robots with different morphologies, in the context of agents that have to perform several tasks.  A core component of the proposed approach is to use a task-invariant future space, which can be shared between tasks & between agents.\n\nCompared to previous work (Ammar et al. 2015), it seems the main contribution here is to \u201cassume that good correspondences in episodic tasks can be extracted through time alignment\u201d (Sec. 2).  This is an interesting hypothesis. There is also similarity to work by Raimalwala et al (2016), but the authors argue their method is better equipped to handle non-linear dynamics. These are two interesting hypotheses, however I don\u2019t see that they have been verified in the presented empirical results.  In particular, the question of the pairing correspondence seems crucial. What happens when the time alignment is not suitable. Is it possible to use dynamic time warping (or similar method) to achieve reasonable results?  Robustness to misspecification of the pairing correspondence P seems a major concern.\n\nIn general, more comparison to other transfer methods, including those listed in Sec.2, would be very valuable.  The addition of Sec.5.1 is definitely a right step in this direction, but represents a small portion of the recent work on transfer learning.  I appreciate that other methods transfer other pieces of information (e.g. the policy), but still if the end goal is better performance, what is worth transferring (in addition to how to do the transfer) should be a reasonable question to explore.\n\nOverall, the paper tackles an important problem, but this is a very active area of research, and further comparison to other methods would be worthwhile.  The method proposed of transferring the representation is well motivated, cleanly described, and conceptually sound.  The assumption that time alignment can be used for the state pairing seems problematic, and should be further validated."
  },
  {
    "people": [
      "Ammar",
      "Bou Ammar et al",
      "Bou Ammar"
    ],
    "review": "Dear Reviewers and Area Chair,\nThe reviewers for the paper provided a positive evaluation, with suggestions for additional experiments providing comparisons with several prior methods, some clarifications, and brought up the important problem of addressing time-based correspondence alignment. We have edited the paper to include additional experiments that address these issues, and therefore believe that the reviewer concerns about the work have been addressed. We discuss the specifics below.\n\nAnonReviewer 1 Comments: \n\u201cCompared to previous work (Ammar et al. 2015)\u201d\n- Performed this comparison and added the results into the Figures 5 and 8. We found that our method performed significantly better than this prior work. \n\u201cIs it possible to use dynamic time warping (or similar method) to achieve reasonable results?  Robustness to misspecification of the pairing correspondence P seems a major concern.\u201d\n- We introduced an EM style algorithm which removes this assumption. Description of this method are in Section 3.3.2, and results in Figures 5 and 8. \n\u201cmore comparison to other transfer methods, including those listed in Sec.2, would be very valuable\u201d\n- We have implemented several more baselines: KCCA, Unsupervised Manifold Alignment as suggested by Bou Ammar et al, Direct mappings, random projections, CCA in Figures 5 and 8.\n\nAnonReviewer3 Comments: \n\u201cpreferable to see comparisons for something more current and up to date, such as manifold alignment or kernel CCA\u201d, \u201ccomparisons to related approaches is not very up to date\u201d \n-Performed comparisons to Kernel CCA and Manifold Alignment using Unsupervised Manifold Alignment (Wang and Mahadevan, Bou Ammar et al) and added to our experiments in Figures 5 and 8. \n\nAnonReviewer4 Comments: \n\u201cA limitation of the paper is that the authors suppose that time alignment is trivial [...] could be dealt with through subsampling, dynamic time warping, or learning a matching function\u201d \n-We introduce a new EM style algorithm alternating between feature learning and dynamic time warping. Description is in Section 3.3.2 and results in Figures 5 and 8. \n\u201cAnother baseline (worse than CCA) would be to just have the random projections for \"f\" and \"g\"\u201d\n-We added this comparison in Figures 5 and 8.\n\u201cat least a much bigger sample budget should be tested [...] control for the fact that the embeddings were trained with more iterations in the case of doing transfer\u201d \n- We ran the baseline with a significantly higher sample budget as shown in Table 1. The poor performance is likely due to not enough guided exploration happening without good reward shaping for the baseline. \n\u201cproblem of learning invariant feature spaces is also linked to metric learning [...] no parallel is drawn with Multi-Task learning in ML \u201d\n-The additional references suggested have been added in the Related Work (Section 2).\n"
  },
  {
    "people": [
      "Abhishek",
      "Sergey"
    ],
    "review": "Dear reviewers,\n\nWe have addressed the concerns presented in the reviews, including comparisons to several prior methods and a method for automatically determining alignment, as detailed in the individual reviewer responses. If you have suggestions for additional comparisons, we would be happy to add them in the final version. As such, we hope you will consider adjusting your reviews accordingly. \n\nRegards\nAbhishek, Coline, Yuxuan, Pieter, Sergey"
  },
  {
    "people": [
      "Ammar"
    ],
    "review": "We thank the reviewers for their comments. Most comments asked for additional comparisons and learned state correspondences. \nIn response to reviewer comments, we have added the following to our paper:\n\n1. Learning state correspondences by using an alternating optimization to jointly assign the state pairs in P and learn the embedding functions f and g. This significantly relaxes the assumption that states can be paired by time-step and provides better performance in experiment 1. See section 3.3.2 for a description of this method and figures 5 and 7a for results.\n\n2. All of the comparisons suggested by the reviewers: kernel-CCA, unsupervised manifold alignment (Ammar et al. 2015), and random projections. Results are in Figures 5 and 7a.\n\n3. Connections to metric learning and multitask learning in Section 2.\n\nThe comparisons have been run on experiments 1 and 2 and will be added to experiment 3 for the final version."
  },
  {
    "people": [
      "Xing"
    ],
    "review": "This paper presents an approach for skills transfer from one task to another in a control setting (trained by RL) by forcing the embeddings learned on two different tasks to be close (L2 penalty). The experiments are conducted in MuJoCo, with a set of experiments being from the state of the joints/links (5.2/5.3) and a set of experiments on the pixels (5.4). They exhibit transfer from arms with different number of links, and from a torque-driven arm to a tendon-driven arm.\n\nOne limitation of the paper is that the authors suppose that time alignment is trivial, because the tasks are all episodic and in the same domain. Time alignment is one form of domain adaptation / transfer that is not dealt with in the paper, that could be dealt with through subsampling, dynamic time warping, or learning a matching function (e.g. neural network).\n\nGeneral remarks: The approach is compared to CCA, which is a relevant baseline. However, as the paper is purely experimental, another baseline (worse than CCA) would be to just have the random projections for \"f\" and \"g\" (the embedding functions on the two domains), to check that the bad performance of the \"no transfer\" version of the model is due to over-specialisation of these embeddings. I would also add (for information) that the problem of learning invariant feature spaces is also linked to metric learning (e.g. [Xing et al. 2002]). More generally, no parallel is drawn with multi-task learning in ML. In the case of knowledge transfer (4.1.1), it may make sense to anneal \\alpha.\n\nThe experiments feel a bit rushed. In particular, the performance of the baseline being always 0 (no transfer at all) is uninformative, at least a much bigger sample budget should be tested. Also, why does Figure 7.b contain no \"CCA\" nor \"direct mapping\" results? Another concern that I have with the experiments: (if/how) did the author control for the fact that the embeddings were trained with more iterations in the case of doing transfer?\n\nOverall, the study of transfer is most welcomed in RL. The experiments in this paper are interesting enough for publication, but the paper could have been more thorough."
  },
  {
    "people": [
      "Ammar",
      "Raimalwala"
    ],
    "review": "The paper considers the problem of transferring skills between robots with different morphologies, in the context of agents that have to perform several tasks.  A core component of the proposed approach is to use a task-invariant future space, which can be shared between tasks & between agents.\n\nCompared to previous work (Ammar et al. 2015), it seems the main contribution here is to \u201cassume that good correspondences in episodic tasks can be extracted through time alignment\u201d (Sec. 2).  This is an interesting hypothesis. There is also similarity to work by Raimalwala et al (2016), but the authors argue their method is better equipped to handle non-linear dynamics. These are two interesting hypotheses, however I don\u2019t see that they have been verified in the presented empirical results.  In particular, the question of the pairing correspondence seems crucial. What happens when the time alignment is not suitable. Is it possible to use dynamic time warping (or similar method) to achieve reasonable results?  Robustness to misspecification of the pairing correspondence P seems a major concern.\n\nIn general, more comparison to other transfer methods, including those listed in Sec.2, would be very valuable.  The addition of Sec.5.1 is definitely a right step in this direction, but represents a small portion of the recent work on transfer learning.  I appreciate that other methods transfer other pieces of information (e.g. the policy), but still if the end goal is better performance, what is worth transferring (in addition to how to do the transfer) should be a reasonable question to explore.\n\nOverall, the paper tackles an important problem, but this is a very active area of research, and further comparison to other methods would be worthwhile.  The method proposed of transferring the representation is well motivated, cleanly described, and conceptually sound.  The assumption that time alignment can be used for the state pairing seems problematic, and should be further validated."
  },
  {
    "people": [
      "Soroush Mehri"
    ],
    "review": "The paper looks very nice and the results are even better. However, I must clearly state that the way that you have defined your model lacks any mathematical formalism. The main definition in the paper is literally Figure 1, with literally 2 equations in the whole paper, where one of them is just defining an ordered cascade definition of a joint distribution (eq 1). It is stated that each of the modules operates on a different FS_k time window, but how are these combined with the previous h-vector and with the one coming from the upper modules is not stated and remains a mystery, unless I go and read your code. Also interestingly enough the lowest level module uses an MLP rather than some form of an RNN to combine the observation in its window and the conditioning vector c. I'm very confused on why is that the case, and did you just stack all of the FS_1 vectors and input them to the MLP? Did you try to have another RNN there rather than an MLP? Does this implies that at every new iteration t, we need to compute the MLP at every level a new? \n\nAlso your answer to Soroush Mehri 3) - \"You can find the code and best found hyper-parameters in the following Github repo\" is really unsatisfactory. As a researcher I will need to spent several hours digging trough your code, written in Theano, which for that matter many people might have never used before, in order to just understand what exactly are you doing. On the other hand you can just explain in probably 3 equations the whole model and another 2 sentences about what were the number of hidden units etc... Also speaking about research, you should NOT just report the best chosen hyper parameters. The whole community will benefit a lot more if you list all of the things you tried and how well did they do (if needed add this in an Appendix). \n\nI really hope you guys take this constructively and write at least the model definition better."
  },
  {
    "people": [
      "Soroush Mehri"
    ],
    "review": "The paper looks very nice and the results are even better. However, I must clearly state that the way that you have defined your model lacks any mathematical formalism. The main definition in the paper is literally Figure 1, with literally 2 equations in the whole paper, where one of them is just defining an ordered cascade definition of a joint distribution (eq 1). It is stated that each of the modules operates on a different FS_k time window, but how are these combined with the previous h-vector and with the one coming from the upper modules is not stated and remains a mystery, unless I go and read your code. Also interestingly enough the lowest level module uses an MLP rather than some form of an RNN to combine the observation in its window and the conditioning vector c. I'm very confused on why is that the case, and did you just stack all of the FS_1 vectors and input them to the MLP? Did you try to have another RNN there rather than an MLP? Does this implies that at every new iteration t, we need to compute the MLP at every level a new? \n\nAlso your answer to Soroush Mehri 3) - \"You can find the code and best found hyper-parameters in the following Github repo\" is really unsatisfactory. As a researcher I will need to spent several hours digging trough your code, written in Theano, which for that matter many people might have never used before, in order to just understand what exactly are you doing. On the other hand you can just explain in probably 3 equations the whole model and another 2 sentences about what were the number of hidden units etc... Also speaking about research, you should NOT just report the best chosen hyper parameters. The whole community will benefit a lot more if you list all of the things you tried and how well did they do (if needed add this in an Appendix). \n\nI really hope you guys take this constructively and write at least the model definition better."
  },
  {
    "people": [
      "Bousmalis"
    ],
    "review": "Update: I thank the authors for their comments. I still think that the method needs more experimental evaluation: for now, it's restricted to the settings in which one can use pre-trained ImageNet model, but it's also important to show the effectiveness in scenarios where one needs to train everything from scratch. I would love to see a fair comparison of the state-of-the-art methods on toy datasets (e.g. see (Bousmalis et al., 2016), (Ganin & Lempitsky, 2015)). In my opinion, it's a crucial point that doesn't allow me to increase the rating.\n\nThis paper proposes a simple trick turning batch normalization into a domain adaptation technique. The authors show that per-batch means and variances normally computed as a part of the BN procedure are sufficient to discriminate the domain. This observation leads to an idea that adaptation for the target domain can be performed by replacing population statistics computed on the source dataset by the corresponding statistics from the target dataset.\n\nOverall, I think the paper is more suitable for a workshop track rather than for the main conference track. My main concerns are the following:\n\n1. Although the main idea is very simple, it feels like the paper is composed in such a way to make the main contribution less obvious (e.g. the idea could have been described in the abstract but the authors avoided doing so). \n\n2. (This one is from the pre-review questions) The authors are using much stronger base CNN which may account for the bulk of the reported improvement. In order to prove the effectiveness of the trick, the authors would need to conduct a fair comparison against competing methods. It would be highly desirable to conduct this comparison also in the case of a model trained from scratch (as opposed to reusing ImageNet-trained networks).\n"
  },
  {
    "people": [
      "Bousmalis"
    ],
    "review": "Update: I thank the authors for their comments. I still think that the method needs more experimental evaluation: for now, it's restricted to the settings in which one can use pre-trained ImageNet model, but it's also important to show the effectiveness in scenarios where one needs to train everything from scratch. I would love to see a fair comparison of the state-of-the-art methods on toy datasets (e.g. see (Bousmalis et al., 2016), (Ganin & Lempitsky, 2015)). In my opinion, it's a crucial point that doesn't allow me to increase the rating.\n\nThis paper proposes a simple trick turning batch normalization into a domain adaptation technique. The authors show that per-batch means and variances normally computed as a part of the BN procedure are sufficient to discriminate the domain. This observation leads to an idea that adaptation for the target domain can be performed by replacing population statistics computed on the source dataset by the corresponding statistics from the target dataset.\n\nOverall, I think the paper is more suitable for a workshop track rather than for the main conference track. My main concerns are the following:\n\n1. Although the main idea is very simple, it feels like the paper is composed in such a way to make the main contribution less obvious (e.g. the idea could have been described in the abstract but the authors avoided doing so). \n\n2. (This one is from the pre-review questions) The authors are using much stronger base CNN which may account for the bulk of the reported improvement. In order to prove the effectiveness of the trick, the authors would need to conduct a fair comparison against competing methods. It would be highly desirable to conduct this comparison also in the case of a model trained from scratch (as opposed to reusing ImageNet-trained networks).\n"
  },
  {
    "people": [
      "Mallet"
    ],
    "review": "The authors advocate use of chirplets as a basis for modeling audio signals.  They introduce a fast chiplet transform for efficient computation. Also introduced is the idea of initializing (pre-training) CNN layers to mimic chirplet transform of audio signal (similar to ideas proposed by Mallet et al. on scattering transforms).  The paper is fairly easy to follow but in a few places contains undefined terms (e.g. AM-FM, MAP).\n\nWhile the idea of using chirplet transform is interesting, my main concern is that the empirical evidence provided is in a rather narrow domain of bird call classification.  Furthermore, the accuracy gains shown in that domain are relatively small (61% MAP for log-Mel features vs 61.5% for chirplet transforms).  I would recommend that authors provide evidence for how this generalizes to other audio (including speech) tasks."
  },
  {
    "people": [
      "Mallet"
    ],
    "review": "The authors advocate use of chirplets as a basis for modeling audio signals.  They introduce a fast chiplet transform for efficient computation. Also introduced is the idea of initializing (pre-training) CNN layers to mimic chirplet transform of audio signal (similar to ideas proposed by Mallet et al. on scattering transforms).  The paper is fairly easy to follow but in a few places contains undefined terms (e.g. AM-FM, MAP).\n\nWhile the idea of using chirplet transform is interesting, my main concern is that the empirical evidence provided is in a rather narrow domain of bird call classification.  Furthermore, the accuracy gains shown in that domain are relatively small (61% MAP for log-Mel features vs 61.5% for chirplet transforms).  I would recommend that authors provide evidence for how this generalizes to other audio (including speech) tasks."
  },
  {
    "people": [
      "Andreas",
      "Andreas"
    ],
    "review": "The paper proposes to use reinforcement learning to learn how to compose the words in a sentence, i.e. parse tree, that can be helpful for the downstream tasks. To do that, the shift-reduce framework is employed and RL is used to learn the policy of the two actions SHIFT and REDUCE. The experiments on four datasets (SST, SICK, IMDB, and SNLI) show that the proposed approach outperformed the approach using predefined tree structures (e.g. left-to-right, right-to-left). \n\nThe paper is well written and has two good points. Firstly, the idea of using RL to learn parse trees using downstream tasks is very interesting and novel. And employing the shift-reduce framework is a very smart choice because the set of actions is minimal (shift and reduce). Secondly, what shown in the paper somewhat confirms the need of parse trees. This is indeed interesting because of the current debate on whether syntax is helpful.\n\nI have the following comments:\n- it seems that the authors weren't aware of some recent work using RL to learn structures for composition, e.g. Andreas et al (2016).\n- because different composition functions (e.g. LSTM, GRU, or classical recursive neural net) have different inductive biases, I was wondering if the tree structures found by the model would be independent from the composition function choice.\n- because RNNs in theory are equivalent to Turing machines, I was wondering if restricting the expressiveness of the model (e.g. reducing the dimension) can help the model focus on discovering more helpful tree structures.\n\nRef:\nAndreas et al. Learning to Compose Neural Networks for Question Answering. NAACL 2016"
  },
  {
    "people": [
      "Socher",
      "Kazuma"
    ],
    "review": "Dear authors,\n\nI've read the interesting paper and learned nice ideas.\n\nNow I've found somewhat incorrect mention in your paper.\nIn Introduction, it is said that trees are provided with sentences in Socher et al. (2011), but they jointly learn the binary tree structures according to the target task (sentiment classification) although the approach is different (reinforcement learning or autoencoder).\n\nBest,\n  Kazuma"
  },
  {
    "people": [
      "Tai"
    ],
    "review": "In several of the evaluation, the \"constituency tree LSTM\" and/or \"dependency tree LSTM\" methods perform much better than all of your proposed models, including the \"supervised syntax\" model, and sometimes even with the same number of parameters. What is the difference between your \"supervised syntax\" method and these tree LSTMs? why doesn't the supervised syntax approach perform at least as good as the Tai et al models?"
  },
  {
    "people": [
      "Munkhdalai",
      "Tsendsuren",
      "Munkhdalai",
      "Tsendsuren"
    ],
    "review": "Table 2 is missing some recent results on this task. Please see the NTI and NSE results on the same task [1,2]. NTI is particularly relevant to this work because it encodes a sentence with an n-ary tree (i.e. binary tree) instead of using a parser output or learning to compose.\n\n\nThanks,\n\n\n\nRef:\n\n1. Munkhdalai, Tsendsuren, and Hong Yu. \"Neural Tree Indexers for Text Understanding.\" arXiv preprint arXiv:1607.04492 (2016).\n2. Munkhdalai, Tsendsuren, and Hong Yu. \"Neural Semantic Encoders.\" arXiv preprint arXiv:1607.04315 (2016)."
  },
  {
    "people": [
      "Andreas",
      "Andreas"
    ],
    "review": "The paper proposes to use reinforcement learning to learn how to compose the words in a sentence, i.e. parse tree, that can be helpful for the downstream tasks. To do that, the shift-reduce framework is employed and RL is used to learn the policy of the two actions SHIFT and REDUCE. The experiments on four datasets (SST, SICK, IMDB, and SNLI) show that the proposed approach outperformed the approach using predefined tree structures (e.g. left-to-right, right-to-left). \n\nThe paper is well written and has two good points. Firstly, the idea of using RL to learn parse trees using downstream tasks is very interesting and novel. And employing the shift-reduce framework is a very smart choice because the set of actions is minimal (shift and reduce). Secondly, what shown in the paper somewhat confirms the need of parse trees. This is indeed interesting because of the current debate on whether syntax is helpful.\n\nI have the following comments:\n- it seems that the authors weren't aware of some recent work using RL to learn structures for composition, e.g. Andreas et al (2016).\n- because different composition functions (e.g. LSTM, GRU, or classical recursive neural net) have different inductive biases, I was wondering if the tree structures found by the model would be independent from the composition function choice.\n- because RNNs in theory are equivalent to Turing machines, I was wondering if restricting the expressiveness of the model (e.g. reducing the dimension) can help the model focus on discovering more helpful tree structures.\n\nRef:\nAndreas et al. Learning to Compose Neural Networks for Question Answering. NAACL 2016"
  },
  {
    "people": [
      "Socher",
      "Kazuma"
    ],
    "review": "Dear authors,\n\nI've read the interesting paper and learned nice ideas.\n\nNow I've found somewhat incorrect mention in your paper.\nIn Introduction, it is said that trees are provided with sentences in Socher et al. (2011), but they jointly learn the binary tree structures according to the target task (sentiment classification) although the approach is different (reinforcement learning or autoencoder).\n\nBest,\n  Kazuma"
  },
  {
    "people": [
      "Tai"
    ],
    "review": "In several of the evaluation, the \"constituency tree LSTM\" and/or \"dependency tree LSTM\" methods perform much better than all of your proposed models, including the \"supervised syntax\" model, and sometimes even with the same number of parameters. What is the difference between your \"supervised syntax\" method and these tree LSTMs? why doesn't the supervised syntax approach perform at least as good as the Tai et al models?"
  },
  {
    "people": [
      "Munkhdalai",
      "Tsendsuren",
      "Munkhdalai",
      "Tsendsuren"
    ],
    "review": "Table 2 is missing some recent results on this task. Please see the NTI and NSE results on the same task [1,2]. NTI is particularly relevant to this work because it encodes a sentence with an n-ary tree (i.e. binary tree) instead of using a parser output or learning to compose.\n\n\nThanks,\n\n\n\nRef:\n\n1. Munkhdalai, Tsendsuren, and Hong Yu. \"Neural Tree Indexers for Text Understanding.\" arXiv preprint arXiv:1607.04492 (2016).\n2. Munkhdalai, Tsendsuren, and Hong Yu. \"Neural Semantic Encoders.\" arXiv preprint arXiv:1607.04315 (2016)."
  },
  {
    "people": [
      "tanh"
    ],
    "review": "Summary:\nIn this paper, the authors explore the advantages/disadvantages of using a sin activation function.\nThey first demonstrate that even with simple tasks, using sin activations can result in complex to optimize loss functions.\nThey then compare networks trained with different activations on the MNIST dataset, and discover that the periodicity of the sin activation is not necessary for learning the task well.\nThey then try different algorithmic tasks, where the periodicity of the functions is helpful.\n\nPros:\nThe closed form derivations of the loss surface were interesting to see, and the clarity of tone on the advantages *and* disadvantages was educational.\n\nCons: \nSeems like more of a preliminary investigation of the potential benefits of sin, and more evidence (to support or in contrary) is needed to conclude anything significant -- the results on MNIST seem to indicate truncated sin is just as good, and while it is interesting that tanh maybe uses more of the saturated part, the two seem relatively interchangeable. The toy algorithmic tasks are hard to conclude something concrete from."
  },
  {
    "people": [
      "tanh"
    ],
    "review": "Summary:\nIn this paper, the authors explore the advantages/disadvantages of using a sin activation function.\nThey first demonstrate that even with simple tasks, using sin activations can result in complex to optimize loss functions.\nThey then compare networks trained with different activations on the MNIST dataset, and discover that the periodicity of the sin activation is not necessary for learning the task well.\nThey then try different algorithmic tasks, where the periodicity of the functions is helpful.\n\nPros:\nThe closed form derivations of the loss surface were interesting to see, and the clarity of tone on the advantages *and* disadvantages was educational.\n\nCons: \nSeems like more of a preliminary investigation of the potential benefits of sin, and more evidence (to support or in contrary) is needed to conclude anything significant -- the results on MNIST seem to indicate truncated sin is just as good, and while it is interesting that tanh maybe uses more of the saturated part, the two seem relatively interchangeable. The toy algorithmic tasks are hard to conclude something concrete from."
  },
  {
    "people": [
      "Boltzmann"
    ],
    "review": "This paper proposes a new generative model that uses real-valued non-volume preserving transformations in order to achieve efficient and exact inference and sampling of data points.\nThe authors use the change-of-variable technique to obtain a model distribution of the data from a simple prior distribution on a latent variable. By carefully designing the bijective function used in the change-of-variable technique, they obtain a Jacobian that is triangular and allows for efficient computation.\n\nGenerative models with tractable inference and efficient sampling are an active research area and this paper definitely contributes to this field.\n\nWhile not achieving state-of-the-art, they are not far behind. This doesn't change the fact that the proposed method is innovative and worth exploring as it tries to bridge the gap between auto-regressive models, variational autoencoders and generative adversarial networks.\n\nThe authors clearly mention the difference and similarities with other types of generative models that are being actively researched.\nCompared to autoregressive models, the proposed approach offers fast sampling.\nCompared to generative adversarial networks, Real NVP offers a tractable log-likelihood evaluation.\nCompared to variational autoencoders, the inference is exact.\nCompared to deep Boltzmann machines, the learning of the proposed method is tractable.\nIt is clear that Real NVP goal is to bridge the gap between existing and popular generative models.\n\nThe paper presents a lot of interesting experiments showing the capabilities of the proposed technique. Making the code available online will certainly contribute to the field. Is there any intention of releasing the code?\n\nTypo: (Section 3.7) We also \"use apply\" batch normalization"
  },
  {
    "people": [
      "Boltzmann"
    ],
    "review": "This paper proposes a new generative model that uses real-valued non-volume preserving transformations in order to achieve efficient and exact inference and sampling of data points.\nThe authors use the change-of-variable technique to obtain a model distribution of the data from a simple prior distribution on a latent variable. By carefully designing the bijective function used in the change-of-variable technique, they obtain a Jacobian that is triangular and allows for efficient computation.\n\nGenerative models with tractable inference and efficient sampling are an active research area and this paper definitely contributes to this field.\n\nWhile not achieving state-of-the-art, they are not far behind. This doesn't change the fact that the proposed method is innovative and worth exploring as it tries to bridge the gap between auto-regressive models, variational autoencoders and generative adversarial networks.\n\nThe authors clearly mention the difference and similarities with other types of generative models that are being actively researched.\nCompared to autoregressive models, the proposed approach offers fast sampling.\nCompared to generative adversarial networks, Real NVP offers a tractable log-likelihood evaluation.\nCompared to variational autoencoders, the inference is exact.\nCompared to deep Boltzmann machines, the learning of the proposed method is tractable.\nIt is clear that Real NVP goal is to bridge the gap between existing and popular generative models.\n\nThe paper presents a lot of interesting experiments showing the capabilities of the proposed technique. Making the code available online will certainly contribute to the field. Is there any intention of releasing the code?\n\nTypo: (Section 3.7) We also \"use apply\" batch normalization"
  },
  {
    "people": [
      "Saxe",
      "Henaff"
    ],
    "review": "This paper investigates the impact of orthogonal weight matrices on learning dynamics in RNNs. The paper proposes a variety of interesting optimization formulations that enforce orthogonality in the recurrent weight matrix to varying degrees. The experimental results demonstrate several conclusions: enforcing exact orthogonality does not help learning, while enforcing soft orthogonality or initializing to orthogonal weights can substantially improve learning. While some of the optimization methods proposed currently require matrix inversion and are therefore slow in wall clock time, orthogonal initialization and some of the soft orthogonality constraints are relatively inexpensive and may find their way into practical use.\n\nThe experiments are generally done to a high standard and yield a variety of useful insights, and the writing is clear.\n\nThe experimental results are based on using a fixed learning rate for the different regularization strengths. Learning speed might be highly dependent on this, and different strengths may admit different maximal stable learning rates. It would be instructive to optimize the learning rate for each margin separately (maybe on one of the shorter sequence lengths) to see how soft orthogonality impacts the stability of the learning process. Fig. 5, for instance, shows that a sigmoid improves stability\u2014but perhaps slightly reducing the learning rate for the non-sigmoid Gaussian prior RNN would make the learning well-behaved again for weightings less than 1.\n\nFig. 4 shows singular values converging around 1.05 rather than 1. Does initializing to orthogonal matrices multiplied by 1.05 confer any noticeable advantage over standard orthogonal matrices? Especially on the T=10K copy task?\n\n\u201cCuriously, larger margins and even models without sigmoidal constraints on the spectrum (no margin) performed well as long as they were initialized to be orthogonal suggesting that evolution away from orthogonality is not a serious problem on this task.\u201d This is consistent with the analysis given in Saxe et al. 2013, where for deep linear nets, if a singular value is initialized to 1 but dies away during training, this is because it must be zero to implement the desired input-output map. More broadly, an open question has been whether orthogonality is useful as an initialization, as proposed by Saxe et al., where its role is mainly as a preconditioner which makes optimization proceed quickly but doesn\u2019t fundamentally change the optimization problem; or whether it is useful as a regularizer, as proposed by Arjovsky et al. 2015 and Henaff et al. 2015, that is, as an additional constraint in the optimization problem (minimize loss subject to weights being orthogonal). These experiments seem to show that mere initialization to orthogonal weights is enough to reap an optimization speed advantage, and that too much regularization begins to hurt performance\u2014i.e., substantially changing the optimization problem is undesirable. This point is also apparent in Fig. 2: In terms of the training loss on MNIST (Fig. 2), no margin does almost indistinguishably from a margin of 1 or .1. However in terms of accuracy, a margin of .1 is best. This shows that large or nonexistent margins (i.e., orthogonal initializations) enable fast optimization of the training loss, but among models that attain similar training loss, the more nearly orthogonal weights perform better. This starts to separate out the optimization speed advantage conferred by orthogonality from the regularization advantage it confers. It may be useful to more explicitly discuss the initialization vs regularization dimension in the text.\n\nOverall, this paper contributes a variety of techniques and intuitions which are likely to be useful in training RNNs.\n"
  },
  {
    "people": [
      "Saxe",
      "Henaff"
    ],
    "review": "This paper investigates the impact of orthogonal weight matrices on learning dynamics in RNNs. The paper proposes a variety of interesting optimization formulations that enforce orthogonality in the recurrent weight matrix to varying degrees. The experimental results demonstrate several conclusions: enforcing exact orthogonality does not help learning, while enforcing soft orthogonality or initializing to orthogonal weights can substantially improve learning. While some of the optimization methods proposed currently require matrix inversion and are therefore slow in wall clock time, orthogonal initialization and some of the soft orthogonality constraints are relatively inexpensive and may find their way into practical use.\n\nThe experiments are generally done to a high standard and yield a variety of useful insights, and the writing is clear.\n\nThe experimental results are based on using a fixed learning rate for the different regularization strengths. Learning speed might be highly dependent on this, and different strengths may admit different maximal stable learning rates. It would be instructive to optimize the learning rate for each margin separately (maybe on one of the shorter sequence lengths) to see how soft orthogonality impacts the stability of the learning process. Fig. 5, for instance, shows that a sigmoid improves stability\u2014but perhaps slightly reducing the learning rate for the non-sigmoid Gaussian prior RNN would make the learning well-behaved again for weightings less than 1.\n\nFig. 4 shows singular values converging around 1.05 rather than 1. Does initializing to orthogonal matrices multiplied by 1.05 confer any noticeable advantage over standard orthogonal matrices? Especially on the T=10K copy task?\n\n\u201cCuriously, larger margins and even models without sigmoidal constraints on the spectrum (no margin) performed well as long as they were initialized to be orthogonal suggesting that evolution away from orthogonality is not a serious problem on this task.\u201d This is consistent with the analysis given in Saxe et al. 2013, where for deep linear nets, if a singular value is initialized to 1 but dies away during training, this is because it must be zero to implement the desired input-output map. More broadly, an open question has been whether orthogonality is useful as an initialization, as proposed by Saxe et al., where its role is mainly as a preconditioner which makes optimization proceed quickly but doesn\u2019t fundamentally change the optimization problem; or whether it is useful as a regularizer, as proposed by Arjovsky et al. 2015 and Henaff et al. 2015, that is, as an additional constraint in the optimization problem (minimize loss subject to weights being orthogonal). These experiments seem to show that mere initialization to orthogonal weights is enough to reap an optimization speed advantage, and that too much regularization begins to hurt performance\u2014i.e., substantially changing the optimization problem is undesirable. This point is also apparent in Fig. 2: In terms of the training loss on MNIST (Fig. 2), no margin does almost indistinguishably from a margin of 1 or .1. However in terms of accuracy, a margin of .1 is best. This shows that large or nonexistent margins (i.e., orthogonal initializations) enable fast optimization of the training loss, but among models that attain similar training loss, the more nearly orthogonal weights perform better. This starts to separate out the optimization speed advantage conferred by orthogonality from the regularization advantage it confers. It may be useful to more explicitly discuss the initialization vs regularization dimension in the text.\n\nOverall, this paper contributes a variety of techniques and intuitions which are likely to be useful in training RNNs.\n"
  },
  {
    "people": [
      "Stanovsky"
    ],
    "review": "There are many other types of contexts which should be discussed; see \"Open IE as an Intermediate Structure for Semantic Tasks\" (Stanovsky et al., ACL 2015)."
  },
  {
    "people": [
      "Stanovsky"
    ],
    "review": "There are many other types of contexts which should be discussed; see \"Open IE as an Intermediate Structure for Semantic Tasks\" (Stanovsky et al., ACL 2015)."
  },
  {
    "people": [
      "Po-Sen Huang",
      "Jianfeng Gao",
      "Weizhu Chen"
    ],
    "review": "The proposed model in the paper is very similar to the ReasoNet model[1], which is first public available in arXiv in Sep, 17th, 2016. \n\n1. The organization of the paper, especially motivation and related work work part, is very similar to ReasoNet paper [1].\n2. The idea of termination Gate in Comprehension task is first proposed in ReasoNet, the paper fails to explain the connection between this work and ReasoNet model [1].\n\n1. Yelong Shen, Po-Sen Huang, Jianfeng Gao, Weizhu Chen. \"ReasoNet: Learning to Stop Reading in Machine Comprehension. arXiv preprint arXiv:1609.05284 (Sep-17-2016)."
  },
  {
    "people": [
      "Po-Sen Huang",
      "Jianfeng Gao",
      "Weizhu Chen"
    ],
    "review": "The proposed model in the paper is very similar to the ReasoNet model[1], which is first public available in arXiv in Sep, 17th, 2016. \n\n1. The organization of the paper, especially motivation and related work work part, is very similar to ReasoNet paper [1].\n2. The idea of termination Gate in Comprehension task is first proposed in ReasoNet, the paper fails to explain the connection between this work and ReasoNet model [1].\n\n1. Yelong Shen, Po-Sen Huang, Jianfeng Gao, Weizhu Chen. \"ReasoNet: Learning to Stop Reading in Machine Comprehension. arXiv preprint arXiv:1609.05284 (Sep-17-2016)."
  },
  {
    "people": [
      "Hariharan",
      "Girshick"
    ],
    "review": "The paper extends a regularizer on the gradients recently proposed by Hariharan and Girshick. I agree with the reviewers that while the analysis is interesting, it is unclear why this particular regularizer is especially relevant for low-shot learning. And the experimental validation is not strong enough to warrant acceptance."
  },
  {
    "people": [
      "Hariharan",
      "Hariharan",
      "Girshick"
    ],
    "review": "Summary\n===\nThis paper extends and analyzes the gradient regularizer of Hariharan and\nGirshick 2016. In that paper a regularizer was proposed which penalizes\ngradient magnitudes and it was shown to aid low-shot learning performance.\nThis work shows that the previous regularizer is equivalent to a direct penalty\non the magnitude of feature values weighted differently per example.\n\nThe analysis goes to to provide two examples where a feature penalty\nfavors a better representation. The first example addresses the XOR\nproblem, constructing a network where a feature penalty encourages\na representation where XOR is linearly separable.\nThe second example analyzes a 2 layer linear network, showing improved stability\nof a 2nd order optimizer when the feature penalty is added.\nOne last bit of analysis shows how this regularizer can be interpreted as\na Gaussian prior on both features and weights. Since the prior can be\ninterpreted as having a soft whitening effect, the feature regularizer\nis like a soft version of Batch Normalization.\n\nExperiments show small improvements on a synthetic XOR test set.\nOn the Omniglot dataset feature regularization is better than most baselines,\nbut is worse than Moment Matching Networks. An experiment on ImageNet similar\nto Hariharan and Girshick 2016 also shows effective low-shot learning.\n\n\nStrengths\n===\n\n* The core proposal is a simple modification of Hariharan and Girshick 2016.\n\n* The idea of feature regularization is analyzed from multiple angles\nboth theoretically and empirically.\n\n* The connection with Batch Normalization could have broader impact.\n\n\nWeaknesses\n===\n\n* In section 2 the gradient regularizer of Hariharan and Girshick is introduced.\nWhile introducing the concept, some concern is expressed about the motivation:\n\"And it is not very clear why small gradients on every sample produces\ngood generalization experimentally.\" This seems to be the central issue to me.\nThe paper details some related analysis, it does not offer a clear answer to\nthis problem.\n\n\n* The purpose and generality of section 2.1 is not clear.\n\nThe analysis provides a specific case (XOR with a non-standard architecture)\nwhere feature regularization intuitively helps learn a better representation.\nHowever, the intended take-away is not clear.\n\nThe take-away may be that since a feature penalty helps in this case it\nshould help in other cases. I am hesitant to buy that argument because of the\nspecific architecture used in this section. The result seems to rely on the\nchoice of an x^2 non-linearity, which is not often encountered in recent neural\nnet literature.\n\nThe point might also be to highlight the difference between a weight\npenalty and a feature penalty because the two seem to encourage\ndifferent values of b in this case. However, there is no comparison to\na weight penalty on b in section 2.1.\n\n\n* As far as I can tell, eq. 3 depends on either assuming an L2 or cross-entropy\nloss. A more general class of losses for which eq. 3 holds is not provided. This\nshould be made clear before eq. 3 is presented.\n\n\n* The Omniglot and ImageNet experiments are performed with Batch Normalization,\nyet the paper points out that feature regularization may be similar in effect\nto Batch Norm. Since the ResNet CNN baseline includes Batch Norm and there are\nclear improvements over that baseline, the proposed regularizer has a clear\nadditional positive effect. However, results should be provided without\nBatch Norm so a 1-1 comparison between the two methods can be performed.\n\n\n* The ImageNet experiment should be more like Hariharan and Girshick.\nIn particular, the same split of classes should be used (provided in\nthe appendix) and performance should be measured using n > 1 novel examples\nper class (using k nearest neighbors).\n\n\nMinor:\n\n* A brief comparison to Matching Networks is provided in section 3.2, but the\nperformance of Matching Networks should also be reported in Table 1.\n\n* From the approach section: \"Intuitively when close to convergence, about half\nof the data-cases recommend to update a parameter to go left, while\nthe other half recommend to go right.\"\n\nCould the intuition be clarified? There are many directions in high\ndimensional space and many ways to divide them into two groups.\n\n* Is the SGM penalty of Hariharan and Girshick implemented for this paper\nor using their code? Either is acceptable, but clarification would be appreciated.\n\n* Should the first equal sign in eq. 13 be proportional to, not equal to?\n\n* The work is dense in nature, but I think the presentation could be improved.\nIn particular, more detailed derivations could be provided in an appendix\nand some details could be removed from the main version in order to increase\nfocus on the results (e.g., the derviation in section 2.2.1).\n\n\nOverall Evaluation\n===\n\nThis paper provides an interesting set of analyses, but their value is not clear.\nThere is no clear reason why a gradient or feature regularizer should improve\nlow-shot learning performance. Despite that, experiments support that conclusion,\nthe analysis is interesting by itself, and the analysis may help lead to a\nclearer explanation.\n\nThe work is a somewhat novel extension and analysis of Hariharan and Girshick 2016.\nSome points are not completely clear, as mentioned above."
  },
  {
    "people": [
      "Hariharan",
      "Girshick"
    ],
    "review": "The paper extends a regularizer on the gradients recently proposed by Hariharan and Girshick. I agree with the reviewers that while the analysis is interesting, it is unclear why this particular regularizer is especially relevant for low-shot learning. And the experimental validation is not strong enough to warrant acceptance."
  },
  {
    "people": [
      "Hariharan",
      "Hariharan",
      "Girshick"
    ],
    "review": "Summary\n===\nThis paper extends and analyzes the gradient regularizer of Hariharan and\nGirshick 2016. In that paper a regularizer was proposed which penalizes\ngradient magnitudes and it was shown to aid low-shot learning performance.\nThis work shows that the previous regularizer is equivalent to a direct penalty\non the magnitude of feature values weighted differently per example.\n\nThe analysis goes to to provide two examples where a feature penalty\nfavors a better representation. The first example addresses the XOR\nproblem, constructing a network where a feature penalty encourages\na representation where XOR is linearly separable.\nThe second example analyzes a 2 layer linear network, showing improved stability\nof a 2nd order optimizer when the feature penalty is added.\nOne last bit of analysis shows how this regularizer can be interpreted as\na Gaussian prior on both features and weights. Since the prior can be\ninterpreted as having a soft whitening effect, the feature regularizer\nis like a soft version of Batch Normalization.\n\nExperiments show small improvements on a synthetic XOR test set.\nOn the Omniglot dataset feature regularization is better than most baselines,\nbut is worse than Moment Matching Networks. An experiment on ImageNet similar\nto Hariharan and Girshick 2016 also shows effective low-shot learning.\n\n\nStrengths\n===\n\n* The core proposal is a simple modification of Hariharan and Girshick 2016.\n\n* The idea of feature regularization is analyzed from multiple angles\nboth theoretically and empirically.\n\n* The connection with Batch Normalization could have broader impact.\n\n\nWeaknesses\n===\n\n* In section 2 the gradient regularizer of Hariharan and Girshick is introduced.\nWhile introducing the concept, some concern is expressed about the motivation:\n\"And it is not very clear why small gradients on every sample produces\ngood generalization experimentally.\" This seems to be the central issue to me.\nThe paper details some related analysis, it does not offer a clear answer to\nthis problem.\n\n\n* The purpose and generality of section 2.1 is not clear.\n\nThe analysis provides a specific case (XOR with a non-standard architecture)\nwhere feature regularization intuitively helps learn a better representation.\nHowever, the intended take-away is not clear.\n\nThe take-away may be that since a feature penalty helps in this case it\nshould help in other cases. I am hesitant to buy that argument because of the\nspecific architecture used in this section. The result seems to rely on the\nchoice of an x^2 non-linearity, which is not often encountered in recent neural\nnet literature.\n\nThe point might also be to highlight the difference between a weight\npenalty and a feature penalty because the two seem to encourage\ndifferent values of b in this case. However, there is no comparison to\na weight penalty on b in section 2.1.\n\n\n* As far as I can tell, eq. 3 depends on either assuming an L2 or cross-entropy\nloss. A more general class of losses for which eq. 3 holds is not provided. This\nshould be made clear before eq. 3 is presented.\n\n\n* The Omniglot and ImageNet experiments are performed with Batch Normalization,\nyet the paper points out that feature regularization may be similar in effect\nto Batch Norm. Since the ResNet CNN baseline includes Batch Norm and there are\nclear improvements over that baseline, the proposed regularizer has a clear\nadditional positive effect. However, results should be provided without\nBatch Norm so a 1-1 comparison between the two methods can be performed.\n\n\n* The ImageNet experiment should be more like Hariharan and Girshick.\nIn particular, the same split of classes should be used (provided in\nthe appendix) and performance should be measured using n > 1 novel examples\nper class (using k nearest neighbors).\n\n\nMinor:\n\n* A brief comparison to Matching Networks is provided in section 3.2, but the\nperformance of Matching Networks should also be reported in Table 1.\n\n* From the approach section: \"Intuitively when close to convergence, about half\nof the data-cases recommend to update a parameter to go left, while\nthe other half recommend to go right.\"\n\nCould the intuition be clarified? There are many directions in high\ndimensional space and many ways to divide them into two groups.\n\n* Is the SGM penalty of Hariharan and Girshick implemented for this paper\nor using their code? Either is acceptable, but clarification would be appreciated.\n\n* Should the first equal sign in eq. 13 be proportional to, not equal to?\n\n* The work is dense in nature, but I think the presentation could be improved.\nIn particular, more detailed derivations could be provided in an appendix\nand some details could be removed from the main version in order to increase\nfocus on the results (e.g., the derviation in section 2.2.1).\n\n\nOverall Evaluation\n===\n\nThis paper provides an interesting set of analyses, but their value is not clear.\nThere is no clear reason why a gradient or feature regularizer should improve\nlow-shot learning performance. Despite that, experiments support that conclusion,\nthe analysis is interesting by itself, and the analysis may help lead to a\nclearer explanation.\n\nThe work is a somewhat novel extension and analysis of Hariharan and Girshick 2016.\nSome points are not completely clear, as mentioned above."
  },
  {
    "people": [
      "Martens"
    ],
    "review": "It is an interesting idea to go after saddle points in the optimization with an SR1 update and a good start in experiments, but missing important comparisons to recent 2nd order optimizations such as Adam, other Hessian free methods (Martens 2012), Pearlmutter fast exact multiplication by the Hessian. From the mnist/cifar curves it is not really showing an advantage to AdaDelta/Nag (although this is stated), and much more experimentation is needed to make a claim about mini-batch insensitivity to performance, can you show error rates on a larger scale task?"
  },
  {
    "people": [
      "Nocedal",
      "Wright",
      "Byrd",
      "Dauphin",
      "Martens"
    ],
    "review": "The paper proposes a new second-order method L-SR1 to train deep neural networks. It is claimed that the method addresses two important optimization problems in this setting: poor conditioning of the Hessian and proliferation of saddle points. The method can be viewed as a concatenation of SR1 algorithm of Nocedal & Wright (2006) and limited-memory representations Byrd et al. (1994). First of all, I am missing a more formal, theoretical argument in this work (in general providing more intuition would be helpful too), which instead is provided in the works of Dauphin (2014) or Martens. The experimental section in not very convincing considering that the performance in terms of the wall-clock time is not reported and the advantage over some competitor methods is not very strong even in terms of epochs. I understand that the authors are optimizing their implementation still, but the question is: considering the experiments are not convincing, why would anybody bother to implement L-SR1 to train their deep models? The work is not ready to be published."
  },
  {
    "people": [
      "Adam"
    ],
    "review": "L-SR1 seems to have O(mn) time complexity. I miss this information in your paper. \nYour experimental results suggest that L-SR1 does not outperform Adadelta (I suppose the same for Adam). \nGiven the time complexity of L-SR1, the x-axis showing time would suggest that L-SR1 is much (say, m times) slower. \n\"The memory size of 2 had the lowest minimum test loss over 90\" suggests that the main driven force of L-SR1 \nwas its momentum, i.e., the second-order information was rather useless."
  },
  {
    "people": [
      "Martens"
    ],
    "review": "It is an interesting idea to go after saddle points in the optimization with an SR1 update and a good start in experiments, but missing important comparisons to recent 2nd order optimizations such as Adam, other Hessian free methods (Martens 2012), Pearlmutter fast exact multiplication by the Hessian. From the mnist/cifar curves it is not really showing an advantage to AdaDelta/Nag (although this is stated), and much more experimentation is needed to make a claim about mini-batch insensitivity to performance, can you show error rates on a larger scale task?"
  },
  {
    "people": [
      "Martens"
    ],
    "review": "It is an interesting idea to go after saddle points in the optimization with an SR1 update and a good start in experiments, but missing important comparisons to recent 2nd order optimizations such as Adam, other Hessian free methods (Martens 2012), Pearlmutter fast exact multiplication by the Hessian. From the mnist/cifar curves it is not really showing an advantage to AdaDelta/Nag (although this is stated), and much more experimentation is needed to make a claim about mini-batch insensitivity to performance, can you show error rates on a larger scale task?"
  },
  {
    "people": [
      "Nocedal",
      "Wright",
      "Byrd",
      "Dauphin",
      "Martens"
    ],
    "review": "The paper proposes a new second-order method L-SR1 to train deep neural networks. It is claimed that the method addresses two important optimization problems in this setting: poor conditioning of the Hessian and proliferation of saddle points. The method can be viewed as a concatenation of SR1 algorithm of Nocedal & Wright (2006) and limited-memory representations Byrd et al. (1994). First of all, I am missing a more formal, theoretical argument in this work (in general providing more intuition would be helpful too), which instead is provided in the works of Dauphin (2014) or Martens. The experimental section in not very convincing considering that the performance in terms of the wall-clock time is not reported and the advantage over some competitor methods is not very strong even in terms of epochs. I understand that the authors are optimizing their implementation still, but the question is: considering the experiments are not convincing, why would anybody bother to implement L-SR1 to train their deep models? The work is not ready to be published."
  },
  {
    "people": [
      "Adam"
    ],
    "review": "L-SR1 seems to have O(mn) time complexity. I miss this information in your paper. \nYour experimental results suggest that L-SR1 does not outperform Adadelta (I suppose the same for Adam). \nGiven the time complexity of L-SR1, the x-axis showing time would suggest that L-SR1 is much (say, m times) slower. \n\"The memory size of 2 had the lowest minimum test loss over 90\" suggests that the main driven force of L-SR1 \nwas its momentum, i.e., the second-order information was rather useless."
  },
  {
    "people": [
      "Martens"
    ],
    "review": "It is an interesting idea to go after saddle points in the optimization with an SR1 update and a good start in experiments, but missing important comparisons to recent 2nd order optimizations such as Adam, other Hessian free methods (Martens 2012), Pearlmutter fast exact multiplication by the Hessian. From the mnist/cifar curves it is not really showing an advantage to AdaDelta/Nag (although this is stated), and much more experimentation is needed to make a claim about mini-batch insensitivity to performance, can you show error rates on a larger scale task?"
  },
  {
    "people": [
      "Weiss",
      "Bowman"
    ],
    "review": "The paper introduce a way to train joint models for many NLP tasks. Traditionally, we treat these tasks as \u201cpipeline\u201d \u2014 the later tasks will depending on the output of the previous tasks. Here, the authors propose a neural approach which includes all the tasks in one single model. The higher level tasks takes (1) the predictions from the lower level tasks and (2) the hidden representations of the lower level tasks. Also proposed in this paper, is the successive regularization. Intuitively, this means that, when training the high level tasks, we don\u2019t want to change the model in the lower levels by too much so that the lower level tasks can keep a reasonable accuracy of prediction.\n\nOn the modeling side, I think the proposed model is very similar comparing to (Zhang and Weiss, ACL 2016) and SPINN (Bowman et al, 2016) in a even simpler way. The number of the experiments are good. But I am not sure I am convinced by the numbers in Table 1 since the patterns are not very clear there \u2014 sometimes, the performance of the higher level tasks even goes down when training with more tasks (sometimes it does go up, but also not very significant and stable). The dependency scores, although I don\u2019t think this is a serious problem, comparing the UAS/LAS when the output is not guaranteed to be a well-formed tree isn\u2019t strictly speaking fair.\n\nI admit that the successive regularization make sense intuitively and is a very interesting direction to try. However, without a careful study of the training schema of such model, the current results on successive regularization do not convince me that it should be the right thing to do in such models (the current results are not strong enough to show that). The training methods need to be explored here including things as iteratively train on different tasks, and the relationship between the number of training iterations of a task and it\u2019s training set size (and loss on this task etc)."
  },
  {
    "people": [
      "Weiss",
      "Bowman"
    ],
    "review": "The paper introduce a way to train joint models for many NLP tasks. Traditionally, we treat these tasks as \u201cpipeline\u201d \u2014 the later tasks will depending on the output of the previous tasks. Here, the authors propose a neural approach which includes all the tasks in one single model. The higher level tasks takes (1) the predictions from the lower level tasks and (2) the hidden representations of the lower level tasks. Also proposed in this paper, is the successive regularization. Intuitively, this means that, when training the high level tasks, we don\u2019t want to change the model in the lower levels by too much so that the lower level tasks can keep a reasonable accuracy of prediction.\n\nOn the modeling side, I think the proposed model is very similar comparing to (Zhang and Weiss, ACL 2016) and SPINN (Bowman et al, 2016) in a even simpler way. The number of the experiments are good. But I am not sure I am convinced by the numbers in Table 1 since the patterns are not very clear there \u2014 sometimes, the performance of the higher level tasks even goes down when training with more tasks (sometimes it does go up, but also not very significant and stable). The dependency scores, although I don\u2019t think this is a serious problem, comparing the UAS/LAS when the output is not guaranteed to be a well-formed tree isn\u2019t strictly speaking fair.\n\nI admit that the successive regularization make sense intuitively and is a very interesting direction to try. However, without a careful study of the training schema of such model, the current results on successive regularization do not convince me that it should be the right thing to do in such models (the current results are not strong enough to show that). The training methods need to be explored here including things as iteratively train on different tasks, and the relationship between the number of training iterations of a task and it\u2019s training set size (and loss on this task etc)."
  },
  {
    "people": [
      "Mathieu",
      "Marszalek"
    ],
    "review": "1) Summary\n\nThis paper investigates the usefulness of decoupling appearance and motion information for the problem of future frame prediction in natural videos. The method introduces a novel two-stream encoder-decoder architecture, MCNet, consisting of two separate encoders -- a convnet on single frames and a convnet+LSTM on sequences of temporal differences -- followed by combination layers (stacking + convolutions) and a deconvolutional network decoder leveraging also residual connections from the two encoders. The architecture is trained end-to-end using the objective and adversarial training strategy of Mathieu et al.\n\n2) Contributions\n\n+ The architecture seems novel and is well motivated. It is also somewhat related to the two-stream networks of Simonyan & Zisserman, which are very effective for real-world action recognition.\n+ The qualitative results are numerous, insightful, and very convincing (including quantitatively) on KTH & Weizmann, showing the benefits of decoupling content and motion for simple scenes with periodic motions, as well as the need for residual connections.\n\n3) Suggestions for improvement\n\nStatic dataset bias:\nIn response to the pre-review concerns about the observed static nature of the qualitative results, the authors added a simple baseline consisting in copying the pixels of the last observed frame. On the one hand, the updated experiments on KTH confirm the good results of the method in these conditions. On the other hand, the fact that this baseline is better than all other methods (not just the authors's) on UCF101 casts some doubts on whether reporting average statistics on UCF101 is insightful enough. Although the authors provide some qualitative analysis pertaining to the quantity of motion, further quantitative analysis seems necessary to validate the performance of this and other methods on future frame prediction. At least, the results on UCF101 should be disambiguated with respect to the type of scene, for instance by measuring the overall quantity of motion (e.g., l2 norm of time differences) and reporting PSNR and SSIM per quartile / decile. Ideally, other realistic datasets than UCF101 should be considered in complement. For instance, the Hollywood 2 dataset of Marszalek et al would be a good candidate, as it focuses on movies and often contains complex actor, camera, and background motions that would make the \"pixel-copying\" baseline very poor. Experiments on video datasets beyond actions, like the KITTI tracking benchmark, would also greatly improve the paper.\n\nAdditional recognition experiments:\nAs mentioned in pre-review questions, further UCF-101 experiments on action recognition tasks by fine-tuning would also greatly improve the paper. Classifying videos indeed requires learning both appearance and motion features, and the two-stream encoder + combination layers of the MCNet+Res architecture seem particularly adapted, if they indeed allowed for unsupervised pre-trainining of content and motion representations, as postulated by the authors. These experiments would also contribute to dispelling the aforementioned concerns about the static nature of the learned representations.\n\n4) Conclusion\n\nOverall, this paper proposes an interesting architecture for an important problem, but requires additional experiments to substantiate the claims made by the authors. If the authors make the aforementioned additional experiments and the results are convincing, then this paper would be clearly relevant for ICLR.\n\n5) Post-rebuttal final decision\n\nThe authors did a significant amount of additional work, following the suggestions made by the reviewers, and providing additional compelling experimental evidence. This makes this one of the most experimentally thorough ones for this problem. I, therefore, increase my rating, and suggest to accept this paper. Good job!"
  },
  {
    "people": [
      "Mathieu",
      "Matthieu"
    ],
    "review": "The paper presents a method for predicting video sequences in the lines of Mathieu et al. The contribution is the separation of the predictor into two different networks, picking up motion and content, respectively.\n\nThe paper is very interesting, but the novelty is low compared to the referenced work. As also pointed out by AnonReviewer1, there is a similarity with two-stream networks (and also a whole body of work building on this seminal paper). Separating motion and content has also been proposed for other applications, e.g. pose estimation.\n\nDetails :\n\nThe paper can be clearly understood if the basic frameworks (like GANs) are known, but the presentation is not general and good enough for a broad public.\n\nExample : Losses (7) to (9) are well known from the Matthieu et al. paper. However, to make the paper self-contained, they should be properly explained, and it should be mentioned that they are \"additional\" losses. The main target is the GAN loss. The adversarial part of the paper is not properly enough introduced. I do agree, that adversarial training is now well enough known in the community, but it should still be properly introduced. This also involves the explanation that L_Disc is the loss for a second network, the discriminator and explaining the role of both etc.\n\nEquation (1) : c is not explained (are these motion vectors)? c is also overloaded with the feature dimension c'.\n\nThe residual nature of the layer should be made more apparent in equation (3).\n\nThere are several typos, absence of articles and prepositions (\"of\" etc.). The paper should be reread carefully.\n"
  },
  {
    "people": [
      "Mathieu",
      "Marszalek"
    ],
    "review": "1) Summary\n\nThis paper investigates the usefulness of decoupling appearance and motion information for the problem of future frame prediction in natural videos. The method introduces a novel two-stream encoder-decoder architecture, MCNet, consisting of two separate encoders -- a convnet on single frames and a convnet+LSTM on sequences of temporal differences -- followed by combination layers (stacking + convolutions) and a deconvolutional network decoder leveraging also residual connections from the two encoders. The architecture is trained end-to-end using the objective and adversarial training strategy of Mathieu et al.\n\n2) Contributions\n\n+ The architecture seems novel and is well motivated. It is also somewhat related to the two-stream networks of Simonyan & Zisserman, which are very effective for real-world action recognition.\n+ The qualitative results are numerous, insightful, and very convincing (including quantitatively) on KTH & Weizmann, showing the benefits of decoupling content and motion for simple scenes with periodic motions, as well as the need for residual connections.\n\n3) Suggestions for improvement\n\nStatic dataset bias:\nIn response to the pre-review concerns about the observed static nature of the qualitative results, the authors added a simple baseline consisting in copying the pixels of the last observed frame. On the one hand, the updated experiments on KTH confirm the good results of the method in these conditions. On the other hand, the fact that this baseline is better than all other methods (not just the authors's) on UCF101 casts some doubts on whether reporting average statistics on UCF101 is insightful enough. Although the authors provide some qualitative analysis pertaining to the quantity of motion, further quantitative analysis seems necessary to validate the performance of this and other methods on future frame prediction. At least, the results on UCF101 should be disambiguated with respect to the type of scene, for instance by measuring the overall quantity of motion (e.g., l2 norm of time differences) and reporting PSNR and SSIM per quartile / decile. Ideally, other realistic datasets than UCF101 should be considered in complement. For instance, the Hollywood 2 dataset of Marszalek et al would be a good candidate, as it focuses on movies and often contains complex actor, camera, and background motions that would make the \"pixel-copying\" baseline very poor. Experiments on video datasets beyond actions, like the KITTI tracking benchmark, would also greatly improve the paper.\n\nAdditional recognition experiments:\nAs mentioned in pre-review questions, further UCF-101 experiments on action recognition tasks by fine-tuning would also greatly improve the paper. Classifying videos indeed requires learning both appearance and motion features, and the two-stream encoder + combination layers of the MCNet+Res architecture seem particularly adapted, if they indeed allowed for unsupervised pre-trainining of content and motion representations, as postulated by the authors. These experiments would also contribute to dispelling the aforementioned concerns about the static nature of the learned representations.\n\n4) Conclusion\n\nOverall, this paper proposes an interesting architecture for an important problem, but requires additional experiments to substantiate the claims made by the authors. If the authors make the aforementioned additional experiments and the results are convincing, then this paper would be clearly relevant for ICLR.\n\n5) Post-rebuttal final decision\n\nThe authors did a significant amount of additional work, following the suggestions made by the reviewers, and providing additional compelling experimental evidence. This makes this one of the most experimentally thorough ones for this problem. I, therefore, increase my rating, and suggest to accept this paper. Good job!"
  },
  {
    "people": [
      "Mathieu",
      "Marszalek"
    ],
    "review": "1) Summary\n\nThis paper investigates the usefulness of decoupling appearance and motion information for the problem of future frame prediction in natural videos. The method introduces a novel two-stream encoder-decoder architecture, MCNet, consisting of two separate encoders -- a convnet on single frames and a convnet+LSTM on sequences of temporal differences -- followed by combination layers (stacking + convolutions) and a deconvolutional network decoder leveraging also residual connections from the two encoders. The architecture is trained end-to-end using the objective and adversarial training strategy of Mathieu et al.\n\n2) Contributions\n\n+ The architecture seems novel and is well motivated. It is also somewhat related to the two-stream networks of Simonyan & Zisserman, which are very effective for real-world action recognition.\n+ The qualitative results are numerous, insightful, and very convincing (including quantitatively) on KTH & Weizmann, showing the benefits of decoupling content and motion for simple scenes with periodic motions, as well as the need for residual connections.\n\n3) Suggestions for improvement\n\nStatic dataset bias:\nIn response to the pre-review concerns about the observed static nature of the qualitative results, the authors added a simple baseline consisting in copying the pixels of the last observed frame. On the one hand, the updated experiments on KTH confirm the good results of the method in these conditions. On the other hand, the fact that this baseline is better than all other methods (not just the authors's) on UCF101 casts some doubts on whether reporting average statistics on UCF101 is insightful enough. Although the authors provide some qualitative analysis pertaining to the quantity of motion, further quantitative analysis seems necessary to validate the performance of this and other methods on future frame prediction. At least, the results on UCF101 should be disambiguated with respect to the type of scene, for instance by measuring the overall quantity of motion (e.g., l2 norm of time differences) and reporting PSNR and SSIM per quartile / decile. Ideally, other realistic datasets than UCF101 should be considered in complement. For instance, the Hollywood 2 dataset of Marszalek et al would be a good candidate, as it focuses on movies and often contains complex actor, camera, and background motions that would make the \"pixel-copying\" baseline very poor. Experiments on video datasets beyond actions, like the KITTI tracking benchmark, would also greatly improve the paper.\n\nAdditional recognition experiments:\nAs mentioned in pre-review questions, further UCF-101 experiments on action recognition tasks by fine-tuning would also greatly improve the paper. Classifying videos indeed requires learning both appearance and motion features, and the two-stream encoder + combination layers of the MCNet+Res architecture seem particularly adapted, if they indeed allowed for unsupervised pre-trainining of content and motion representations, as postulated by the authors. These experiments would also contribute to dispelling the aforementioned concerns about the static nature of the learned representations.\n\n4) Conclusion\n\nOverall, this paper proposes an interesting architecture for an important problem, but requires additional experiments to substantiate the claims made by the authors. If the authors make the aforementioned additional experiments and the results are convincing, then this paper would be clearly relevant for ICLR.\n\n5) Post-rebuttal final decision\n\nThe authors did a significant amount of additional work, following the suggestions made by the reviewers, and providing additional compelling experimental evidence. This makes this one of the most experimentally thorough ones for this problem. I, therefore, increase my rating, and suggest to accept this paper. Good job!"
  },
  {
    "people": [
      "Mathieu",
      "Matthieu"
    ],
    "review": "The paper presents a method for predicting video sequences in the lines of Mathieu et al. The contribution is the separation of the predictor into two different networks, picking up motion and content, respectively.\n\nThe paper is very interesting, but the novelty is low compared to the referenced work. As also pointed out by AnonReviewer1, there is a similarity with two-stream networks (and also a whole body of work building on this seminal paper). Separating motion and content has also been proposed for other applications, e.g. pose estimation.\n\nDetails :\n\nThe paper can be clearly understood if the basic frameworks (like GANs) are known, but the presentation is not general and good enough for a broad public.\n\nExample : Losses (7) to (9) are well known from the Matthieu et al. paper. However, to make the paper self-contained, they should be properly explained, and it should be mentioned that they are \"additional\" losses. The main target is the GAN loss. The adversarial part of the paper is not properly enough introduced. I do agree, that adversarial training is now well enough known in the community, but it should still be properly introduced. This also involves the explanation that L_Disc is the loss for a second network, the discriminator and explaining the role of both etc.\n\nEquation (1) : c is not explained (are these motion vectors)? c is also overloaded with the feature dimension c'.\n\nThe residual nature of the layer should be made more apparent in equation (3).\n\nThere are several typos, absence of articles and prepositions (\"of\" etc.). The paper should be reread carefully.\n"
  },
  {
    "people": [
      "Mathieu",
      "Marszalek"
    ],
    "review": "1) Summary\n\nThis paper investigates the usefulness of decoupling appearance and motion information for the problem of future frame prediction in natural videos. The method introduces a novel two-stream encoder-decoder architecture, MCNet, consisting of two separate encoders -- a convnet on single frames and a convnet+LSTM on sequences of temporal differences -- followed by combination layers (stacking + convolutions) and a deconvolutional network decoder leveraging also residual connections from the two encoders. The architecture is trained end-to-end using the objective and adversarial training strategy of Mathieu et al.\n\n2) Contributions\n\n+ The architecture seems novel and is well motivated. It is also somewhat related to the two-stream networks of Simonyan & Zisserman, which are very effective for real-world action recognition.\n+ The qualitative results are numerous, insightful, and very convincing (including quantitatively) on KTH & Weizmann, showing the benefits of decoupling content and motion for simple scenes with periodic motions, as well as the need for residual connections.\n\n3) Suggestions for improvement\n\nStatic dataset bias:\nIn response to the pre-review concerns about the observed static nature of the qualitative results, the authors added a simple baseline consisting in copying the pixels of the last observed frame. On the one hand, the updated experiments on KTH confirm the good results of the method in these conditions. On the other hand, the fact that this baseline is better than all other methods (not just the authors's) on UCF101 casts some doubts on whether reporting average statistics on UCF101 is insightful enough. Although the authors provide some qualitative analysis pertaining to the quantity of motion, further quantitative analysis seems necessary to validate the performance of this and other methods on future frame prediction. At least, the results on UCF101 should be disambiguated with respect to the type of scene, for instance by measuring the overall quantity of motion (e.g., l2 norm of time differences) and reporting PSNR and SSIM per quartile / decile. Ideally, other realistic datasets than UCF101 should be considered in complement. For instance, the Hollywood 2 dataset of Marszalek et al would be a good candidate, as it focuses on movies and often contains complex actor, camera, and background motions that would make the \"pixel-copying\" baseline very poor. Experiments on video datasets beyond actions, like the KITTI tracking benchmark, would also greatly improve the paper.\n\nAdditional recognition experiments:\nAs mentioned in pre-review questions, further UCF-101 experiments on action recognition tasks by fine-tuning would also greatly improve the paper. Classifying videos indeed requires learning both appearance and motion features, and the two-stream encoder + combination layers of the MCNet+Res architecture seem particularly adapted, if they indeed allowed for unsupervised pre-trainining of content and motion representations, as postulated by the authors. These experiments would also contribute to dispelling the aforementioned concerns about the static nature of the learned representations.\n\n4) Conclusion\n\nOverall, this paper proposes an interesting architecture for an important problem, but requires additional experiments to substantiate the claims made by the authors. If the authors make the aforementioned additional experiments and the results are convincing, then this paper would be clearly relevant for ICLR.\n\n5) Post-rebuttal final decision\n\nThe authors did a significant amount of additional work, following the suggestions made by the reviewers, and providing additional compelling experimental evidence. This makes this one of the most experimentally thorough ones for this problem. I, therefore, increase my rating, and suggest to accept this paper. Good job!"
  },
  {
    "people": [
      "Gulcehre",
      "Gu",
      "Wen"
    ],
    "review": "This paper introduces pointer-network neural networks, which are applied to referring expressions in three small-scale language modeling tasks: dialogue modeling, recipe modeling and news article modeling. When conditioned on the co-reference chain, the proposed models outperform standard sequence-to-sequence models with attention.\n\nThe proposed models are essentially variants of pointer networks with copy mechanisms (Gulcehre et al., 2016; Gu et al., 2016; Ling et al., 2016), which have been modified to take into account reference chains. As such, the main architectural novelty lies in 1) restricting the pointer mechanism to focus on co-referenced entities, 2) applying pointer mechanism to 2D arrays (tables), and 3) training with supervised alignments. Although useful in practice, these are minor contributions from an architectural perspective.\n\nThe empirical contributions are centred around measuring perplexity on the three language modeling tasks. Measuring perplexity is typical for standard language modeling tasks, but is really an unreliable proxy for dialogue modeling and recipe generation performance. In addition to this, both the dialogue and recipe tasks are tiny compared to standard language modeling tasks. This makes it difficult to evaluate the impact of the dialogue and recipe modeling results. For example, if one was to bootstrap from a larger corpus, it seems likely that a standard sequence-to-sequence model with attention would yield performance comparable to the proposed models (with enough data, the attention mechanism could learn to align referring entities by itself). The language modeling task on news article (Gigaword) seems to yield the most conclusive results. However, the dataset for this task is non-standard and results are provided for only a single baseline. Overall, this limits the conclusions we can draw from the empirical experiments.\n\n\nFinally, the paper itself contains many errors, including mathematical errors, grammatical errors and typos:\n- Eq. (1) is missing a sum over $z_i$.\n- \"into the a decoder LSTM\" -> \"into the decoder LSTM\"\n- \"denoted as his\" -> \"denoted as\"\n- \"Surprising,\" -> \"Surprisingly,\"\n- \"torkens\" -> \"tokens\"\n- \"if follows that the next token\" -> \"the next token\"\n- In the \"COREFERENCE BASED LANGUAGE MODEL\" sub-section, what does $M$ denote?\n- In the sentence: \"The attribute of each column is denoted as $s_c, where $c$ is the c-th attribute\". For these definitions to be make sense, $s_c$ has to be a one-hot vector. If yes, please clarify this in the text.\n- \"the weighted sum is performed\" -> \"the weighted sum is computed\"\n- \"a attribute\" -> \"an attribute\"\n- In the paragraph on Pointer Switch, change $p(z_{i,v} |s_{i,v}) = 1$ -> $p(z_{i,v} |s_{i,v}) = 0$.\n- In the \"Table Pointer\" paragraph, I assume you mean outer-product instead of cross-product? Otherwise, I don't see how the equations add up.\n\n\nOther comments:\n- For the \"Attention based decoder\", is the attention computed using the word embeddings themselves or the hidden states of the sentence encoder? Also, it applied only to the previous turn of the dialogue or to the entire dialogue history? Please clarify this.\n- What's the advantage of using an \"Entity state update\" rule, compared to a pointer network or copy network, which you used in the dialogue and recipe tasks? Please elaborate on this.\n- In the Related Work section, the following sentence is not quite accurate: \"For the task oriented dialogues, most of them embed the seq2seq model in traditional dialogue systems while our model queries the database directly.\". There are task-oriented dialogue models which do query databases during natural language generation. See, for example, \"A Network-based End-to-End Trainable Task-oriented Dialogue System\" by Wen et al."
  },
  {
    "people": [
      "Gulcehre",
      "Gu",
      "Wen"
    ],
    "review": "This paper introduces pointer-network neural networks, which are applied to referring expressions in three small-scale language modeling tasks: dialogue modeling, recipe modeling and news article modeling. When conditioned on the co-reference chain, the proposed models outperform standard sequence-to-sequence models with attention.\n\nThe proposed models are essentially variants of pointer networks with copy mechanisms (Gulcehre et al., 2016; Gu et al., 2016; Ling et al., 2016), which have been modified to take into account reference chains. As such, the main architectural novelty lies in 1) restricting the pointer mechanism to focus on co-referenced entities, 2) applying pointer mechanism to 2D arrays (tables), and 3) training with supervised alignments. Although useful in practice, these are minor contributions from an architectural perspective.\n\nThe empirical contributions are centred around measuring perplexity on the three language modeling tasks. Measuring perplexity is typical for standard language modeling tasks, but is really an unreliable proxy for dialogue modeling and recipe generation performance. In addition to this, both the dialogue and recipe tasks are tiny compared to standard language modeling tasks. This makes it difficult to evaluate the impact of the dialogue and recipe modeling results. For example, if one was to bootstrap from a larger corpus, it seems likely that a standard sequence-to-sequence model with attention would yield performance comparable to the proposed models (with enough data, the attention mechanism could learn to align referring entities by itself). The language modeling task on news article (Gigaword) seems to yield the most conclusive results. However, the dataset for this task is non-standard and results are provided for only a single baseline. Overall, this limits the conclusions we can draw from the empirical experiments.\n\n\nFinally, the paper itself contains many errors, including mathematical errors, grammatical errors and typos:\n- Eq. (1) is missing a sum over $z_i$.\n- \"into the a decoder LSTM\" -> \"into the decoder LSTM\"\n- \"denoted as his\" -> \"denoted as\"\n- \"Surprising,\" -> \"Surprisingly,\"\n- \"torkens\" -> \"tokens\"\n- \"if follows that the next token\" -> \"the next token\"\n- In the \"COREFERENCE BASED LANGUAGE MODEL\" sub-section, what does $M$ denote?\n- In the sentence: \"The attribute of each column is denoted as $s_c, where $c$ is the c-th attribute\". For these definitions to be make sense, $s_c$ has to be a one-hot vector. If yes, please clarify this in the text.\n- \"the weighted sum is performed\" -> \"the weighted sum is computed\"\n- \"a attribute\" -> \"an attribute\"\n- In the paragraph on Pointer Switch, change $p(z_{i,v} |s_{i,v}) = 1$ -> $p(z_{i,v} |s_{i,v}) = 0$.\n- In the \"Table Pointer\" paragraph, I assume you mean outer-product instead of cross-product? Otherwise, I don't see how the equations add up.\n\n\nOther comments:\n- For the \"Attention based decoder\", is the attention computed using the word embeddings themselves or the hidden states of the sentence encoder? Also, it applied only to the previous turn of the dialogue or to the entire dialogue history? Please clarify this.\n- What's the advantage of using an \"Entity state update\" rule, compared to a pointer network or copy network, which you used in the dialogue and recipe tasks? Please elaborate on this.\n- In the Related Work section, the following sentence is not quite accurate: \"For the task oriented dialogues, most of them embed the seq2seq model in traditional dialogue systems while our model queries the database directly.\". There are task-oriented dialogue models which do query databases during natural language generation. See, for example, \"A Network-based End-to-End Trainable Task-oriented Dialogue System\" by Wen et al."
  },
  {
    "people": [
      "Gulcehre",
      "Gu",
      "Wen"
    ],
    "review": "This paper introduces pointer-network neural networks, which are applied to referring expressions in three small-scale language modeling tasks: dialogue modeling, recipe modeling and news article modeling. When conditioned on the co-reference chain, the proposed models outperform standard sequence-to-sequence models with attention.\n\nThe proposed models are essentially variants of pointer networks with copy mechanisms (Gulcehre et al., 2016; Gu et al., 2016; Ling et al., 2016), which have been modified to take into account reference chains. As such, the main architectural novelty lies in 1) restricting the pointer mechanism to focus on co-referenced entities, 2) applying pointer mechanism to 2D arrays (tables), and 3) training with supervised alignments. Although useful in practice, these are minor contributions from an architectural perspective.\n\nThe empirical contributions are centred around measuring perplexity on the three language modeling tasks. Measuring perplexity is typical for standard language modeling tasks, but is really an unreliable proxy for dialogue modeling and recipe generation performance. In addition to this, both the dialogue and recipe tasks are tiny compared to standard language modeling tasks. This makes it difficult to evaluate the impact of the dialogue and recipe modeling results. For example, if one was to bootstrap from a larger corpus, it seems likely that a standard sequence-to-sequence model with attention would yield performance comparable to the proposed models (with enough data, the attention mechanism could learn to align referring entities by itself). The language modeling task on news article (Gigaword) seems to yield the most conclusive results. However, the dataset for this task is non-standard and results are provided for only a single baseline. Overall, this limits the conclusions we can draw from the empirical experiments.\n\n\nFinally, the paper itself contains many errors, including mathematical errors, grammatical errors and typos:\n- Eq. (1) is missing a sum over $z_i$.\n- \"into the a decoder LSTM\" -> \"into the decoder LSTM\"\n- \"denoted as his\" -> \"denoted as\"\n- \"Surprising,\" -> \"Surprisingly,\"\n- \"torkens\" -> \"tokens\"\n- \"if follows that the next token\" -> \"the next token\"\n- In the \"COREFERENCE BASED LANGUAGE MODEL\" sub-section, what does $M$ denote?\n- In the sentence: \"The attribute of each column is denoted as $s_c, where $c$ is the c-th attribute\". For these definitions to be make sense, $s_c$ has to be a one-hot vector. If yes, please clarify this in the text.\n- \"the weighted sum is performed\" -> \"the weighted sum is computed\"\n- \"a attribute\" -> \"an attribute\"\n- In the paragraph on Pointer Switch, change $p(z_{i,v} |s_{i,v}) = 1$ -> $p(z_{i,v} |s_{i,v}) = 0$.\n- In the \"Table Pointer\" paragraph, I assume you mean outer-product instead of cross-product? Otherwise, I don't see how the equations add up.\n\n\nOther comments:\n- For the \"Attention based decoder\", is the attention computed using the word embeddings themselves or the hidden states of the sentence encoder? Also, it applied only to the previous turn of the dialogue or to the entire dialogue history? Please clarify this.\n- What's the advantage of using an \"Entity state update\" rule, compared to a pointer network or copy network, which you used in the dialogue and recipe tasks? Please elaborate on this.\n- In the Related Work section, the following sentence is not quite accurate: \"For the task oriented dialogues, most of them embed the seq2seq model in traditional dialogue systems while our model queries the database directly.\". There are task-oriented dialogue models which do query databases during natural language generation. See, for example, \"A Network-based End-to-End Trainable Task-oriented Dialogue System\" by Wen et al."
  },
  {
    "people": [
      "Gulcehre",
      "Gu",
      "Wen"
    ],
    "review": "This paper introduces pointer-network neural networks, which are applied to referring expressions in three small-scale language modeling tasks: dialogue modeling, recipe modeling and news article modeling. When conditioned on the co-reference chain, the proposed models outperform standard sequence-to-sequence models with attention.\n\nThe proposed models are essentially variants of pointer networks with copy mechanisms (Gulcehre et al., 2016; Gu et al., 2016; Ling et al., 2016), which have been modified to take into account reference chains. As such, the main architectural novelty lies in 1) restricting the pointer mechanism to focus on co-referenced entities, 2) applying pointer mechanism to 2D arrays (tables), and 3) training with supervised alignments. Although useful in practice, these are minor contributions from an architectural perspective.\n\nThe empirical contributions are centred around measuring perplexity on the three language modeling tasks. Measuring perplexity is typical for standard language modeling tasks, but is really an unreliable proxy for dialogue modeling and recipe generation performance. In addition to this, both the dialogue and recipe tasks are tiny compared to standard language modeling tasks. This makes it difficult to evaluate the impact of the dialogue and recipe modeling results. For example, if one was to bootstrap from a larger corpus, it seems likely that a standard sequence-to-sequence model with attention would yield performance comparable to the proposed models (with enough data, the attention mechanism could learn to align referring entities by itself). The language modeling task on news article (Gigaword) seems to yield the most conclusive results. However, the dataset for this task is non-standard and results are provided for only a single baseline. Overall, this limits the conclusions we can draw from the empirical experiments.\n\n\nFinally, the paper itself contains many errors, including mathematical errors, grammatical errors and typos:\n- Eq. (1) is missing a sum over $z_i$.\n- \"into the a decoder LSTM\" -> \"into the decoder LSTM\"\n- \"denoted as his\" -> \"denoted as\"\n- \"Surprising,\" -> \"Surprisingly,\"\n- \"torkens\" -> \"tokens\"\n- \"if follows that the next token\" -> \"the next token\"\n- In the \"COREFERENCE BASED LANGUAGE MODEL\" sub-section, what does $M$ denote?\n- In the sentence: \"The attribute of each column is denoted as $s_c, where $c$ is the c-th attribute\". For these definitions to be make sense, $s_c$ has to be a one-hot vector. If yes, please clarify this in the text.\n- \"the weighted sum is performed\" -> \"the weighted sum is computed\"\n- \"a attribute\" -> \"an attribute\"\n- In the paragraph on Pointer Switch, change $p(z_{i,v} |s_{i,v}) = 1$ -> $p(z_{i,v} |s_{i,v}) = 0$.\n- In the \"Table Pointer\" paragraph, I assume you mean outer-product instead of cross-product? Otherwise, I don't see how the equations add up.\n\n\nOther comments:\n- For the \"Attention based decoder\", is the attention computed using the word embeddings themselves or the hidden states of the sentence encoder? Also, it applied only to the previous turn of the dialogue or to the entire dialogue history? Please clarify this.\n- What's the advantage of using an \"Entity state update\" rule, compared to a pointer network or copy network, which you used in the dialogue and recipe tasks? Please elaborate on this.\n- In the Related Work section, the following sentence is not quite accurate: \"For the task oriented dialogues, most of them embed the seq2seq model in traditional dialogue systems while our model queries the database directly.\". There are task-oriented dialogue models which do query databases during natural language generation. See, for example, \"A Network-based End-to-End Trainable Task-oriented Dialogue System\" by Wen et al."
  },
  {
    "people": [
      "Parzen"
    ],
    "review": "We thank the reviewers for their helpful comments and suggestions.  Based on them, we have significantly updated the paper to include:\n\n1. Comparisons with mVAE (mixture VAE), an ablated version of eVAE that does not share parameters between epitomes\n2. Section 4.2 analyzing the effect of epitome size on generative performance\n3. Section 4.3 analyzing the effect of encoder / decoder architectures on over-pruning and generative performance\n4. Fig. 7 with qualitative samples from best eVAE models\n5.  Section 8.3 in the Appendix comparing the effectiveness of likelihood lower bound and Parzen log-density as a metric for generation ability, and reporting numbers for both\n6. Updating and clarifying Parzen experiments to be on MNIST and lower bound experiments to be on binarized MNIST, consistent with literature\n\nWe believe these updates address the reviewers' comments as well as provide additional insight.\n"
  },
  {
    "people": [
      "Parzen",
      "Parzen",
      "Parzen"
    ],
    "review": "This paper replaces the Gaussian prior often used in a VAE with a group sparse prior. They modify the approximate posterior function so that it also generates group sparse samples. The development of novel forms for the generative model and inference process in VAEs is an active and important area of research. I don't believe the specific choice of prior proposed in this paper is very well motivated however. I believe several of the conceptual claims are incorrect. The experimental results are unconvincing, and I suspect compare log likelihoods in bits against competing algorithms in nats.\n\nSome more detailed comments:\n\nIn Table 1, the log likelihoods reported for competing techniques are all in nats. The reported log likelihood of cVAE using 10K samples is not only higher than the likelihood of true data samples, but is also higher than the log likelihood that can be achieved by fitting a 10K k-means mixture model to the data (eg as done in \"A note on the evaluation of generative models\"). It should nearly impossible to outperform a 10K k-means mixture on Parzen estimation, which makes me extremely skeptical of these eVAE results. However, if you assume that the eVAE log likelihood is actually in bits, and multiply it by log 2 to convert to nats, then it corresponds to a totally believable log likelihood. Note that some Parzen window implementations report log likelihood in bits. Is this experiment comparing log likelihood in bits to competing log likelihoods in nats? (also, label units -- eg bits or nats -- in table)\n\nIt would be really, really, good to report and compare the variational lower bound on the log likelihood!! Alternatively, if you are concerned your bound is loose, you can use AIS to get a more exact measure of the log likelihood. Even if the Parzen window results are correct, Parzen estimates of log likelihood are extremely poor. They possess any drawback of log likelihood evaluation (which they approximate), and then have many additional drawbacks as well.\n\nThe MNIST sample quality does not appear to be visually competitive. Also -- it appears that the images are of the probability of activation for each pixel, rather than actual samples from the model. Samples would be more accurate, but either way make sure to describe what is shown in the figure.\n\nThere are no experiments on non-toy datasets.\n\nI am still concerned about most of the issues I raised in my questions below. Briefly, some comments on the authors' response:\n\n1. \"minibatches are constructed to not only have a random subset of training examples but also be balanced w.r.t. to epitome assignment (Alg. 1, ln. 4).\"\nNice! This makes me feel better about why all the epitomes will be used.\n\n2. I don't think your response addresses why C_vae would trade off between data reconstruction and being factorial. The approximate posterior is factorial by construction -- there's nothing in C_vae that can make it more or less factorial.\n\n3. \"For C_vae to have zero contribution from the KL term of a particular z_d (in other words, that unit is deactivated), it has to have all the examples in the training set be deactivated (KL term of zero) for that unit\"\nThis isn't true. A standard VAE can set the variance to 1 and the mean to 0 (KL term of 0) for some examples in the training set, and have non-zero KL for other training examples.\n\n4. The VAE loss is trained on a lower bound on the log likelihood, though it does have a term that looks like reconstruction error. Naively, I would imagine that if it overfits, this would correspond to data samples becoming more likely under the generative model.\n\n5/6. See Parzen concerns above. It's strange to train a binary model, and then treat it's probability of activation as a sample in a continuous space.\n\n6. \"we can only evaluate the model from its samples\"\nI don't believe this is true. You are training on a lower bound on the log likelihood, which immediately provides another method of quantitative evaluation. Additionally, you could use techniques such as AIS to compute the exact log likelihood.\n\n7. I don't believe Parzen window evaluation is a better measure of model quality, even in terms of sample generation, than log likelihood."
  },
  {
    "people": [
      "Parzen"
    ],
    "review": "We thank the reviewers for their helpful comments and suggestions.  Based on them, we have significantly updated the paper to include:\n\n1. Comparisons with mVAE (mixture VAE), an ablated version of eVAE that does not share parameters between epitomes\n2. Section 4.2 analyzing the effect of epitome size on generative performance\n3. Section 4.3 analyzing the effect of encoder / decoder architectures on over-pruning and generative performance\n4. Fig. 7 with qualitative samples from best eVAE models\n5.  Section 8.3 in the Appendix comparing the effectiveness of likelihood lower bound and Parzen log-density as a metric for generation ability, and reporting numbers for both\n6. Updating and clarifying Parzen experiments to be on MNIST and lower bound experiments to be on binarized MNIST, consistent with literature\n\nWe believe these updates address the reviewers' comments as well as provide additional insight.\n"
  },
  {
    "people": [
      "Parzen",
      "Parzen",
      "Parzen"
    ],
    "review": "This paper replaces the Gaussian prior often used in a VAE with a group sparse prior. They modify the approximate posterior function so that it also generates group sparse samples. The development of novel forms for the generative model and inference process in VAEs is an active and important area of research. I don't believe the specific choice of prior proposed in this paper is very well motivated however. I believe several of the conceptual claims are incorrect. The experimental results are unconvincing, and I suspect compare log likelihoods in bits against competing algorithms in nats.\n\nSome more detailed comments:\n\nIn Table 1, the log likelihoods reported for competing techniques are all in nats. The reported log likelihood of cVAE using 10K samples is not only higher than the likelihood of true data samples, but is also higher than the log likelihood that can be achieved by fitting a 10K k-means mixture model to the data (eg as done in \"A note on the evaluation of generative models\"). It should nearly impossible to outperform a 10K k-means mixture on Parzen estimation, which makes me extremely skeptical of these eVAE results. However, if you assume that the eVAE log likelihood is actually in bits, and multiply it by log 2 to convert to nats, then it corresponds to a totally believable log likelihood. Note that some Parzen window implementations report log likelihood in bits. Is this experiment comparing log likelihood in bits to competing log likelihoods in nats? (also, label units -- eg bits or nats -- in table)\n\nIt would be really, really, good to report and compare the variational lower bound on the log likelihood!! Alternatively, if you are concerned your bound is loose, you can use AIS to get a more exact measure of the log likelihood. Even if the Parzen window results are correct, Parzen estimates of log likelihood are extremely poor. They possess any drawback of log likelihood evaluation (which they approximate), and then have many additional drawbacks as well.\n\nThe MNIST sample quality does not appear to be visually competitive. Also -- it appears that the images are of the probability of activation for each pixel, rather than actual samples from the model. Samples would be more accurate, but either way make sure to describe what is shown in the figure.\n\nThere are no experiments on non-toy datasets.\n\nI am still concerned about most of the issues I raised in my questions below. Briefly, some comments on the authors' response:\n\n1. \"minibatches are constructed to not only have a random subset of training examples but also be balanced w.r.t. to epitome assignment (Alg. 1, ln. 4).\"\nNice! This makes me feel better about why all the epitomes will be used.\n\n2. I don't think your response addresses why C_vae would trade off between data reconstruction and being factorial. The approximate posterior is factorial by construction -- there's nothing in C_vae that can make it more or less factorial.\n\n3. \"For C_vae to have zero contribution from the KL term of a particular z_d (in other words, that unit is deactivated), it has to have all the examples in the training set be deactivated (KL term of zero) for that unit\"\nThis isn't true. A standard VAE can set the variance to 1 and the mean to 0 (KL term of 0) for some examples in the training set, and have non-zero KL for other training examples.\n\n4. The VAE loss is trained on a lower bound on the log likelihood, though it does have a term that looks like reconstruction error. Naively, I would imagine that if it overfits, this would correspond to data samples becoming more likely under the generative model.\n\n5/6. See Parzen concerns above. It's strange to train a binary model, and then treat it's probability of activation as a sample in a continuous space.\n\n6. \"we can only evaluate the model from its samples\"\nI don't believe this is true. You are training on a lower bound on the log likelihood, which immediately provides another method of quantitative evaluation. Additionally, you could use techniques such as AIS to compute the exact log likelihood.\n\n7. I don't believe Parzen window evaluation is a better measure of model quality, even in terms of sample generation, than log likelihood."
  },
  {
    "people": [
      "Neal"
    ],
    "review": "This paper proposes a new kind of generative model based on an annealing process, where the transition probabilities are learned directly to maximize a variational lower bound on the log-likelihood. Overall, the idea is clever and appealing, but I think the paper needs more quantitative validation and better discussion of the relationship with prior work.\n\nIn terms of prior work, AIS and RAISE are both closely related algorithms, and share much of the mathematical structure with the proposed method. For this reason, it\u2019s not sufficient to mention them in passing in the related work section; those methods and their relationship to variational walkback need to be discussed in detail. If I understand correctly, the proposed method is essentially an extension of RAISE where the transition probabilities are learned rather than fixed based on an existing MRF. I think this is an interesting and worthwhile extension, but the relationship to existing work needs to be clarified.\n\nThe analysis of Appendix D seems incorrect. It derives a formula for the ratios of prior and posterior probabilities, but this formula only holds under the assumption of constant temperature (in which case the ratio is very large). When the temperature is varied, the analysis of Neal (2001) applies, and the answer is different. \n\nOne of the main selling points of the method is that it optimizes a variational lower bound on the log-likelihood; even more accurate estimates can be obtained using importance sampling. It ought to be easy to report log-likelihood estimates for this method, so I wonder why such estimates aren\u2019t reported. There are lots of prior results to compare against on MNIST. (In addition, a natural baseline would be RAISE, so that one can check if the ability to learn the transitions actually helps.)\n\nI think the basic idea here is a sound one, so I would be willing to raise my score if the above issues are addressed in a revised version.\n\n\nMinor comments:\n\n\u201cA recognized obstacle to training undirected graphical models\u2026 is that ML training requires sampling from MCMC chains in the inner loop of training, for each example.\u201d This seems like an unfair characterization, since the standard algorithm is PCD, which usually takes only a single step per mini-batch.\n\nSome of the methods discussed in the related work are missing citations.\n\nThe method is justified in terms of \u201ccarving the energy function in the right direction at each point\u201d, but I\u2019m not sure this is actually what\u2019s happening. Isn\u2019t the point of the method that it can optimize a lower bound on the log-likelihood, and therefore learn a globally correct allocation of probability mass?"
  },
  {
    "people": [
      "Neal"
    ],
    "review": "This paper proposes a new kind of generative model based on an annealing process, where the transition probabilities are learned directly to maximize a variational lower bound on the log-likelihood. Overall, the idea is clever and appealing, but I think the paper needs more quantitative validation and better discussion of the relationship with prior work.\n\nIn terms of prior work, AIS and RAISE are both closely related algorithms, and share much of the mathematical structure with the proposed method. For this reason, it\u2019s not sufficient to mention them in passing in the related work section; those methods and their relationship to variational walkback need to be discussed in detail. If I understand correctly, the proposed method is essentially an extension of RAISE where the transition probabilities are learned rather than fixed based on an existing MRF. I think this is an interesting and worthwhile extension, but the relationship to existing work needs to be clarified.\n\nThe analysis of Appendix D seems incorrect. It derives a formula for the ratios of prior and posterior probabilities, but this formula only holds under the assumption of constant temperature (in which case the ratio is very large). When the temperature is varied, the analysis of Neal (2001) applies, and the answer is different. \n\nOne of the main selling points of the method is that it optimizes a variational lower bound on the log-likelihood; even more accurate estimates can be obtained using importance sampling. It ought to be easy to report log-likelihood estimates for this method, so I wonder why such estimates aren\u2019t reported. There are lots of prior results to compare against on MNIST. (In addition, a natural baseline would be RAISE, so that one can check if the ability to learn the transitions actually helps.)\n\nI think the basic idea here is a sound one, so I would be willing to raise my score if the above issues are addressed in a revised version.\n\n\nMinor comments:\n\n\u201cA recognized obstacle to training undirected graphical models\u2026 is that ML training requires sampling from MCMC chains in the inner loop of training, for each example.\u201d This seems like an unfair characterization, since the standard algorithm is PCD, which usually takes only a single step per mini-batch.\n\nSome of the methods discussed in the related work are missing citations.\n\nThe method is justified in terms of \u201ccarving the energy function in the right direction at each point\u201d, but I\u2019m not sure this is actually what\u2019s happening. Isn\u2019t the point of the method that it can optimize a lower bound on the log-likelihood, and therefore learn a globally correct allocation of probability mass?\n"
  },
  {
    "people": [
      "Neal"
    ],
    "review": "This paper proposes a new kind of generative model based on an annealing process, where the transition probabilities are learned directly to maximize a variational lower bound on the log-likelihood. Overall, the idea is clever and appealing, but I think the paper needs more quantitative validation and better discussion of the relationship with prior work.\n\nIn terms of prior work, AIS and RAISE are both closely related algorithms, and share much of the mathematical structure with the proposed method. For this reason, it\u2019s not sufficient to mention them in passing in the related work section; those methods and their relationship to variational walkback need to be discussed in detail. If I understand correctly, the proposed method is essentially an extension of RAISE where the transition probabilities are learned rather than fixed based on an existing MRF. I think this is an interesting and worthwhile extension, but the relationship to existing work needs to be clarified.\n\nThe analysis of Appendix D seems incorrect. It derives a formula for the ratios of prior and posterior probabilities, but this formula only holds under the assumption of constant temperature (in which case the ratio is very large). When the temperature is varied, the analysis of Neal (2001) applies, and the answer is different. \n\nOne of the main selling points of the method is that it optimizes a variational lower bound on the log-likelihood; even more accurate estimates can be obtained using importance sampling. It ought to be easy to report log-likelihood estimates for this method, so I wonder why such estimates aren\u2019t reported. There are lots of prior results to compare against on MNIST. (In addition, a natural baseline would be RAISE, so that one can check if the ability to learn the transitions actually helps.)\n\nI think the basic idea here is a sound one, so I would be willing to raise my score if the above issues are addressed in a revised version.\n\n\nMinor comments:\n\n\u201cA recognized obstacle to training undirected graphical models\u2026 is that ML training requires sampling from MCMC chains in the inner loop of training, for each example.\u201d This seems like an unfair characterization, since the standard algorithm is PCD, which usually takes only a single step per mini-batch.\n\nSome of the methods discussed in the related work are missing citations.\n\nThe method is justified in terms of \u201ccarving the energy function in the right direction at each point\u201d, but I\u2019m not sure this is actually what\u2019s happening. Isn\u2019t the point of the method that it can optimize a lower bound on the log-likelihood, and therefore learn a globally correct allocation of probability mass?"
  },
  {
    "people": [
      "Neal"
    ],
    "review": "This paper proposes a new kind of generative model based on an annealing process, where the transition probabilities are learned directly to maximize a variational lower bound on the log-likelihood. Overall, the idea is clever and appealing, but I think the paper needs more quantitative validation and better discussion of the relationship with prior work.\n\nIn terms of prior work, AIS and RAISE are both closely related algorithms, and share much of the mathematical structure with the proposed method. For this reason, it\u2019s not sufficient to mention them in passing in the related work section; those methods and their relationship to variational walkback need to be discussed in detail. If I understand correctly, the proposed method is essentially an extension of RAISE where the transition probabilities are learned rather than fixed based on an existing MRF. I think this is an interesting and worthwhile extension, but the relationship to existing work needs to be clarified.\n\nThe analysis of Appendix D seems incorrect. It derives a formula for the ratios of prior and posterior probabilities, but this formula only holds under the assumption of constant temperature (in which case the ratio is very large). When the temperature is varied, the analysis of Neal (2001) applies, and the answer is different. \n\nOne of the main selling points of the method is that it optimizes a variational lower bound on the log-likelihood; even more accurate estimates can be obtained using importance sampling. It ought to be easy to report log-likelihood estimates for this method, so I wonder why such estimates aren\u2019t reported. There are lots of prior results to compare against on MNIST. (In addition, a natural baseline would be RAISE, so that one can check if the ability to learn the transitions actually helps.)\n\nI think the basic idea here is a sound one, so I would be willing to raise my score if the above issues are addressed in a revised version.\n\n\nMinor comments:\n\n\u201cA recognized obstacle to training undirected graphical models\u2026 is that ML training requires sampling from MCMC chains in the inner loop of training, for each example.\u201d This seems like an unfair characterization, since the standard algorithm is PCD, which usually takes only a single step per mini-batch.\n\nSome of the methods discussed in the related work are missing citations.\n\nThe method is justified in terms of \u201ccarving the energy function in the right direction at each point\u201d, but I\u2019m not sure this is actually what\u2019s happening. Isn\u2019t the point of the method that it can optimize a lower bound on the log-likelihood, and therefore learn a globally correct allocation of probability mass?\n"
  },
  {
    "people": [
      "Levitis"
    ],
    "review": "Dear reviewers,\n\nThank you all for your feedback and suggestions.\n\n** We have added the following changes to the paper: \n- better explanation of figures 4 and 5 in their captions\n- clearer definition a bout (Section 5.1, first paragraph)\n- analogy between diagonal connections and skip connections added (Section 3.1, first paragraph)\n\n** Regarding scaling to other data\nIn Section 4, paragraph 1, we describe how data should be represented to fit our framework, and we have added a discussion at the end of the paper about scaling to more complex scenarios (Section 6, paragraph 2).\n\n** In response to modeling \u201cbehavior\u201d being too strong of a claim and that it should be replaced with \u201cspatiotemporal behavior\u201d or \u201cfruit fly behavior\u201d:\n1) We qualify our statement in the abstract, stating both that we focus on motion sequences (i.e. trajectories) and that we test it on fruit flies and handwriting, which we further elaborate on in the introduction.\n\n2) I would argue that behavior is spatiotemporal.\nPeople from different fields may think of behavior differently, ranging from generating to observing behavior. A roboticist may think of it in terms of moving joints, an animator in terms of moving vertices, and a computer vision researcher in terms of moving pixels.\n\nAs noted by Levitis et al. ("
  },
  {
    "people": [
      "Levitis"
    ],
    "review": "Dear reviewers,\n\nThank you all for your feedback and suggestions.\n\n** We have added the following changes to the paper: \n- better explanation of figures 4 and 5 in their captions\n- clearer definition a bout (Section 5.1, first paragraph)\n- analogy between diagonal connections and skip connections added (Section 3.1, first paragraph)\n\n** Regarding scaling to other data\nIn Section 4, paragraph 1, we describe how data should be represented to fit our framework, and we have added a discussion at the end of the paper about scaling to more complex scenarios (Section 6, paragraph 2).\n\n** In response to modeling \u201cbehavior\u201d being too strong of a claim and that it should be replaced with \u201cspatiotemporal behavior\u201d or \u201cfruit fly behavior\u201d:\n1) We qualify our statement in the abstract, stating both that we focus on motion sequences (i.e. trajectories) and that we test it on fruit flies and handwriting, which we further elaborate on in the introduction.\n\n2) I would argue that behavior is spatiotemporal.\nPeople from different fields may think of behavior differently, ranging from generating to observing behavior. A roboticist may think of it in terms of moving joints, an animator in terms of moving vertices, and a computer vision researcher in terms of moving pixels.\n\nAs noted by Levitis et al. ("
  },
  {
    "people": [
      "Raychev"
    ],
    "review": "Dear reviewers,\n\nWe have improved our approaches and revised the paper based on the comments. We thank all reviewers for the valuable comments, and hope our work has the chance to be discussed further!\n\nWe are still working to update the paper, and some new results will be available by tomorrow.\n\nHere is the list of all changes as of Jan-15-2017:\n\n1) We have fine-tuned the model to achieve better performance. Now, our approaches can achieve a better performance than prior art. We have updated the following related section:\n   a) Abstract & Introduction\n   b) Section 5.2. Training details\n   c) All results in Section 5.3 & 5.4. In 5.3, the baseline is updated to Raychev et al. (2016a).\n   d) Some results in Section 5.5 are still pending. So this section is removed right now.\n\n2) The related work section and items in the reference have been revised.\n\n3) We have added a paragraph in Section 3.3 to explain the difference between next node prediction and next token prediction.\n\n"
  },
  {
    "people": [
      "Bielik"
    ],
    "review": "This paper considers the code completion problem: given partially written source code produce a distribution over the next token or sequence of tokens. This is an interesting and important problem with relevance to industry and research. The authors propose an LSTM model that sequentially generates a depth-first traversal over an AST. Not surprisingly the results improve over previous approaches with more brittle conditioning mechanisms (Bielik et al. 2016). Still, simply augmenting previous work with LSTM-based conditioning is not enough of a contribution to justify an entire paper. Some directions that would greatly improve the contribution include: considering distinct traversal orders, does this change the predictive accuracy? Any other ways of dealing with UNK tokens? The ultimate goal of this paper is to improve code completion, and it would be great to go beyond simply neurifying previous methods.\n\nComments:\n\n- Last two sentences of related work claim that other methods can only \"examine a limited subset of source code\". Aside from being a vague statement, it isn't accurate. The models described in Bielik et al. 2016 and Maddison & Tarlow 2014 can in principle condition on any part of the AST already generated. The difference in this work is that the LSTM can learn to condition in a flexible way that doesn't increase the complexity of the computation.\n\n- In the denying prediction experiments, the most interesting number is the Prediction Accuracy, which is P(accurate | model doesn't predict UNK). I think it would also be interesting to see P(accurate | UNK is not ground truth). Clearly the models trained to ignore UNK losses will do worse overall, but do they do worse on non-UNK tokens?"
  },
  {
    "people": [
      "Bielik",
      "Maddison"
    ],
    "review": "This paper studies the problem of source code completion using neural network models. A variety of models are presented, all of which are simple variations on LSTMs, adapted to the peculiarities of the data representation chosen (code is represented as a sequence of (nonterminal, terminal) pairs with terminals being allowed to be EMPTY). Another minor tweak is the option to \"deny prediction,\" which makes sense in the context of code completion in an IDE, as it's probably better to not make a prediction if the model is very unsure about what comes next.\n\nEmpirically, results show that performance is worse than previous work on predicting terminals but better at predicting nonterminals. However, I find the split between terminals and nonterminals to be strange, and it's not clear to me what the takeaway is. Surely a simple proxy for what we care about is how often the system is going to suggest the next token that actually appears in the code. Why not compute this and report a single number to summarize the performance?\n\nOverall the paper is OK, but it has a flavor of \"we ran LSTMs on an existing dataset\". The results are OK but not amazing. There are also some issues with the writing that could be improved (see below). In total, I don't think there is a big enough contribution to warrant publication at ICLR.\n\nDetailed comments:\n\n* I find the NT2NT model strange, in that it predicts the nonterminal and the terminal independently conditional upon the hidden state.\n\n* The discussion of related work needs reworking. For example, Bielik et al. does not generalize all of the works listed at the start of section 2, and the Maddison (2016) citation is wrong\n"
  },
  {
    "people": [
      "Raychev"
    ],
    "review": "Dear reviewers,\n\nWe have improved our approaches and revised the paper based on the comments. We thank all reviewers for the valuable comments, and hope our work has the chance to be discussed further!\n\nWe are still working to update the paper, and some new results will be available by tomorrow.\n\nHere is the list of all changes as of Jan-15-2017:\n\n1) We have fine-tuned the model to achieve better performance. Now, our approaches can achieve a better performance than prior art. We have updated the following related section:\n   a) Abstract & Introduction\n   b) Section 5.2. Training details\n   c) All results in Section 5.3 & 5.4. In 5.3, the baseline is updated to Raychev et al. (2016a).\n   d) Some results in Section 5.5 are still pending. So this section is removed right now.\n\n2) The related work section and items in the reference have been revised.\n\n3) We have added a paragraph in Section 3.3 to explain the difference between next node prediction and next token prediction.\n\n"
  },
  {
    "people": [
      "Bielik"
    ],
    "review": "This paper considers the code completion problem: given partially written source code produce a distribution over the next token or sequence of tokens. This is an interesting and important problem with relevance to industry and research. The authors propose an LSTM model that sequentially generates a depth-first traversal over an AST. Not surprisingly the results improve over previous approaches with more brittle conditioning mechanisms (Bielik et al. 2016). Still, simply augmenting previous work with LSTM-based conditioning is not enough of a contribution to justify an entire paper. Some directions that would greatly improve the contribution include: considering distinct traversal orders, does this change the predictive accuracy? Any other ways of dealing with UNK tokens? The ultimate goal of this paper is to improve code completion, and it would be great to go beyond simply neurifying previous methods.\n\nComments:\n\n- Last two sentences of related work claim that other methods can only \"examine a limited subset of source code\". Aside from being a vague statement, it isn't accurate. The models described in Bielik et al. 2016 and Maddison & Tarlow 2014 can in principle condition on any part of the AST already generated. The difference in this work is that the LSTM can learn to condition in a flexible way that doesn't increase the complexity of the computation.\n\n- In the denying prediction experiments, the most interesting number is the Prediction Accuracy, which is P(accurate | model doesn't predict UNK). I think it would also be interesting to see P(accurate | UNK is not ground truth). Clearly the models trained to ignore UNK losses will do worse overall, but do they do worse on non-UNK tokens?"
  },
  {
    "people": [
      "Bielik",
      "Maddison"
    ],
    "review": "This paper studies the problem of source code completion using neural network models. A variety of models are presented, all of which are simple variations on LSTMs, adapted to the peculiarities of the data representation chosen (code is represented as a sequence of (nonterminal, terminal) pairs with terminals being allowed to be EMPTY). Another minor tweak is the option to \"deny prediction,\" which makes sense in the context of code completion in an IDE, as it's probably better to not make a prediction if the model is very unsure about what comes next.\n\nEmpirically, results show that performance is worse than previous work on predicting terminals but better at predicting nonterminals. However, I find the split between terminals and nonterminals to be strange, and it's not clear to me what the takeaway is. Surely a simple proxy for what we care about is how often the system is going to suggest the next token that actually appears in the code. Why not compute this and report a single number to summarize the performance?\n\nOverall the paper is OK, but it has a flavor of \"we ran LSTMs on an existing dataset\". The results are OK but not amazing. There are also some issues with the writing that could be improved (see below). In total, I don't think there is a big enough contribution to warrant publication at ICLR.\n\nDetailed comments:\n\n* I find the NT2NT model strange, in that it predicts the nonterminal and the terminal independently conditional upon the hidden state.\n\n* The discussion of related work needs reworking. For example, Bielik et al. does not generalize all of the works listed at the start of section 2, and the Maddison (2016) citation is wrong\n"
  },
  {
    "people": [
      "Watter"
    ],
    "review": "The authors propose a time-series model with discrete states for robotics applications. I think the proposed method is too simplistic to be useful in the presented form, eg. 1) the state space (dimensionality & topology) is exactly matched to the experiments 2) displacements in the transition model are linear in the actions 3) observations are one-dimensional. This seems to be quite behind the current state of the art, eg \u201cEmbed to Control\u201d by Watter et al 2015, where a state representation is learned directly from pixels.\nFurthermore the authors do not compare to any other method except for an out-of-the-box LSTM model. Also, I feel like there must be a lot of prior work for combining HMMs + NNs out there, I think it would be necessary for the authors to relate their work to this literature."
  },
  {
    "people": [
      "Haarnoja"
    ],
    "review": "Summary: This paper presents a differentiable histogram filter for state estimation/tracking. The proposed histogram filter is a particular Bayesian filter that represents the discretized states using beliefs. The prediction step is parameterized by a locally linear and translation-invariant motion model while the measurement model is represented by a multi-layered neural network. The whole system is learned with both supervised and unsupervised objectives and experiments are carried out on two synthetic robot localization tasks (1D and 2D). The major claim of this paper is that the problem-specific model structure (Bayesian filter for state estimation) should improve pure deep learning approach in data-efficiency and generalization ability. \n+This paper has nice arguments about the importance of prior knowledge to deep learning approach for specific tasks. \n+An end-to-end histogram filter is derived for state estimation and unsupervised learning is possible in this model.\n-This paper seems to have a hidden assumption that deep learning (RNN) is a natural choice for recursive state estimation and the rest of paper is built upon this assumption including LSTM baselines. However, this assumption itself may not be true, because Bayesian filter is a first-established approach for this classic problem, so it it more important to justify if deep learning is even necessary for solving the tasks presented. This requests pure Bayesian filter baselines in the experiments. \n-The derived histogram filter seems to be particularly designed for discretized state space. It is not clear how well it can be generalized to continuous state space using the notation \"x\". More interestingly, the observation is discrete (binary) as well, which eventually makes it possible to derive a closed-form measurement update model. This setup might be too constrained. Generalizing to continuous observations is not a trivial task, not even to mention using images as observations like Haarnoja et al 2016. These design choices overall narrow down the scope of applicability."
  },
  {
    "people": [
      "Watter"
    ],
    "review": "The authors propose a time-series model with discrete states for robotics applications. I think the proposed method is too simplistic to be useful in the presented form, eg. 1) the state space (dimensionality & topology) is exactly matched to the experiments 2) displacements in the transition model are linear in the actions 3) observations are one-dimensional. This seems to be quite behind the current state of the art, eg \u201cEmbed to Control\u201d by Watter et al 2015, where a state representation is learned directly from pixels.\nFurthermore the authors do not compare to any other method except for an out-of-the-box LSTM model. Also, I feel like there must be a lot of prior work for combining HMMs + NNs out there, I think it would be necessary for the authors to relate their work to this literature. \u00a0\u00a0"
  },
  {
    "people": [
      "Watter"
    ],
    "review": "The authors propose a time-series model with discrete states for robotics applications. I think the proposed method is too simplistic to be useful in the presented form, eg. 1) the state space (dimensionality & topology) is exactly matched to the experiments 2) displacements in the transition model are linear in the actions 3) observations are one-dimensional. This seems to be quite behind the current state of the art, eg \u201cEmbed to Control\u201d by Watter et al 2015, where a state representation is learned directly from pixels.\nFurthermore the authors do not compare to any other method except for an out-of-the-box LSTM model. Also, I feel like there must be a lot of prior work for combining HMMs + NNs out there, I think it would be necessary for the authors to relate their work to this literature."
  },
  {
    "people": [
      "Haarnoja"
    ],
    "review": "Summary: This paper presents a differentiable histogram filter for state estimation/tracking. The proposed histogram filter is a particular Bayesian filter that represents the discretized states using beliefs. The prediction step is parameterized by a locally linear and translation-invariant motion model while the measurement model is represented by a multi-layered neural network. The whole system is learned with both supervised and unsupervised objectives and experiments are carried out on two synthetic robot localization tasks (1D and 2D). The major claim of this paper is that the problem-specific model structure (Bayesian filter for state estimation) should improve pure deep learning approach in data-efficiency and generalization ability. \n+This paper has nice arguments about the importance of prior knowledge to deep learning approach for specific tasks. \n+An end-to-end histogram filter is derived for state estimation and unsupervised learning is possible in this model.\n-This paper seems to have a hidden assumption that deep learning (RNN) is a natural choice for recursive state estimation and the rest of paper is built upon this assumption including LSTM baselines. However, this assumption itself may not be true, because Bayesian filter is a first-established approach for this classic problem, so it it more important to justify if deep learning is even necessary for solving the tasks presented. This requests pure Bayesian filter baselines in the experiments. \n-The derived histogram filter seems to be particularly designed for discretized state space. It is not clear how well it can be generalized to continuous state space using the notation \"x\". More interestingly, the observation is discrete (binary) as well, which eventually makes it possible to derive a closed-form measurement update model. This setup might be too constrained. Generalizing to continuous observations is not a trivial task, not even to mention using images as observations like Haarnoja et al 2016. These design choices overall narrow down the scope of applicability."
  },
  {
    "people": [
      "Watter"
    ],
    "review": "The authors propose a time-series model with discrete states for robotics applications. I think the proposed method is too simplistic to be useful in the presented form, eg. 1) the state space (dimensionality & topology) is exactly matched to the experiments 2) displacements in the transition model are linear in the actions 3) observations are one-dimensional. This seems to be quite behind the current state of the art, eg \u201cEmbed to Control\u201d by Watter et al 2015, where a state representation is learned directly from pixels.\nFurthermore the authors do not compare to any other method except for an out-of-the-box LSTM model. Also, I feel like there must be a lot of prior work for combining HMMs + NNs out there, I think it would be necessary for the authors to relate their work to this literature. \u00a0\u00a0"
  },
  {
    "people": [
      "Tillmann"
    ],
    "review": "This paper adapts NMT to a noisy channel formulation utilizing the recently developed SSNT framework. The paper is well-written and has solid experimental results. Howver, the paper can be improved with a bit more originality and impact. In, short the pros and cons of the paper are:\n \n Pro: \n - Clarity: All agree paper was very \"well written\" \n - Quality: Reviewers note the \"strong experimental section\". Comprehensive results. \n \n Cons: \n - Originality: There were concerns about technical novelty: (a) \"this paper does not present anything that is particular novel on top of the SSNT\" (b) not that \"conceptually different from the work of Tillmann et al\". \n - Impact: Reviewers were not completely convinced that this method could not work with simpler means. For instance by using clever reranking or utilizing the deep speech style unprincipled combination. However, this paper does produce a better approach for this problem."
  },
  {
    "people": [
      "Alkhouli",
      "Alkhouli"
    ],
    "review": "This is an interesting paper that investigates alternative neural machine translation approaches. Falling back to traditional concepts machine translation while using powerful neural models is essential to understanding the leap achieved by neural machine translation over conventional methods.\n\nIn terms of related work, there is a close relation to the paper Alignment-Based Neural Machine Translation (Alkhouli et al. 2016)*, whether in the decomposition of the model into neural alignment and  lexical/word probabilities, or using a weighted log-linear combination to combine the models which are eventually used in a standalone decoder. A notable difference is that the paper uses direct translation models (as opposed to the inverted ones proposed by the authors here), and the fact that the models are trained on word-aligned data which speeds up training. As for the alignment model, the former choses to model non-monotone source jumps whereas the current paper choses two classes (emit and shift) to model monotone alignments. In (Alkhouli et al. 2016), decoding is done using a beam search decoder that hypothesizes alignments and word translations\n\nAs for the MT experiments, did you try the model combination using larger data (i.e. >=100M tokens). How long did the current models take to train on the 184K sentence pairs you are using? It would also be interesting to see how much you gain/lose if the models were to be trained using given word alignments (e.g. obtained using GIZA++). Training then should be faster.\n\n* "
  },
  {
    "people": [
      "Li",
      "Yu"
    ],
    "review": "This paper proposes to use an SSNT model of p(x|y) to allow for a noisy channel model of conditional generation that (still) allows for incremental generation of y. The authors also propose an approximate search strategy for decoding, and do an extensive empirical evaluation.\n\nPROs: This paper is generally well written, and the SSNT model is quite interesting and its application here well motivated. Furthermore, the empirical evaluation is very well done, and the authors obtain good results.\n\nCONs: One might be concerned about whether the additional training and decoding complexity is warranted. For instance, one might plausibly obtain the benefits of the proposed approach by reranking (full) outputs from a standard seq2seq model with a score combining p(y|x), p(x|y), and p(y). (It's worth noting that Li et al. (NAACL 2016) do something similar for conversation modeling). At the same time, being able to rerank during search may be helpful, and so it might be nice to see some experiments addressing this.\n\nOther Comments: \n - Given that the main thrust of the paper is to provide a model for p(x|y), the paper might be slightly clearer if Section 2 were presented from the perspective of modeling p(x|y) instead of switching back to p(y|x) as in the original Yu et al. paper. \n\n - It initially seems strange to suggest a noisy-channel model as a way of addressing the \"explaining away\" problem, since now you have an explicit, uncalibrated p(y) term. However, since seq2seq models appear to naturally do a lot of target-side language modeling, incorporating an explicit p(x|y) term seems quite clever.\n"
  },
  {
    "people": [
      "Tillmann",
      "Tillmann"
    ],
    "review": "The paper proposes an online variant of segment to segment transducers, which allows to circumvent the necessity of observing whole sentence, before making target predictions. Authors mostly build on their previous work, allowing additionally to leverage independent priors on the target hypotheses, like the language grammar or sentence length.\n\nStrong points:\n- well written, interesting idea of combining various sources of information in a Bayesian framework for seq2seq models\nHandling something in an online manner typically makes things more difficult, and this is what the authors are trying to do here - which is definitely of interest to the community\n- strong experimental section, with some strong results (though not complete: see weak points)\n\nWeak points:\n- Authors do not improve on computational complexity (w.r.t Tillmann proposal), hence the algorithms may be found difficult to apply in scenarios where inputs may be long (this already takes into account a rather constrained model of alignment latent variables)\n- What about the baseline where you only combine direct, LM and bias contributions (no channel)? Was there any (non-obvious) algorithmic constraint why - this has not been included?\n\nSome other (minor) comments:\n\n- Related to the first weak point: can you elaborate more on how the clue of your work is conceptually different from the work of Tillmann et al. (1997) (except, of course, the fact you use connectionist discriminative models to derive particular conditional probabilities). \n- How sensitive is the model to different choices of hyper-parameters in eq (3). Do you naively search through the search space of those, or do something more clever?\n- Some more comments on details of the auxiliary direct model would be definitely of interest.\n- How crucial is the correct choice of the pruning variables (K1 and K2)? \n- Sec. 2: makes no Markovian assumptions -> no first-order Markovian assumption?\n\nTypos:\nTable 1: chanel -> channel (one before last row)\n\nApologies for late review."
  },
  {
    "people": [
      "Tillmann"
    ],
    "review": "This paper adapts NMT to a noisy channel formulation utilizing the recently developed SSNT framework. The paper is well-written and has solid experimental results. Howver, the paper can be improved with a bit more originality and impact. In, short the pros and cons of the paper are:\n \n Pro: \n - Clarity: All agree paper was very \"well written\" \n - Quality: Reviewers note the \"strong experimental section\". Comprehensive results. \n \n Cons: \n - Originality: There were concerns about technical novelty: (a) \"this paper does not present anything that is particular novel on top of the SSNT\" (b) not that \"conceptually different from the work of Tillmann et al\". \n - Impact: Reviewers were not completely convinced that this method could not work with simpler means. For instance by using clever reranking or utilizing the deep speech style unprincipled combination. However, this paper does produce a better approach for this problem."
  },
  {
    "people": [
      "Alkhouli",
      "Alkhouli"
    ],
    "review": "This is an interesting paper that investigates alternative neural machine translation approaches. Falling back to traditional concepts machine translation while using powerful neural models is essential to understanding the leap achieved by neural machine translation over conventional methods.\n\nIn terms of related work, there is a close relation to the paper Alignment-Based Neural Machine Translation (Alkhouli et al. 2016)*, whether in the decomposition of the model into neural alignment and  lexical/word probabilities, or using a weighted log-linear combination to combine the models which are eventually used in a standalone decoder. A notable difference is that the paper uses direct translation models (as opposed to the inverted ones proposed by the authors here), and the fact that the models are trained on word-aligned data which speeds up training. As for the alignment model, the former choses to model non-monotone source jumps whereas the current paper choses two classes (emit and shift) to model monotone alignments. In (Alkhouli et al. 2016), decoding is done using a beam search decoder that hypothesizes alignments and word translations\n\nAs for the MT experiments, did you try the model combination using larger data (i.e. >=100M tokens). How long did the current models take to train on the 184K sentence pairs you are using? It would also be interesting to see how much you gain/lose if the models were to be trained using given word alignments (e.g. obtained using GIZA++). Training then should be faster.\n\n* "
  },
  {
    "people": [
      "Li",
      "Yu"
    ],
    "review": "This paper proposes to use an SSNT model of p(x|y) to allow for a noisy channel model of conditional generation that (still) allows for incremental generation of y. The authors also propose an approximate search strategy for decoding, and do an extensive empirical evaluation.\n\nPROs: This paper is generally well written, and the SSNT model is quite interesting and its application here well motivated. Furthermore, the empirical evaluation is very well done, and the authors obtain good results.\n\nCONs: One might be concerned about whether the additional training and decoding complexity is warranted. For instance, one might plausibly obtain the benefits of the proposed approach by reranking (full) outputs from a standard seq2seq model with a score combining p(y|x), p(x|y), and p(y). (It's worth noting that Li et al. (NAACL 2016) do something similar for conversation modeling). At the same time, being able to rerank during search may be helpful, and so it might be nice to see some experiments addressing this.\n\nOther Comments: \n - Given that the main thrust of the paper is to provide a model for p(x|y), the paper might be slightly clearer if Section 2 were presented from the perspective of modeling p(x|y) instead of switching back to p(y|x) as in the original Yu et al. paper. \n\n - It initially seems strange to suggest a noisy-channel model as a way of addressing the \"explaining away\" problem, since now you have an explicit, uncalibrated p(y) term. However, since seq2seq models appear to naturally do a lot of target-side language modeling, incorporating an explicit p(x|y) term seems quite clever.\n"
  },
  {
    "people": [
      "Tillmann",
      "Tillmann"
    ],
    "review": "The paper proposes an online variant of segment to segment transducers, which allows to circumvent the necessity of observing whole sentence, before making target predictions. Authors mostly build on their previous work, allowing additionally to leverage independent priors on the target hypotheses, like the language grammar or sentence length.\n\nStrong points:\n- well written, interesting idea of combining various sources of information in a Bayesian framework for seq2seq models\nHandling something in an online manner typically makes things more difficult, and this is what the authors are trying to do here - which is definitely of interest to the community\n- strong experimental section, with some strong results (though not complete: see weak points)\n\nWeak points:\n- Authors do not improve on computational complexity (w.r.t Tillmann proposal), hence the algorithms may be found difficult to apply in scenarios where inputs may be long (this already takes into account a rather constrained model of alignment latent variables)\n- What about the baseline where you only combine direct, LM and bias contributions (no channel)? Was there any (non-obvious) algorithmic constraint why - this has not been included?\n\nSome other (minor) comments:\n\n- Related to the first weak point: can you elaborate more on how the clue of your work is conceptually different from the work of Tillmann et al. (1997) (except, of course, the fact you use connectionist discriminative models to derive particular conditional probabilities). \n- How sensitive is the model to different choices of hyper-parameters in eq (3). Do you naively search through the search space of those, or do something more clever?\n- Some more comments on details of the auxiliary direct model would be definitely of interest.\n- How crucial is the correct choice of the pruning variables (K1 and K2)? \n- Sec. 2: makes no Markovian assumptions -> no first-order Markovian assumption?\n\nTypos:\nTable 1: chanel -> channel (one before last row)\n\nApologies for late review."
  },
  {
    "people": [
      "Kim",
      "Bengio",
      "Kim",
      "Bengio",
      "Rasmus"
    ],
    "review": "This paper introduces an energy-based Generative Adversarial Network (GAN) and provides theoretical and empirical results modeling a number of image datasets (including large-scale versions of categories of ImageNet). As far as I know energy-based GANs (EBGAN) were introduced in Kim and Bengio (2016), but the proposed version makes a number of different design choices. \n\nFirst, it does away with the entropy regularization term that Kim and Bengio (2016) introduced to ensure that the GAN discriminator converged to an energy function proportional to the log density of the data (at optimum). This implies that the discriminator in the proposed scheme will become uniform at convergence as discussed in the theoretical section of the paper, however the introductory text seems to imply otherwise -- that one could recover a meaningful score function from the trained energy-function (discriminator). This should be clarified. \n\nSecond, this version of the EBGAN setting includes two innovations: (1) the introduction of the hinge loss in the value function, and (2) the use of an auto-encoder parametrization for the energy function. These innovations are not empirically justified in any way - this is disappointing, as it would be really good to see empirical results supporting the arguments made in support of their introduction. \n\nThe two significant contributions of this paper are the theoretical analysis of the energy-baesd GAN formalism (showing that the optimum corresponds to a Nash equilibrium) and the impressive empirical results on large images that set a new standard in what straight GAN-style models can achieve.\n\nThe theoretical results seem solid to me and make a nice contribution.\n\nRegarding the quantitative results in Table 2, it seems not appropriate to bold the EBGAN line when it seems to be statistically indistinguishable form the Rasmus et al (2015) results. Though it is not mentioned here, the use of bold typically indicates the state-of-the-art. \n\nI think this paper could be be much stronger if the two novel contributions to the energy-based GAN setting were more thoroughly explored with ablation experiments. That being said, I think this paper has already become a contribution other are building on (including at least two other ICLR submissions) and so I think it should be accepted for publication at ICLR. \n"
  },
  {
    "people": [
      "Springenberg",
      "Jost Tobias",
      "Kim",
      "Taesup",
      "Yoshua Bengio"
    ],
    "review": "This paper proposes a novel extension of generative adversarial networks that replaces the traditional binary classifier discriminator with one that assigns a scalar energy to each point in the generator's output domain. The discriminator minimizes a hinge loss while the generator attempts to generate samples with low energy under the discriminator. The authors show that a Nash equilibrium under these conditions yields a generator that matches the data distribution (assuming infinite capacity). Experiments are conducted with the discriminator taking the form of an autoencoder, optionally including a regularizer that penalizes generated samples having a high cosine similarity to other samples in the minibatch.\n\nPros:\n* The paper is well-written.\n* The topic will be of interest to many because it sets the stage for the exploration of a wider variety of discriminators than currently used for training GANs.\n* The theorems regarding optimality of the Nash equilibrium appear to be correct.\n* Thorough exploration of hyperparameters in the MNIST experiments.\n* Semi-supervised results show that contrastive samples from the generator improve classification performance.\n\nCons:\n* The relationship to other works that broaden the scope of the discriminator (e.g. [1]) or use a generative network to provide contrastive samples to an energy-based model ([2]) is not made clear in the paper.\n* From visual inspection alone it is difficult to conclude whether EB-GANs produce better samples than DC-GANs on the LSUN and CelebA datasets.\n* It is difficult to assess the effect of the PT regularizer beyond visual inspection as the Inception score results are computed with the vanilla EB-GAN.\n\nSpecific Comments\n* Sec 2.3: It is unclear to me why a reconstruction loss will necessarily produce very different gradient directions.\n* Sec 2.4: It is confusing that \"pulling-away\" is abbreviated as \"PT\".\n* Sec 4.1: It seems strange that the Inception model (trained on natural images) is being used to compute KL scores for MNIST. Using an MNIST-trained CNN to compute Inception-style scores seems to be more appropriate here.\n* Figure 3: There is little variation across the histograms, so this figure is not very enlightening.\n* Appendix A: In the proof of theorem 2, it is unclear to me why a Nash equilibrium of the system exists.\n\nTypos / Minor Comments\n* Abstract: \"probabilistic GANs\" should probably be \"traditional\" or \"classical\" GANs.\n* Theorem 2: \"A Nash equilibrium ... exists\"\n* Sec 3: Should be \"Several papers were presented\"\n\nOverall, I have some concerns with the related work and experimental evaluation sections, but I feel the model is novel enough and is well-justified by the optimality proofs and the quality of the generated samples.\n\n[1] Springenberg, Jost Tobias. \"Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks.\" arXiv preprint arXiv:1511.06390 (2015).\n[2] Kim, Taesup, and Yoshua Bengio. \"Deep Directed Generative Models with Energy-Based Probability Estimation.\" arXiv preprint arXiv:1606.03439 (2016)."
  },
  {
    "people": [
      "Junbo",
      "Yuchen"
    ],
    "review": "Hi Junbo,\n\nThis is an interesting paper with appealing samples and thorough comparisons with normal GAN, but I am not sure what is the motivation behind using hinge loss. It seems to be an arbitrary choice since I don't see any explanation in the paper. Does the experimental and theoretical result still hold for other penalty functions?\n\nThanks,\nYuchen"
  },
  {
    "people": [
      "Kim",
      "Bengio",
      "Kim",
      "Bengio",
      "Rasmus"
    ],
    "review": "This paper introduces an energy-based Generative Adversarial Network (GAN) and provides theoretical and empirical results modeling a number of image datasets (including large-scale versions of categories of ImageNet). As far as I know energy-based GANs (EBGAN) were introduced in Kim and Bengio (2016), but the proposed version makes a number of different design choices. \n\nFirst, it does away with the entropy regularization term that Kim and Bengio (2016) introduced to ensure that the GAN discriminator converged to an energy function proportional to the log density of the data (at optimum). This implies that the discriminator in the proposed scheme will become uniform at convergence as discussed in the theoretical section of the paper, however the introductory text seems to imply otherwise -- that one could recover a meaningful score function from the trained energy-function (discriminator). This should be clarified. \n\nSecond, this version of the EBGAN setting includes two innovations: (1) the introduction of the hinge loss in the value function, and (2) the use of an auto-encoder parametrization for the energy function. These innovations are not empirically justified in any way - this is disappointing, as it would be really good to see empirical results supporting the arguments made in support of their introduction. \n\nThe two significant contributions of this paper are the theoretical analysis of the energy-baesd GAN formalism (showing that the optimum corresponds to a Nash equilibrium) and the impressive empirical results on large images that set a new standard in what straight GAN-style models can achieve.\n\nThe theoretical results seem solid to me and make a nice contribution.\n\nRegarding the quantitative results in Table 2, it seems not appropriate to bold the EBGAN line when it seems to be statistically indistinguishable form the Rasmus et al (2015) results. Though it is not mentioned here, the use of bold typically indicates the state-of-the-art. \n\nI think this paper could be be much stronger if the two novel contributions to the energy-based GAN setting were more thoroughly explored with ablation experiments. That being said, I think this paper has already become a contribution other are building on (including at least two other ICLR submissions) and so I think it should be accepted for publication at ICLR. \n"
  },
  {
    "people": [
      "Springenberg",
      "Jost Tobias",
      "Kim",
      "Taesup",
      "Yoshua Bengio"
    ],
    "review": "This paper proposes a novel extension of generative adversarial networks that replaces the traditional binary classifier discriminator with one that assigns a scalar energy to each point in the generator's output domain. The discriminator minimizes a hinge loss while the generator attempts to generate samples with low energy under the discriminator. The authors show that a Nash equilibrium under these conditions yields a generator that matches the data distribution (assuming infinite capacity). Experiments are conducted with the discriminator taking the form of an autoencoder, optionally including a regularizer that penalizes generated samples having a high cosine similarity to other samples in the minibatch.\n\nPros:\n* The paper is well-written.\n* The topic will be of interest to many because it sets the stage for the exploration of a wider variety of discriminators than currently used for training GANs.\n* The theorems regarding optimality of the Nash equilibrium appear to be correct.\n* Thorough exploration of hyperparameters in the MNIST experiments.\n* Semi-supervised results show that contrastive samples from the generator improve classification performance.\n\nCons:\n* The relationship to other works that broaden the scope of the discriminator (e.g. [1]) or use a generative network to provide contrastive samples to an energy-based model ([2]) is not made clear in the paper.\n* From visual inspection alone it is difficult to conclude whether EB-GANs produce better samples than DC-GANs on the LSUN and CelebA datasets.\n* It is difficult to assess the effect of the PT regularizer beyond visual inspection as the Inception score results are computed with the vanilla EB-GAN.\n\nSpecific Comments\n* Sec 2.3: It is unclear to me why a reconstruction loss will necessarily produce very different gradient directions.\n* Sec 2.4: It is confusing that \"pulling-away\" is abbreviated as \"PT\".\n* Sec 4.1: It seems strange that the Inception model (trained on natural images) is being used to compute KL scores for MNIST. Using an MNIST-trained CNN to compute Inception-style scores seems to be more appropriate here.\n* Figure 3: There is little variation across the histograms, so this figure is not very enlightening.\n* Appendix A: In the proof of theorem 2, it is unclear to me why a Nash equilibrium of the system exists.\n\nTypos / Minor Comments\n* Abstract: \"probabilistic GANs\" should probably be \"traditional\" or \"classical\" GANs.\n* Theorem 2: \"A Nash equilibrium ... exists\"\n* Sec 3: Should be \"Several papers were presented\"\n\nOverall, I have some concerns with the related work and experimental evaluation sections, but I feel the model is novel enough and is well-justified by the optimality proofs and the quality of the generated samples.\n\n[1] Springenberg, Jost Tobias. \"Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks.\" arXiv preprint arXiv:1511.06390 (2015).\n[2] Kim, Taesup, and Yoshua Bengio. \"Deep Directed Generative Models with Energy-Based Probability Estimation.\" arXiv preprint arXiv:1606.03439 (2016)."
  },
  {
    "people": [
      "Junbo",
      "Yuchen"
    ],
    "review": "Hi Junbo,\n\nThis is an interesting paper with appealing samples and thorough comparisons with normal GAN, but I am not sure what is the motivation behind using hinge loss. It seems to be an arbitrary choice since I don't see any explanation in the paper. Does the experimental and theoretical result still hold for other penalty functions?\n\nThanks,\nYuchen"
  },
  {
    "people": [
      "Swersky",
      "Snoek",
      "Adams"
    ],
    "review": "This paper discusses Hyperband, an extension of successive halving by Jamieson & Talwalkar (AISTATS 2016). Successive halving is a very nice algorithm that starts evaluating many configurations and repeatedly cuts off the current worst half to explore many configuration for a limited budget.\n\nHaving read the paper for the question period and just rereading it again, I am now not entirely sure what its contribution is meant to be: the only improvement of Hyperband vs. successive halving is in the theoretical worst case bounds (not more than 5x worse than random search), but you can (a) trivially obtain that bound by using a fifth of your time for running random configurations to completion and (b) the theoretical analysis to show this is said to be beyond the scope of the paper. That makes me wonder whether the theoretical results are the contribution of this paper, or whether they are the subject of a different paper and the current paper is mostly an empirical study of the method?\nI hope to get a response by the authors and see this made clearer in an updated version of the paper.\n\nIn terms of experiments, the paper fails to show a case where Hyperband actually performs better than the authors' previous algorithm successive halving with its most agressive setting of bracket b=4. Literally, in every figure, bracket b=4 is at least as good (and sometimes substantially better) than Hyperband. That makes me think that in practice I would prefer successive halving with b=4 over Hyperband. (And if I really want Hyperband's guarantee of not being more than 5x worse than random search I can run random search on a fifth of my machines.) \nThe experiments also compare to some Bayesian optimization methods, but not to the most relevant very closely related Multi-Task Bayesian Optimization methods that have been dominating effective methods for deep learning in that area in the last 3 years: \"Multi-Task Bayesian Optimization\" by Swersky, Snoek, and Adams (2013) already showed 5x speedups for deep learning by starting with smaller datasets, and there have been several follow-up papers showing even larger speedups. \n\nGiven that this prominent work on multitask Bayesian optimization exists, I also think the introduction, which sells Hyperband as a very new approach to hyperparameter optimization is misleading. I would've much preferred a more down-to-earth pitch that says \"configuration evaluation\" has been becoming a very important feature in hyperparameter optimization, including Bayesian optimization, that sometimes yields very large speedups (this can be quantified by examples from existing papers) and this paper adds some much-needed theoretical understanding to this and demonstrates how important configuration evaluation is even in the simplest case of being used with random search. I think this could be done easily and locally by adding a paragraph to the intro.\n\nAs another point regarding novelty, I think the authors should make clear that approaches for adaptively deciding how many resources to use for which evaluation have been studied for (at least) 23 years in the ML community -- see Maron & Moore, NIPS 1993: \"Hoeffding Races: Accelerating Model Selection Search for Classification and Function Approximation\" ("
  },
  {
    "people": [
      "Miguel Hern\u2021ndez Lobato",
      "Matthew Hoffman",
      "Nando de Freitas",
      "Ziyu Wang"
    ],
    "review": "This paper presents a simple strategy for hyperparameter optimization that gives strong empirical results. The reviewers all agreed that the paper should be accepted and that it would be interesting and useful to the ICLR community. However, they did have strong reservations about the claims made in the paper and one reviewer stated that their accept decision was conditional on a better treatment of the related literature. \n \n While it is natural for authors to argue for the advantages of their approach over existing methods, some of the claims made are unfounded. For example, the claim that the proposed method is guaranteed to converge while \"methods that rely on these heuristics are not endowed with any\n theoretical consistency guarantees\" is weak. Any optimization method can trivially add this guarantee by adopting a simple strategy of adding a random experiment 1/n of the time (in fact, SMAC does this I believe). This claim is true of random search compared to gradient based optimization on non-convex functions as well, yet no one optimizes their deep nets via random search. Also, the authors claim to compare to state-of-the-art in hyperparameter optimization but all the comparisons are to algorithms published in either 2011 or 2012. Four years of continued research on the subject are ignored (e.g. methods in Bayesian optimization for hyperparameter tuning have evolved considerably since 2012 - see e.g. the work of Miguel Hern\u2021ndez Lobato, Matthew Hoffman, Nando de Freitas, Ziyu Wang, etc.). It's understood that it is difficult to compare to the latest literature (and the authors state that they had trouble running recent algorithms), but one can't claim to compare to state-of-the-art without actually comparing to state-of-the-art.\n \n Please address the reviewers concerns and tone down the claims of the paper."
  },
  {
    "people": [
      "Domhan"
    ],
    "review": "We thank the reviewers for providing thoughtful comments and feedback for our paper.  Below please find our responses to these comments, grouped by theme. \n\n***Main Contribution***\n\nTo borrow the words of AnonReviewer1, the contribution of this paper is a \"very simple method [with] great empirical results for several deep learning tasks\u201d that is also endowed with theoretical guarantees. While we briefly allude to the theoretical properties of Hyperband in Section 3, a thorough theoretical treatment is beyond the scope of the paper (see the arXiv version for detailed theoretical analysis).  We have updated the introduction to clarify our contributions in this work.\n\n***Related Work***\n\nWe did not intend to suggest that optimizing configuration evaluation is a new idea and recognize that there is a long line of work in this area that stretches back many years.  To address AnonReviewer1\u2019s concerns, we moved the related work to section 2, expanded on existing configuration evaluation approaches, and added further discussion of existing approaches that combine configuration selection and evaluation.  Also, as per the suggestion of AnonReviewer2, we have added a reference to Bergstra & Bengio 2012 for the random search baseline.  \n\n***Parallelism***\n\nThe reviewers are correct that there are non-trivial design decisions in parallelizing Hyperband, and the updated version of the paper only talks about this topic as an interesting and important avenue of future research.\n\n***Worst Case Bound and Theoretical Properties***\n\n- \u201cIt's only [Hyperband\u2019s] worst-case analysis that makes no assumption [on the convergence behavior]\u201d\n\nHyperband is endowed with both worst-case and sample complexity guarantees, and neither of these results rely on any assumptions on the shape or rate of convergence of the validation error. In both cases, our only assumption is that the validation error eventually converges.\n\n- \u201conly improvement of Hyperband vs. successive halving is in the theoretical worst case bounds (not more than 5x worse than random search), but you can (a) trivially obtain that bound by using a fifth of your time for running random configurations to completion\u201d; \n\nHyperband crucially extends the previous SH work by addressing the n vs B/n problem that we discuss in Section 3. Indeed, our assumption-free complexity results for Hyperband match the oracle SH results up to a log factor (which we discuss further in the next paragraph).  We also note that our current work also introduces a novel variant of SH amenable to the finite horizon, where the maximum budget per configuration is limited.\n\nAs a further note regarding this log factor, Hyperband\u2019s guarantee actually shows that it is no worse than 5x the best SH bracket, in hindsight. If the best bracket is 50x faster than uniform allocation with random search, then overall, Hyperband is at least 10x faster than uniform allocation. But we agree that the fall-back guarantee is somewhat pessimistic. Ideally, instead of looping over the different kinds of brackets, one would treat each bracket as a separate \u201cmeta-arm\u201d of an outerloop bandit algorithm so that we could claim something like \u201cthe sub-optimal brackets are only played a finite number of times.\u201d However, this remains as future work.\n\n***Empirical Studies***\n\n- \u201cFigure 3 run out far enough that the other methods have had time to converge in order to see what this gap between optimal and near-optimal really is\u201d; \u201cif random2x is shown, then I would also like to see SMAC2x, Spearmint2x, TPE2x...it would be worth seeing 3x, 10x, and so forth.\u201d\n\nThe experiments in Section 4 are resource intensive and we chose our comparison set to be as informative as possible given the high cost of running these experiments (the total cost was over 10k in EC2 credits and the CNN experiments took over 10k GPU hours).  That said, we agree that extending the results would be interesting.  To this end, we are currently running experiments to extend the chart for CIFAR-10 trained using CNN to twice the current x-axis range to bring all competitors closer to convergence.  We will update the paper once the additional trials are complete.\n\n- \u201cThe experiments also compare to some Bayesian optimization methods, but not to the most relevant very closely related Multi-Task Bayesian Optimization\u201d\n\nWe did not compare to Multi-Task Bayesian Optimization for two main reasons: (1) the method mainly addresses the problem of transfer learning/meta-learning; and (2) in its simplest form, it may suffer from the \u201cn vs B/n\u201d problem since trying several subset sizes in conjunction exponentially increases the number of MTBO\u2019s own hyperparameters. Instead, we opted to use the early stopping method presented in Domhan et. al. 2015 to get an idea of how combined configuration selection and configuration evaluation approaches would perform. We have however added a citation for MTBO in our related work.\n\n- \u201cI am looking forward to seeing the details on these [CVST] experiments\u201d\n\nWe have added a comparison CVST to the updated paper (see Section 2 and also Appendix A.1).  \n\n- \u201cbracket b=4 is at least as good (and sometimes substantially better) than Hyperband.\u201d \n\nFor the deep learning and kernel experiments studied in Section 4, bracket s=4 does indeed perform very well and one could conceivably just run SH with that particular point in the n vs B/n tradeoff. However, in the LeNet experiment discussed in Section 3, Figure 2 shows that bracket s=3 performed the best. In practice, if meta-data or previous experience suggests that a certain n vs B/n tradeoff is likely to work well in practice, one can exploit this information and simply run SH with that tradeoff. However, oftentimes, this a priori knowledge is not available, thus motivating the use of Hyperband. Indeed, our empirical results demonstrate the efficacy of Hyperband in this standard setting where the choice of n vs B/n is unknown."
  },
  {
    "people": [
      "Swersky",
      "Snoek",
      "Adams"
    ],
    "review": "This paper discusses Hyperband, an extension of successive halving by Jamieson & Talwalkar (AISTATS 2016). Successive halving is a very nice algorithm that starts evaluating many configurations and repeatedly cuts off the current worst half to explore many configuration for a limited budget.\n\nHaving read the paper for the question period and just rereading it again, I am now not entirely sure what its contribution is meant to be: the only improvement of Hyperband vs. successive halving is in the theoretical worst case bounds (not more than 5x worse than random search), but you can (a) trivially obtain that bound by using a fifth of your time for running random configurations to completion and (b) the theoretical analysis to show this is said to be beyond the scope of the paper. That makes me wonder whether the theoretical results are the contribution of this paper, or whether they are the subject of a different paper and the current paper is mostly an empirical study of the method?\nI hope to get a response by the authors and see this made clearer in an updated version of the paper.\n\nIn terms of experiments, the paper fails to show a case where Hyperband actually performs better than the authors' previous algorithm successive halving with its most agressive setting of bracket b=4. Literally, in every figure, bracket b=4 is at least as good (and sometimes substantially better) than Hyperband. That makes me think that in practice I would prefer successive halving with b=4 over Hyperband. (And if I really want Hyperband's guarantee of not being more than 5x worse than random search I can run random search on a fifth of my machines.) \nThe experiments also compare to some Bayesian optimization methods, but not to the most relevant very closely related Multi-Task Bayesian Optimization methods that have been dominating effective methods for deep learning in that area in the last 3 years: \"Multi-Task Bayesian Optimization\" by Swersky, Snoek, and Adams (2013) already showed 5x speedups for deep learning by starting with smaller datasets, and there have been several follow-up papers showing even larger speedups. \n\nGiven that this prominent work on multitask Bayesian optimization exists, I also think the introduction, which sells Hyperband as a very new approach to hyperparameter optimization is misleading. I would've much preferred a more down-to-earth pitch that says \"configuration evaluation\" has been becoming a very important feature in hyperparameter optimization, including Bayesian optimization, that sometimes yields very large speedups (this can be quantified by examples from existing papers) and this paper adds some much-needed theoretical understanding to this and demonstrates how important configuration evaluation is even in the simplest case of being used with random search. I think this could be done easily and locally by adding a paragraph to the intro.\n\nAs another point regarding novelty, I think the authors should make clear that approaches for adaptively deciding how many resources to use for which evaluation have been studied for (at least) 23 years in the ML community -- see Maron & Moore, NIPS 1993: \"Hoeffding Races: Accelerating Model Selection Search for Classification and Function Approximation\" ("
  },
  {
    "people": [
      "Jamieson",
      "Talwalkar"
    ],
    "review": "This paper presents Hyperband, a method for hyperparameter optimization where the model is trained by gradient descent or some other iterative scheme. The paper builds on the successive halving + random search approach of Jamieson and Talwalkar and addresses the tradeoff between training fewer models for a longer amount of time, or many models for a shorter amount of time. Effectively, the idea is to perform multiple rounds of successive halving, starting from the most exploratory setting, and then in each round exponentially decreasing the number of experiments, but granting them exponentially more resources. In contrast to other recent papers on this topic, the approach here does not rely on any specific model of the underlying learning curves and therefore makes fewer assumptions about the nature of the model. The results seem to show that this approach can be highly effective, often providing several factors of speedup over sequential approaches.\n\nOverall I think this paper is a good contribution to the hyperparameter optimization literature. It\u2019s relatively simple to implement, and seems to be quite effective for many problems. It seems like a natural extension of the random search methodology to the case of early stopping. To me, it seems like Hyperband would be most useful on problems where a) random search itself is expected to perform well and b) the computational budget is sufficiently constrained so that squeezing out the absolute best performance is not feasible and near-optimal performance is sufficient. I would personally like to see the plots in Figure 3 run out far enough that the other methods have had time to converge in order to see what this gap between optimal and near-optimal really is (if there is one).\n\nI\u2019m not sure I agree with the use of random2x as a baseline. I can see why it\u2019s a useful comparison because it demonstrates the benefit of parallelism over sequential methods, but virtually all of these other methods also have parallel extensions. I think if random2x is shown, then I would also like to see SMAC2x, Spearmint2x, TPE2x, etc. I also think it would be worth seeing 3x, 10x, and so forth and how Hyperband fares against these baselines.\n"
  },
  {
    "people": [
      "Swersky",
      "Snoek",
      "Adams"
    ],
    "review": "This paper discusses Hyperband, an extension of successive halving by Jamieson & Talwalkar (AISTATS 2016). Successive halving is a very nice algorithm that starts evaluating many configurations and repeatedly cuts off the current worst half to explore many configuration for a limited budget.\n\nHaving read the paper for the question period and just rereading it again, I am now not entirely sure what its contribution is meant to be: the only improvement of Hyperband vs. successive halving is in the theoretical worst case bounds (not more than 5x worse than random search), but you can (a) trivially obtain that bound by using a fifth of your time for running random configurations to completion and (b) the theoretical analysis to show this is said to be beyond the scope of the paper. That makes me wonder whether the theoretical results are the contribution of this paper, or whether they are the subject of a different paper and the current paper is mostly an empirical study of the method?\nI hope to get a response by the authors and see this made clearer in an updated version of the paper.\n\nIn terms of experiments, the paper fails to show a case where Hyperband actually performs better than the authors' previous algorithm successive halving with its most agressive setting of bracket b=4. Literally, in every figure, bracket b=4 is at least as good (and sometimes substantially better) than Hyperband. That makes me think that in practice I would prefer successive halving with b=4 over Hyperband. (And if I really want Hyperband's guarantee of not being more than 5x worse than random search I can run random search on a fifth of my machines.) \nThe experiments also compare to some Bayesian optimization methods, but not to the most relevant very closely related Multi-Task Bayesian Optimization methods that have been dominating effective methods for deep learning in that area in the last 3 years: \"Multi-Task Bayesian Optimization\" by Swersky, Snoek, and Adams (2013) already showed 5x speedups for deep learning by starting with smaller datasets, and there have been several follow-up papers showing even larger speedups. \n\nGiven that this prominent work on multitask Bayesian optimization exists, I also think the introduction, which sells Hyperband as a very new approach to hyperparameter optimization is misleading. I would've much preferred a more down-to-earth pitch that says \"configuration evaluation\" has been becoming a very important feature in hyperparameter optimization, including Bayesian optimization, that sometimes yields very large speedups (this can be quantified by examples from existing papers) and this paper adds some much-needed theoretical understanding to this and demonstrates how important configuration evaluation is even in the simplest case of being used with random search. I think this could be done easily and locally by adding a paragraph to the intro.\n\nAs another point regarding novelty, I think the authors should make clear that approaches for adaptively deciding how many resources to use for which evaluation have been studied for (at least) 23 years in the ML community -- see Maron & Moore, NIPS 1993: \"Hoeffding Races: Accelerating Model Selection Search for Classification and Function Approximation\" ("
  },
  {
    "people": [
      "Miguel Hern\u2021ndez Lobato",
      "Matthew Hoffman",
      "Nando de Freitas",
      "Ziyu Wang"
    ],
    "review": "This paper presents a simple strategy for hyperparameter optimization that gives strong empirical results. The reviewers all agreed that the paper should be accepted and that it would be interesting and useful to the ICLR community. However, they did have strong reservations about the claims made in the paper and one reviewer stated that their accept decision was conditional on a better treatment of the related literature. \n \n While it is natural for authors to argue for the advantages of their approach over existing methods, some of the claims made are unfounded. For example, the claim that the proposed method is guaranteed to converge while \"methods that rely on these heuristics are not endowed with any\n theoretical consistency guarantees\" is weak. Any optimization method can trivially add this guarantee by adopting a simple strategy of adding a random experiment 1/n of the time (in fact, SMAC does this I believe). This claim is true of random search compared to gradient based optimization on non-convex functions as well, yet no one optimizes their deep nets via random search. Also, the authors claim to compare to state-of-the-art in hyperparameter optimization but all the comparisons are to algorithms published in either 2011 or 2012. Four years of continued research on the subject are ignored (e.g. methods in Bayesian optimization for hyperparameter tuning have evolved considerably since 2012 - see e.g. the work of Miguel Hern\u2021ndez Lobato, Matthew Hoffman, Nando de Freitas, Ziyu Wang, etc.). It's understood that it is difficult to compare to the latest literature (and the authors state that they had trouble running recent algorithms), but one can't claim to compare to state-of-the-art without actually comparing to state-of-the-art.\n \n Please address the reviewers concerns and tone down the claims of the paper."
  },
  {
    "people": [
      "Domhan"
    ],
    "review": "We thank the reviewers for providing thoughtful comments and feedback for our paper.  Below please find our responses to these comments, grouped by theme. \n\n***Main Contribution***\n\nTo borrow the words of AnonReviewer1, the contribution of this paper is a \"very simple method [with] great empirical results for several deep learning tasks\u201d that is also endowed with theoretical guarantees. While we briefly allude to the theoretical properties of Hyperband in Section 3, a thorough theoretical treatment is beyond the scope of the paper (see the arXiv version for detailed theoretical analysis).  We have updated the introduction to clarify our contributions in this work.\n\n***Related Work***\n\nWe did not intend to suggest that optimizing configuration evaluation is a new idea and recognize that there is a long line of work in this area that stretches back many years.  To address AnonReviewer1\u2019s concerns, we moved the related work to section 2, expanded on existing configuration evaluation approaches, and added further discussion of existing approaches that combine configuration selection and evaluation.  Also, as per the suggestion of AnonReviewer2, we have added a reference to Bergstra & Bengio 2012 for the random search baseline.  \n\n***Parallelism***\n\nThe reviewers are correct that there are non-trivial design decisions in parallelizing Hyperband, and the updated version of the paper only talks about this topic as an interesting and important avenue of future research.\n\n***Worst Case Bound and Theoretical Properties***\n\n- \u201cIt's only [Hyperband\u2019s] worst-case analysis that makes no assumption [on the convergence behavior]\u201d\n\nHyperband is endowed with both worst-case and sample complexity guarantees, and neither of these results rely on any assumptions on the shape or rate of convergence of the validation error. In both cases, our only assumption is that the validation error eventually converges.\n\n- \u201conly improvement of Hyperband vs. successive halving is in the theoretical worst case bounds (not more than 5x worse than random search), but you can (a) trivially obtain that bound by using a fifth of your time for running random configurations to completion\u201d; \n\nHyperband crucially extends the previous SH work by addressing the n vs B/n problem that we discuss in Section 3. Indeed, our assumption-free complexity results for Hyperband match the oracle SH results up to a log factor (which we discuss further in the next paragraph).  We also note that our current work also introduces a novel variant of SH amenable to the finite horizon, where the maximum budget per configuration is limited.\n\nAs a further note regarding this log factor, Hyperband\u2019s guarantee actually shows that it is no worse than 5x the best SH bracket, in hindsight. If the best bracket is 50x faster than uniform allocation with random search, then overall, Hyperband is at least 10x faster than uniform allocation. But we agree that the fall-back guarantee is somewhat pessimistic. Ideally, instead of looping over the different kinds of brackets, one would treat each bracket as a separate \u201cmeta-arm\u201d of an outerloop bandit algorithm so that we could claim something like \u201cthe sub-optimal brackets are only played a finite number of times.\u201d However, this remains as future work.\n\n***Empirical Studies***\n\n- \u201cFigure 3 run out far enough that the other methods have had time to converge in order to see what this gap between optimal and near-optimal really is\u201d; \u201cif random2x is shown, then I would also like to see SMAC2x, Spearmint2x, TPE2x...it would be worth seeing 3x, 10x, and so forth.\u201d\n\nThe experiments in Section 4 are resource intensive and we chose our comparison set to be as informative as possible given the high cost of running these experiments (the total cost was over 10k in EC2 credits and the CNN experiments took over 10k GPU hours).  That said, we agree that extending the results would be interesting.  To this end, we are currently running experiments to extend the chart for CIFAR-10 trained using CNN to twice the current x-axis range to bring all competitors closer to convergence.  We will update the paper once the additional trials are complete.\n\n- \u201cThe experiments also compare to some Bayesian optimization methods, but not to the most relevant very closely related Multi-Task Bayesian Optimization\u201d\n\nWe did not compare to Multi-Task Bayesian Optimization for two main reasons: (1) the method mainly addresses the problem of transfer learning/meta-learning; and (2) in its simplest form, it may suffer from the \u201cn vs B/n\u201d problem since trying several subset sizes in conjunction exponentially increases the number of MTBO\u2019s own hyperparameters. Instead, we opted to use the early stopping method presented in Domhan et. al. 2015 to get an idea of how combined configuration selection and configuration evaluation approaches would perform. We have however added a citation for MTBO in our related work.\n\n- \u201cI am looking forward to seeing the details on these [CVST] experiments\u201d\n\nWe have added a comparison CVST to the updated paper (see Section 2 and also Appendix A.1).  \n\n- \u201cbracket b=4 is at least as good (and sometimes substantially better) than Hyperband.\u201d \n\nFor the deep learning and kernel experiments studied in Section 4, bracket s=4 does indeed perform very well and one could conceivably just run SH with that particular point in the n vs B/n tradeoff. However, in the LeNet experiment discussed in Section 3, Figure 2 shows that bracket s=3 performed the best. In practice, if meta-data or previous experience suggests that a certain n vs B/n tradeoff is likely to work well in practice, one can exploit this information and simply run SH with that tradeoff. However, oftentimes, this a priori knowledge is not available, thus motivating the use of Hyperband. Indeed, our empirical results demonstrate the efficacy of Hyperband in this standard setting where the choice of n vs B/n is unknown."
  },
  {
    "people": [
      "Swersky",
      "Snoek",
      "Adams"
    ],
    "review": "This paper discusses Hyperband, an extension of successive halving by Jamieson & Talwalkar (AISTATS 2016). Successive halving is a very nice algorithm that starts evaluating many configurations and repeatedly cuts off the current worst half to explore many configuration for a limited budget.\n\nHaving read the paper for the question period and just rereading it again, I am now not entirely sure what its contribution is meant to be: the only improvement of Hyperband vs. successive halving is in the theoretical worst case bounds (not more than 5x worse than random search), but you can (a) trivially obtain that bound by using a fifth of your time for running random configurations to completion and (b) the theoretical analysis to show this is said to be beyond the scope of the paper. That makes me wonder whether the theoretical results are the contribution of this paper, or whether they are the subject of a different paper and the current paper is mostly an empirical study of the method?\nI hope to get a response by the authors and see this made clearer in an updated version of the paper.\n\nIn terms of experiments, the paper fails to show a case where Hyperband actually performs better than the authors' previous algorithm successive halving with its most agressive setting of bracket b=4. Literally, in every figure, bracket b=4 is at least as good (and sometimes substantially better) than Hyperband. That makes me think that in practice I would prefer successive halving with b=4 over Hyperband. (And if I really want Hyperband's guarantee of not being more than 5x worse than random search I can run random search on a fifth of my machines.) \nThe experiments also compare to some Bayesian optimization methods, but not to the most relevant very closely related Multi-Task Bayesian Optimization methods that have been dominating effective methods for deep learning in that area in the last 3 years: \"Multi-Task Bayesian Optimization\" by Swersky, Snoek, and Adams (2013) already showed 5x speedups for deep learning by starting with smaller datasets, and there have been several follow-up papers showing even larger speedups. \n\nGiven that this prominent work on multitask Bayesian optimization exists, I also think the introduction, which sells Hyperband as a very new approach to hyperparameter optimization is misleading. I would've much preferred a more down-to-earth pitch that says \"configuration evaluation\" has been becoming a very important feature in hyperparameter optimization, including Bayesian optimization, that sometimes yields very large speedups (this can be quantified by examples from existing papers) and this paper adds some much-needed theoretical understanding to this and demonstrates how important configuration evaluation is even in the simplest case of being used with random search. I think this could be done easily and locally by adding a paragraph to the intro.\n\nAs another point regarding novelty, I think the authors should make clear that approaches for adaptively deciding how many resources to use for which evaluation have been studied for (at least) 23 years in the ML community -- see Maron & Moore, NIPS 1993: \"Hoeffding Races: Accelerating Model Selection Search for Classification and Function Approximation\" ("
  },
  {
    "people": [
      "Jamieson",
      "Talwalkar"
    ],
    "review": "This paper presents Hyperband, a method for hyperparameter optimization where the model is trained by gradient descent or some other iterative scheme. The paper builds on the successive halving + random search approach of Jamieson and Talwalkar and addresses the tradeoff between training fewer models for a longer amount of time, or many models for a shorter amount of time. Effectively, the idea is to perform multiple rounds of successive halving, starting from the most exploratory setting, and then in each round exponentially decreasing the number of experiments, but granting them exponentially more resources. In contrast to other recent papers on this topic, the approach here does not rely on any specific model of the underlying learning curves and therefore makes fewer assumptions about the nature of the model. The results seem to show that this approach can be highly effective, often providing several factors of speedup over sequential approaches.\n\nOverall I think this paper is a good contribution to the hyperparameter optimization literature. It\u2019s relatively simple to implement, and seems to be quite effective for many problems. It seems like a natural extension of the random search methodology to the case of early stopping. To me, it seems like Hyperband would be most useful on problems where a) random search itself is expected to perform well and b) the computational budget is sufficiently constrained so that squeezing out the absolute best performance is not feasible and near-optimal performance is sufficient. I would personally like to see the plots in Figure 3 run out far enough that the other methods have had time to converge in order to see what this gap between optimal and near-optimal really is (if there is one).\n\nI\u2019m not sure I agree with the use of random2x as a baseline. I can see why it\u2019s a useful comparison because it demonstrates the benefit of parallelism over sequential methods, but virtually all of these other methods also have parallel extensions. I think if random2x is shown, then I would also like to see SMAC2x, Spearmint2x, TPE2x, etc. I also think it would be worth seeing 3x, 10x, and so forth and how Hyperband fares against these baselines.\n"
  },
  {
    "people": [
      "Yarin Gal\u2019s"
    ],
    "review": "The authors propose a conceptually simple method for regularisation of recurrent neural networks. The idea is related to dropout, but instead of zeroing out units, they are instead set to their respective values at the preceding time step element-wise with a certain probability.\n\nOverall, the paper is well written. The method is clearly represented up to issues raised by reviewers during the pre-review question phase. The related work is complete and probably the best currently available on the matter of regularising RNNs.\n\nThe experimental section focuses on comparing the method with the current SOTA on a set of NLP benchmarks and a synthetic problem. All of the experiments focus on sequences over discrete values. An additional experiment also shows that the sequential Jacobian is far higher for long-term dependencies than in the dropout case.\n\nOverall, the paper bears great potential. However, I do see some points.\n\n1) As raised during the pre-review questions, I would like to see the results of experiments that feature a complete hyper parameter search. I.e. a proper model selection process,as it should be standard in the community. I do not see why this was not done, especially as the author count seems to indicate that the necessary resources are available.\n\nI want to repeat at this point that Table 2 of the paper shows that validation error is not a reliable estimator for testing error in the respective data set. Thus, overfitting the model selection process is a serious concern here.\nZoneout does not seem to improve that much in the other tasks.\n\n2) Zoneout is not investigated well mathematically. E.g. an analysis of the of the form of gradients from unit K at time step T to unit K\u2019 at time step T-R would have been interesting, especially as these are not necessarily non-zero for dropout. Also, the question whether zoneout has a variational interpretation in the spirit of Yarin Gal\u2019s work is an obvious one. I can see that it is if we treat zoneout in a resnet framework and dropout on the incremental parts. Overall, little effort is done answering the question *why* zoneout works well, even though the literature bears plenty of starting points for such analysis.\n\n3) The data sets used are only symbolic. It would have been great if more ground was covered, i.e. continuous data such as from dynamical systems. To me it is not obvious whether it will transfer right away.\n\n\nAn extreme amount of \u201ctricks\u201d is being published currently for improved RNN training. How does zoneout stand out? It is a nice idea, and simple to implement. However, the paper under delivers: the experiments do not convince me (see 1) and 3)). There authors do not provide convincing theoretical insights either. (2)\n\nConsequently, the paper reduces to a \u201cepsilon improvement, great text, mediocre experimental evaluation, little theoretical insight\u201d.\n"
  },
  {
    "people": [
      "Yarin Gal\u2019s"
    ],
    "review": "The authors propose a conceptually simple method for regularisation of recurrent neural networks. The idea is related to dropout, but instead of zeroing out units, they are instead set to their respective values at the preceding time step element-wise with a certain probability.\n\nOverall, the paper is well written. The method is clearly represented up to issues raised by reviewers during the pre-review question phase. The related work is complete and probably the best currently available on the matter of regularising RNNs.\n\nThe experimental section focuses on comparing the method with the current SOTA on a set of NLP benchmarks and a synthetic problem. All of the experiments focus on sequences over discrete values. An additional experiment also shows that the sequential Jacobian is far higher for long-term dependencies than in the dropout case.\n\nOverall, the paper bears great potential. However, I do see some points.\n\n1) As raised during the pre-review questions, I would like to see the results of experiments that feature a complete hyper parameter search. I.e. a proper model selection process,as it should be standard in the community. I do not see why this was not done, especially as the author count seems to indicate that the necessary resources are available.\n\nI want to repeat at this point that Table 2 of the paper shows that validation error is not a reliable estimator for testing error in the respective data set. Thus, overfitting the model selection process is a serious concern here.\nZoneout does not seem to improve that much in the other tasks.\n\n2) Zoneout is not investigated well mathematically. E.g. an analysis of the of the form of gradients from unit K at time step T to unit K\u2019 at time step T-R would have been interesting, especially as these are not necessarily non-zero for dropout. Also, the question whether zoneout has a variational interpretation in the spirit of Yarin Gal\u2019s work is an obvious one. I can see that it is if we treat zoneout in a resnet framework and dropout on the incremental parts. Overall, little effort is done answering the question *why* zoneout works well, even though the literature bears plenty of starting points for such analysis.\n\n3) The data sets used are only symbolic. It would have been great if more ground was covered, i.e. continuous data such as from dynamical systems. To me it is not obvious whether it will transfer right away.\n\n\nAn extreme amount of \u201ctricks\u201d is being published currently for improved RNN training. How does zoneout stand out? It is a nice idea, and simple to implement. However, the paper under delivers: the experiments do not convince me (see 1) and 3)). There authors do not provide convincing theoretical insights either. (2)\n\nConsequently, the paper reduces to a \u201cepsilon improvement, great text, mediocre experimental evaluation, little theoretical insight\u201d.\n"
  },
  {
    "people": [
      "Waldspurger"
    ],
    "review": "I find the general direction of the work is promising but, in my opinion, the paper has three main drawback. While the motivation and overall idea seem very reasonable, the derivation is not convincing mathematically. The experiments are limited and the presentation needs significant improvement.  The writing and wording are in general poorly structured to the point that it is sometimes difficult to follow the proposed ideas. The overall organization needs improvement and the connection between sections is not properly established. The paper could be significantly improved by simply re-writing it.\n\nI'm not fully convinced by the motivation for the proposed non-linearity (|c|^2), as described on page 5. The authors argue that  (Waldspurger, 2016) suggests that higher order nonlinearities might be  beneficial for sparsity. But unless I'm missing something, that work seems  to suggest that in the general case higher order nonlinearities can be neglected. Could you please comment on this?\n\nOn the other hand, adding a second order term to the descriptor seems\nan interesting direction, as long as stability to small variations is preserved (which should be shown experimentally)\n\nThe experimental section is rather limited. The paper would be stronger with a thorough numerical evaluation. The presented results, in my opinion, do not show convincingly a clear advantage of the proposed method over a standard implementation of the scattering transform. In order to show the merits of the proposed approach, it would be really helpful to directly compare running times and compression rates.\n\nQuestions: \n- Can you show empirically that the proposed higher order nonlinearity\nproduces sparser representations than the complex modulus?\n\nOther minor issues:\n- The proof of Section 2.1, should be preceded by a clear statement in the form of a proposition\n- \"Hadamart\" -> Hadamard\n- \"Valid set\" -> Validation set\n- \"nonzeros coefficients\" -> nonzero coefficients\n- Figure 3 is difficult to understand. Please provide more details.\n- Figure 5 is supposed to show a comparison to a standard implementation of the Scattering network, but it doesn't seem to be such comparison in that figure. Please explain.\n- Please verify the references. The first reference states \"MALLAT\".\n"
  },
  {
    "people": [
      "Waldspurger"
    ],
    "review": "I find the general direction of the work is promising but, in my opinion, the paper has three main drawback. While the motivation and overall idea seem very reasonable, the derivation is not convincing mathematically. The experiments are limited and the presentation needs significant improvement.  The writing and wording are in general poorly structured to the point that it is sometimes difficult to follow the proposed ideas. The overall organization needs improvement and the connection between sections is not properly established. The paper could be significantly improved by simply re-writing it.\n\nI'm not fully convinced by the motivation for the proposed non-linearity (|c|^2), as described on page 5. The authors argue that  (Waldspurger, 2016) suggests that higher order nonlinearities might be  beneficial for sparsity. But unless I'm missing something, that work seems  to suggest that in the general case higher order nonlinearities can be neglected. Could you please comment on this?\n\nOn the other hand, adding a second order term to the descriptor seems\nan interesting direction, as long as stability to small variations is preserved (which should be shown experimentally)\n\nThe experimental section is rather limited. The paper would be stronger with a thorough numerical evaluation. The presented results, in my opinion, do not show convincingly a clear advantage of the proposed method over a standard implementation of the scattering transform. In order to show the merits of the proposed approach, it would be really helpful to directly compare running times and compression rates.\n\nQuestions: \n- Can you show empirically that the proposed higher order nonlinearity\nproduces sparser representations than the complex modulus?\n\nOther minor issues:\n- The proof of Section 2.1, should be preceded by a clear statement in the form of a proposition\n- \"Hadamart\" -> Hadamard\n- \"Valid set\" -> Validation set\n- \"nonzeros coefficients\" -> nonzero coefficients\n- Figure 3 is difficult to understand. Please provide more details.\n- Figure 5 is supposed to show a comparison to a standard implementation of the Scattering network, but it doesn't seem to be such comparison in that figure. Please explain.\n- Please verify the references. The first reference states \"MALLAT\".\n"
  },
  {
    "people": [
      "Langevin"
    ],
    "review": "The authors propose to add noise to the gradients computed while optimizing deep neural networks with stochastic gradient based methods. They show results multiple data sets which indicate that the method can counteract bad parameter initialization and that it can be especially beneficial for training more complicated architectures.\n\nThe method is tested on a multitude of different tasks and architectures. The results would be more convincing if they would be accompanied by confidence intervals but I understand that some of the experiments must have taken very long to run. I like that the results include both situations in which the gradient noise helps a lot and situations in which it doesn\u2019t seem to add much to the other optimization or initialization tools employed. The quantity of the experiments and the variety of the models provide quite convincing evidence that the effect of the gradient noise generalizes to many settings. The results were not always that convincing. In Section 4.2, the method only helped significantly when a sub-optimal training scheme was used, for example. The results on MNIST are not very good compared to the state-of-the-art. Since the method is so simple, I was hoping to see more theoretical arguments for its usefulness. That said, the experimental investigations into the importance of the annealing procedure, the comparison with the effect of gradient stochasticity and the comparison with weight noise, provide some additional insight.\n\nThe paper is well written and cites relevant prior work. The proposed method is described clearly and concisely, which is to be expected given its simplicity. \n\nThe proposed idea is not very original. As the authors acknowledge, very similar algorithms have been used for training and it is pretty much identical to simulating Langevin dynamics but with the goal of finding a single optimum in mind rather than approximating an expected value. The work is the evaluation of an old tool in a new era where models have become bigger and more complex.\n\nDespite the lack of novelty of the method, I do think that the results are valuable. The method is so easy to implement and seems to be so useful for complicated model which are hard to initialize, that it is important for others in the field to know about it. I suspect many people will at least try the method. The variety of the architectures and tasks for which the method was useful suggests that many people may also add it to their repertoire of optimization tricks. \n\n\nPros:\n* The idea is easy to implement.\n* The method is evaluated on a variety of tasks and for very different models.\n* Some interesting experiments which compare the method with similar approaches and investigate the importance of the annealing scheme.\n* The paper is well-written.\n\n\nCons:\n* The idea is not very original.\n* There is no clear theoretical motivation of analysis.\n* Not all the results are convincing."
  },
  {
    "people": [
      "Laurent",
      "Cooijmans"
    ],
    "review": "This paper presents a simple method of adding gradient noise to improve the training of deep neural networks. This paper first appeared on arXiv over a year ago and while there have been many innovations in the area of improving the training of deep neural networks in tha time (batch normalization for RNNs, layer normalization, normalization propagation, etc.) this paper does not mention or compare to these methods. \n\nIn particular, the authors state \"However, recent work on applying batch normalization to recurrent networks (Laurent et al., 2015) has not shown promise in improving generalization ability for recur- rent architectures, which are the focus of this work.\" This statement is simply incorrect and was thoroughly explored in, e.g. Cooijmans et al. (2016) that establish that batch normalization is effective for RNNs.\n\nThe proposed method itself is extremely simple and is similar to numerous training strategies that have previously been advocated in the literature. As a result the contribution would be incremental at best and could be significant with sufficiently strong empirical results supporting this particular variant. However, as discussed above there are now multiple training strategies and algorithms in the literature that are not empirically compared.\n\nUnfortunately, this paper is now fairly seriously out of date. It would not be appropriate to publish this at ICLR 2017. "
  },
  {
    "people": [
      "Langevin"
    ],
    "review": "The authors propose to add noise to the gradients computed while optimizing deep neural networks with stochastic gradient based methods. They show results multiple data sets which indicate that the method can counteract bad parameter initialization and that it can be especially beneficial for training more complicated architectures.\n\nThe method is tested on a multitude of different tasks and architectures. The results would be more convincing if they would be accompanied by confidence intervals but I understand that some of the experiments must have taken very long to run. I like that the results include both situations in which the gradient noise helps a lot and situations in which it doesn\u2019t seem to add much to the other optimization or initialization tools employed. The quantity of the experiments and the variety of the models provide quite convincing evidence that the effect of the gradient noise generalizes to many settings. The results were not always that convincing. In Section 4.2, the method only helped significantly when a sub-optimal training scheme was used, for example. The results on MNIST are not very good compared to the state-of-the-art. Since the method is so simple, I was hoping to see more theoretical arguments for its usefulness. That said, the experimental investigations into the importance of the annealing procedure, the comparison with the effect of gradient stochasticity and the comparison with weight noise, provide some additional insight.\n\nThe paper is well written and cites relevant prior work. The proposed method is described clearly and concisely, which is to be expected given its simplicity. \n\nThe proposed idea is not very original. As the authors acknowledge, very similar algorithms have been used for training and it is pretty much identical to simulating Langevin dynamics but with the goal of finding a single optimum in mind rather than approximating an expected value. The work is the evaluation of an old tool in a new era where models have become bigger and more complex.\n\nDespite the lack of novelty of the method, I do think that the results are valuable. The method is so easy to implement and seems to be so useful for complicated model which are hard to initialize, that it is important for others in the field to know about it. I suspect many people will at least try the method. The variety of the architectures and tasks for which the method was useful suggests that many people may also add it to their repertoire of optimization tricks. \n\n\nPros:\n* The idea is easy to implement.\n* The method is evaluated on a variety of tasks and for very different models.\n* Some interesting experiments which compare the method with similar approaches and investigate the importance of the annealing scheme.\n* The paper is well-written.\n\n\nCons:\n* The idea is not very original.\n* There is no clear theoretical motivation of analysis.\n* Not all the results are convincing.\n"
  },
  {
    "people": [
      "Langevin"
    ],
    "review": "The authors propose to add noise to the gradients computed while optimizing deep neural networks with stochastic gradient based methods. They show results multiple data sets which indicate that the method can counteract bad parameter initialization and that it can be especially beneficial for training more complicated architectures.\n\nThe method is tested on a multitude of different tasks and architectures. The results would be more convincing if they would be accompanied by confidence intervals but I understand that some of the experiments must have taken very long to run. I like that the results include both situations in which the gradient noise helps a lot and situations in which it doesn\u2019t seem to add much to the other optimization or initialization tools employed. The quantity of the experiments and the variety of the models provide quite convincing evidence that the effect of the gradient noise generalizes to many settings. The results were not always that convincing. In Section 4.2, the method only helped significantly when a sub-optimal training scheme was used, for example. The results on MNIST are not very good compared to the state-of-the-art. Since the method is so simple, I was hoping to see more theoretical arguments for its usefulness. That said, the experimental investigations into the importance of the annealing procedure, the comparison with the effect of gradient stochasticity and the comparison with weight noise, provide some additional insight.\n\nThe paper is well written and cites relevant prior work. The proposed method is described clearly and concisely, which is to be expected given its simplicity. \n\nThe proposed idea is not very original. As the authors acknowledge, very similar algorithms have been used for training and it is pretty much identical to simulating Langevin dynamics but with the goal of finding a single optimum in mind rather than approximating an expected value. The work is the evaluation of an old tool in a new era where models have become bigger and more complex.\n\nDespite the lack of novelty of the method, I do think that the results are valuable. The method is so easy to implement and seems to be so useful for complicated model which are hard to initialize, that it is important for others in the field to know about it. I suspect many people will at least try the method. The variety of the architectures and tasks for which the method was useful suggests that many people may also add it to their repertoire of optimization tricks. \n\n\nPros:\n* The idea is easy to implement.\n* The method is evaluated on a variety of tasks and for very different models.\n* Some interesting experiments which compare the method with similar approaches and investigate the importance of the annealing scheme.\n* The paper is well-written.\n\n\nCons:\n* The idea is not very original.\n* There is no clear theoretical motivation of analysis.\n* Not all the results are convincing."
  },
  {
    "people": [
      "Laurent",
      "Cooijmans"
    ],
    "review": "This paper presents a simple method of adding gradient noise to improve the training of deep neural networks. This paper first appeared on arXiv over a year ago and while there have been many innovations in the area of improving the training of deep neural networks in tha time (batch normalization for RNNs, layer normalization, normalization propagation, etc.) this paper does not mention or compare to these methods. \n\nIn particular, the authors state \"However, recent work on applying batch normalization to recurrent networks (Laurent et al., 2015) has not shown promise in improving generalization ability for recur- rent architectures, which are the focus of this work.\" This statement is simply incorrect and was thoroughly explored in, e.g. Cooijmans et al. (2016) that establish that batch normalization is effective for RNNs.\n\nThe proposed method itself is extremely simple and is similar to numerous training strategies that have previously been advocated in the literature. As a result the contribution would be incremental at best and could be significant with sufficiently strong empirical results supporting this particular variant. However, as discussed above there are now multiple training strategies and algorithms in the literature that are not empirically compared.\n\nUnfortunately, this paper is now fairly seriously out of date. It would not be appropriate to publish this at ICLR 2017. "
  },
  {
    "people": [
      "Langevin"
    ],
    "review": "The authors propose to add noise to the gradients computed while optimizing deep neural networks with stochastic gradient based methods. They show results multiple data sets which indicate that the method can counteract bad parameter initialization and that it can be especially beneficial for training more complicated architectures.\n\nThe method is tested on a multitude of different tasks and architectures. The results would be more convincing if they would be accompanied by confidence intervals but I understand that some of the experiments must have taken very long to run. I like that the results include both situations in which the gradient noise helps a lot and situations in which it doesn\u2019t seem to add much to the other optimization or initialization tools employed. The quantity of the experiments and the variety of the models provide quite convincing evidence that the effect of the gradient noise generalizes to many settings. The results were not always that convincing. In Section 4.2, the method only helped significantly when a sub-optimal training scheme was used, for example. The results on MNIST are not very good compared to the state-of-the-art. Since the method is so simple, I was hoping to see more theoretical arguments for its usefulness. That said, the experimental investigations into the importance of the annealing procedure, the comparison with the effect of gradient stochasticity and the comparison with weight noise, provide some additional insight.\n\nThe paper is well written and cites relevant prior work. The proposed method is described clearly and concisely, which is to be expected given its simplicity. \n\nThe proposed idea is not very original. As the authors acknowledge, very similar algorithms have been used for training and it is pretty much identical to simulating Langevin dynamics but with the goal of finding a single optimum in mind rather than approximating an expected value. The work is the evaluation of an old tool in a new era where models have become bigger and more complex.\n\nDespite the lack of novelty of the method, I do think that the results are valuable. The method is so easy to implement and seems to be so useful for complicated model which are hard to initialize, that it is important for others in the field to know about it. I suspect many people will at least try the method. The variety of the architectures and tasks for which the method was useful suggests that many people may also add it to their repertoire of optimization tricks. \n\n\nPros:\n* The idea is easy to implement.\n* The method is evaluated on a variety of tasks and for very different models.\n* Some interesting experiments which compare the method with similar approaches and investigate the importance of the annealing scheme.\n* The paper is well-written.\n\n\nCons:\n* The idea is not very original.\n* There is no clear theoretical motivation of analysis.\n* Not all the results are convincing.\n"
  },
  {
    "people": [
      "Chen"
    ],
    "review": "This paper proposes relation networks in order to model the pairwise interactions between objects in a visual scene. \nThe model is very straight forward, first an MLP (with shared weights) is applied to each pair of objects. Finally a prediction is created by an MLP which operates by summing non-linear functions of these pairs of objects. \nExperimental evaluation is done in a synthetic dataset that is generated to fit the architecture hand-crafted in this paper. \n\nThe title of the paper claims much more than the paper delivers. Discovering objects and their relations is a very important task.\nHowever, this paper does not discover objects or their relations, instead, each objects is represented with hand coded ground truth attributes, and only a small set of trivial relationships are \"discovered\", e.g., relative position. \n\nDiscovering objects and their relationships has been tackled for several decades in computer vision (CV). The paper does not cite or compare to any technique in this body of literature. This is typically refer to as \"contextual models\".\n\nCan the proposed architecture help object detection and/or scene classification? would it work in the presence of noise (e.g, missing detections, non accurate detection estimates, complex texture)? would it work when the attributes of objects are estimated from real images? \n \nI'll be more convinced if experiments where done in real scenes. In the case of indoor scenes, datasets such as NYUv2, Sun-RGB-D, SceneNN, Chen et al CVPR 14 (text-to-image-correference) could be used. In outdoor scenes, KITTI and the relationships between cars, pedestrians and cyclist could also serve as benchmark. \n\nWithout showing real scenes, this paper tackles a too toy problem with a very simple model which does not go much further than current context models, which model pairwise relationships between objects (with MRFs, with deep nets, etc). \n"
  },
  {
    "people": [
      "Chen"
    ],
    "review": "This paper proposes relation networks in order to model the pairwise interactions between objects in a visual scene. \nThe model is very straight forward, first an MLP (with shared weights) is applied to each pair of objects. Finally a prediction is created by an MLP which operates by summing non-linear functions of these pairs of objects. \nExperimental evaluation is done in a synthetic dataset that is generated to fit the architecture hand-crafted in this paper. \n\nThe title of the paper claims much more than the paper delivers. Discovering objects and their relations is a very important task.\nHowever, this paper does not discover objects or their relations, instead, each objects is represented with hand coded ground truth attributes, and only a small set of trivial relationships are \"discovered\", e.g., relative position. \n\nDiscovering objects and their relationships has been tackled for several decades in computer vision (CV). The paper does not cite or compare to any technique in this body of literature. This is typically refer to as \"contextual models\".\n\nCan the proposed architecture help object detection and/or scene classification? would it work in the presence of noise (e.g, missing detections, non accurate detection estimates, complex texture)? would it work when the attributes of objects are estimated from real images? \n \nI'll be more convinced if experiments where done in real scenes. In the case of indoor scenes, datasets such as NYUv2, Sun-RGB-D, SceneNN, Chen et al CVPR 14 (text-to-image-correference) could be used. In outdoor scenes, KITTI and the relationships between cars, pedestrians and cyclist could also serve as benchmark. \n\nWithout showing real scenes, this paper tackles a too toy problem with a very simple model which does not go much further than current context models, which model pairwise relationships between objects (with MRFs, with deep nets, etc). \n"
  },
  {
    "people": [
      "Graham Taylor",
      "Geoffrey Hinton"
    ],
    "review": "Paper proposes a novel Variational Encoder architecture that contains discrete variables. Model contains an undirected discrete component that captures distribution over disconnected manifolds and a directed hierarchical continuous component that models the actual manifolds (induced by the discrete variables). In essence the model clusters the data and at the same time learns a continuous manifold representation for the clusters. The training procedure for such models is also presented and is quite involved. Experiments illustrate state-of-the-art performance on public datasets (including MNIST, Omniglot, Caltech-101). \n\nOverall the model is interesting and could be useful in a variety of applications and domains. The approach is complex and somewhat mathematically involved. It's not exactly clear how the model compares or relates to other RBM formulations, particularly those that contain discrete latent variables and continuous outputs. As a prime example:\n\nGraham Taylor and Geoffrey Hinton. Factored conditional restricted Boltzmann machines for modeling motion style. In Proc. of the 26th International Conference on Machine Learning (ICML), 1025\u20131032, 2009.\n\nDiscussion of this should certainly be added. \n"
  },
  {
    "people": [
      "Boltzmann"
    ],
    "review": "This paper presents a way of training deep generative models with discrete hidden variables using the reparameterization trick. It then applies it to a particular DBN-like architecture, and shows that this architecture achieves state-of-the-art density modeling performance on MNIST and similar datasets. \n\nThe paper is well written, and the exposition is both thorough and precise. There are several appendices which justify various design decisions in detail. I wish more papers in our field would take this degree of care with the exposition!\n\nThe log-likelihood results are quite strong, especially given that most of the competitive algorithms are based on continuous latent variables. Probably the main thing missing from the experiments is some way to separate out the contributions of the architecture and the inference algorithm. (E.g., what if a comparable architecture is trained with VIMCO, or if the algorithm is applied to a previously published discrete architecture?)\n\nI\u2019m a bit concerned about the variance of the gradients in the general formulation of the algorithm. See my comment \u201cvariance of the derivatives of F^{-1}\u201d below. I think the response is convincing, but the problem (as well as \u201cengineering principles\u201d for the smoothing distribution) are probably worth pointing out in the paper itself, since the problem seems likely to occur unless the user is aware of it. (E.g., my proposal of widely separated normals would be a natural distribution to consider until one actually works through the gradients \u2014 something not commonly done in the age of autodiff frameworks.)\n\nAnother concern is how many sequential operations are needed for inference in the RBM model. (Note: is this actually an RBM, or a general Boltzmann machine?) The q distribution takes the form of an autoregressive model where the variables are processed one at a time. Section 3 mentions the possibility of grouping together variables in the q distribution, and this is elaborated in detail in Appendix A. But the solution requires decomposing the joint into a product of conditionals and applying the CDFs sequentially. So either way, it seems like we\u2019re stuck handling all the variables sequentially, which might get expensive. \n\nMinor: the second paragraph of Section 3 needs a reference to Appendix A.\n"
  },
  {
    "people": [
      "Courville",
      "Gregor",
      "Danihelka",
      "Mnih",
      "Blundell",
      "Wierstra",
      "D.",
      "Mnih",
      "Gregor",
      "Bornschein",
      "Bengio",
      "Gu",
      "Levine",
      "Sutskever",
      "Mnih",
      "Mnih",
      "Rezende"
    ],
    "review": "This paper proposes a complex new architecture with some discrete latent variables along with a specialized method for training them. The trouble is that it's unclear how effective the method actually is at training the discrete part of the system. Is it better or worse at this than the existing methods?There're many such methods to choose from:\nHinton, G. E., Dayan, P., Frey, B. J., & Neal, R. M. (1995). The \"wake-sleep\" algorithm for unsupervised neural networks. \nBengio, Y., Leonard, N., & Courville, A. (2013). Estimating or propagating gradients through stochastic neurons for conditional computation.\nGregor, K., Danihelka, I., Mnih, A., Blundell, C., & Wierstra, D. (2014). Deep autoregressive networks.\nMnih, A., & Gregor, K. (2014). Neural variational inference and learning in belief networks. \nBornschein, J., & Bengio, Y. (2014). Reweighted wake-sleep.\nGu, S., Levine, S., Sutskever, I., & Mnih, A. (2015). MuProp: Unbiased Backpropagation for Stochastic Neural Networks.\nMnih, A., & Rezende, D. J. (2016). Variational inference for Monte Carlo objectives.\nAll of these algorithms have been used to train models with hundreds of variables, so they should be able to handle models with up to 128 discrete variables used in the paper easily, as long as the reparameterization trick is used for the Gaussian latent variables. Moreover, the above methods are applicable to any architecture with discrete latent variables, unlike the algorithm proposed in the paper, which can only be used if the discrete variables are smoothed out with the continuous ones. Why not compare to at least one of these more general algorithms? It would be good to know whether anything is gained by taking advantage of the model structure in this case."
  },
  {
    "people": [
      "Graham Taylor",
      "Geoffrey Hinton"
    ],
    "review": "Paper proposes a novel Variational Encoder architecture that contains discrete variables. Model contains an undirected discrete component that captures distribution over disconnected manifolds and a directed hierarchical continuous component that models the actual manifolds (induced by the discrete variables). In essence the model clusters the data and at the same time learns a continuous manifold representation for the clusters. The training procedure for such models is also presented and is quite involved. Experiments illustrate state-of-the-art performance on public datasets (including MNIST, Omniglot, Caltech-101). \n\nOverall the model is interesting and could be useful in a variety of applications and domains. The approach is complex and somewhat mathematically involved. It's not exactly clear how the model compares or relates to other RBM formulations, particularly those that contain discrete latent variables and continuous outputs. As a prime example:\n\nGraham Taylor and Geoffrey Hinton. Factored conditional restricted Boltzmann machines for modeling motion style. In Proc. of the 26th International Conference on Machine Learning (ICML), 1025\u20131032, 2009.\n\nDiscussion of this should certainly be added. \n"
  },
  {
    "people": [
      "Boltzmann"
    ],
    "review": "This paper presents a way of training deep generative models with discrete hidden variables using the reparameterization trick. It then applies it to a particular DBN-like architecture, and shows that this architecture achieves state-of-the-art density modeling performance on MNIST and similar datasets. \n\nThe paper is well written, and the exposition is both thorough and precise. There are several appendices which justify various design decisions in detail. I wish more papers in our field would take this degree of care with the exposition!\n\nThe log-likelihood results are quite strong, especially given that most of the competitive algorithms are based on continuous latent variables. Probably the main thing missing from the experiments is some way to separate out the contributions of the architecture and the inference algorithm. (E.g., what if a comparable architecture is trained with VIMCO, or if the algorithm is applied to a previously published discrete architecture?)\n\nI\u2019m a bit concerned about the variance of the gradients in the general formulation of the algorithm. See my comment \u201cvariance of the derivatives of F^{-1}\u201d below. I think the response is convincing, but the problem (as well as \u201cengineering principles\u201d for the smoothing distribution) are probably worth pointing out in the paper itself, since the problem seems likely to occur unless the user is aware of it. (E.g., my proposal of widely separated normals would be a natural distribution to consider until one actually works through the gradients \u2014 something not commonly done in the age of autodiff frameworks.)\n\nAnother concern is how many sequential operations are needed for inference in the RBM model. (Note: is this actually an RBM, or a general Boltzmann machine?) The q distribution takes the form of an autoregressive model where the variables are processed one at a time. Section 3 mentions the possibility of grouping together variables in the q distribution, and this is elaborated in detail in Appendix A. But the solution requires decomposing the joint into a product of conditionals and applying the CDFs sequentially. So either way, it seems like we\u2019re stuck handling all the variables sequentially, which might get expensive. \n\nMinor: the second paragraph of Section 3 needs a reference to Appendix A.\n"
  },
  {
    "people": [
      "Courville",
      "Gregor",
      "Danihelka",
      "Mnih",
      "Blundell",
      "Wierstra",
      "D.",
      "Mnih",
      "Gregor",
      "Bornschein",
      "Bengio",
      "Gu",
      "Levine",
      "Sutskever",
      "Mnih",
      "Mnih",
      "Rezende"
    ],
    "review": "This paper proposes a complex new architecture with some discrete latent variables along with a specialized method for training them. The trouble is that it's unclear how effective the method actually is at training the discrete part of the system. Is it better or worse at this than the existing methods?There're many such methods to choose from:\nHinton, G. E., Dayan, P., Frey, B. J., & Neal, R. M. (1995). The \"wake-sleep\" algorithm for unsupervised neural networks. \nBengio, Y., Leonard, N., & Courville, A. (2013). Estimating or propagating gradients through stochastic neurons for conditional computation.\nGregor, K., Danihelka, I., Mnih, A., Blundell, C., & Wierstra, D. (2014). Deep autoregressive networks.\nMnih, A., & Gregor, K. (2014). Neural variational inference and learning in belief networks. \nBornschein, J., & Bengio, Y. (2014). Reweighted wake-sleep.\nGu, S., Levine, S., Sutskever, I., & Mnih, A. (2015). MuProp: Unbiased Backpropagation for Stochastic Neural Networks.\nMnih, A., & Rezende, D. J. (2016). Variational inference for Monte Carlo objectives.\nAll of these algorithms have been used to train models with hundreds of variables, so they should be able to handle models with up to 128 discrete variables used in the paper easily, as long as the reparameterization trick is used for the Gaussian latent variables. Moreover, the above methods are applicable to any architecture with discrete latent variables, unlike the algorithm proposed in the paper, which can only be used if the discrete variables are smoothed out with the continuous ones. Why not compare to at least one of these more general algorithms? It would be good to know whether anything is gained by taking advantage of the model structure in this case."
  },
  {
    "people": [
      "Lsup",
      "Lunsup"
    ],
    "review": "[EDIT: The thoughtful author responses addressed my major concerns. The github links for data and code will be really helpful for reproducing results (I haven't looked carefully, but this is great). The revision addressed many issues, including the additional results. As such I am upgrading my rating from a 5 to a 6 and recommend acceptance of the paper.]\n\nThe paper proposes to apply deep nets to perform detection and localization of extreme weather events in simulated weather data. The problem is related to object detection in computer vision in that the input is a 2D \u201cimage\u201d (multichannel spatial weather data) or 3D \u201cvideo\u201d (temporal version of the data) and the output is a bounding box (spatial-temporal localization of a weather event) and class label (weather event type). It differs from standard object detection in that the input has multiple heterogenous channels and labeled data is scarce.\n\nA simple but quite reasonable deep net is proposed for the task based on similar approaches in computer vision. While proposal based systems are most popular in vision currently (in particular Faster-RCNN) the proposed approach is simple and a fine starting point. There is little innovation on the part of the detection system, but as noted, it is a valid application of ideas from computer vision to the task at hand. \n\nThe authors propose both a supervised approach (only ground truth bounding box location/label is used) and a semi-supervised approach that additionally incorporates the reconstruction loss as a regularization. In all cases the losses are fairly standard and again, reasonable. The only confusing bit is that the \u201csemi-supervised\u201d loss actually has all the labels used for the \u201csupervised\u201d loss and additionally incorporates the reconstruction loss. Hence, the \u201csemi-supervised\u201d loss is actually stronger, which makes the terminology a bit confusing.\n\nThe paper is easy to follow, but notation is sloppy. For example, above equation 5 it states that \u201cthe loss is a weighted combination of reconstruction error and bounding box regression loss\u201d; actually it\u2019s a combination of the supervised and unsupervised loss (Lsup and Lunsup), and Lrec is not defined (although I assume Lrec=Lunsup). The paper is fairly non-technical, but nevertheless these minor issues should be fixed. (E.g., see also reference to \u201cfigure 4 and 4\u201d). \n\nThe biggest concern w the paper though is experimental results. Only a single figure and table of results are shown (figure 4 and table 4). The metrics are not defined (what is mean average recall?). Only 2D versus 3D version of the model are shown, and supervised and semi-supervised. Moreover, numbers seem a bit all over the place, without consistent patterns (e.g., why is 2D supervised better than the seemingly much strong 3D semi-supervised?). One of the things that is unclear is how many events are actually in the training/testing data, and more importantly, how good are these results in absolute terms? Regardless, the experiments are fairly sparse and ablation studies and more discussion lacking. It is also unclear if future researchers will be able to reproduce the experimental setting (a commitment to open-source the data or a way to reproduce the experiments would be critical for future authors).\n\nMinor nit: the authors use both a classification loss and an \u201cobjectness\u201d loss. I\u2019ve never seen both used together like this (normally objectness is used in two-stage object proposal systems where in the first stage class-agnostic proposals are given and in the second stage these are cropped and a class-specific classifier is applied). I strongly suspect removing the objectness loss would not impact results since the classification loss should provide strictly stronger supervisory signal. Regardless, this is a fairly non-standard choice and should be justified (experimentally).\n\nOverall this is a borderline paper. I do believe that it is valuable to apply computer vision techniques to a domain that I\u2019ve see little work on in our community. That being said, I have no expertise on this type of data -- it\u2019s possible this deep learning techniques are now routinely used in the climate science literature (I suspect not, though). Overall, there is little novelty on the algorithmic side in this paper (the equations in section 3 are commonly used in the cv literature). The use of reconstruction loss to improve results in the data-sparse setting is interesting, but the experimental results are inconclusive. The experimental validation is generally insufficient. Reproducibility for future research is difficult unless the data is open-sourced. Overall, I think this paper is a good start, and with improved experiments and more careful writing I think could eventually make for a decent paper."
  },
  {
    "people": [
      "Lsup",
      "Lunsup"
    ],
    "review": "[EDIT: The thoughtful author responses addressed my major concerns. The github links for data and code will be really helpful for reproducing results (I haven't looked carefully, but this is great). The revision addressed many issues, including the additional results. As such I am upgrading my rating from a 5 to a 6 and recommend acceptance of the paper.]\n\nThe paper proposes to apply deep nets to perform detection and localization of extreme weather events in simulated weather data. The problem is related to object detection in computer vision in that the input is a 2D \u201cimage\u201d (multichannel spatial weather data) or 3D \u201cvideo\u201d (temporal version of the data) and the output is a bounding box (spatial-temporal localization of a weather event) and class label (weather event type). It differs from standard object detection in that the input has multiple heterogenous channels and labeled data is scarce.\n\nA simple but quite reasonable deep net is proposed for the task based on similar approaches in computer vision. While proposal based systems are most popular in vision currently (in particular Faster-RCNN) the proposed approach is simple and a fine starting point. There is little innovation on the part of the detection system, but as noted, it is a valid application of ideas from computer vision to the task at hand. \n\nThe authors propose both a supervised approach (only ground truth bounding box location/label is used) and a semi-supervised approach that additionally incorporates the reconstruction loss as a regularization. In all cases the losses are fairly standard and again, reasonable. The only confusing bit is that the \u201csemi-supervised\u201d loss actually has all the labels used for the \u201csupervised\u201d loss and additionally incorporates the reconstruction loss. Hence, the \u201csemi-supervised\u201d loss is actually stronger, which makes the terminology a bit confusing.\n\nThe paper is easy to follow, but notation is sloppy. For example, above equation 5 it states that \u201cthe loss is a weighted combination of reconstruction error and bounding box regression loss\u201d; actually it\u2019s a combination of the supervised and unsupervised loss (Lsup and Lunsup), and Lrec is not defined (although I assume Lrec=Lunsup). The paper is fairly non-technical, but nevertheless these minor issues should be fixed. (E.g., see also reference to \u201cfigure 4 and 4\u201d). \n\nThe biggest concern w the paper though is experimental results. Only a single figure and table of results are shown (figure 4 and table 4). The metrics are not defined (what is mean average recall?). Only 2D versus 3D version of the model are shown, and supervised and semi-supervised. Moreover, numbers seem a bit all over the place, without consistent patterns (e.g., why is 2D supervised better than the seemingly much strong 3D semi-supervised?). One of the things that is unclear is how many events are actually in the training/testing data, and more importantly, how good are these results in absolute terms? Regardless, the experiments are fairly sparse and ablation studies and more discussion lacking. It is also unclear if future researchers will be able to reproduce the experimental setting (a commitment to open-source the data or a way to reproduce the experiments would be critical for future authors).\n\nMinor nit: the authors use both a classification loss and an \u201cobjectness\u201d loss. I\u2019ve never seen both used together like this (normally objectness is used in two-stage object proposal systems where in the first stage class-agnostic proposals are given and in the second stage these are cropped and a class-specific classifier is applied). I strongly suspect removing the objectness loss would not impact results since the classification loss should provide strictly stronger supervisory signal. Regardless, this is a fairly non-standard choice and should be justified (experimentally).\n\nOverall this is a borderline paper. I do believe that it is valuable to apply computer vision techniques to a domain that I\u2019ve see little work on in our community. That being said, I have no expertise on this type of data -- it\u2019s possible this deep learning techniques are now routinely used in the climate science literature (I suspect not, though). Overall, there is little novelty on the algorithmic side in this paper (the equations in section 3 are commonly used in the cv literature). The use of reconstruction loss to improve results in the data-sparse setting is interesting, but the experimental results are inconclusive. The experimental validation is generally insufficient. Reproducibility for future research is difficult unless the data is open-sourced. Overall, I think this paper is a good start, and with improved experiments and more careful writing I think could eventually make for a decent paper."
  },
  {
    "people": [
      "Kalman",
      "Johnson"
    ],
    "review": "This paper presents a variational inference based method for learning nonlinear dynamical systems. Unlike the deep Kalman filter, the proposed method learns a state space model, which forces the latent state to maintain all of the information relevant to predictions, rather than leaving it implicit in the observations. Experiments show the proposed method is better able to learn meaningful representations of sequence data.\n\nThe proposed DVBF is well motivated, and for the most part the presentation is clear. The experiments show interesting results on illustrative toy examples. I think the contribution is interesting and potentially useful, so I\u2019d recommend acceptance.\n\nThe SVAE method of Johnson et al. (2016) deserves more discussion than the two sentences devoted to it, since the method seems pretty closely related. Like the DVBF, the SVAE imposes a Markovianity assumption, and it is able to handle similar kinds of problems. From what I understand, the most important algorithmic difference is that the SVAE q network predicts potentials, whereas the DVBF q network predicts innovations. What are the tradeoffs between the two?  Section 2.2 says they do the latter in the interest of solving control-related tasks, but I\u2019m not clear why this follows. \n\nIs there a reason SVAEs don\u2019t meet all the desiderata mentioned at the end of the Introduction?\n\nSince the SVAE code is publicly available, one could probably compare against it in the experiments. \n\nI\u2019m a bit confused about the role of uncertainty about v. In principle, one could estimate the transition parameters by maximum likelihood (i.e. fitting a point estimate of v), but this isn\u2019t what\u2019s done. Instead, v is integrated out as part of the marginal likelihood, which I interpret as giving the flexibility to model different dynamics for different sequences. But if this is the case, then shouldn\u2019t the q distribution for v depend on the data, rather than being data-independent as in Eqn. (9)?"
  },
  {
    "people": [
      "Kalman",
      "Johnson"
    ],
    "review": "This paper presents a variational inference based method for learning nonlinear dynamical systems. Unlike the deep Kalman filter, the proposed method learns a state space model, which forces the latent state to maintain all of the information relevant to predictions, rather than leaving it implicit in the observations. Experiments show the proposed method is better able to learn meaningful representations of sequence data.\n\nThe proposed DVBF is well motivated, and for the most part the presentation is clear. The experiments show interesting results on illustrative toy examples. I think the contribution is interesting and potentially useful, so I\u2019d recommend acceptance.\n\nThe SVAE method of Johnson et al. (2016) deserves more discussion than the two sentences devoted to it, since the method seems pretty closely related. Like the DVBF, the SVAE imposes a Markovianity assumption, and it is able to handle similar kinds of problems. From what I understand, the most important algorithmic difference is that the SVAE q network predicts potentials, whereas the DVBF q network predicts innovations. What are the tradeoffs between the two?  Section 2.2 says they do the latter in the interest of solving control-related tasks, but I\u2019m not clear why this follows. \n\nIs there a reason SVAEs don\u2019t meet all the desiderata mentioned at the end of the Introduction?\n\nSince the SVAE code is publicly available, one could probably compare against it in the experiments. \n\nI\u2019m a bit confused about the role of uncertainty about v. In principle, one could estimate the transition parameters by maximum likelihood (i.e. fitting a point estimate of v), but this isn\u2019t what\u2019s done. Instead, v is integrated out as part of the marginal likelihood, which I interpret as giving the flexibility to model different dynamics for different sequences. But if this is the case, then shouldn\u2019t the q distribution for v depend on the data, rather than being data-independent as in Eqn. (9)?\n"
  },
  {
    "people": [
      "Watter"
    ],
    "review": "The paper proposes to use the very standard SVGB in a sequential setting like several previous works did. However, they proposes to have a clear state space constraints similar to Linear Gaussian Models: Markovian latent space and conditional independence of observed variables given the latent variables. However the model is in this case non-linear. These assumptions are well motivated by the goal of having meaningful latent variables.\nThe experiments are interesting but I'm still not completely convinced by the regression results in Figure 3, namely that one could obtain the angle and velocity from the state but using a function more powerful than a linear function. Also, why isn't the model from (Watter et al., 2015) not included ?\nAfter rereading I'm not sure I understand why the coordinates should be combined in a 3x3 checkerboard as said in Figure 5a. \nThen paper is well motivated and the resulting model is novel enough, the bouncing ball experiment is not quite convincing, especially in prediction, as the problem is fully determined by its initial velocity and position. "
  },
  {
    "people": [
      "Valpola",
      "Karhunen",
      "Honkela"
    ],
    "review": "Thanks for the interesting paper!\n\nIt seems you may have missed some very relevant early prior work on variational inference for nonlinear state-space models in Valpola and Karhunen (Neural Computation 2002, doi:10.1162/089976602760408017) further developed in Honkela et al. (JMLR 2010)."
  },
  {
    "people": [
      "Kalman",
      "Johnson"
    ],
    "review": "This paper presents a variational inference based method for learning nonlinear dynamical systems. Unlike the deep Kalman filter, the proposed method learns a state space model, which forces the latent state to maintain all of the information relevant to predictions, rather than leaving it implicit in the observations. Experiments show the proposed method is better able to learn meaningful representations of sequence data.\n\nThe proposed DVBF is well motivated, and for the most part the presentation is clear. The experiments show interesting results on illustrative toy examples. I think the contribution is interesting and potentially useful, so I\u2019d recommend acceptance.\n\nThe SVAE method of Johnson et al. (2016) deserves more discussion than the two sentences devoted to it, since the method seems pretty closely related. Like the DVBF, the SVAE imposes a Markovianity assumption, and it is able to handle similar kinds of problems. From what I understand, the most important algorithmic difference is that the SVAE q network predicts potentials, whereas the DVBF q network predicts innovations. What are the tradeoffs between the two?  Section 2.2 says they do the latter in the interest of solving control-related tasks, but I\u2019m not clear why this follows. \n\nIs there a reason SVAEs don\u2019t meet all the desiderata mentioned at the end of the Introduction?\n\nSince the SVAE code is publicly available, one could probably compare against it in the experiments. \n\nI\u2019m a bit confused about the role of uncertainty about v. In principle, one could estimate the transition parameters by maximum likelihood (i.e. fitting a point estimate of v), but this isn\u2019t what\u2019s done. Instead, v is integrated out as part of the marginal likelihood, which I interpret as giving the flexibility to model different dynamics for different sequences. But if this is the case, then shouldn\u2019t the q distribution for v depend on the data, rather than being data-independent as in Eqn. (9)?"
  },
  {
    "people": [
      "Kalman",
      "Johnson"
    ],
    "review": "This paper presents a variational inference based method for learning nonlinear dynamical systems. Unlike the deep Kalman filter, the proposed method learns a state space model, which forces the latent state to maintain all of the information relevant to predictions, rather than leaving it implicit in the observations. Experiments show the proposed method is better able to learn meaningful representations of sequence data.\n\nThe proposed DVBF is well motivated, and for the most part the presentation is clear. The experiments show interesting results on illustrative toy examples. I think the contribution is interesting and potentially useful, so I\u2019d recommend acceptance.\n\nThe SVAE method of Johnson et al. (2016) deserves more discussion than the two sentences devoted to it, since the method seems pretty closely related. Like the DVBF, the SVAE imposes a Markovianity assumption, and it is able to handle similar kinds of problems. From what I understand, the most important algorithmic difference is that the SVAE q network predicts potentials, whereas the DVBF q network predicts innovations. What are the tradeoffs between the two?  Section 2.2 says they do the latter in the interest of solving control-related tasks, but I\u2019m not clear why this follows. \n\nIs there a reason SVAEs don\u2019t meet all the desiderata mentioned at the end of the Introduction?\n\nSince the SVAE code is publicly available, one could probably compare against it in the experiments. \n\nI\u2019m a bit confused about the role of uncertainty about v. In principle, one could estimate the transition parameters by maximum likelihood (i.e. fitting a point estimate of v), but this isn\u2019t what\u2019s done. Instead, v is integrated out as part of the marginal likelihood, which I interpret as giving the flexibility to model different dynamics for different sequences. But if this is the case, then shouldn\u2019t the q distribution for v depend on the data, rather than being data-independent as in Eqn. (9)?\n"
  },
  {
    "people": [
      "Watter"
    ],
    "review": "The paper proposes to use the very standard SVGB in a sequential setting like several previous works did. However, they proposes to have a clear state space constraints similar to Linear Gaussian Models: Markovian latent space and conditional independence of observed variables given the latent variables. However the model is in this case non-linear. These assumptions are well motivated by the goal of having meaningful latent variables.\nThe experiments are interesting but I'm still not completely convinced by the regression results in Figure 3, namely that one could obtain the angle and velocity from the state but using a function more powerful than a linear function. Also, why isn't the model from (Watter et al., 2015) not included ?\nAfter rereading I'm not sure I understand why the coordinates should be combined in a 3x3 checkerboard as said in Figure 5a. \nThen paper is well motivated and the resulting model is novel enough, the bouncing ball experiment is not quite convincing, especially in prediction, as the problem is fully determined by its initial velocity and position. "
  },
  {
    "people": [
      "Valpola",
      "Karhunen",
      "Honkela"
    ],
    "review": "Thanks for the interesting paper!\n\nIt seems you may have missed some very relevant early prior work on variational inference for nonlinear state-space models in Valpola and Karhunen (Neural Computation 2002, doi:10.1162/089976602760408017) further developed in Honkela et al. (JMLR 2010)."
  },
  {
    "people": [
      "Lucas Theis",
      "Aaron van den Oord",
      "Matthias Bethge",
      "Hugo Larochelle",
      "Iain Murray"
    ],
    "review": "The bulk of the reviewer (R) comments pertain to the experiments on MNIST that we address collectively here to facilitate a richer, holistic discussion. Any remaining questions raised by the reviewers are addressed individually as follow-ups to the reviews.\n\nWe have rewritten the entire experimental section based on fresh experimentation, focusing on aspects that we believe are crucial to resolving the concerns raised by the reviewers. For brevity, we have also removed some portions from the earlier draft that we do not believe are central to the discussion. In the spirit of the constructive feedback loop that ICLR facilitates, we hope the reviewers will be able to read the revised experiments section. Our response to the reviewer comments:\n\n1. A bigger VAE vs. BGMs containing smaller models (R1)\nOur earlier submission demonstrated improved performance of BGMs for sampling with gains in computational requirements, but did not directly address the reviewer\u2019s question about model capacity. Our latest submission considers larger baseline models with equal or more capacity than BGM sequences of simpler models (Sec. 3.2.2, Table 2), and shows the superior performance of BGMs for this task (Figure 3) . As before, we also illustrate that the performance gains of BGM sequences are not at the cost of increased training time (Sec 3.2.3, Table 2).\n\n2. Samples from MNIST look weak (R4)\nOur experiments on MNIST do not (or even intended to) make any state-of-the-art claims, but instead demonstrate the improvements in computational and statistical performance that simpler models can attain for the task of generating samples as a result of the boosting procedure. Having said that, we respectfully disagree that the samples produced by BGM sequences look weak keeping in mind that the dataset under consideration is the harder \u2018binarized\u2019 version of MNIST. In fact, although we understand that it is somewhat subjective, we believe the BGM samples look better than NADE samples on the same dataset (Figure 2 (Left) in [2]). Note that we display actual samples produced by our models (without any cherry-picking), and hence, these samples should not be compared with the probabilities from sampling pixels (Figure 2 (Middle) in [2]). \n\n3. Strategies for setting weights (\\alpha\u2019s) for intermediate models (R3)\nIn our earlier submission, we discussed model weights in the theoretical section and listed heuristic strategies for setting them in practice as a future work. Our latest submission includes experimental results and discussion of some heuristic strategies that we propose for both density estimation and sampling. See Sec. 3.1.3 and Appendix A.2.1.\n\n4. Procedure for drawing samples from BGM sequences (R1, R3)\nSee Appendix A.2.3 for details.  In contrast to results in the previous submission where the independent chains were run from the same start seeds across BGM sequences and samples arranged for easy comparison, the latest results show samples produced from chains that are run independently, initialized with different random start seeds, and have an increased burn-in time of 100,000 samples (10x the previous) to mitigate the possibility of any mixing-related issues. We also observed visually that the samples from any given chain vary over time transforming smoothly across multiple digits within the burn-in time, a further indication of successful mixing. \n\n5. Quantitative estimation of log-likelihoods for sampling (R1, R3, R4)\nFor evaluating samples, log-likelihoods can be problematic. See [1], Sec. 3.2 for a detailed discussion on why log-likelihoods are not a good proxy for sample quality. Additionally, generative models such as VAEs, RBMs have intractable likelihoods and hence, resort to their own approximations for generating density estimates. These approximations are known to exhibit different behaviors; for instance, RBMs estimated using AIS are typically believed to overestimate the log-likelihoods, whereas VAEs make variational approximations that only provide lower bounds -- it is difficult to make definitive statements based on such approximations. We tried AIS on the BGM sequences considered in the paper with about 15,000 intermediate annealing distributions (step-size of 1e-3 between for the first 500 distributions, 1e-4 for the next 4000, and 1e-5 for the remaining where every transition from one distribution to another was implemented using 10 steps of Metropolis-Hastings), but got unrealistically optimistic log-likelihood evaluations that we do not feel confident reporting for the purpose of a meaningful comparison with the lower bounds provided by VAEs.  \n\nReferences:\n[1] Lucas Theis, Aaron van den Oord, and Matthias Bethge. \u201cA note on the evaluation of generative models.\u201d In ICLR, 2016.\n[2] Hugo Larochelle and Iain Murray. \"The Neural Autoregressive Distribution Estimator.\"  In AISTATS, 2011."
  },
  {
    "people": [
      "Lucas Theis",
      "Aaron van den Oord",
      "Matthias Bethge",
      "Hugo Larochelle",
      "Iain Murray"
    ],
    "review": "The bulk of the reviewer (R) comments pertain to the experiments on MNIST that we address collectively here to facilitate a richer, holistic discussion. Any remaining questions raised by the reviewers are addressed individually as follow-ups to the reviews.\n\nWe have rewritten the entire experimental section based on fresh experimentation, focusing on aspects that we believe are crucial to resolving the concerns raised by the reviewers. For brevity, we have also removed some portions from the earlier draft that we do not believe are central to the discussion. In the spirit of the constructive feedback loop that ICLR facilitates, we hope the reviewers will be able to read the revised experiments section. Our response to the reviewer comments:\n\n1. A bigger VAE vs. BGMs containing smaller models (R1)\nOur earlier submission demonstrated improved performance of BGMs for sampling with gains in computational requirements, but did not directly address the reviewer\u2019s question about model capacity. Our latest submission considers larger baseline models with equal or more capacity than BGM sequences of simpler models (Sec. 3.2.2, Table 2), and shows the superior performance of BGMs for this task (Figure 3) . As before, we also illustrate that the performance gains of BGM sequences are not at the cost of increased training time (Sec 3.2.3, Table 2).\n\n2. Samples from MNIST look weak (R4)\nOur experiments on MNIST do not (or even intended to) make any state-of-the-art claims, but instead demonstrate the improvements in computational and statistical performance that simpler models can attain for the task of generating samples as a result of the boosting procedure. Having said that, we respectfully disagree that the samples produced by BGM sequences look weak keeping in mind that the dataset under consideration is the harder \u2018binarized\u2019 version of MNIST. In fact, although we understand that it is somewhat subjective, we believe the BGM samples look better than NADE samples on the same dataset (Figure 2 (Left) in [2]). Note that we display actual samples produced by our models (without any cherry-picking), and hence, these samples should not be compared with the probabilities from sampling pixels (Figure 2 (Middle) in [2]). \n\n3. Strategies for setting weights (\\alpha\u2019s) for intermediate models (R3)\nIn our earlier submission, we discussed model weights in the theoretical section and listed heuristic strategies for setting them in practice as a future work. Our latest submission includes experimental results and discussion of some heuristic strategies that we propose for both density estimation and sampling. See Sec. 3.1.3 and Appendix A.2.1.\n\n4. Procedure for drawing samples from BGM sequences (R1, R3)\nSee Appendix A.2.3 for details.  In contrast to results in the previous submission where the independent chains were run from the same start seeds across BGM sequences and samples arranged for easy comparison, the latest results show samples produced from chains that are run independently, initialized with different random start seeds, and have an increased burn-in time of 100,000 samples (10x the previous) to mitigate the possibility of any mixing-related issues. We also observed visually that the samples from any given chain vary over time transforming smoothly across multiple digits within the burn-in time, a further indication of successful mixing. \n\n5. Quantitative estimation of log-likelihoods for sampling (R1, R3, R4)\nFor evaluating samples, log-likelihoods can be problematic. See [1], Sec. 3.2 for a detailed discussion on why log-likelihoods are not a good proxy for sample quality. Additionally, generative models such as VAEs, RBMs have intractable likelihoods and hence, resort to their own approximations for generating density estimates. These approximations are known to exhibit different behaviors; for instance, RBMs estimated using AIS are typically believed to overestimate the log-likelihoods, whereas VAEs make variational approximations that only provide lower bounds -- it is difficult to make definitive statements based on such approximations. We tried AIS on the BGM sequences considered in the paper with about 15,000 intermediate annealing distributions (step-size of 1e-3 between for the first 500 distributions, 1e-4 for the next 4000, and 1e-5 for the remaining where every transition from one distribution to another was implemented using 10 steps of Metropolis-Hastings), but got unrealistically optimistic log-likelihood evaluations that we do not feel confident reporting for the purpose of a meaningful comparison with the lower bounds provided by VAEs.  \n\nReferences:\n[1] Lucas Theis, Aaron van den Oord, and Matthias Bethge. \u201cA note on the evaluation of generative models.\u201d In ICLR, 2016.\n[2] Hugo Larochelle and Iain Murray. \"The Neural Autoregressive Distribution Estimator.\"  In AISTATS, 2011."
  },
  {
    "people": [
      "Laurent",
      "Amodei",
      "Laurent",
      "Amodei"
    ],
    "review": "Contributions\nThe paper presents an adaptation of batch normalization for RNNs in the case of LSTMs, along the horizontal depth. Contrary to previous work from (Laurent 2015; Amodei 2016), the work demonstrates that batch-normalizing the hidden states of RNNs can improve optimization, and argues with quantitative experiments that the key factor to making this work is proper initialization of parameters, in particular gamma. Experiments show some gain in performance over vanilla LSTMs on Sequential MNIST, PTB, Text8 and CNN Question-Answering.\n\nNovelty+Significance\nBatch normalization has been key for training deeper and deeper networks (e.g. ResNets) and it seems natural that we would want to extend it to RNNs.  The paper shows that it is possible to do so with proper initialization of parameters, contrary to previous work from (Laurent 2015; Amodei 2016). Novelty comes from where to batch norm (i.e. not in the cell update) and in the per-time step statistics. \n\nAdding batch normalization to LSTMs incurs additional computational cost and bookkeeping; for training speed comparisons (e.g. Figure 2) the paper only compares LSTM and BN-LSTM by iteration count; given the additional complexity of the BN-LSTM I would have also liked to see a wall-clock comparison.\n\nAs RNNs are used across many tasks, this work is of interest to many.  However, the results gains are generally minor and require several tricks to work in practice. Also, this work doesn\u2019t address a question about batch normalization that it seems natural that it helps with faster training, but why would it also improve generalization? \n\nClarity\nThe paper is overall very clear and well-motivated. The model is well described and easy to understand, and the plots illustrate the points clearly.\n\nSummary\nInteresting though relatively incremental adaptation, but shows batch normalization to work for RNNs where previous works have not succeeded. Comprehensive set of experiments though it is questionable if the empirical gains are significant enough to justify the increased model complexity as well as computational overhead.\n\nPros\n- Shows batch normalization to work for RNNs where previous works have not succeeded\n- Good empirical analysis of hyper-parameter choices and of the activations\n- Experiments on multiple tasks\n- Clarity\n\nCons\n- Relatively incremental\n- Several \u2018hacks\u2019 for the method (per-time step statistics, adding noise for exploding variance, sequence-wise normalization)\n- No mention of computational overhead\n- Only character or pixel-level tasks, what about word-level?"
  },
  {
    "people": [
      "Laurent",
      "Amodei",
      "Laurent",
      "Amodei"
    ],
    "review": "Contributions\nThe paper presents an adaptation of batch normalization for RNNs in the case of LSTMs, along the horizontal depth. Contrary to previous work from (Laurent 2015; Amodei 2016), the work demonstrates that batch-normalizing the hidden states of RNNs can improve optimization, and argues with quantitative experiments that the key factor to making this work is proper initialization of parameters, in particular gamma. Experiments show some gain in performance over vanilla LSTMs on Sequential MNIST, PTB, Text8 and CNN Question-Answering.\n\nNovelty+Significance\nBatch normalization has been key for training deeper and deeper networks (e.g. ResNets) and it seems natural that we would want to extend it to RNNs.  The paper shows that it is possible to do so with proper initialization of parameters, contrary to previous work from (Laurent 2015; Amodei 2016). Novelty comes from where to batch norm (i.e. not in the cell update) and in the per-time step statistics. \n\nAdding batch normalization to LSTMs incurs additional computational cost and bookkeeping; for training speed comparisons (e.g. Figure 2) the paper only compares LSTM and BN-LSTM by iteration count; given the additional complexity of the BN-LSTM I would have also liked to see a wall-clock comparison.\n\nAs RNNs are used across many tasks, this work is of interest to many.  However, the results gains are generally minor and require several tricks to work in practice. Also, this work doesn\u2019t address a question about batch normalization that it seems natural that it helps with faster training, but why would it also improve generalization? \n\nClarity\nThe paper is overall very clear and well-motivated. The model is well described and easy to understand, and the plots illustrate the points clearly.\n\nSummary\nInteresting though relatively incremental adaptation, but shows batch normalization to work for RNNs where previous works have not succeeded. Comprehensive set of experiments though it is questionable if the empirical gains are significant enough to justify the increased model complexity as well as computational overhead.\n\nPros\n- Shows batch normalization to work for RNNs where previous works have not succeeded\n- Good empirical analysis of hyper-parameter choices and of the activations\n- Experiments on multiple tasks\n- Clarity\n\nCons\n- Relatively incremental\n- Several \u2018hacks\u2019 for the method (per-time step statistics, adding noise for exploding variance, sequence-wise normalization)\n- No mention of computational overhead\n- Only character or pixel-level tasks, what about word-level?"
  },
  {
    "people": [
      "Laurent",
      "Amodei",
      "Laurent",
      "Amodei"
    ],
    "review": "Contributions\nThe paper presents an adaptation of batch normalization for RNNs in the case of LSTMs, along the horizontal depth. Contrary to previous work from (Laurent 2015; Amodei 2016), the work demonstrates that batch-normalizing the hidden states of RNNs can improve optimization, and argues with quantitative experiments that the key factor to making this work is proper initialization of parameters, in particular gamma. Experiments show some gain in performance over vanilla LSTMs on Sequential MNIST, PTB, Text8 and CNN Question-Answering.\n\nNovelty+Significance\nBatch normalization has been key for training deeper and deeper networks (e.g. ResNets) and it seems natural that we would want to extend it to RNNs.  The paper shows that it is possible to do so with proper initialization of parameters, contrary to previous work from (Laurent 2015; Amodei 2016). Novelty comes from where to batch norm (i.e. not in the cell update) and in the per-time step statistics. \n\nAdding batch normalization to LSTMs incurs additional computational cost and bookkeeping; for training speed comparisons (e.g. Figure 2) the paper only compares LSTM and BN-LSTM by iteration count; given the additional complexity of the BN-LSTM I would have also liked to see a wall-clock comparison.\n\nAs RNNs are used across many tasks, this work is of interest to many.  However, the results gains are generally minor and require several tricks to work in practice. Also, this work doesn\u2019t address a question about batch normalization that it seems natural that it helps with faster training, but why would it also improve generalization? \n\nClarity\nThe paper is overall very clear and well-motivated. The model is well described and easy to understand, and the plots illustrate the points clearly.\n\nSummary\nInteresting though relatively incremental adaptation, but shows batch normalization to work for RNNs where previous works have not succeeded. Comprehensive set of experiments though it is questionable if the empirical gains are significant enough to justify the increased model complexity as well as computational overhead.\n\nPros\n- Shows batch normalization to work for RNNs where previous works have not succeeded\n- Good empirical analysis of hyper-parameter choices and of the activations\n- Experiments on multiple tasks\n- Clarity\n\nCons\n- Relatively incremental\n- Several \u2018hacks\u2019 for the method (per-time step statistics, adding noise for exploding variance, sequence-wise normalization)\n- No mention of computational overhead\n- Only character or pixel-level tasks, what about word-level?"
  },
  {
    "people": [
      "Laurent",
      "Amodei",
      "Laurent",
      "Amodei"
    ],
    "review": "Contributions\nThe paper presents an adaptation of batch normalization for RNNs in the case of LSTMs, along the horizontal depth. Contrary to previous work from (Laurent 2015; Amodei 2016), the work demonstrates that batch-normalizing the hidden states of RNNs can improve optimization, and argues with quantitative experiments that the key factor to making this work is proper initialization of parameters, in particular gamma. Experiments show some gain in performance over vanilla LSTMs on Sequential MNIST, PTB, Text8 and CNN Question-Answering.\n\nNovelty+Significance\nBatch normalization has been key for training deeper and deeper networks (e.g. ResNets) and it seems natural that we would want to extend it to RNNs.  The paper shows that it is possible to do so with proper initialization of parameters, contrary to previous work from (Laurent 2015; Amodei 2016). Novelty comes from where to batch norm (i.e. not in the cell update) and in the per-time step statistics. \n\nAdding batch normalization to LSTMs incurs additional computational cost and bookkeeping; for training speed comparisons (e.g. Figure 2) the paper only compares LSTM and BN-LSTM by iteration count; given the additional complexity of the BN-LSTM I would have also liked to see a wall-clock comparison.\n\nAs RNNs are used across many tasks, this work is of interest to many.  However, the results gains are generally minor and require several tricks to work in practice. Also, this work doesn\u2019t address a question about batch normalization that it seems natural that it helps with faster training, but why would it also improve generalization? \n\nClarity\nThe paper is overall very clear and well-motivated. The model is well described and easy to understand, and the plots illustrate the points clearly.\n\nSummary\nInteresting though relatively incremental adaptation, but shows batch normalization to work for RNNs where previous works have not succeeded. Comprehensive set of experiments though it is questionable if the empirical gains are significant enough to justify the increased model complexity as well as computational overhead.\n\nPros\n- Shows batch normalization to work for RNNs where previous works have not succeeded\n- Good empirical analysis of hyper-parameter choices and of the activations\n- Experiments on multiple tasks\n- Clarity\n\nCons\n- Relatively incremental\n- Several \u2018hacks\u2019 for the method (per-time step statistics, adding noise for exploding variance, sequence-wise normalization)\n- No mention of computational overhead\n- Only character or pixel-level tasks, what about word-level?"
  },
  {
    "people": [
      "Taylor",
      "Taylor"
    ],
    "review": "It is interesting to derive such a bound and show it satisfies a regret bound along with empirical evidence on the CIFAR-10 for cross entropy loss and auto encoder for MSE loss. At least empirically, by comparing the observed training loss and taylor loss, the better a particular optimizer performs (training loss statement, not a validation or observed test statement) the smaller the difference between these two. Also shown is the regret loss is satisfied at different scales of the network, by layer, neuron and whole network. \n\nThe Taylor approximation can be used to investigate activation configurations of the network, and used to connect this to difficulty in optimizing  at kinks in the loss surface, along with an empirical study of exploration of activation surface of the SGD/Adam/RMSprop optimizers, the more exploration the better the resulting training loss.\n\nNot that it impacts the paper but the weaker performance of the SGD could be related to the fixed learning rate, if we anneal this learning rate, which should improve performance, does this translate to more exploration and tightening between the actual loss and the Taylor loss? \n\n- It might be useful to use a cross validation set for some of the empirical studies, in the end we would like to say something about generalization of the resulting network\n\n- Is there a reason the subscript on the Jacobian changes to a_l in the"
  },
  {
    "people": [
      "Taylor"
    ],
    "review": "This paper develops a theoretical guarantee for the convergence of the training error. The result is quite general that covers the training of a wide range of neural network models. The key idea of the paper is approximate the training loss by its linear approximation. Since its linearity in the variables (thus convex), the authors plug in results that has been developed in the literature of online learning. \n\nThis paper has good novelty in using the Taylor approximation thus greatly simplifying the analysis of the behaviour of the model. However, there are two problems about the main result of this paper, Theorem 2.\n\n1. It is not clear if the Taylor optimum would converge or not. \nAs noticed by the authors, the upper bound is path dependent. Appendix 3 tries to claim that this Taylor optimum indeed converges, but the proof is buggy. In the proof of Lemma 2, it is proved that the difference between two sequential Taylor optimum is approaching 0. Note that this is actually weaker than being Cauchy sequence and insufficient to guarantee convergence.\n\n2. The lefthand side of Equation (3) (I will denote it by L3 in this review) is not equivalent to training error. \nAn upper bound on this average error is not sufficient to guarantee the convergence of the training error neither. Take the gradient descent for example (thus each minibatch x_0^n is the whole training set), the convergence of the training error should be lim_{n -> \\infty} l(f_{w^n}(x_0^n), y^n). The convergence of L3 is necessary but not sufficient to imply the convergence of the training error.\n\nAnother concern about Theorem 2 (but it is minor compared to the two problems mentioned above) is that to achieve the O(1/\\sqrt{n}) rate, the algorithm has to pick a particular learning rate. Larger or smaller learning rate (in the order of n) will lead to significantly worse regret. But in the experiments of the paper, the learning rates are not picked according to the theorem.\n\nOverall, this paper has a good motivation and good novelty. It could be further developed into a good paper. But due to the two problems and a buggy proof mentioned above, I think it is not ready for publish yet.\n"
  },
  {
    "people": [
      "Taylor",
      "Taylor"
    ],
    "review": "This paper adopts Taylor approximations of neural nets for separating convex and non-convex components of the optimization. This enables them to bound the training error by the Taylor optimum and regret (theorem 2). This is a nice theoretical result applicable to popular deep nets. The empirical studies back up the theoretical claim."
  },
  {
    "people": [
      "Taylor",
      "Taylor"
    ],
    "review": "It is interesting to derive such a bound and show it satisfies a regret bound along with empirical evidence on the CIFAR-10 for cross entropy loss and auto encoder for MSE loss. At least empirically, by comparing the observed training loss and taylor loss, the better a particular optimizer performs (training loss statement, not a validation or observed test statement) the smaller the difference between these two. Also shown is the regret loss is satisfied at different scales of the network, by layer, neuron and whole network. \n\nThe Taylor approximation can be used to investigate activation configurations of the network, and used to connect this to difficulty in optimizing  at kinks in the loss surface, along with an empirical study of exploration of activation surface of the SGD/Adam/RMSprop optimizers, the more exploration the better the resulting training loss.\n\nNot that it impacts the paper but the weaker performance of the SGD could be related to the fixed learning rate, if we anneal this learning rate, which should improve performance, does this translate to more exploration and tightening between the actual loss and the Taylor loss? \n\n- It might be useful to use a cross validation set for some of the empirical studies, in the end we would like to say something about generalization of the resulting network\n\n- Is there a reason the subscript on the Jacobian changes to a_l in the "
  },
  {
    "people": [
      "Taylor",
      "Taylor"
    ],
    "review": "It is interesting to derive such a bound and show it satisfies a regret bound along with empirical evidence on the CIFAR-10 for cross entropy loss and auto encoder for MSE loss. At least empirically, by comparing the observed training loss and taylor loss, the better a particular optimizer performs (training loss statement, not a validation or observed test statement) the smaller the difference between these two. Also shown is the regret loss is satisfied at different scales of the network, by layer, neuron and whole network. \n\nThe Taylor approximation can be used to investigate activation configurations of the network, and used to connect this to difficulty in optimizing  at kinks in the loss surface, along with an empirical study of exploration of activation surface of the SGD/Adam/RMSprop optimizers, the more exploration the better the resulting training loss.\n\nNot that it impacts the paper but the weaker performance of the SGD could be related to the fixed learning rate, if we anneal this learning rate, which should improve performance, does this translate to more exploration and tightening between the actual loss and the Taylor loss? \n\n- It might be useful to use a cross validation set for some of the empirical studies, in the end we would like to say something about generalization of the resulting network\n\n- Is there a reason the subscript on the Jacobian changes to a_l in the"
  },
  {
    "people": [
      "Taylor"
    ],
    "review": "This paper develops a theoretical guarantee for the convergence of the training error. The result is quite general that covers the training of a wide range of neural network models. The key idea of the paper is approximate the training loss by its linear approximation. Since its linearity in the variables (thus convex), the authors plug in results that has been developed in the literature of online learning. \n\nThis paper has good novelty in using the Taylor approximation thus greatly simplifying the analysis of the behaviour of the model. However, there are two problems about the main result of this paper, Theorem 2.\n\n1. It is not clear if the Taylor optimum would converge or not. \nAs noticed by the authors, the upper bound is path dependent. Appendix 3 tries to claim that this Taylor optimum indeed converges, but the proof is buggy. In the proof of Lemma 2, it is proved that the difference between two sequential Taylor optimum is approaching 0. Note that this is actually weaker than being Cauchy sequence and insufficient to guarantee convergence.\n\n2. The lefthand side of Equation (3) (I will denote it by L3 in this review) is not equivalent to training error. \nAn upper bound on this average error is not sufficient to guarantee the convergence of the training error neither. Take the gradient descent for example (thus each minibatch x_0^n is the whole training set), the convergence of the training error should be lim_{n -> \\infty} l(f_{w^n}(x_0^n), y^n). The convergence of L3 is necessary but not sufficient to imply the convergence of the training error.\n\nAnother concern about Theorem 2 (but it is minor compared to the two problems mentioned above) is that to achieve the O(1/\\sqrt{n}) rate, the algorithm has to pick a particular learning rate. Larger or smaller learning rate (in the order of n) will lead to significantly worse regret. But in the experiments of the paper, the learning rates are not picked according to the theorem.\n\nOverall, this paper has a good motivation and good novelty. It could be further developed into a good paper. But due to the two problems and a buggy proof mentioned above, I think it is not ready for publish yet.\n"
  },
  {
    "people": [
      "Taylor",
      "Taylor"
    ],
    "review": "This paper adopts Taylor approximations of neural nets for separating convex and non-convex components of the optimization. This enables them to bound the training error by the Taylor optimum and regret (theorem 2). This is a nice theoretical result applicable to popular deep nets. The empirical studies back up the theoretical claim."
  },
  {
    "people": [
      "Taylor",
      "Taylor"
    ],
    "review": "It is interesting to derive such a bound and show it satisfies a regret bound along with empirical evidence on the CIFAR-10 for cross entropy loss and auto encoder for MSE loss. At least empirically, by comparing the observed training loss and taylor loss, the better a particular optimizer performs (training loss statement, not a validation or observed test statement) the smaller the difference between these two. Also shown is the regret loss is satisfied at different scales of the network, by layer, neuron and whole network. \n\nThe Taylor approximation can be used to investigate activation configurations of the network, and used to connect this to difficulty in optimizing  at kinks in the loss surface, along with an empirical study of exploration of activation surface of the SGD/Adam/RMSprop optimizers, the more exploration the better the resulting training loss.\n\nNot that it impacts the paper but the weaker performance of the SGD could be related to the fixed learning rate, if we anneal this learning rate, which should improve performance, does this translate to more exploration and tightening between the actual loss and the Taylor loss? \n\n- It might be useful to use a cross validation set for some of the empirical studies, in the end we would like to say something about generalization of the resulting network\n\n- Is there a reason the subscript on the Jacobian changes to a_l in the "
  },
  {
    "people": [
      "W. Shang",
      "K. Sohn",
      "D. Almeida",
      "H. Lee",
      "S. Arora",
      "Y. Liang",
      "T. Ma",
      "R. Giryes",
      "A. M. Bronstein",
      "W. Shang"
    ],
    "review": "Summary:\nThe paper proposes to use Model-RIP, a simple concept from compressed sensing community, to theoretically describe a invertibility of a single convolution + max-pool operation (i.e., without non-linear activation function) assuming that the convolution filters are drawn from random Gaussian distribution. In the high level, they attempted to show that convolution weights consisting of random filters form a roughly orthogonal matrix, hence its transpose times the activation roughly reconstruct the input. Empirically, this submission tries to verify the model-RIP property of learned CNN filters by computing the norm ratio between the activation and the weight transpose multiplying the activation, and verify the reconstruction bounds by computing the relative errors between the input and the reconstructed input through the proposed reconstruction algorithm. Qualitatively, they also try to invert the activation to the previous layer output through the same algorithm then followed up by a learned decoder to reconstruct the raw input.\n\n\nHowever, there are many flaws in their assumptions but little useful consequence from enforcing CNNs to their mathematical model. The most evident flaws in their theorems are (1) the weight matrix of learned CNNs does not meet their model-RIP assumption (i.e., low coherence) and (2) even without the first flaw, their reconstruction bound doesn\u2019t hold analytically with ReLU non-linearity: RIP can only bound the norm of the transposed weights times the activation (see Eq. 1) instead of providing enough conditions for a metric-preserving property between input space and activation space [3] since ReLU zeroes out the negative activations, i.e., the activation can be arbitrarily close to zero. Furthermore, the presentations of their experimental results are very misleading and self-contradicting: a pre-trained decoder with switch units is responsible for most of the reconstruction and they only applied their proposed recon algorithm on one layer; the \u201crelative error\u201d does not give reasonable measurement of the reconstruction property, especially in the pixel space; the norm ratio ||W^Tc||/||c|| is also an inappropriate measurement for model-RIP condition, not to mention the discrepancy between the presented norm ratio and the learned filter\u2019s coherence level (if ||W^Tc||/||c|| is close to 1, coherence should be close to 0, but it is actually much larger than 0).  \n\n\n[1] W. Shang, K. Sohn, D. Almeida, and H. Lee. Understanding and improving convolutional neural networks via concatenated rectified linear units. In ICML, 2016.\u2028\n[2] S. Arora, Y. Liang, and T. Ma. Why are deep nets reversible: A simple theory, with implications for training.\n[3] R. Giryes, G. Sapiro, and A. M. Bronstein. Deep neural networks with random gaussian weights: A universal classification strategy? IEEE Transactions on Signal Processing, 2016.\u2028\n[4] A. Coates, and A. Ng. The importance of encoding versus training with sparse coding and vector quantization. In ICML, 2015. \n[5] W. Shang. A Preliminary Study of the Norm Preservation Properties of CNN\n. In WiML Workshop, 2015. ("
  },
  {
    "people": [
      "W. Shang",
      "K. Sohn",
      "D. Almeida",
      "H. Lee",
      "S. Arora",
      "Y. Liang",
      "T. Ma",
      "R. Giryes",
      "A. M. Bronstein",
      "W. Shang"
    ],
    "review": "Summary:\nThe paper proposes to use Model-RIP, a simple concept from compressed sensing community, to theoretically describe a invertibility of a single convolution + max-pool operation (i.e., without non-linear activation function) assuming that the convolution filters are drawn from random Gaussian distribution. In the high level, they attempted to show that convolution weights consisting of random filters form a roughly orthogonal matrix, hence its transpose times the activation roughly reconstruct the input. Empirically, this submission tries to verify the model-RIP property of learned CNN filters by computing the norm ratio between the activation and the weight transpose multiplying the activation, and verify the reconstruction bounds by computing the relative errors between the input and the reconstructed input through the proposed reconstruction algorithm. Qualitatively, they also try to invert the activation to the previous layer output through the same algorithm then followed up by a learned decoder to reconstruct the raw input.\n\n\nHowever, there are many flaws in their assumptions but little useful consequence from enforcing CNNs to their mathematical model. The most evident flaws in their theorems are (1) the weight matrix of learned CNNs does not meet their model-RIP assumption (i.e., low coherence) and (2) even without the first flaw, their reconstruction bound doesn\u2019t hold analytically with ReLU non-linearity: RIP can only bound the norm of the transposed weights times the activation (see Eq. 1) instead of providing enough conditions for a metric-preserving property between input space and activation space [3] since ReLU zeroes out the negative activations, i.e., the activation can be arbitrarily close to zero. Furthermore, the presentations of their experimental results are very misleading and self-contradicting: a pre-trained decoder with switch units is responsible for most of the reconstruction and they only applied their proposed recon algorithm on one layer; the \u201crelative error\u201d does not give reasonable measurement of the reconstruction property, especially in the pixel space; the norm ratio ||W^Tc||/||c|| is also an inappropriate measurement for model-RIP condition, not to mention the discrepancy between the presented norm ratio and the learned filter\u2019s coherence level (if ||W^Tc||/||c|| is close to 1, coherence should be close to 0, but it is actually much larger than 0).  \n\n\n[1] W. Shang, K. Sohn, D. Almeida, and H. Lee. Understanding and improving convolutional neural networks via concatenated rectified linear units. In ICML, 2016.\u2028\n[2] S. Arora, Y. Liang, and T. Ma. Why are deep nets reversible: A simple theory, with implications for training.\n[3] R. Giryes, G. Sapiro, and A. M. Bronstein. Deep neural networks with random gaussian weights: A universal classification strategy? IEEE Transactions on Signal Processing, 2016.\u2028\n[4] A. Coates, and A. Ng. The importance of encoding versus training with sparse coding and vector quantization. In ICML, 2015. \n[5] W. Shang. A Preliminary Study of the Norm Preservation Properties of CNN\n. In WiML Workshop, 2015. ("
  },
  {
    "people": [
      "Salakhutdinov",
      "Hinton"
    ],
    "review": "This work proposes a model that can learn short binary codes via paragraph vectors to allow fast retrieval of documents. The experiments show that this is superior to semantic hashing. The approach is simple and not very technically interesting. For a code size of 128, the loss compared to a continuous paragraph vector seems moderate.\n\nThe paper asks the reader to refer to the Salakhutdinov and Hinton paper for the baseline numbers but I think they should be placed in the paper for easy reference. For simplicity, the paper could show the precision at 12.5%, 25% and 50% recall for the proposed model and semantic hashing. It also seems that the semantic hashing paper shows results on RCV2 and not RCV1. RCV1 is twice the size of RCV2 and is English only so it seems that these results are not comparable. It would be interesting to see how many binary bits are required to match the performance of the continuous representation. A comparison to the continuous PV-DBOW trained with bigrams would also make it a more fair comparison.\n\nFigure 7 in the paper shows a loss from using the real-binary PV-DBOW. It seems that if a user needed high quality ranking after the retrieval stage and they could afford the extra space and computation, then it would be better for them to use a standard PV-DBOW to obtain the continuous representation at that stage.\n\nMinor comments:\nFirst line after the introduction: is sheer -> is the sheer\n4th line from the bottom of P1: words embeddings -> word embeddings\nIn table 1: What does code size refer to for PV-DBOW? Is this the number of elements in the continuous vector?\n5th line from the bottom of P5: W -> We\n5th line after section 3.1: covers wide -> covers a wide"
  },
  {
    "people": [
      "Le",
      "Le",
      "Lin",
      "Salakhutdinov",
      "Hinton"
    ],
    "review": "The method in this paper introduces a binary encoding level in the PV-DBOW and PV-DM document embedding methods (from Le & Mikolov'14). The binary encoding consists in a sigmoid with trained parameters that is inserted after the standard training stage of the embedding.\n \nFor a document to encode, the binary vector is obtained by forcing the sigmoid to output a binary output for each of the embedding vector components. The binary vector can then be used for compact storage and fast comparison of documents.\n \nPros:\n \n- the binary representation outperforms the Semantic hashing method from Salakhutdinov & Hinton '09\n \n- the experimental approach sound: they compare on the same experimental setup as Salakhutdinov & Hinton '09, but since in the meantime document representations improved (Le & Mikolov'14), they also combine this new representation with an RBM to show the benefit of their binary PV-DBOW/PV-DM\n \nCons:\n \n- the insertion of the sigmoid to produce binary codes (from Lin & al. '15) in the training process is incremental\n \n- the explanation is too abstract and difficult to follow for a non-expert (see details below)\n \n- a comparison with efficient indexing methods used in image retrieval is missing. For large-scale indexing of embedding vectors, derivations of the Inverted multi-index are probably more interesting than binary codes. See eg. Babenko & Lempitsky, Efficient Indexing of Billion-Scale Datasets of Deep Descriptors, CVPR'16\n \nDetailed comments:\n \nSection 1: the motivation for producing binary codes is not given. Also, the experimental section could give some timings and mem usage numbers to show the benefit of binary embeddings\n \nfigure 1, 2, 3: there is enough space to include more information on the representation of the model: model parameters + training objective + characteristic sizes + dropout. In particular, in fig 2, it is not clear why \"embedding lookup\" and \"linear projection\" cannot be merged in a single smaller lookup table (presumably because there is an intermediate training objective that prevents this).\n \np2: \"This way, the length of binary codes is not tied to the dimensionality of word embeddings.\" -> why not?\n \nsection 3: This is the experimental setup of  Salakhutdinov & Hinton 2009. Specify this and whether there is any difference between the setups.\n \n\"similarity of the inferred codes\": say here that codes are compared using Hamming distances.\n \n\"binary codes perform very well, despite their far lower capacity\" -> do you mean smaller size than real vectors?\n \nfig 5: these plots could be dropped if space is needed.\n \nsection 3.1: one could argue that \"transferring\" from Wikipedia to anything else cannot be called transferring, since Wikipedia's purpose is to include all topics and lexical domains\n \nsection 3.2: specify how the 300D real vectors are compared. L2 distance? inner product?\n \nfig4: specify what the raw performance of the large embedding vectors is (without pre-filtering with binary codes), or equivalently, the perf of (code-size, Hamming dis) = (28, 28), (24, 24), etc.\n"
  },
  {
    "people": [
      "Krizhevsky",
      "Kulis",
      "Darrell",
      "Gong",
      "Norouzi",
      "Fleet",
      "Bengio",
      "Norouzi"
    ],
    "review": "This paper presents a method to represent text documents and paragraphs as short binary codes to allow fast similarity search and retrieval by using hashing techniques. The real-valued paragraph vectors by Le & Mikolov is extended by adding a stochastic binary layer on top of the neural network architecture. Two methods for binarizing the final activations are compared: (1) simply adding noise to sigmoid activations to encourage discritization. (2) binarizing the activations in the forward pass and keeping them real-valued in the backward pass (straight-through estimation). The paper presents encouraging results by using straight-through estimation on 20 newsgroup and RCV1 text datasets by using 128 and 32 bit binary codes.\n\nOn the plus side, the application presented in the paper is interesting and important. The exposition of the paper is clean and clear. However, the novelty of the approach is limited from a machine learning standpoint. The literature on binary hashing beyond semantic hashing and Krizhevsky's binary autoencoders in 2011 is not explained. An important baseline is missing where real-valued paragraph vectors are learned first, and then converted to binary codes using off-the-shelf hashing methods (e.g. random projection LSH by Charikar, BRE by Kulis & Darrell, ITQ by Gong & Lazebnik, MLH by Norouzi & Fleet, etc.)\n\nGiven the lack of novelty and the missing baseline, I do not recommend this paper in its current for publication in the ICLR conference's proceeding. Moving forward, this paper may be more suitable for NLP conferences as it is more on the applied side.\n\nMore comments:\n- I believe from an practical perspective it may be easier to first learn real-valued paragraph vectors and then quantize them for indexing. That said, an end-to-end approach as proposed in this paper may perform better. I would like to see an empirical comparison between the proposed end-to-end approach and a simpler two stage quantization method suggested here.\n- See \"Estimating or Propagating Gradients Through Stochastic Neurons\" By Bengio et al - discussing straight through estimation and some other alternatives.\n- The paper argues that the length of binary codes cannot be longer than 32 bits because longer codes are not suitable for document hashing. This is not quite right given multi-probe hashing mechanisms, for example see \"Mult-index Hashing\" by Norouzi et al.\n- See \"Hashing for Similarity Search: A Survey\" by Wang et al. for a survey of related work on binary hashing and quantization. You seem to ignore the extensive work done on binary hashing.\n"
  },
  {
    "people": [
      "Salakhutdinov",
      "Hinton"
    ],
    "review": "This work proposes a model that can learn short binary codes via paragraph vectors to allow fast retrieval of documents. The experiments show that this is superior to semantic hashing. The approach is simple and not very technically interesting. For a code size of 128, the loss compared to a continuous paragraph vector seems moderate.\n\nThe paper asks the reader to refer to the Salakhutdinov and Hinton paper for the baseline numbers but I think they should be placed in the paper for easy reference. For simplicity, the paper could show the precision at 12.5%, 25% and 50% recall for the proposed model and semantic hashing. It also seems that the semantic hashing paper shows results on RCV2 and not RCV1. RCV1 is twice the size of RCV2 and is English only so it seems that these results are not comparable. It would be interesting to see how many binary bits are required to match the performance of the continuous representation. A comparison to the continuous PV-DBOW trained with bigrams would also make it a more fair comparison.\n\nFigure 7 in the paper shows a loss from using the real-binary PV-DBOW. It seems that if a user needed high quality ranking after the retrieval stage and they could afford the extra space and computation, then it would be better for them to use a standard PV-DBOW to obtain the continuous representation at that stage.\n\nMinor comments:\nFirst line after the introduction: is sheer -> is the sheer\n4th line from the bottom of P1: words embeddings -> word embeddings\nIn table 1: What does code size refer to for PV-DBOW? Is this the number of elements in the continuous vector?\n5th line from the bottom of P5: W -> We\n5th line after section 3.1: covers wide -> covers a wide\n"
  },
  {
    "people": [
      "Salakhutdinov",
      "Hinton"
    ],
    "review": "This work proposes a model that can learn short binary codes via paragraph vectors to allow fast retrieval of documents. The experiments show that this is superior to semantic hashing. The approach is simple and not very technically interesting. For a code size of 128, the loss compared to a continuous paragraph vector seems moderate.\n\nThe paper asks the reader to refer to the Salakhutdinov and Hinton paper for the baseline numbers but I think they should be placed in the paper for easy reference. For simplicity, the paper could show the precision at 12.5%, 25% and 50% recall for the proposed model and semantic hashing. It also seems that the semantic hashing paper shows results on RCV2 and not RCV1. RCV1 is twice the size of RCV2 and is English only so it seems that these results are not comparable. It would be interesting to see how many binary bits are required to match the performance of the continuous representation. A comparison to the continuous PV-DBOW trained with bigrams would also make it a more fair comparison.\n\nFigure 7 in the paper shows a loss from using the real-binary PV-DBOW. It seems that if a user needed high quality ranking after the retrieval stage and they could afford the extra space and computation, then it would be better for them to use a standard PV-DBOW to obtain the continuous representation at that stage.\n\nMinor comments:\nFirst line after the introduction: is sheer -> is the sheer\n4th line from the bottom of P1: words embeddings -> word embeddings\nIn table 1: What does code size refer to for PV-DBOW? Is this the number of elements in the continuous vector?\n5th line from the bottom of P5: W -> We\n5th line after section 3.1: covers wide -> covers a wide"
  },
  {
    "people": [
      "Le",
      "Le",
      "Lin",
      "Salakhutdinov",
      "Hinton"
    ],
    "review": "The method in this paper introduces a binary encoding level in the PV-DBOW and PV-DM document embedding methods (from Le & Mikolov'14). The binary encoding consists in a sigmoid with trained parameters that is inserted after the standard training stage of the embedding.\n \nFor a document to encode, the binary vector is obtained by forcing the sigmoid to output a binary output for each of the embedding vector components. The binary vector can then be used for compact storage and fast comparison of documents.\n \nPros:\n \n- the binary representation outperforms the Semantic hashing method from Salakhutdinov & Hinton '09\n \n- the experimental approach sound: they compare on the same experimental setup as Salakhutdinov & Hinton '09, but since in the meantime document representations improved (Le & Mikolov'14), they also combine this new representation with an RBM to show the benefit of their binary PV-DBOW/PV-DM\n \nCons:\n \n- the insertion of the sigmoid to produce binary codes (from Lin & al. '15) in the training process is incremental\n \n- the explanation is too abstract and difficult to follow for a non-expert (see details below)\n \n- a comparison with efficient indexing methods used in image retrieval is missing. For large-scale indexing of embedding vectors, derivations of the Inverted multi-index are probably more interesting than binary codes. See eg. Babenko & Lempitsky, Efficient Indexing of Billion-Scale Datasets of Deep Descriptors, CVPR'16\n \nDetailed comments:\n \nSection 1: the motivation for producing binary codes is not given. Also, the experimental section could give some timings and mem usage numbers to show the benefit of binary embeddings\n \nfigure 1, 2, 3: there is enough space to include more information on the representation of the model: model parameters + training objective + characteristic sizes + dropout. In particular, in fig 2, it is not clear why \"embedding lookup\" and \"linear projection\" cannot be merged in a single smaller lookup table (presumably because there is an intermediate training objective that prevents this).\n \np2: \"This way, the length of binary codes is not tied to the dimensionality of word embeddings.\" -> why not?\n \nsection 3: This is the experimental setup of  Salakhutdinov & Hinton 2009. Specify this and whether there is any difference between the setups.\n \n\"similarity of the inferred codes\": say here that codes are compared using Hamming distances.\n \n\"binary codes perform very well, despite their far lower capacity\" -> do you mean smaller size than real vectors?\n \nfig 5: these plots could be dropped if space is needed.\n \nsection 3.1: one could argue that \"transferring\" from Wikipedia to anything else cannot be called transferring, since Wikipedia's purpose is to include all topics and lexical domains\n \nsection 3.2: specify how the 300D real vectors are compared. L2 distance? inner product?\n \nfig4: specify what the raw performance of the large embedding vectors is (without pre-filtering with binary codes), or equivalently, the perf of (code-size, Hamming dis) = (28, 28), (24, 24), etc.\n"
  },
  {
    "people": [
      "Krizhevsky",
      "Kulis",
      "Darrell",
      "Gong",
      "Norouzi",
      "Fleet",
      "Bengio",
      "Norouzi"
    ],
    "review": "This paper presents a method to represent text documents and paragraphs as short binary codes to allow fast similarity search and retrieval by using hashing techniques. The real-valued paragraph vectors by Le & Mikolov is extended by adding a stochastic binary layer on top of the neural network architecture. Two methods for binarizing the final activations are compared: (1) simply adding noise to sigmoid activations to encourage discritization. (2) binarizing the activations in the forward pass and keeping them real-valued in the backward pass (straight-through estimation). The paper presents encouraging results by using straight-through estimation on 20 newsgroup and RCV1 text datasets by using 128 and 32 bit binary codes.\n\nOn the plus side, the application presented in the paper is interesting and important. The exposition of the paper is clean and clear. However, the novelty of the approach is limited from a machine learning standpoint. The literature on binary hashing beyond semantic hashing and Krizhevsky's binary autoencoders in 2011 is not explained. An important baseline is missing where real-valued paragraph vectors are learned first, and then converted to binary codes using off-the-shelf hashing methods (e.g. random projection LSH by Charikar, BRE by Kulis & Darrell, ITQ by Gong & Lazebnik, MLH by Norouzi & Fleet, etc.)\n\nGiven the lack of novelty and the missing baseline, I do not recommend this paper in its current for publication in the ICLR conference's proceeding. Moving forward, this paper may be more suitable for NLP conferences as it is more on the applied side.\n\nMore comments:\n- I believe from an practical perspective it may be easier to first learn real-valued paragraph vectors and then quantize them for indexing. That said, an end-to-end approach as proposed in this paper may perform better. I would like to see an empirical comparison between the proposed end-to-end approach and a simpler two stage quantization method suggested here.\n- See \"Estimating or Propagating Gradients Through Stochastic Neurons\" By Bengio et al - discussing straight through estimation and some other alternatives.\n- The paper argues that the length of binary codes cannot be longer than 32 bits because longer codes are not suitable for document hashing. This is not quite right given multi-probe hashing mechanisms, for example see \"Mult-index Hashing\" by Norouzi et al.\n- See \"Hashing for Similarity Search: A Survey\" by Wang et al. for a survey of related work on binary hashing and quantization. You seem to ignore the extensive work done on binary hashing.\n"
  },
  {
    "people": [
      "Salakhutdinov",
      "Hinton"
    ],
    "review": "This work proposes a model that can learn short binary codes via paragraph vectors to allow fast retrieval of documents. The experiments show that this is superior to semantic hashing. The approach is simple and not very technically interesting. For a code size of 128, the loss compared to a continuous paragraph vector seems moderate.\n\nThe paper asks the reader to refer to the Salakhutdinov and Hinton paper for the baseline numbers but I think they should be placed in the paper for easy reference. For simplicity, the paper could show the precision at 12.5%, 25% and 50% recall for the proposed model and semantic hashing. It also seems that the semantic hashing paper shows results on RCV2 and not RCV1. RCV1 is twice the size of RCV2 and is English only so it seems that these results are not comparable. It would be interesting to see how many binary bits are required to match the performance of the continuous representation. A comparison to the continuous PV-DBOW trained with bigrams would also make it a more fair comparison.\n\nFigure 7 in the paper shows a loss from using the real-binary PV-DBOW. It seems that if a user needed high quality ranking after the retrieval stage and they could afford the extra space and computation, then it would be better for them to use a standard PV-DBOW to obtain the continuous representation at that stage.\n\nMinor comments:\nFirst line after the introduction: is sheer -> is the sheer\n4th line from the bottom of P1: words embeddings -> word embeddings\nIn table 1: What does code size refer to for PV-DBOW? Is this the number of elements in the continuous vector?\n5th line from the bottom of P5: W -> We\n5th line after section 3.1: covers wide -> covers a wide\n"
  },
  {
    "people": [
      "Cantor"
    ],
    "review": "This work introduces a novel method for training GANs by displacing simultaneous SGD, and unrolling the inner optimization in the minmax game as a computational graph. The paper is very clearly written, and explains the justification very well. The problem being attacked is very significant and important. The approach is novel, however, similar ideas have been tried to solve problems unrelated to GANs.\n\nThe first quantitative experiment is section 3.3.1, where the authors attempt to find the best z which can generate training examples. This is done by using L-BFGS on |G(z) - x|. The claim is that if we're able to find such a z, then the generator can generate this particular training example. It's demonstrated that 0-step GANs are not able to generate many training examples, while unrolled GANs do. However, I find this experiment unreasonable. Being able to find a certain z, which generates a certain sample does not guarantee that this particular mode is high probability. In fact, an identity function can potentially beat all the GAN models in the proposed metric. And due to Cantor's proof of equivalence between all powers of real spaces, this applies to smaller dimension of z as well. More realistically, it should be possible to generate *any* image from a generator by finding a very specific z. That a certain z exists which can generate a sample does not prove that the generator is not missing modes. It just proves that the generator is similar enough to an identity function to be able to generate any possible image. This metric is thus measuring something potentially tangential to diversity or mode-dropping. Another problem with this metric is that that showing that the optimization is not able to find a z for a specific training examples does not prove that such a z does not exist, only that it's harder to find. So, this comparison might just be showing that unrolled GANs have a smoother function than 0-step GANs, and thus easier to optimize for z.\n\nThe second quantitative experiment considers mean pairwise distance between generated samples, and between data samples. The first number is likely to be small in the case of a mode-dropping GAN. The authors argue that the two numbers being closer to each other is an indication of the generated samples being as diverse as the data. Once again, this metric is not convincing. 1. The distances are being measured in pixel-space. 2. A GAN model could be generating garbage, and yet still perform very well in this metric.\n\nThere are no other quantitative results in the paper. Even though the method is optimizing diversity, for a sanity check, scores for quality such as Inception scores or SSL performance would have been useful. Another metric that the authors can consider is training GAN using this approach on the tri-MNIST dataset (concatenation of 3 MNIST digits), which results in 1000 easily-identifiable modes. Then, demonstrate that the GAN is able to generate all the 1000 modes with equal probability. This is not a perfect metric either, but arguably much better than the metrics in this paper. This metric is used in this ICLR submission: "
  },
  {
    "people": [
      "Cantor"
    ],
    "review": "This work introduces a novel method for training GANs by displacing simultaneous SGD, and unrolling the inner optimization in the minmax game as a computational graph. The paper is very clearly written, and explains the justification very well. The problem being attacked is very significant and important. The approach is novel, however, similar ideas have been tried to solve problems unrelated to GANs.\n\nThe first quantitative experiment is section 3.3.1, where the authors attempt to find the best z which can generate training examples. This is done by using L-BFGS on |G(z) - x|. The claim is that if we're able to find such a z, then the generator can generate this particular training example. It's demonstrated that 0-step GANs are not able to generate many training examples, while unrolled GANs do. However, I find this experiment unreasonable. Being able to find a certain z, which generates a certain sample does not guarantee that this particular mode is high probability. In fact, an identity function can potentially beat all the GAN models in the proposed metric. And due to Cantor's proof of equivalence between all powers of real spaces, this applies to smaller dimension of z as well. More realistically, it should be possible to generate *any* image from a generator by finding a very specific z. That a certain z exists which can generate a sample does not prove that the generator is not missing modes. It just proves that the generator is similar enough to an identity function to be able to generate any possible image. This metric is thus measuring something potentially tangential to diversity or mode-dropping. Another problem with this metric is that that showing that the optimization is not able to find a z for a specific training examples does not prove that such a z does not exist, only that it's harder to find. So, this comparison might just be showing that unrolled GANs have a smoother function than 0-step GANs, and thus easier to optimize for z.\n\nThe second quantitative experiment considers mean pairwise distance between generated samples, and between data samples. The first number is likely to be small in the case of a mode-dropping GAN. The authors argue that the two numbers being closer to each other is an indication of the generated samples being as diverse as the data. Once again, this metric is not convincing. 1. The distances are being measured in pixel-space. 2. A GAN model could be generating garbage, and yet still perform very well in this metric.\n\nThere are no other quantitative results in the paper. Even though the method is optimizing diversity, for a sanity check, scores for quality such as Inception scores or SSL performance would have been useful. Another metric that the authors can consider is training GAN using this approach on the tri-MNIST dataset (concatenation of 3 MNIST digits), which results in 1000 easily-identifiable modes. Then, demonstrate that the GAN is able to generate all the 1000 modes with equal probability. This is not a perfect metric either, but arguably much better than the metrics in this paper. This metric is used in this ICLR submission: "
  },
  {
    "people": [
      "Rezende"
    ],
    "review": "The authors propose to sample from VAEs through a Markov chain [z_t ~ q(z|x=x_{t-1}), x_t ~ p(x|z=z_t)]. The paper uses confusing notation, oversells the novelty, ignoring some relevant previous results. The qualitative difference between regular sampling and this Gibbs chain is not very convincing, judging from the figures. It would be a great workshop paper (perhaps more), if the authors fix the notation, fix the discussion to related work, and produce more convincing (perhaps simply upscaled?) figures.\n\nComments: \n - Rezende et al's (2014) original VAE paper already discusses the Markov chain, which is ignored in this paper\n - Notation is nonstandard / confusing. At page 1, it\u2019s unclear what the authors mean with \u201cp(x|z) which is approximated as q(x|z)\u201d.\n- It\u2019s also not clear what\u2019s meant with q(z). At page 2, q(z) is called the learned distribution, while p(z) can in general also be a learned distribution.\n- It\u2019s not true that it\u2019s impossible to draw samples from q(z): one can sample x ~ q(x) from the dataset, then draw z ~ q(z|x).\n- It's not explained whether the analysis only applies to continuous observed spaces, or also discrete observed spaces\n- Figures 3 and 4 are not very convincing."
  },
  {
    "people": [
      "Rezende",
      "Bengio"
    ],
    "review": "This approach taken in this paper is topical, especially since the importance of sampling and generating diverse samples is increasingly discussed in work on generative models. There were several concerns from reviewers, in three areas particularly: connection and comparison to related work; lack of clarity and understanding of the paper; experiments that are not sufficiently convincing. These have been addressed to some extent by the authors, discussing in more detail the related work, especially in connection to Rezende et al., and GSN of Bengio et al., and with improved figures. But these points are still of concern especially in terms of assessing sample diversity in relation to much of the recent work on richer variational posterior methods and other techniques. For these reasons, the paper is not yet ready for acceptance at this years conference."
  },
  {
    "people": [
      "Rezende"
    ],
    "review": "The authors propose to sample from VAEs through a Markov chain [z_t ~ q(z|x=x_{t-1}), x_t ~ p(x|z=z_t)]. The paper uses confusing notation, oversells the novelty, ignoring some relevant previous results. The qualitative difference between regular sampling and this Gibbs chain is not very convincing, judging from the figures. It would be a great workshop paper (perhaps more), if the authors fix the notation, fix the discussion to related work, and produce more convincing (perhaps simply upscaled?) figures.\n\nComments: \n - Rezende et al's (2014) original VAE paper already discusses the Markov chain, which is ignored in this paper\n - Notation is nonstandard / confusing. At page 1, it\u2019s unclear what the authors mean with \u201cp(x|z) which is approximated as q(x|z)\u201d.\n- It\u2019s also not clear what\u2019s meant with q(z). At page 2, q(z) is called the learned distribution, while p(z) can in general also be a learned distribution.\n- It\u2019s not true that it\u2019s impossible to draw samples from q(z): one can sample x ~ q(x) from the dataset, then draw z ~ q(z|x).\n- It's not explained whether the analysis only applies to continuous observed spaces, or also discrete observed spaces\n- Figures 3 and 4 are not very convincing.\n"
  },
  {
    "people": [
      "Rezende",
      "Rezende"
    ],
    "review": "While you cite Rezende et al. 2014 when referring to VAEs, you claim the main contribution of your work is the generation of posterior samples from a Markov Chain. However, Rezende et al. 2014 presented a very similar idea. Let me quote them directly:\n\nWe do not integrate over the missing\nvalues, but use a procedure that simulates a Markov\nchain that we show converges to the true marginal distribution\nof missing given observed pixels.\n\n-- Section 5.4\n\nImage completion can be approximatively achieved by\na simple iterative procedure which consists of (i) initializing\nthe non-observed pixels with random values;\n(ii) sampling from the recognition distribution given\nthe resulting image; (iii) reconstruct the image given\nthe sample from the recognition model; (iv) iterate the\nprocedure.\n\n-- Appendix F\n\nHow is this procedure different from your main contribution? I mean, they motivate with missing data imputation and start in a different way, but the main loop seems the same. Even if there is a significant difference between the procedures, I believe this should be addressed in the paper. I also suggest taking a closer look at appendix F, they provide some proofs which seem relevant to your work.\n"
  },
  {
    "people": [
      "Fischer"
    ],
    "review": "How are the novelty of samples affected as more samples are generated from the Markov Chain? I played around with a similar idea and found that after about 6-7 Monte Carlo samples, the images looked identical to the training data. Do you observe something similar in your experiments?\n\nAlso, the interpolation experiments are interesting. It seems that the proposed method implicitly interpolates along the Fischer metric rather than the Euclidean metric. Some discussion on this might be illuminating. "
  },
  {
    "people": [
      "Rezende"
    ],
    "review": "The authors propose to sample from VAEs through a Markov chain [z_t ~ q(z|x=x_{t-1}), x_t ~ p(x|z=z_t)]. The paper uses confusing notation, oversells the novelty, ignoring some relevant previous results. The qualitative difference between regular sampling and this Gibbs chain is not very convincing, judging from the figures. It would be a great workshop paper (perhaps more), if the authors fix the notation, fix the discussion to related work, and produce more convincing (perhaps simply upscaled?) figures.\n\nComments: \n - Rezende et al's (2014) original VAE paper already discusses the Markov chain, which is ignored in this paper\n - Notation is nonstandard / confusing. At page 1, it\u2019s unclear what the authors mean with \u201cp(x|z) which is approximated as q(x|z)\u201d.\n- It\u2019s also not clear what\u2019s meant with q(z). At page 2, q(z) is called the learned distribution, while p(z) can in general also be a learned distribution.\n- It\u2019s not true that it\u2019s impossible to draw samples from q(z): one can sample x ~ q(x) from the dataset, then draw z ~ q(z|x).\n- It's not explained whether the analysis only applies to continuous observed spaces, or also discrete observed spaces\n- Figures 3 and 4 are not very convincing."
  },
  {
    "people": [
      "Rezende",
      "Bengio"
    ],
    "review": "This approach taken in this paper is topical, especially since the importance of sampling and generating diverse samples is increasingly discussed in work on generative models. There were several concerns from reviewers, in three areas particularly: connection and comparison to related work; lack of clarity and understanding of the paper; experiments that are not sufficiently convincing. These have been addressed to some extent by the authors, discussing in more detail the related work, especially in connection to Rezende et al., and GSN of Bengio et al., and with improved figures. But these points are still of concern especially in terms of assessing sample diversity in relation to much of the recent work on richer variational posterior methods and other techniques. For these reasons, the paper is not yet ready for acceptance at this years conference."
  },
  {
    "people": [
      "Rezende"
    ],
    "review": "The authors propose to sample from VAEs through a Markov chain [z_t ~ q(z|x=x_{t-1}), x_t ~ p(x|z=z_t)]. The paper uses confusing notation, oversells the novelty, ignoring some relevant previous results. The qualitative difference between regular sampling and this Gibbs chain is not very convincing, judging from the figures. It would be a great workshop paper (perhaps more), if the authors fix the notation, fix the discussion to related work, and produce more convincing (perhaps simply upscaled?) figures.\n\nComments: \n - Rezende et al's (2014) original VAE paper already discusses the Markov chain, which is ignored in this paper\n - Notation is nonstandard / confusing. At page 1, it\u2019s unclear what the authors mean with \u201cp(x|z) which is approximated as q(x|z)\u201d.\n- It\u2019s also not clear what\u2019s meant with q(z). At page 2, q(z) is called the learned distribution, while p(z) can in general also be a learned distribution.\n- It\u2019s not true that it\u2019s impossible to draw samples from q(z): one can sample x ~ q(x) from the dataset, then draw z ~ q(z|x).\n- It's not explained whether the analysis only applies to continuous observed spaces, or also discrete observed spaces\n- Figures 3 and 4 are not very convincing.\n"
  },
  {
    "people": [
      "Rezende",
      "Rezende"
    ],
    "review": "While you cite Rezende et al. 2014 when referring to VAEs, you claim the main contribution of your work is the generation of posterior samples from a Markov Chain. However, Rezende et al. 2014 presented a very similar idea. Let me quote them directly:\n\nWe do not integrate over the missing\nvalues, but use a procedure that simulates a Markov\nchain that we show converges to the true marginal distribution\nof missing given observed pixels.\n\n-- Section 5.4\n\nImage completion can be approximatively achieved by\na simple iterative procedure which consists of (i) initializing\nthe non-observed pixels with random values;\n(ii) sampling from the recognition distribution given\nthe resulting image; (iii) reconstruct the image given\nthe sample from the recognition model; (iv) iterate the\nprocedure.\n\n-- Appendix F\n\nHow is this procedure different from your main contribution? I mean, they motivate with missing data imputation and start in a different way, but the main loop seems the same. Even if there is a significant difference between the procedures, I believe this should be addressed in the paper. I also suggest taking a closer look at appendix F, they provide some proofs which seem relevant to your work.\n"
  },
  {
    "people": [
      "Fischer"
    ],
    "review": "How are the novelty of samples affected as more samples are generated from the Markov Chain? I played around with a similar idea and found that after about 6-7 Monte Carlo samples, the images looked identical to the training data. Do you observe something similar in your experiments?\n\nAlso, the interpolation experiments are interesting. It seems that the proposed method implicitly interpolates along the Fischer metric rather than the Euclidean metric. Some discussion on this might be illuminating. "
  },
  {
    "people": [
      "Dauphin",
      "Amari"
    ],
    "review": "This paper investigates the hessian of small deep networks near the end of training. The main result is that many eigenvalues are approximately zero, such that the Hessian is highly singular, which means that a wide amount of theory does not apply.\n\nThe overall point that deep learning algorithms are singular, and that this undercuts many theoretical results, is important but it has already been made: Watanabe. \u201cAlmost All Learning Machines are Singular\u201d, FOCI 2007. This is one paper in a growing body of work investigating this phenomenon. In general, the references for this paper could be fleshed out much further\u2014a variety of prior work has examined the Hessian in deep learning, e.g., Dauphin et al. \u201cIdentifying and attacking the saddle point problem in high dimensional non-convex optimization\u201d NIPS 2014 or the work of Amari and others.\n\nExperimentally, it is hard to tell how results from the small sized networks considered here might translate to much larger networks. It seems likely that the behavior for much larger networks would be different. A reason for optimism, though, is the fact that a clear bulk/outlier behavior emerges even in these networks. Characterizing this behavior for simple systems is valuable. Overall, the results feel preliminary but likely to be of interest when further fleshed out.\n\nThis paper is attacking an important problem, but should do a better job situating itself in the related literature and undertaking experiments of sufficient size to reveal large-scale behavior relevant to practice.\n"
  },
  {
    "people": [
      "Dauphin",
      "Amari"
    ],
    "review": "This paper investigates the hessian of small deep networks near the end of training. The main result is that many eigenvalues are approximately zero, such that the Hessian is highly singular, which means that a wide amount of theory does not apply.\n\nThe overall point that deep learning algorithms are singular, and that this undercuts many theoretical results, is important but it has already been made: Watanabe. \u201cAlmost All Learning Machines are Singular\u201d, FOCI 2007. This is one paper in a growing body of work investigating this phenomenon. In general, the references for this paper could be fleshed out much further\u2014a variety of prior work has examined the Hessian in deep learning, e.g., Dauphin et al. \u201cIdentifying and attacking the saddle point problem in high dimensional non-convex optimization\u201d NIPS 2014 or the work of Amari and others.\n\nExperimentally, it is hard to tell how results from the small sized networks considered here might translate to much larger networks. It seems likely that the behavior for much larger networks would be different. A reason for optimism, though, is the fact that a clear bulk/outlier behavior emerges even in these networks. Characterizing this behavior for simple systems is valuable. Overall, the results feel preliminary but likely to be of interest when further fleshed out.\n\nThis paper is attacking an important problem, but should do a better job situating itself in the related literature and undertaking experiments of sufficient size to reveal large-scale behavior relevant to practice.\n"
  },
  {
    "people": [
      "Domke",
      "Hosseini",
      "Uria",
      "Theis",
      "West"
    ],
    "review": "Summary:\nThis paper on autoregressive generative models explores various extensions of PixelCNNs. The proposed changes are to replace the softmax function with a logistic mixture model, to use dropout for regularization, to use downsampling to increase receptive field size, and the introduction of particular skip connections. The authors find that this allows the PixelCNN to outperform a PixelRNN on CIFAR-10, the previous state-of-the-art model. The authors further explore the performance of PixelCNNs with smaller receptive field sizes.\n\nReview:\nThis is a useful contribution towards better tractable image models. In particular, autoregressive models can be quite slow at test time, and the more efficient architectures described here should help with that.\n\nMy main criticism regards the severe neglect of related work. Mixture models have been used a lot in autoregressive image modeling, including for multivariate conditional densities and including downsampling to increase receptive field size, albeit in a different manner: Domke (2008), Hosseini et al. (2010), Theis et al. (2012), Uria et al. (2013), Theis et al. (2015). Note that the logistic distribution is a special case of the Gaussian scale mixture (West, 1978).\n\nThe main difference seems to be the integration of the density to model integers. While this is clearly a good idea and the right way forward, the authors claim but do not support that not doing this has \u201cproved to be a problem for earlier models based on continuous distributions\u201d. Please elaborate, add a reference, or ideally report the performance achieved by PixelCNN++ without integration (and instead adding uniform noise to make the variables continuous).\n\n60,000 images are not a lot in a high-dimensional space. While I can see the usefulness of regularization for specialized content \u2013 and this can serve as a good example to demonstrate the usefulness of dropout \u2013 why not use \u201c80 million tiny images\u201d (superset of CIFAR-10) for natural images? Semi-supervised learning should be fairly trivial here (because the model\u2019s likelihood is tractable), so this data could even be used in the class-conditional case.\n\nIt would be interesting to know how fast the different models are at test time (i.e., when generating images)."
  },
  {
    "people": [
      "Boltzmann",
      "Boltzmann"
    ],
    "review": "It is refreshing that OpenAI has taken the time to resurrect classic heuristics like down-sampling and dropout into PixelCNN. Some sort of AR technique like PixelCNN probably holds the missing keys needed to eventually have decent originally-created images from CIFAR10 or other real-life data-sets. So any engineering streamlining, as in this paper, is welcome to the general public, especially when helping to avoid expensive clusters of GPUs, only DeepMind can afford. In this sense, OpenAI is fulfilling its mission and we are all very grateful! Thus the paper is a welcome addition and we hope it finds its way into what appears to be an experimental CS conference anyway. \n\nOn a more conceptual level, our hope is that OpenAI, with so talented a team, will stop competing in these contrived contests to improve by basis points certain obscure log-likelihoods and instead focus on the bigger picture problems. Why for example, almost two years later, the class-conditional CIFAR10 samples, as on the left of Figure 4 in this paper (column 8 - class of horses), are still inferior to, say, the samples on Figure 4 of reference [2]? Forgive the pun, but aren't we beating a dead horse here? Yes, resolution and sharpness have improved, due to good engineering but nobody in the general public will take these samples seriously! Despite the claims put forward by some on the DeepMind team, PixelCNN is not a \"fully-generative\" neural net (as rigorously defined in section 3 of reference [1]), but merely a perturbative net, in the vain of the Boltzmann machine. After the Procrustean experience of lost decades on Boltzmann machines, the time perhaps has come to think more about the fundamentals and less about the heuristics?\n\n[1] "
  },
  {
    "people": [
      "Theis",
      "Theis",
      "van den Oord"
    ],
    "review": "Apologies for the late submission of this review, and thank you for the author\u2019s responses to earlier questions.\n\nThis submission proposes an improved implementation of the PixelCNN generative model. Most of the improvements are small and can be considered as specific technical details such as the use of dropout and skip connections, while others are slightly more substantial such as the use of a different likelihood model and multiscale analysis. The submission demonstrates state-of-the-art likelihood results on CIFAR-10.\n\nMy summary of the main contribution:\nAutoregressive-type models - of which PixelCNN is an example - are a nice class of models as their likelihood can be evaluated in closed form. A main differentiator for this type of models is how the conditional likelihood of one pixel conditioned on its causal neighbourhood is modelled:\n\n- In one line of work such as (Theis et al, 2012 MCGSM, Theis et al 2015 Spatial LSTM) the conditional distribution is modelled as a continuous density over real numbers. This approach has limitations: We know that in observed data pixel intensities are quantized to a discrete integer representation so a discrete distribution could give better likelihoods. Furthermore these continuous distributions have a tail and assign some probability mass outside the valid range of pixel intensities, which may hurt the likelihood.\n- In more recent work by van den Oord and colleagues the conditional likelihood is modelled as an arbitrary discrete distribution over the 256 possible values for pixel intensities. This does not suffer from the limitations of continuous likelihoods, but it also seems wasteful and is not very data efficient.\n\nThe authors propose something in the middle by keeping the discretized nature of the conditional likelihood, but restricting the discrete distribution to ones whose CDF that can be modeled as a linear combination of sigmoids. This approach makes sense to me, and is new in a way, but it doesn\u2019t appear to be very revolutionary or significant to me.\n\nThe second somewhat significant modification is the use of downsampling and multiscale modelling (as opposed to dilated convolutions). The main motivation for the authors to do this is saving computation time while keeping the multiscale flexibility of the model. The authors also introduce shortcut connections to compensate for the potential loss of information as they perform downsampling. Again, I feel that this modification not particularly revolutionary. Multiscale image analysis with autoregressive generative models has been done for example in (Theis et al, 2012) and several other papers.\n\nOverall I felt that this submission falls short on presenting substantially new ideas, and reads more like documentation for a particular implementation of an existing idea."
  },
  {
    "people": [
      "Domke",
      "Hosseini",
      "Uria",
      "Theis",
      "West"
    ],
    "review": "Summary:\nThis paper on autoregressive generative models explores various extensions of PixelCNNs. The proposed changes are to replace the softmax function with a logistic mixture model, to use dropout for regularization, to use downsampling to increase receptive field size, and the introduction of particular skip connections. The authors find that this allows the PixelCNN to outperform a PixelRNN on CIFAR-10, the previous state-of-the-art model. The authors further explore the performance of PixelCNNs with smaller receptive field sizes.\n\nReview:\nThis is a useful contribution towards better tractable image models. In particular, autoregressive models can be quite slow at test time, and the more efficient architectures described here should help with that.\n\nMy main criticism regards the severe neglect of related work. Mixture models have been used a lot in autoregressive image modeling, including for multivariate conditional densities and including downsampling to increase receptive field size, albeit in a different manner: Domke (2008), Hosseini et al. (2010), Theis et al. (2012), Uria et al. (2013), Theis et al. (2015). Note that the logistic distribution is a special case of the Gaussian scale mixture (West, 1978).\n\nThe main difference seems to be the integration of the density to model integers. While this is clearly a good idea and the right way forward, the authors claim but do not support that not doing this has \u201cproved to be a problem for earlier models based on continuous distributions\u201d. Please elaborate, add a reference, or ideally report the performance achieved by PixelCNN++ without integration (and instead adding uniform noise to make the variables continuous).\n\n60,000 images are not a lot in a high-dimensional space. While I can see the usefulness of regularization for specialized content \u2013 and this can serve as a good example to demonstrate the usefulness of dropout \u2013 why not use \u201c80 million tiny images\u201d (superset of CIFAR-10) for natural images? Semi-supervised learning should be fairly trivial here (because the model\u2019s likelihood is tractable), so this data could even be used in the class-conditional case.\n\nIt would be interesting to know how fast the different models are at test time (i.e., when generating images)."
  },
  {
    "people": [
      "Domke",
      "Hosseini",
      "Uria",
      "Theis",
      "West"
    ],
    "review": "Summary:\nThis paper on autoregressive generative models explores various extensions of PixelCNNs. The proposed changes are to replace the softmax function with a logistic mixture model, to use dropout for regularization, to use downsampling to increase receptive field size, and the introduction of particular skip connections. The authors find that this allows the PixelCNN to outperform a PixelRNN on CIFAR-10, the previous state-of-the-art model. The authors further explore the performance of PixelCNNs with smaller receptive field sizes.\n\nReview:\nThis is a useful contribution towards better tractable image models. In particular, autoregressive models can be quite slow at test time, and the more efficient architectures described here should help with that.\n\nMy main criticism regards the severe neglect of related work. Mixture models have been used a lot in autoregressive image modeling, including for multivariate conditional densities and including downsampling to increase receptive field size, albeit in a different manner: Domke (2008), Hosseini et al. (2010), Theis et al. (2012), Uria et al. (2013), Theis et al. (2015). Note that the logistic distribution is a special case of the Gaussian scale mixture (West, 1978).\n\nThe main difference seems to be the integration of the density to model integers. While this is clearly a good idea and the right way forward, the authors claim but do not support that not doing this has \u201cproved to be a problem for earlier models based on continuous distributions\u201d. Please elaborate, add a reference, or ideally report the performance achieved by PixelCNN++ without integration (and instead adding uniform noise to make the variables continuous).\n\n60,000 images are not a lot in a high-dimensional space. While I can see the usefulness of regularization for specialized content \u2013 and this can serve as a good example to demonstrate the usefulness of dropout \u2013 why not use \u201c80 million tiny images\u201d (superset of CIFAR-10) for natural images? Semi-supervised learning should be fairly trivial here (because the model\u2019s likelihood is tractable), so this data could even be used in the class-conditional case.\n\nIt would be interesting to know how fast the different models are at test time (i.e., when generating images)."
  },
  {
    "people": [
      "Boltzmann",
      "Boltzmann"
    ],
    "review": "It is refreshing that OpenAI has taken the time to resurrect classic heuristics like down-sampling and dropout into PixelCNN. Some sort of AR technique like PixelCNN probably holds the missing keys needed to eventually have decent originally-created images from CIFAR10 or other real-life data-sets. So any engineering streamlining, as in this paper, is welcome to the general public, especially when helping to avoid expensive clusters of GPUs, only DeepMind can afford. In this sense, OpenAI is fulfilling its mission and we are all very grateful! Thus the paper is a welcome addition and we hope it finds its way into what appears to be an experimental CS conference anyway. \n\nOn a more conceptual level, our hope is that OpenAI, with so talented a team, will stop competing in these contrived contests to improve by basis points certain obscure log-likelihoods and instead focus on the bigger picture problems. Why for example, almost two years later, the class-conditional CIFAR10 samples, as on the left of Figure 4 in this paper (column 8 - class of horses), are still inferior to, say, the samples on Figure 4 of reference [2]? Forgive the pun, but aren't we beating a dead horse here? Yes, resolution and sharpness have improved, due to good engineering but nobody in the general public will take these samples seriously! Despite the claims put forward by some on the DeepMind team, PixelCNN is not a \"fully-generative\" neural net (as rigorously defined in section 3 of reference [1]), but merely a perturbative net, in the vain of the Boltzmann machine. After the Procrustean experience of lost decades on Boltzmann machines, the time perhaps has come to think more about the fundamentals and less about the heuristics?\n\n[1] "
  },
  {
    "people": [
      "Theis",
      "Theis",
      "van den Oord"
    ],
    "review": "Apologies for the late submission of this review, and thank you for the author\u2019s responses to earlier questions.\n\nThis submission proposes an improved implementation of the PixelCNN generative model. Most of the improvements are small and can be considered as specific technical details such as the use of dropout and skip connections, while others are slightly more substantial such as the use of a different likelihood model and multiscale analysis. The submission demonstrates state-of-the-art likelihood results on CIFAR-10.\n\nMy summary of the main contribution:\nAutoregressive-type models - of which PixelCNN is an example - are a nice class of models as their likelihood can be evaluated in closed form. A main differentiator for this type of models is how the conditional likelihood of one pixel conditioned on its causal neighbourhood is modelled:\n\n- In one line of work such as (Theis et al, 2012 MCGSM, Theis et al 2015 Spatial LSTM) the conditional distribution is modelled as a continuous density over real numbers. This approach has limitations: We know that in observed data pixel intensities are quantized to a discrete integer representation so a discrete distribution could give better likelihoods. Furthermore these continuous distributions have a tail and assign some probability mass outside the valid range of pixel intensities, which may hurt the likelihood.\n- In more recent work by van den Oord and colleagues the conditional likelihood is modelled as an arbitrary discrete distribution over the 256 possible values for pixel intensities. This does not suffer from the limitations of continuous likelihoods, but it also seems wasteful and is not very data efficient.\n\nThe authors propose something in the middle by keeping the discretized nature of the conditional likelihood, but restricting the discrete distribution to ones whose CDF that can be modeled as a linear combination of sigmoids. This approach makes sense to me, and is new in a way, but it doesn\u2019t appear to be very revolutionary or significant to me.\n\nThe second somewhat significant modification is the use of downsampling and multiscale modelling (as opposed to dilated convolutions). The main motivation for the authors to do this is saving computation time while keeping the multiscale flexibility of the model. The authors also introduce shortcut connections to compensate for the potential loss of information as they perform downsampling. Again, I feel that this modification not particularly revolutionary. Multiscale image analysis with autoregressive generative models has been done for example in (Theis et al, 2012) and several other papers.\n\nOverall I felt that this submission falls short on presenting substantially new ideas, and reads more like documentation for a particular implementation of an existing idea."
  },
  {
    "people": [
      "Domke",
      "Hosseini",
      "Uria",
      "Theis",
      "West"
    ],
    "review": "Summary:\nThis paper on autoregressive generative models explores various extensions of PixelCNNs. The proposed changes are to replace the softmax function with a logistic mixture model, to use dropout for regularization, to use downsampling to increase receptive field size, and the introduction of particular skip connections. The authors find that this allows the PixelCNN to outperform a PixelRNN on CIFAR-10, the previous state-of-the-art model. The authors further explore the performance of PixelCNNs with smaller receptive field sizes.\n\nReview:\nThis is a useful contribution towards better tractable image models. In particular, autoregressive models can be quite slow at test time, and the more efficient architectures described here should help with that.\n\nMy main criticism regards the severe neglect of related work. Mixture models have been used a lot in autoregressive image modeling, including for multivariate conditional densities and including downsampling to increase receptive field size, albeit in a different manner: Domke (2008), Hosseini et al. (2010), Theis et al. (2012), Uria et al. (2013), Theis et al. (2015). Note that the logistic distribution is a special case of the Gaussian scale mixture (West, 1978).\n\nThe main difference seems to be the integration of the density to model integers. While this is clearly a good idea and the right way forward, the authors claim but do not support that not doing this has \u201cproved to be a problem for earlier models based on continuous distributions\u201d. Please elaborate, add a reference, or ideally report the performance achieved by PixelCNN++ without integration (and instead adding uniform noise to make the variables continuous).\n\n60,000 images are not a lot in a high-dimensional space. While I can see the usefulness of regularization for specialized content \u2013 and this can serve as a good example to demonstrate the usefulness of dropout \u2013 why not use \u201c80 million tiny images\u201d (superset of CIFAR-10) for natural images? Semi-supervised learning should be fairly trivial here (because the model\u2019s likelihood is tractable), so this data could even be used in the class-conditional case.\n\nIt would be interesting to know how fast the different models are at test time (i.e., when generating images)."
  },
  {
    "people": [
      "Joseph Lim",
      "Lawrence Zitnick",
      "Rodrigo Benenson"
    ],
    "review": "Paper summary: the authors proposed to use EdgeBoxes + Fast-RCNN with\nbatch normalization for pedestrian detection\n\nReview summary: results do not cover enough datasets, the reported\nresults do not improve over state of the art, writing is poor, and\noverall the work lacks novelty. This is a clear reject.\n\nPros:\n* Shows that using batch normalization does improve results\n\nCons:\n* Only results on ETH and INRIA. Should include Caltech or KITTI.\n* Reported results are fair, but not improving over state of the art\n* Overall idea of limited interest when considering works like S.\nZhang CVPR 2016 (Fast R-CNN for pedestrian detection) and L. Zhang\nECCV 2017 (Faster R-CNN for pedestrian detection)\n* Issues with the text quality\n* Limited takeaways\n\nQuality: low\nClarity: fair, but poor English\nOriginality: low\nSignificance: low\n\nFor acceptance at future conferences, this work would need more\npolish, improving over best known results on INRA, ETH, and Caltech or\nKITTI. And ideally, present additional new insights.\n\nMinor comments:\n* The text lacks polish. E.g. influent -> influence, has maken ->\nmade, is usually very important -> is important, achieve more\nexcellent results -> achieve better results; etc. Please consider\nasking help from a native speaker for future submissions. There are\nalso non-sense sentences such as \u201cit is computational\u201d.\n* Citations should be in parentheses\n* Some of the citations are incorrect because the family name is in\nthe wrong position, e.g. Joseph Lim, Lawrence Zitnick, and Rodrigo Benenson."
  },
  {
    "people": [
      "Joseph Lim",
      "Lawrence Zitnick",
      "Rodrigo Benenson"
    ],
    "review": "Paper summary: the authors proposed to use EdgeBoxes + Fast-RCNN with\nbatch normalization for pedestrian detection\n\nReview summary: results do not cover enough datasets, the reported\nresults do not improve over state of the art, writing is poor, and\noverall the work lacks novelty. This is a clear reject.\n\nPros:\n* Shows that using batch normalization does improve results\n\nCons:\n* Only results on ETH and INRIA. Should include Caltech or KITTI.\n* Reported results are fair, but not improving over state of the art\n* Overall idea of limited interest when considering works like S.\nZhang CVPR 2016 (Fast R-CNN for pedestrian detection) and L. Zhang\nECCV 2017 (Faster R-CNN for pedestrian detection)\n* Issues with the text quality\n* Limited takeaways\n\nQuality: low\nClarity: fair, but poor English\nOriginality: low\nSignificance: low\n\nFor acceptance at future conferences, this work would need more\npolish, improving over best known results on INRA, ETH, and Caltech or\nKITTI. And ideally, present additional new insights.\n\nMinor comments:\n* The text lacks polish. E.g. influent -> influence, has maken ->\nmade, is usually very important -> is important, achieve more\nexcellent results -> achieve better results; etc. Please consider\nasking help from a native speaker for future submissions. There are\nalso non-sense sentences such as \u201cit is computational\u201d.\n* Citations should be in parentheses\n* Some of the citations are incorrect because the family name is in\nthe wrong position, e.g. Joseph Lim, Lawrence Zitnick, and Rodrigo Benenson."
  },
  {
    "people": [
      "Obama"
    ],
    "review": "This paper addresses the practical problem of generating rare or unseen words in the context of language modeling. Since language follows a Zipf\u2019s law, most approaches limit the vocabulary (because of computation reasons) and hence rare words are often mapped to a UNK token. Rare words are especially important in context of applications such as question answering. MT etc. This paper proposes a language modeling technique which incorporates facts from knowledge bases (KBs) and thus has the ability to generate (potentially unseen) words from KBs. This paper also releases a dataset by aligning words with Freebase facts and corresponding Wikipedia descriptions.\n\nThe model first selects a KB fact based on the previously generated words and facts. Based on the selected fact, it then predicts whether to generate a word based on the vocabulary or to output a symbolic word from the KB. For the latter, the model is trained to predict the position of the word from the fact description.\n\nOverall the paper could use some rewriting especially the notations in section 3. The experiments are well executed and they definitely get good results. The heat maps at the end are very insightful. \n\nComments\n\nThis contributions of this paper would be much stronger if it showed improvements in a practical applications such as Question Answering (although the paper clearly mentions that this technique could be applied to improve QA)\nIn section 3, it is unclear why the authors refer the entity as a \u2018topic'. This makes the text a little confusing since a topic can also be associated with something abstract, but in this case the topic is always a freebase entity. \nIs it really necessary to predict a fact at every step before generating a word. In other words, how many distinct facts on average does the model choose to generate a sentence. Intuitively a natural language sentence would be describe few facts about an entity. If the fact generation step could be avoided (by adding a latent variable which decides if the fact should be generated or not), the model will also be faster.\nIn equation 2, the model has to make a hard decision to choose the fact. For this to be end to end trained, every word needs to be annotated with a corresponding fact which might not be always a realistic scenario. For e.g., in domains such as social media text.\nLearning position embeddings for copying knowledge words seems a little counter-intuitive. Does the sequence of knowledge words follow any particular structure like word O_2 is always the last name (e.g. Obama).\nIt would also be nice to compare to char-level LM's which inherently solves the unknown token problem."
  },
  {
    "people": [
      "Obama",
      "Barack Obama"
    ],
    "review": "This paper proposes to incorporate knowledge base facts into language modeling, thus at each time step, a word is either generated from the full vocabulary or relevant KB entities.\n\nThe authors demonstrate the effectiveness on a new generated dataset WikiFacts which aligns Wikipedia articles with Freebase facts.  The authors also suggest a modified perplexity metric which penalizes the likelihood of unknown words.\n\nAt a high level, I do like the motivation of this paper -- named entity words are usually important for downstream tasks, but difficult to learn solely based on statistical co-occurrences. The facts encoded in KB could be a great supply for this.\n\nHowever, I find it difficult to follow the details of the paper (mainly Section 3) and think the paper writing needs to be much improved. \n- I cannot find where  f_{symbkey} / f_{voca} / f_{copy} are defined\n- w^v, w^s are confusing.\n- e_k seems to be the average of all previous fact embeddings? It is necessary to make it clear enough.\n- (h_t, c_t) = f_LSTM(x_{t\u22121}, h_{t\u22121})  c_t is not used?\n- The notion of \u201cfact embeddings\u201d is also not that clear (I understand that they are taken as the concatenation of relation and entity (object) entities in the end).  For the anchor / \u201ctopic-itself\u201d facts, do you learn the embedding for the special relations and use the entity embeddings from TransE?\n\nOn generating words from KB entities (fact description), it sounds a bit strange to me to generate a symbol position first.  Most entities are multiple words, and it is necessary to keep that order. Also it might be helpful to incorporate some prior information, for example, it is common to only mention \u201cObama\u201d for the entity \u201cBarack Obama\u201d?\n"
  },
  {
    "people": [
      "Michelle"
    ],
    "review": "The paper proposes an evolution upon traditional Recurrent Language Models to give the capability to deal with unknown words. It is done by pairing the traditional RNNLM with a module operating on a KB and able to copy from KB facts to generate unseen words. It is shown to be efficient and much better than plain RNNLM on a new dataset.\n\nThe writing could be improved. The beginning of Section 3 in particular is hard to parse.\n\nThere have been similar efforts recently (like \"Pointer Sentinel Mixture Models\" by Merity et al.) that attempt to overcome limitations of RNNLMs with unknown words; but they usually do it by adding a mechanism to copy from a longer past history. The proposal of the current paper is different and more interesting to me in that it try to bring knowledge from another source (KB) to the language model. This is harder because one needs to leverage the large scale of the KB to do so. Being able to train that conveniently is nice.\n\nThe architecture appears sound, but the writing makes it hard to fully understand completely so I can not give a higher rating. \n\n\nOther comments:\n* How to cope with the dependency on the KB? Freebase is not updated anymore so it is likely that a lot of the new unseen words in the making are not going to be in Freebase.\n* What is the performance on standard benchmarks like Penn Tree Bank?\n* How long is it to train compare to a standard RNNLM?\n* What is the importance of the knowledge context $e$?\n* How is initialized the fact embedding $a_{t-1}$ for the first word?\n* When a word from a fact description has been chosen as prediction (copied), how is it encoded in the generation history for following predictions if it has no embedding (unknown word)? In other words, what happens if \"Michelle\" in the example of Section 3.1 is not in the embedding dictionary, when one wants to predict the next word?\n\n\n\n"
  },
  {
    "people": [
      "Obama"
    ],
    "review": "\nThis paper addresses the practical problem of generating rare or unseen words in the context of language modeling. Since language follows a Zipf\u2019s law, most approaches limit the vocabulary (because of computation reasons) and hence rare words are often mapped to a UNK token. Rare words are especially important in context of applications such as question answering. MT etc. This paper proposes a language modeling technique which incorporates facts from knowledge bases (KBs) and thus has the ability to generate (potentially unseen) words from KBs. This paper also releases a dataset by aligning words with Freebase facts and corresponding Wikipedia descriptions.\n\nThe model first selects a KB fact based on the previously generated words and facts. Based on the selected fact, it then predicts whether to generate a word based on the vocabulary or to output a symbolic word from the KB. For the latter, the model is trained to predict the position of the word from the fact description.\n\nOverall the paper could use some rewriting especially the notations in section 3. The experiments are well executed and they definitely get good results. The heat maps at the end are very insightful. \n\nComments\n\nThis contributions of this paper would be much stronger if it showed improvements in a practical applications such as Question Answering (although the paper clearly mentions that this technique could be applied to improve QA)\nIn section 3, it is unclear why the authors refer the entity as a \u2018topic'. This makes the text a little confusing since a topic can also be associated with something abstract, but in this case the topic is always a freebase entity. \nIs it really necessary to predict a fact at every step before generating a word. In other words, how many distinct facts on average does the model choose to generate a sentence. Intuitively a natural language sentence would be describe few facts about an entity. If the fact generation step could be avoided (by adding a latent variable which decides if the fact should be generated or not), the model will also be faster.\nIn equation 2, the model has to make a hard decision to choose the fact. For this to be end to end trained, every word needs to be annotated with a corresponding fact which might not be always a realistic scenario. For e.g., in domains such as social media text.\nLearning position embeddings for copying knowledge words seems a little counter-intuitive. Does the sequence of knowledge words follow any particular structure like word O_2 is always the last name (e.g. Obama).\nIt would also be nice to compare to char-level LM's which inherently solves the unknown token problem. "
  },
  {
    "people": [
      "Obama"
    ],
    "review": "This paper addresses the practical problem of generating rare or unseen words in the context of language modeling. Since language follows a Zipf\u2019s law, most approaches limit the vocabulary (because of computation reasons) and hence rare words are often mapped to a UNK token. Rare words are especially important in context of applications such as question answering. MT etc. This paper proposes a language modeling technique which incorporates facts from knowledge bases (KBs) and thus has the ability to generate (potentially unseen) words from KBs. This paper also releases a dataset by aligning words with Freebase facts and corresponding Wikipedia descriptions.\n\nThe model first selects a KB fact based on the previously generated words and facts. Based on the selected fact, it then predicts whether to generate a word based on the vocabulary or to output a symbolic word from the KB. For the latter, the model is trained to predict the position of the word from the fact description.\n\nOverall the paper could use some rewriting especially the notations in section 3. The experiments are well executed and they definitely get good results. The heat maps at the end are very insightful. \n\nComments\n\nThis contributions of this paper would be much stronger if it showed improvements in a practical applications such as Question Answering (although the paper clearly mentions that this technique could be applied to improve QA)\nIn section 3, it is unclear why the authors refer the entity as a \u2018topic'. This makes the text a little confusing since a topic can also be associated with something abstract, but in this case the topic is always a freebase entity. \nIs it really necessary to predict a fact at every step before generating a word. In other words, how many distinct facts on average does the model choose to generate a sentence. Intuitively a natural language sentence would be describe few facts about an entity. If the fact generation step could be avoided (by adding a latent variable which decides if the fact should be generated or not), the model will also be faster.\nIn equation 2, the model has to make a hard decision to choose the fact. For this to be end to end trained, every word needs to be annotated with a corresponding fact which might not be always a realistic scenario. For e.g., in domains such as social media text.\nLearning position embeddings for copying knowledge words seems a little counter-intuitive. Does the sequence of knowledge words follow any particular structure like word O_2 is always the last name (e.g. Obama).\nIt would also be nice to compare to char-level LM's which inherently solves the unknown token problem."
  },
  {
    "people": [
      "Obama",
      "Barack Obama"
    ],
    "review": "This paper proposes to incorporate knowledge base facts into language modeling, thus at each time step, a word is either generated from the full vocabulary or relevant KB entities.\n\nThe authors demonstrate the effectiveness on a new generated dataset WikiFacts which aligns Wikipedia articles with Freebase facts.  The authors also suggest a modified perplexity metric which penalizes the likelihood of unknown words.\n\nAt a high level, I do like the motivation of this paper -- named entity words are usually important for downstream tasks, but difficult to learn solely based on statistical co-occurrences. The facts encoded in KB could be a great supply for this.\n\nHowever, I find it difficult to follow the details of the paper (mainly Section 3) and think the paper writing needs to be much improved. \n- I cannot find where  f_{symbkey} / f_{voca} / f_{copy} are defined\n- w^v, w^s are confusing.\n- e_k seems to be the average of all previous fact embeddings? It is necessary to make it clear enough.\n- (h_t, c_t) = f_LSTM(x_{t\u22121}, h_{t\u22121})  c_t is not used?\n- The notion of \u201cfact embeddings\u201d is also not that clear (I understand that they are taken as the concatenation of relation and entity (object) entities in the end).  For the anchor / \u201ctopic-itself\u201d facts, do you learn the embedding for the special relations and use the entity embeddings from TransE?\n\nOn generating words from KB entities (fact description), it sounds a bit strange to me to generate a symbol position first.  Most entities are multiple words, and it is necessary to keep that order. Also it might be helpful to incorporate some prior information, for example, it is common to only mention \u201cObama\u201d for the entity \u201cBarack Obama\u201d?\n"
  },
  {
    "people": [
      "Michelle"
    ],
    "review": "The paper proposes an evolution upon traditional Recurrent Language Models to give the capability to deal with unknown words. It is done by pairing the traditional RNNLM with a module operating on a KB and able to copy from KB facts to generate unseen words. It is shown to be efficient and much better than plain RNNLM on a new dataset.\n\nThe writing could be improved. The beginning of Section 3 in particular is hard to parse.\n\nThere have been similar efforts recently (like \"Pointer Sentinel Mixture Models\" by Merity et al.) that attempt to overcome limitations of RNNLMs with unknown words; but they usually do it by adding a mechanism to copy from a longer past history. The proposal of the current paper is different and more interesting to me in that it try to bring knowledge from another source (KB) to the language model. This is harder because one needs to leverage the large scale of the KB to do so. Being able to train that conveniently is nice.\n\nThe architecture appears sound, but the writing makes it hard to fully understand completely so I can not give a higher rating. \n\n\nOther comments:\n* How to cope with the dependency on the KB? Freebase is not updated anymore so it is likely that a lot of the new unseen words in the making are not going to be in Freebase.\n* What is the performance on standard benchmarks like Penn Tree Bank?\n* How long is it to train compare to a standard RNNLM?\n* What is the importance of the knowledge context $e$?\n* How is initialized the fact embedding $a_{t-1}$ for the first word?\n* When a word from a fact description has been chosen as prediction (copied), how is it encoded in the generation history for following predictions if it has no embedding (unknown word)? In other words, what happens if \"Michelle\" in the example of Section 3.1 is not in the embedding dictionary, when one wants to predict the next word?\n\n\n\n"
  },
  {
    "people": [
      "Obama"
    ],
    "review": "\nThis paper addresses the practical problem of generating rare or unseen words in the context of language modeling. Since language follows a Zipf\u2019s law, most approaches limit the vocabulary (because of computation reasons) and hence rare words are often mapped to a UNK token. Rare words are especially important in context of applications such as question answering. MT etc. This paper proposes a language modeling technique which incorporates facts from knowledge bases (KBs) and thus has the ability to generate (potentially unseen) words from KBs. This paper also releases a dataset by aligning words with Freebase facts and corresponding Wikipedia descriptions.\n\nThe model first selects a KB fact based on the previously generated words and facts. Based on the selected fact, it then predicts whether to generate a word based on the vocabulary or to output a symbolic word from the KB. For the latter, the model is trained to predict the position of the word from the fact description.\n\nOverall the paper could use some rewriting especially the notations in section 3. The experiments are well executed and they definitely get good results. The heat maps at the end are very insightful. \n\nComments\n\nThis contributions of this paper would be much stronger if it showed improvements in a practical applications such as Question Answering (although the paper clearly mentions that this technique could be applied to improve QA)\nIn section 3, it is unclear why the authors refer the entity as a \u2018topic'. This makes the text a little confusing since a topic can also be associated with something abstract, but in this case the topic is always a freebase entity. \nIs it really necessary to predict a fact at every step before generating a word. In other words, how many distinct facts on average does the model choose to generate a sentence. Intuitively a natural language sentence would be describe few facts about an entity. If the fact generation step could be avoided (by adding a latent variable which decides if the fact should be generated or not), the model will also be faster.\nIn equation 2, the model has to make a hard decision to choose the fact. For this to be end to end trained, every word needs to be annotated with a corresponding fact which might not be always a realistic scenario. For e.g., in domains such as social media text.\nLearning position embeddings for copying knowledge words seems a little counter-intuitive. Does the sequence of knowledge words follow any particular structure like word O_2 is always the last name (e.g. Obama).\nIt would also be nice to compare to char-level LM's which inherently solves the unknown token problem. "
  },
  {
    "people": [
      "Huang",
      "Li",
      "Yu",
      "Deng",
      "Gong"
    ],
    "review": "This paper proposed a deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network with tensor factorization and end-to-end knowledge sharing. This approach removed the requirement of a user-de\ufb01ned multi-task sharing strategy in conventional approach. Their experimental results indicate that their approach can achieve higher accuracy with fewer design choices.\n\nAlthough factorization ideas have been exploited in the past for other tasks I think applying it to MTL is interesting. The only thing I want to point out is that the saving of parameter is from the low-rank factorization. In the conventional MTL each layer's weight size can also be reduced if SVD is used. \n\nBTW, recent neural network MTL was explored first (earlier than 2014, 2015 work cited) in speech recognition community. see, e.g., \n\nHuang, J.T., Li, J., Yu, D., Deng, L. and Gong, Y., 2013, May. Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (pp. 7304-7308). IEEE."
  },
  {
    "people": [
      "Huang",
      "Li",
      "Yu",
      "Deng",
      "Gong"
    ],
    "review": "This paper proposed a deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network with tensor factorization and end-to-end knowledge sharing. This approach removed the requirement of a user-de\ufb01ned multi-task sharing strategy in conventional approach. Their experimental results indicate that their approach can achieve higher accuracy with fewer design choices.\n\nAlthough factorization ideas have been exploited in the past for other tasks I think applying it to MTL is interesting. The only thing I want to point out is that the saving of parameter is from the low-rank factorization. In the conventional MTL each layer's weight size can also be reduced if SVD is used. \n\nBTW, recent neural network MTL was explored first (earlier than 2014, 2015 work cited) in speech recognition community. see, e.g., \n\nHuang, J.T., Li, J., Yu, D., Deng, L. and Gong, Y., 2013, May. Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (pp. 7304-7308). IEEE."
  },
  {
    "people": [
      "Graves"
    ],
    "review": "The authors proposed a dynamic neural Turing machine (D-NTM) model that overcomes the rigid location-based memory access used in the original NTM model. The paper has two main contributions: 1) introducing a learnable addressing to NTM. 2) curriculum learning using hybrid discrete and continuous attention. The proposed model was empirically evaluated on Facebook bAbI task and has shown improvement over the original NTM.\n\nPros:\n+ Comprehensive comparisons of feed-forward controllers v.s. recurrent controllers\n+ Encouraging results on the curriculum learning on hybrid discrete and continuous attentions\n\nCons:\n- Very weak NTM baseline (due to some hyper-parameter engineering?) in Table 1, 31% err. comparing to the NTM 20% err. reported in Table 1 in(Graves et al, 2016, Hybrid computing using a neural network with dynamic external memory). In fact, the NTM baseline in (Graves et al 2016) is better than the proposed D-NTM with GRU controller. Maybe it is worthwhile to reproduce their results using the hyper-parameter setting in their Table2 which could potentially lead to better D-NTM performance?\n- Section 3 of the paper is hard to follow. The overall clarity of the paper needs improvement."
  },
  {
    "people": [
      "Graves"
    ],
    "review": "This paper introduces a variant of the neural Turing machine (NTM, Graves et al. 2014) where key and values are stored. They try both continuous and discrete mechanisms to control the memory.\n\nThe model is quite complicated and seem to require a lot of tricks to work. Overall it seems that more than 10 different terms appear in the cost function and many different hacks are required to learn the model. It is hard to understand the justification for all of these tricks and sophisticated choices. There is no code available nor plan to release it (afaik).\n\nThe model is evaluated on a set of toy problems (the \u201cbabi task\u201d) and achieves performance that are only slightly above those of a vanilla LSTM but are much worse than the different memory augmented models proposed in the last few years.  \n\nIn terms of writing, the description of the model is quite hard to follow, describing different blocks independently, optimization tricks and regularization. The equations are hard to read, using non standard notation (e.g., \u201csoftplus\u201d), overloading notations (w_t, b\u2026), or write similar equations in different ways (for example, eq (8-9) compared to (10-11). Why are two equations in scalar and the other in vectors? Why is there an arrow instead of an equal?\u2026).\n\nOverall it is very hard to put together all the pieces of this model(s), there is no code available and I\u2019m afraid there is not enough details to be able to reproduce their numbers. Finally, the performance on the bAbI tasks are quite poor compared to other memory augmented models.\n"
  },
  {
    "people": [
      "Weston",
      "Sukhbaatar",
      "Bordes",
      "Dodge",
      "Weston"
    ],
    "review": "The model described in his paper is quite complicated and reproducing the method from its description may be challenging: Are you planing to release the code and what is your estimate release date?\n\nDifferent cost functions and regularization are introduce in the paper. Would it be possible to summarize the overall cost function minimized by this model? \n\nSome variables seems to have different definition. In particular w_t and b, would it be possible to clarify this?\n\n\u201cgamma_t is a shallow MLP\u201d: What does it mean? Is gamma_t a function or a variable? It seems from eq. (10) that it is a vector (or a scalar?).\n\n\"curriculum learning for the discrete attention\": Can you compare this to simpler schemes? Like rounding the continuous attention?\n\nIn your introduction, you state that \"it is possible to use the discrete non-differentiable attention mechanism\", referencing Zaremba & Sutskever, 2016 that use REINFORCE and a simple controller. However, in your sec. 4, you are stating that  \"Training discrete attention with feed-forward controller and REINFORCE is challenging\". It seems that these two statements contradict each other, could you comment on it? \n\nIn the introduction, you state that \"Memory network (Weston et al. 2015b) [..] uses an attention-based mechanism to index them\". To the best of my knowledge, it is actually Sukhbaatar et al., 2015 that has introduce the attention mechanism to memory networks. \n\nIn the introduction, the authors state that \u201cmemory networks [\u2026] [are] used in real tasks (Bordes et al. 2015, Dodge et al. 2015)\u201d. However in both papers, they use memory network systems different from Weston et al. The current statement is quite misleading, would it be possible to clarify it?\n"
  },
  {
    "people": [
      "Graves"
    ],
    "review": "The authors proposed a dynamic neural Turing machine (D-NTM) model that overcomes the rigid location-based memory access used in the original NTM model. The paper has two main contributions: 1) introducing a learnable addressing to NTM. 2) curriculum learning using hybrid discrete and continuous attention. The proposed model was empirically evaluated on Facebook bAbI task and has shown improvement over the original NTM.\n\nPros:\n+ Comprehensive comparisons of feed-forward controllers v.s. recurrent controllers\n+ Encouraging results on the curriculum learning on hybrid discrete and continuous attentions\n\nCons:\n- Very weak NTM baseline (due to some hyper-parameter engineering?) in Table 1, 31% err. comparing to the NTM 20% err. reported in Table 1 in(Graves et al, 2016, Hybrid computing using a neural network with dynamic external memory). In fact, the NTM baseline in (Graves et al 2016) is better than the proposed D-NTM with GRU controller. Maybe it is worthwhile to reproduce their results using the hyper-parameter setting in their Table2 which could potentially lead to better D-NTM performance?\n- Section 3 of the paper is hard to follow. The overall clarity of the paper needs improvement."
  },
  {
    "people": [
      "Graves"
    ],
    "review": "This paper introduces a variant of the neural Turing machine (NTM, Graves et al. 2014) where key and values are stored. They try both continuous and discrete mechanisms to control the memory.\n\nThe model is quite complicated and seem to require a lot of tricks to work. Overall it seems that more than 10 different terms appear in the cost function and many different hacks are required to learn the model. It is hard to understand the justification for all of these tricks and sophisticated choices. There is no code available nor plan to release it (afaik).\n\nThe model is evaluated on a set of toy problems (the \u201cbabi task\u201d) and achieves performance that are only slightly above those of a vanilla LSTM but are much worse than the different memory augmented models proposed in the last few years.  \n\nIn terms of writing, the description of the model is quite hard to follow, describing different blocks independently, optimization tricks and regularization. The equations are hard to read, using non standard notation (e.g., \u201csoftplus\u201d), overloading notations (w_t, b\u2026), or write similar equations in different ways (for example, eq (8-9) compared to (10-11). Why are two equations in scalar and the other in vectors? Why is there an arrow instead of an equal?\u2026).\n\nOverall it is very hard to put together all the pieces of this model(s), there is no code available and I\u2019m afraid there is not enough details to be able to reproduce their numbers. Finally, the performance on the bAbI tasks are quite poor compared to other memory augmented models.\n"
  },
  {
    "people": [
      "Weston",
      "Sukhbaatar",
      "Bordes",
      "Dodge",
      "Weston"
    ],
    "review": "The model described in his paper is quite complicated and reproducing the method from its description may be challenging: Are you planing to release the code and what is your estimate release date?\n\nDifferent cost functions and regularization are introduce in the paper. Would it be possible to summarize the overall cost function minimized by this model? \n\nSome variables seems to have different definition. In particular w_t and b, would it be possible to clarify this?\n\n\u201cgamma_t is a shallow MLP\u201d: What does it mean? Is gamma_t a function or a variable? It seems from eq. (10) that it is a vector (or a scalar?).\n\n\"curriculum learning for the discrete attention\": Can you compare this to simpler schemes? Like rounding the continuous attention?\n\nIn your introduction, you state that \"it is possible to use the discrete non-differentiable attention mechanism\", referencing Zaremba & Sutskever, 2016 that use REINFORCE and a simple controller. However, in your sec. 4, you are stating that  \"Training discrete attention with feed-forward controller and REINFORCE is challenging\". It seems that these two statements contradict each other, could you comment on it? \n\nIn the introduction, you state that \"Memory network (Weston et al. 2015b) [..] uses an attention-based mechanism to index them\". To the best of my knowledge, it is actually Sukhbaatar et al., 2015 that has introduce the attention mechanism to memory networks. \n\nIn the introduction, the authors state that \u201cmemory networks [\u2026] [are] used in real tasks (Bordes et al. 2015, Dodge et al. 2015)\u201d. However in both papers, they use memory network systems different from Weston et al. The current statement is quite misleading, would it be possible to clarify it?\n"
  },
  {
    "people": [
      "Welch"
    ],
    "review": "We have just uploaded the newest version of our paper. Based on the reviewers' helpful comments, the changes include:\n\n- Minimal Gated Unit for Recurrent Neural Networks citation\n- Improved descriptions of training tasks in Appendix (added missing details, such as time steps unrolled before computing loss)\n- Added range of perceptron capacity HP b and plot showing that optimal dataset size selected by the HP tuner is only slightly more than mutual information calculated (Figure 1 of Appendix)\n- Small text changes to emphasize that our results indicate the promise of the UGRNN and +RNN, but of course much more extensive testing is needed to declare that they are comparable to well-known, tried and true architectures\n- Welch\u2019s t-test statistics to test the significance of differences between architecture losses as a result of randomly selected hyperparameters (the experiment shown in Figure 5 and Table 1, and test statistics are reported in Table 2 of Appendix)\n- Softening of our recommendation regarding the +RNN in the discussion.\n\nWe are also in the process of releasing a dataset of the HPs and associated losses for all experiments described in the paper. While we can not be positive how long the release process will take, we plan to have the HP dataset publicly available by February 2017!\n"
  },
  {
    "people": [
      "Zhou"
    ],
    "review": "CONTRIBUTIONS\nLarge-scale experiments are used to measure the capacity and trainability of different RNN architectures. Capacity experiments suggest that across all architectures, RNNs can store between three and six bits of information per parameter, with ungated RNNs having the highest per-parameter capacity. All architectures are able to store approximately one floating point number per hidden unit. Trainability experiments show that ungated architectures (RNN, IRNN) are much harder to train than gated architectures (GRU, LSTM, UGRNN, +RNN). The paper also proposes two novel RNN architectures (UGRNN and +RNN); experiments suggest that the UGRNN has similar per-parameter capacity as the ungated RNN but is much easier to train, and that deep (8-layer) +RNN models are easier to train than existing architectures.\n\nCLARITY\nThe paper is well-written and easy to follow.\n\nNOVELTY\nThis paper is the first to my knowledge to empirically measure the number of bits of information that can be stored per learnable parameter. The idea of measuring network capacity by finding the dataset size and other hyperparameters that maximizes mutual information is a particularly novel experimental setup.\n\nThe proposed UGRNN is similar but not identical to the minimal gated unit proposed by Zhou et al, \u201cMinimal Gated Unit for Recurrent Neural Networks\u201d, International Journal of Automation and Computing, 2016.\n\nSIGNIFICANCE\nI have mixed feelings about the significance of this paper. I found the experiments interesting, but I don\u2019t feel that they reveal anything particularly surprising or unexpected about recurrent networks; it is hard to see how any of the experimental results will change the way either that I think about RNNs, or the way that I will use them in my own future work. On the other hand it is valuable to see intuitive results about RNNs confirmed by rigorous experiments, especially since few researchers have the computational resources to perform such large-scale experiments.\n\nThe capacity experiments (both per-parameter capacity and per-unit capacity) essentially force the network to model random data. For most applications of RNNs, however, we do not expect them to work with random data; instead when applied in machine translation or language modeling or image captioning or any number of real-world tasks, we hope that RNNs can learn to model data that is anything but random. It is not clear to me that an architecture\u2019s ability to model random data should be beneficial in modeling real-world data; indeed, the experiments in Section 2.1 show that architectures vary in their capacity to model random data, but the text8 experiments in Section 3 show that these same architectures do not significantly vary in their capacity to model real-world data.\n\nI do not think that the experimental results in the paper are sufficient to prove the significance of the proposed UGRNN and +RNN architectures. It is interesting that the UGRNN can achieve comparable bits per parameter as the ungated RNN and that the deep +RNNs are more easily trainable than other architectures, but the only experiments on a real-world task (language modeling on text8) do not show these architectures to be significantly better than GRU or LSTM.\n\nSUMMARY\nI wish that the experiments had revealed more surprising insights about RNNs, though there is certainly value in experimentally verifying intuitive results. The proposed UGRNN and +RNN architectures show some promising results on synthetic tasks, but I wish that they showed more convincing performance on real-world tasks. Overall I think that the good outweighs the bad, and that the ideas of this paper are of value to the community.\n\nPROS\n- The paper is the first of my knowledge to explicitly measure the bits per parameter that RNNs can store\n- The paper experimentally confirms several intuitive ideas about RNNs:\n    - RNNs of any architecture can store about one number per hidden unit from the input\n    - Different RNN architectures should be compared by their parameter count, not their hidden unit count\n    - With very careful hyperparameter tuning, all RNN architectures perform about the same on text8 language modeling\n    - Gated architectures are easier to train than non-gated RNNs\n\nCONS\n- Experiments do not reveal anything particularly surprising or unexpected\n- The UGRNN and +RNN architectures do not feel well-motivated\n- The utility of the UGRNN and +RNN architectures is not well-established"
  },
  {
    "people": [
      "Welch"
    ],
    "review": "We have just uploaded the newest version of our paper. Based on the reviewers' helpful comments, the changes include:\n\n- Minimal Gated Unit for Recurrent Neural Networks citation\n- Improved descriptions of training tasks in Appendix (added missing details, such as time steps unrolled before computing loss)\n- Added range of perceptron capacity HP b and plot showing that optimal dataset size selected by the HP tuner is only slightly more than mutual information calculated (Figure 1 of Appendix)\n- Small text changes to emphasize that our results indicate the promise of the UGRNN and +RNN, but of course much more extensive testing is needed to declare that they are comparable to well-known, tried and true architectures\n- Welch\u2019s t-test statistics to test the significance of differences between architecture losses as a result of randomly selected hyperparameters (the experiment shown in Figure 5 and Table 1, and test statistics are reported in Table 2 of Appendix)\n- Softening of our recommendation regarding the +RNN in the discussion.\n\nWe are also in the process of releasing a dataset of the HPs and associated losses for all experiments described in the paper. While we can not be positive how long the release process will take, we plan to have the HP dataset publicly available by February 2017!\n"
  },
  {
    "people": [
      "Zhou"
    ],
    "review": "CONTRIBUTIONS\nLarge-scale experiments are used to measure the capacity and trainability of different RNN architectures. Capacity experiments suggest that across all architectures, RNNs can store between three and six bits of information per parameter, with ungated RNNs having the highest per-parameter capacity. All architectures are able to store approximately one floating point number per hidden unit. Trainability experiments show that ungated architectures (RNN, IRNN) are much harder to train than gated architectures (GRU, LSTM, UGRNN, +RNN). The paper also proposes two novel RNN architectures (UGRNN and +RNN); experiments suggest that the UGRNN has similar per-parameter capacity as the ungated RNN but is much easier to train, and that deep (8-layer) +RNN models are easier to train than existing architectures.\n\nCLARITY\nThe paper is well-written and easy to follow.\n\nNOVELTY\nThis paper is the first to my knowledge to empirically measure the number of bits of information that can be stored per learnable parameter. The idea of measuring network capacity by finding the dataset size and other hyperparameters that maximizes mutual information is a particularly novel experimental setup.\n\nThe proposed UGRNN is similar but not identical to the minimal gated unit proposed by Zhou et al, \u201cMinimal Gated Unit for Recurrent Neural Networks\u201d, International Journal of Automation and Computing, 2016.\n\nSIGNIFICANCE\nI have mixed feelings about the significance of this paper. I found the experiments interesting, but I don\u2019t feel that they reveal anything particularly surprising or unexpected about recurrent networks; it is hard to see how any of the experimental results will change the way either that I think about RNNs, or the way that I will use them in my own future work. On the other hand it is valuable to see intuitive results about RNNs confirmed by rigorous experiments, especially since few researchers have the computational resources to perform such large-scale experiments.\n\nThe capacity experiments (both per-parameter capacity and per-unit capacity) essentially force the network to model random data. For most applications of RNNs, however, we do not expect them to work with random data; instead when applied in machine translation or language modeling or image captioning or any number of real-world tasks, we hope that RNNs can learn to model data that is anything but random. It is not clear to me that an architecture\u2019s ability to model random data should be beneficial in modeling real-world data; indeed, the experiments in Section 2.1 show that architectures vary in their capacity to model random data, but the text8 experiments in Section 3 show that these same architectures do not significantly vary in their capacity to model real-world data.\n\nI do not think that the experimental results in the paper are sufficient to prove the significance of the proposed UGRNN and +RNN architectures. It is interesting that the UGRNN can achieve comparable bits per parameter as the ungated RNN and that the deep +RNNs are more easily trainable than other architectures, but the only experiments on a real-world task (language modeling on text8) do not show these architectures to be significantly better than GRU or LSTM.\n\nSUMMARY\nI wish that the experiments had revealed more surprising insights about RNNs, though there is certainly value in experimentally verifying intuitive results. The proposed UGRNN and +RNN architectures show some promising results on synthetic tasks, but I wish that they showed more convincing performance on real-world tasks. Overall I think that the good outweighs the bad, and that the ideas of this paper are of value to the community.\n\nPROS\n- The paper is the first of my knowledge to explicitly measure the bits per parameter that RNNs can store\n- The paper experimentally confirms several intuitive ideas about RNNs:\n    - RNNs of any architecture can store about one number per hidden unit from the input\n    - Different RNN architectures should be compared by their parameter count, not their hidden unit count\n    - With very careful hyperparameter tuning, all RNN architectures perform about the same on text8 language modeling\n    - Gated architectures are easier to train than non-gated RNNs\n\nCONS\n- Experiments do not reveal anything particularly surprising or unexpected\n- The UGRNN and +RNN architectures do not feel well-motivated\n- The utility of the UGRNN and +RNN architectures is not well-established"
  },
  {
    "people": [
      "Springenberg",
      "Diederik Kingma",
      "Tim Salimans",
      "Diederik P Kingma"
    ],
    "review": "We thank all the reviewers for their comments. We would like to provide some clarification regarding the experiments in the paper, and address some of the concerns which were raised.\n\n>> The CIFAR-10 results are worse than those in the paper that originally proposed the network architecture they use (Springenberg et al). The VAE results on MNIST are also horrible.\n\nIf we use exactly the same architecture of Springenberg et al., then our results on CIFAR are, as predicted by the theory, comparable asymptotically, and better for smaller nets. We have added experiments that show this in the revised version to be uploaded soon. Also, our results on VAE are comparable to [KW13] for a similar architecture.\n\nNote, however, that the goal of our experiments is not to improve state-of-the-art on CIFAR-10 or MNIST, but to illustrate the effect of Information Dropout when compared to other forms of dropout, and to validate the intuition derived from the theory. For this reason, for the experiments in the paper we chose the simplest empirical settings, and modified the All Convolutional Net to isolate potentially confounding factors: we removed weight decay, increased the batch size to reduce gradient noise, simplified the architecture by removing the initial dropout layer, and used less aggressive learning rates and no fine tuning.  We also replaced ReLU with Softplus to make the results comparable with those of [KSW15]. This also served to validate the theory which applies to both ReLU and Softplus. \n\nMany factors affect empirical performance, only few of which are relevant to validating our theory. To the latter hand, we went to great length to ensure that the experiments are *controlled*. Only under careful control can the experiments be convincing in validating the theory.\n\nNevertheless, as suggested by the reviewers, we are currently exploring other experiments that would further illustrate the tradeoff between invariance to nuisances and sufficiency as mediated by the coefficient \\beta. We will add these along with the further tests using the same architecture of Springerberg, as described above.\n\n>> The results on CIFAR-10 in Figure 3(b) seem to be on a validation set\n\nWe are using the same nomenclature of [KSW15], since we want to make a direct comparison with their experiment. As customary for CIFAR, the data is divided into a disjoint training set (50,000 samples) and validation/test set (10,000 samples). We feel that \"validation\" here is more appropriate.\n\n[KSW15] Diederik Kingma, Tim Salimans, and Max Welling, \"Variational Dropout and the Local Reparameterization Trick\", 2015\n\n[KW13] Diederik P Kingma, Max Welling, \"Auto-Encoding Variational Bayes\", 2013\n"
  },
  {
    "people": [
      "Springenberg"
    ],
    "review": "Paper summary\nThis paper develops a generalization of dropout using information theoretic\nprinciples. The basic idea is that when learning a representation z of input x\nwith the aim of predicting y, we must choose a z such that it carries the least\namount of information about x, as long as it can predict y. This idea can be\nformalized using the Information Bottleneck Lagrangian. This leads to an\noptimization problem which is similar to the one derived for variational\ndropout, the difference being that Information dropout allows for a scaling\nfactor associated with the KL divergence term that encourages noise. The amount\nof noise being added is made a parameterized function of the data and this\nfunction is optimized along with the rest of the model. Experimental results on\nCIFAR-10 and MNIST show (small) improvements over binary dropout.\n\nStrengths\n- The paper highlights an important conceptual link between probabilistic\n  variational methods and information theoretic methods, showing that dropout\ncan be generalized using both formalisms to arrive at very similar models.\n- The presentation of the model is excellent.\n- The experimental results on cluttered MNIST are impressive.\n\nWeaknesses\n- The results on CIFAR-10 in Figure 3(b) seem to be on a validation set (unless\n  the axis label is a typo). It is not clear why the test set was not used. This\nmakes it hard to compare to results reported in Springenberg et al, as well as\nother results in literature.\n\nQuality\nThe theoretical exposition is high quality. Figure 2 gives a nice qualitative\nassessment of what the model is doing. However, the experimental results\nsection can be made better, for example, by matching the results on CIFAR-10 as\nreported in Springenberg et al. and trying to improve on those using information\ndropout.\n\nClarity\nThe paper is well written and easy to follow.\n\nOriginality\nThe derivation of the information dropout optimization problem using IB\nLagrangian is novel. However, the final model is quite close to variational\ndropout.\n\nSignificance\nThis paper will be of general interest to researchers in representation learning\nbecause it highlights an alternative way to think about latent variables (as\ninformation bottlenecks). However, unless the model can be shown to achieve\nsignificant improvements over simple dropout, its wider impact is likely to be\nlimited.\n\nOverall\nThe paper presents an insightful theoretical derivation and good preliminary\nresults. The experimental section can be improved.\n\nMinor comments and suggestions -\n- expecially -> especially\n- trough -> through\n- There is probably a minus sign missing in the expression for H(y|z) above Eq (2).\n- Figure 3(a) has error bars, but 3(b) doesn't. It might be a good idea to have those\nfor Figure 3(b) as well.\n- Please consider comparing Figure 2 with the activity map of a standard CNN\n  trained with binary dropout, so we can see if similar filtering out is\nhappening there already."
  },
  {
    "people": [
      "Tishby"
    ],
    "review": "An interesting connection is made between dropout, Tishby et al's \"information bottleneck\" and VAEs. Specifically, classification of 'y' from 'x' is split in two faces: an inference model z ~ q(z|x), a prior p(z), and a classifier y ~ p(y|z). By optimizing the objective E_{(x,y)~data} [ E_{z~q(z|x)}[log p(x|y)] + lambda * KL(q(z|x)||p(z))], with lambda <= 1, an information bottleneck 'z' is formed, where lambda controls an upper bound on the number of bits traveling through 'z'.\n\nThe objective is equivalent to a VAE objective with downweighted KL(posterior|prior), an encoder that takes as input 'x', and a decoder that only predicts 'x'.\n\n- Related work (section 2) is discussed sufficiently. \n- In section 3, would be better to remind us the definition of mutual information.\n- Connection to VAEs in section 5 is interesting.\n- Unfortunately, the MNIST/CIFAR-10 results are not great. Since the method is potentially more flexible than other forms of dropout, this is slightly disappointing.\n- It's unclear why the CIFAR-10 results seem to be substantially worse than the results originally reported for that architecture.\n- It's unclear which version of 'beta' was used in figure 3a.\n\nOverall I think the theory presented in the paper is promising. However, the paper lacks sufficiently convincing experimental results, and I encourage the authors to do further experiments that prove significant improvements, at least on CIFAR-10, perhaps on larger problems."
  },
  {
    "people": [
      "Springenberg",
      "Diederik Kingma",
      "Tim Salimans",
      "Diederik P Kingma"
    ],
    "review": "We thank all the reviewers for their comments. We would like to provide some clarification regarding the experiments in the paper, and address some of the concerns which were raised.\n\n>> The CIFAR-10 results are worse than those in the paper that originally proposed the network architecture they use (Springenberg et al). The VAE results on MNIST are also horrible.\n\nIf we use exactly the same architecture of Springenberg et al., then our results on CIFAR are, as predicted by the theory, comparable asymptotically, and better for smaller nets. We have added experiments that show this in the revised version to be uploaded soon. Also, our results on VAE are comparable to [KW13] for a similar architecture.\n\nNote, however, that the goal of our experiments is not to improve state-of-the-art on CIFAR-10 or MNIST, but to illustrate the effect of Information Dropout when compared to other forms of dropout, and to validate the intuition derived from the theory. For this reason, for the experiments in the paper we chose the simplest empirical settings, and modified the All Convolutional Net to isolate potentially confounding factors: we removed weight decay, increased the batch size to reduce gradient noise, simplified the architecture by removing the initial dropout layer, and used less aggressive learning rates and no fine tuning.  We also replaced ReLU with Softplus to make the results comparable with those of [KSW15]. This also served to validate the theory which applies to both ReLU and Softplus. \n\nMany factors affect empirical performance, only few of which are relevant to validating our theory. To the latter hand, we went to great length to ensure that the experiments are *controlled*. Only under careful control can the experiments be convincing in validating the theory.\n\nNevertheless, as suggested by the reviewers, we are currently exploring other experiments that would further illustrate the tradeoff between invariance to nuisances and sufficiency as mediated by the coefficient \\beta. We will add these along with the further tests using the same architecture of Springerberg, as described above.\n\n>> The results on CIFAR-10 in Figure 3(b) seem to be on a validation set\n\nWe are using the same nomenclature of [KSW15], since we want to make a direct comparison with their experiment. As customary for CIFAR, the data is divided into a disjoint training set (50,000 samples) and validation/test set (10,000 samples). We feel that \"validation\" here is more appropriate.\n\n[KSW15] Diederik Kingma, Tim Salimans, and Max Welling, \"Variational Dropout and the Local Reparameterization Trick\", 2015\n\n[KW13] Diederik P Kingma, Max Welling, \"Auto-Encoding Variational Bayes\", 2013\n"
  },
  {
    "people": [
      "Springenberg"
    ],
    "review": "Paper summary\nThis paper develops a generalization of dropout using information theoretic\nprinciples. The basic idea is that when learning a representation z of input x\nwith the aim of predicting y, we must choose a z such that it carries the least\namount of information about x, as long as it can predict y. This idea can be\nformalized using the Information Bottleneck Lagrangian. This leads to an\noptimization problem which is similar to the one derived for variational\ndropout, the difference being that Information dropout allows for a scaling\nfactor associated with the KL divergence term that encourages noise. The amount\nof noise being added is made a parameterized function of the data and this\nfunction is optimized along with the rest of the model. Experimental results on\nCIFAR-10 and MNIST show (small) improvements over binary dropout.\n\nStrengths\n- The paper highlights an important conceptual link between probabilistic\n  variational methods and information theoretic methods, showing that dropout\ncan be generalized using both formalisms to arrive at very similar models.\n- The presentation of the model is excellent.\n- The experimental results on cluttered MNIST are impressive.\n\nWeaknesses\n- The results on CIFAR-10 in Figure 3(b) seem to be on a validation set (unless\n  the axis label is a typo). It is not clear why the test set was not used. This\nmakes it hard to compare to results reported in Springenberg et al, as well as\nother results in literature.\n\nQuality\nThe theoretical exposition is high quality. Figure 2 gives a nice qualitative\nassessment of what the model is doing. However, the experimental results\nsection can be made better, for example, by matching the results on CIFAR-10 as\nreported in Springenberg et al. and trying to improve on those using information\ndropout.\n\nClarity\nThe paper is well written and easy to follow.\n\nOriginality\nThe derivation of the information dropout optimization problem using IB\nLagrangian is novel. However, the final model is quite close to variational\ndropout.\n\nSignificance\nThis paper will be of general interest to researchers in representation learning\nbecause it highlights an alternative way to think about latent variables (as\ninformation bottlenecks). However, unless the model can be shown to achieve\nsignificant improvements over simple dropout, its wider impact is likely to be\nlimited.\n\nOverall\nThe paper presents an insightful theoretical derivation and good preliminary\nresults. The experimental section can be improved.\n\nMinor comments and suggestions -\n- expecially -> especially\n- trough -> through\n- There is probably a minus sign missing in the expression for H(y|z) above Eq (2).\n- Figure 3(a) has error bars, but 3(b) doesn't. It might be a good idea to have those\nfor Figure 3(b) as well.\n- Please consider comparing Figure 2 with the activity map of a standard CNN\n  trained with binary dropout, so we can see if similar filtering out is\nhappening there already."
  },
  {
    "people": [
      "Tishby"
    ],
    "review": "An interesting connection is made between dropout, Tishby et al's \"information bottleneck\" and VAEs. Specifically, classification of 'y' from 'x' is split in two faces: an inference model z ~ q(z|x), a prior p(z), and a classifier y ~ p(y|z). By optimizing the objective E_{(x,y)~data} [ E_{z~q(z|x)}[log p(x|y)] + lambda * KL(q(z|x)||p(z))], with lambda <= 1, an information bottleneck 'z' is formed, where lambda controls an upper bound on the number of bits traveling through 'z'.\n\nThe objective is equivalent to a VAE objective with downweighted KL(posterior|prior), an encoder that takes as input 'x', and a decoder that only predicts 'x'.\n\n- Related work (section 2) is discussed sufficiently. \n- In section 3, would be better to remind us the definition of mutual information.\n- Connection to VAEs in section 5 is interesting.\n- Unfortunately, the MNIST/CIFAR-10 results are not great. Since the method is potentially more flexible than other forms of dropout, this is slightly disappointing.\n- It's unclear why the CIFAR-10 results seem to be substantially worse than the results originally reported for that architecture.\n- It's unclear which version of 'beta' was used in figure 3a.\n\nOverall I think the theory presented in the paper is promising. However, the paper lacks sufficiently convincing experimental results, and I encourage the authors to do further experiments that prove significant improvements, at least on CIFAR-10, perhaps on larger problems."
  },
  {
    "people": [
      "Bayes"
    ],
    "review": "This paper presents both an analysis of neural net optimization landscapes, and an optimization algorithm that encourages movement in directions of high entropy. The motivation is based on intuitions from physics.\n \n Pros\n  - the main idea is well-motivated, from a non-standard perspective.\n  - There are lots of side-experiments supporting the claims for the motivation.\n Cons\n  - The propose method is very complicated, and it sounds like good performance depended on adding and annealing yet another hyperparameter, referred to as 'scoping'.\n  - The motivating intuition has been around for a long time in different forms. In particular, the proposed method is very closely related to stochastic variational inference, or MCMC methods. Appendix C makes it clear that the two methods aren't identical, but I wish the authors had simply run SVI with their proposed modification, instead of appearing to re-invent the idea of maximizing local volume from scratch. The intuition that good generalization comes from regions of high volume is also exactly what Bayes rule says.\n \n In summary, while there is improvement for the paper, the idea is well-motivated and the experimental results are sound."
  },
  {
    "people": [
      "Hardt"
    ],
    "review": "We thank the reviewers and the area chair for their insightful feedback. We have incorporated all comments into our current draft. We first discuss the updates to the paper and address common questions raised by the reviewers. We have also posted individual comments to the reviewers to address specific questions.\n\nUpdates\n=====\n\na) We have updated the experimental section of the paper with new experiments on MNIST, CIFAR-10 and two datasets on RNNs (PTB and char-LSTM).\n\nb) We have modified the algorithm to introduce a technique called \"scoping\". This increases the scope parameter \\gamma as training progresses instead of fixing it and has the effect of exploring the parameter space in the beginning of training (Sec. 4.3). As a result of this, we can now train all our networks with only the local entropy loss instead of treating it as a regularizer (Eqn. 6).\n\nc) For a fair comparison of the training time, we now plot the error curves in Figs. 4, 5 and 6 against the \"effective number of epochs\", i.e., the number of epochs of Entropy-SGD is multiplied by the number of Langevin iterations L (we set L=1 for SGD/Adam). Thus the x-axis is a direct measure of the wall-clock time agnostic to the underlying implementation and is proportional to the number of back-props as suggested.\n\nWe obtain significant speed-ups with respect to our earlier results due to scoping and the wall-clock training time for all our networks with Entropy-SGD is now comparable to SGD/Adam. In fact, Entropy-SGD is almost twice as fast as SGD on our experiments on RNNs and also obtains a better generalization error (cf. Fig. 6). The acceleration for CNNs on MNIST and CIFAR-10 is about 20%.\n\nTable 1 (page 11) summarizes the experimental section of the paper.\n\nd) Improved exposition of the algorithm in Sec. 4.2 that includes intuition for hyper-parameter tuning. We have expanded the discussion of experiments in Sec. 5.3, 5.4 to provide more details and insights that relate to the energy landscape of deep networks.\n\ne) Appendix C discusses the possible connections to variational inference (this is the same material as the discussion with the AC below). Sec. C.1 presents an experimental comparison of local entropy vs. SGLD. We note here that our results using Entropy-SGD for both CNNs and RNNs are much better than vanilla SGLD in significantly smaller (~3-5x) wall-clock times.\n\nResponse to the reviewers:\n================\n\n>> smoothing of the original loss vs. local entropy\nWe discuss this in detail in the related work in Sec. 2 and Appendix C. While smoothing the original loss function using convolutions or averaging the gradient over perturbations of weights reduces the ruggedness of the energy landscape, it does not help with sharp, narrow valleys. Local entropy introduces a measure that focuses on wide local minima in the energy landscape (cf. Fig. 2 which has a \"global\" minimum at a wide valley); this is the primary reason for its efficacy. Smoothing the loss function can also, for instance, generate an artificial local minimum between two close by sharp valleys, which is detrimental to generalization.\n\n>> unrealistic eigenvalue assumption in Sec. 4.3\nWe have clarified this point in Remark 4. Our analysis employs an assumption that the Hessian \\nabla^2 f(x) does not have eigenvalues in the set [-2\\gamma-c, c] for some c > 0. This is admittedly unrealistic, for instance, the eigenspectrum of the Hessian in Fig. 1 has a large fraction of its eigenvalues almost zero. Let us note though that Fig. 1 is plotted at a local minimum, from our experiments, the eigenspectrum is less sparse in the beginning of training.\n\nWe would like to remark that the bound on uniform stability in Thm. 3 by Hardt et al. assumes global conditions on the smoothness of the loss function; one imagines that Eqn. 9 remains qualitatively the same (in particular, with respect to the number of training iterations) even if this assumption is violated to an extent before convergence happens. Obtaining a rigorous generalization bound without this assumption requires a dynamical analysis of SGD and seems out of reach currently."
  },
  {
    "people": [
      "Hochreiter",
      "Schmidhuber"
    ],
    "review": "Overview: \n\nThis paper introduces a biasing term for SGD that, in theoretical results and a toy example, yields solutions with an approximately equal or lower generalization error. This comes at a computational cost of estimating the gradient of the biasing term for each iteration through stochastic gradient Langevin dynamics, approximating an MCMC sample of the log partition function of a modified Gibbs distribution. The cost is equivalent to adding an inner for-loop to the standard SGD algorithm for each minibatch.\n\nPros:\n- Reviews and distills many results and theorems from past 2 decades that suggest a promising way forward for increasing the generalizability of deep neural networks\n- Generally very well written and well presented results, with interesting discussion of eigenvalues of Hessian as a way to characterize \u201cflat\u201d minima\n- Promising mathematical arguments suggest that E-SGD has generalization error bounded below by SGD, motivating further research in the area\n\nCons / points suggested for a rebuttal:\n(1) One claim of the paper given in the abstract is \u201dexperiments on competitive baselines demonstrate that Entropy-SGD leads to improved generalization and has the potential to accelerate training.\u201c This does not appear to be supported by the current set of experiments. As the authors comment in the discussion section, \u201cIn our experiments, Entropy-SGD results in a comparable generalization error as SGD, but always has a lower cross-entropy loss.\u201d It's not clear to me how to reconcile those two claims.\n\n(2) Similarly, the claim of accelerated training is not convincingly supported in the present version of the paper. Vanilla SGD requires a single forward pass through all M minibatches during one epoch for a parameter update, but the new method, E-SGD requires, L*M forward passes during one epoch where L is the number of Langevin updates, which require a minibatch sample each. This could in fact mean that E-SGD has worse computational complexity to reach the same point. In a remark on p.9, the authors note that a single epoch is defined to be \u201cthe number of parameter updates required to run through the dataset once.\u201d It\u2019s not clear to me how this answers the objection to a factor of L additional computations required for the inner-loop SGLD iterations. SGLD appears to introduces a potentially costly tradeoff that must be carefully managed by a user of E-SGD.\n\n(3) As the previous two points suggest, the paper could use some attention to the magnitude of the claims. For example, the introduction reads \u201cActively biasing towards wide valleys aids generalization, in fact, we can optimize solely the free energy term to obtain similar generalization error as SGD on the original loss function.\u201c According the the values reported on pp.9-10, only on MNIST is the generalization error, using only the free energy term (the log partition function of the modified Gibbs distribution), equivalent to using only the SGD loss function. This corresponds to setting rho to 0 in equation (6). On CIFAR-10, rho = 0.01 is used.\n\n(4) Another contribution of this paper, the characterization of the optimization landscape in terms of the eigenvalues of the Hessian and low generalization error being associated with flat local extrema, is helpful and interesting. I found the plots clear and useful. As another reviewer has already pointed out, there are high-level similarities to \u201cFlat Minima\u201d by Hochreiter and Schmidhuber (1997). The authors have responded already by adding a paragraph that helpfully explores some differences with H&S 1997. However, the similarities should also be carefully identified and mentioned. H&S 1997 includes detailed theoretical analysis that could be helpful for future work in this area, and has independently discovered a similar approach to training generalizable networks.\n\n(5) It's not clear how the assumption about the eigenvalues that were made in section 4.4 / Appendix B affect the application of this result to real-world problems. What magnitude of c>0 needs to be chosen? Does this correspond to a measurable characteristic of the dataset? It's a little mysterious in the current version of the paper.\n"
  },
  {
    "people": [
      "Sepp",
      "J\u00fcrgen Schmidhuber",
      "Nitish Shirish"
    ],
    "review": "We have updated the \"Related work\" section of the paper with a discussion of [HS97] and [KS16].\n\n[HS97] Hochreiter, Sepp, and J\u00fcrgen Schmidhuber. \"Flat minima.\" Neural Computation 9.1 (1997): 1-42.\n[KS16] Keskar, Nitish Shirish, et al. \"On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima.\" arXiv:1609.04836 (2016)."
  },
  {
    "people": [
      "Schmidhuber"
    ],
    "review": "Very interesting paper. The authors propose an algorithm which moves the parameters of a neural network towards flat landscapes of the error surface while decreasing the training error. I'm wondering what are the advantages over the related method \"Flat Minima Search\" (FMS) HS97. In HS97 the authors have shown a connection between flat minima and good generalization via MDL. They suggest the FMS algorithm to find such flat regions.\n    \n\nReference:\n\n[HS97] Hochreiter, S. and Schmidhuber, J. (1997), Flat Minima.\n       "
  },
  {
    "people": [
      "Bayes"
    ],
    "review": "This paper presents both an analysis of neural net optimization landscapes, and an optimization algorithm that encourages movement in directions of high entropy. The motivation is based on intuitions from physics.\n \n Pros\n  - the main idea is well-motivated, from a non-standard perspective.\n  - There are lots of side-experiments supporting the claims for the motivation.\n Cons\n  - The propose method is very complicated, and it sounds like good performance depended on adding and annealing yet another hyperparameter, referred to as 'scoping'.\n  - The motivating intuition has been around for a long time in different forms. In particular, the proposed method is very closely related to stochastic variational inference, or MCMC methods. Appendix C makes it clear that the two methods aren't identical, but I wish the authors had simply run SVI with their proposed modification, instead of appearing to re-invent the idea of maximizing local volume from scratch. The intuition that good generalization comes from regions of high volume is also exactly what Bayes rule says.\n \n In summary, while there is improvement for the paper, the idea is well-motivated and the experimental results are sound."
  },
  {
    "people": [
      "Hardt"
    ],
    "review": "We thank the reviewers and the area chair for their insightful feedback. We have incorporated all comments into our current draft. We first discuss the updates to the paper and address common questions raised by the reviewers. We have also posted individual comments to the reviewers to address specific questions.\n\nUpdates\n=====\n\na) We have updated the experimental section of the paper with new experiments on MNIST, CIFAR-10 and two datasets on RNNs (PTB and char-LSTM).\n\nb) We have modified the algorithm to introduce a technique called \"scoping\". This increases the scope parameter \\gamma as training progresses instead of fixing it and has the effect of exploring the parameter space in the beginning of training (Sec. 4.3). As a result of this, we can now train all our networks with only the local entropy loss instead of treating it as a regularizer (Eqn. 6).\n\nc) For a fair comparison of the training time, we now plot the error curves in Figs. 4, 5 and 6 against the \"effective number of epochs\", i.e., the number of epochs of Entropy-SGD is multiplied by the number of Langevin iterations L (we set L=1 for SGD/Adam). Thus the x-axis is a direct measure of the wall-clock time agnostic to the underlying implementation and is proportional to the number of back-props as suggested.\n\nWe obtain significant speed-ups with respect to our earlier results due to scoping and the wall-clock training time for all our networks with Entropy-SGD is now comparable to SGD/Adam. In fact, Entropy-SGD is almost twice as fast as SGD on our experiments on RNNs and also obtains a better generalization error (cf. Fig. 6). The acceleration for CNNs on MNIST and CIFAR-10 is about 20%.\n\nTable 1 (page 11) summarizes the experimental section of the paper.\n\nd) Improved exposition of the algorithm in Sec. 4.2 that includes intuition for hyper-parameter tuning. We have expanded the discussion of experiments in Sec. 5.3, 5.4 to provide more details and insights that relate to the energy landscape of deep networks.\n\ne) Appendix C discusses the possible connections to variational inference (this is the same material as the discussion with the AC below). Sec. C.1 presents an experimental comparison of local entropy vs. SGLD. We note here that our results using Entropy-SGD for both CNNs and RNNs are much better than vanilla SGLD in significantly smaller (~3-5x) wall-clock times.\n\nResponse to the reviewers:\n================\n\n>> smoothing of the original loss vs. local entropy\nWe discuss this in detail in the related work in Sec. 2 and Appendix C. While smoothing the original loss function using convolutions or averaging the gradient over perturbations of weights reduces the ruggedness of the energy landscape, it does not help with sharp, narrow valleys. Local entropy introduces a measure that focuses on wide local minima in the energy landscape (cf. Fig. 2 which has a \"global\" minimum at a wide valley); this is the primary reason for its efficacy. Smoothing the loss function can also, for instance, generate an artificial local minimum between two close by sharp valleys, which is detrimental to generalization.\n\n>> unrealistic eigenvalue assumption in Sec. 4.3\nWe have clarified this point in Remark 4. Our analysis employs an assumption that the Hessian \\nabla^2 f(x) does not have eigenvalues in the set [-2\\gamma-c, c] for some c > 0. This is admittedly unrealistic, for instance, the eigenspectrum of the Hessian in Fig. 1 has a large fraction of its eigenvalues almost zero. Let us note though that Fig. 1 is plotted at a local minimum, from our experiments, the eigenspectrum is less sparse in the beginning of training.\n\nWe would like to remark that the bound on uniform stability in Thm. 3 by Hardt et al. assumes global conditions on the smoothness of the loss function; one imagines that Eqn. 9 remains qualitatively the same (in particular, with respect to the number of training iterations) even if this assumption is violated to an extent before convergence happens. Obtaining a rigorous generalization bound without this assumption requires a dynamical analysis of SGD and seems out of reach currently."
  },
  {
    "people": [
      "Hochreiter",
      "Schmidhuber"
    ],
    "review": "Overview: \n\nThis paper introduces a biasing term for SGD that, in theoretical results and a toy example, yields solutions with an approximately equal or lower generalization error. This comes at a computational cost of estimating the gradient of the biasing term for each iteration through stochastic gradient Langevin dynamics, approximating an MCMC sample of the log partition function of a modified Gibbs distribution. The cost is equivalent to adding an inner for-loop to the standard SGD algorithm for each minibatch.\n\nPros:\n- Reviews and distills many results and theorems from past 2 decades that suggest a promising way forward for increasing the generalizability of deep neural networks\n- Generally very well written and well presented results, with interesting discussion of eigenvalues of Hessian as a way to characterize \u201cflat\u201d minima\n- Promising mathematical arguments suggest that E-SGD has generalization error bounded below by SGD, motivating further research in the area\n\nCons / points suggested for a rebuttal:\n(1) One claim of the paper given in the abstract is \u201dexperiments on competitive baselines demonstrate that Entropy-SGD leads to improved generalization and has the potential to accelerate training.\u201c This does not appear to be supported by the current set of experiments. As the authors comment in the discussion section, \u201cIn our experiments, Entropy-SGD results in a comparable generalization error as SGD, but always has a lower cross-entropy loss.\u201d It's not clear to me how to reconcile those two claims.\n\n(2) Similarly, the claim of accelerated training is not convincingly supported in the present version of the paper. Vanilla SGD requires a single forward pass through all M minibatches during one epoch for a parameter update, but the new method, E-SGD requires, L*M forward passes during one epoch where L is the number of Langevin updates, which require a minibatch sample each. This could in fact mean that E-SGD has worse computational complexity to reach the same point. In a remark on p.9, the authors note that a single epoch is defined to be \u201cthe number of parameter updates required to run through the dataset once.\u201d It\u2019s not clear to me how this answers the objection to a factor of L additional computations required for the inner-loop SGLD iterations. SGLD appears to introduces a potentially costly tradeoff that must be carefully managed by a user of E-SGD.\n\n(3) As the previous two points suggest, the paper could use some attention to the magnitude of the claims. For example, the introduction reads \u201cActively biasing towards wide valleys aids generalization, in fact, we can optimize solely the free energy term to obtain similar generalization error as SGD on the original loss function.\u201c According the the values reported on pp.9-10, only on MNIST is the generalization error, using only the free energy term (the log partition function of the modified Gibbs distribution), equivalent to using only the SGD loss function. This corresponds to setting rho to 0 in equation (6). On CIFAR-10, rho = 0.01 is used.\n\n(4) Another contribution of this paper, the characterization of the optimization landscape in terms of the eigenvalues of the Hessian and low generalization error being associated with flat local extrema, is helpful and interesting. I found the plots clear and useful. As another reviewer has already pointed out, there are high-level similarities to \u201cFlat Minima\u201d by Hochreiter and Schmidhuber (1997). The authors have responded already by adding a paragraph that helpfully explores some differences with H&S 1997. However, the similarities should also be carefully identified and mentioned. H&S 1997 includes detailed theoretical analysis that could be helpful for future work in this area, and has independently discovered a similar approach to training generalizable networks.\n\n(5) It's not clear how the assumption about the eigenvalues that were made in section 4.4 / Appendix B affect the application of this result to real-world problems. What magnitude of c>0 needs to be chosen? Does this correspond to a measurable characteristic of the dataset? It's a little mysterious in the current version of the paper.\n"
  },
  {
    "people": [
      "Sepp",
      "J\u00fcrgen Schmidhuber",
      "Nitish Shirish"
    ],
    "review": "We have updated the \"Related work\" section of the paper with a discussion of [HS97] and [KS16].\n\n[HS97] Hochreiter, Sepp, and J\u00fcrgen Schmidhuber. \"Flat minima.\" Neural Computation 9.1 (1997): 1-42.\n[KS16] Keskar, Nitish Shirish, et al. \"On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima.\" arXiv:1609.04836 (2016)."
  },
  {
    "people": [
      "Schmidhuber"
    ],
    "review": "Very interesting paper. The authors propose an algorithm which moves the parameters of a neural network towards flat landscapes of the error surface while decreasing the training error. I'm wondering what are the advantages over the related method \"Flat Minima Search\" (FMS) HS97. In HS97 the authors have shown a connection between flat minima and good generalization via MDL. They suggest the FMS algorithm to find such flat regions.\n    \n\nReference:\n\n[HS97] Hochreiter, S. and Schmidhuber, J. (1997), Flat Minima.\n       "
  },
  {
    "people": [
      "Sabour",
      "Sabour"
    ],
    "review": "This paper studies the effects of modifying intermediate representations arising in deep convolutional networks, with the purpose of visualizing the role of specific neurons, and also to construct adversarial examples. The paper presents experiments on MNIST as well as faces. \n \n The reviewers agreed that, while this contribution presents an interesting framework, it lacks comparisons with existing methods, and the description of the method lacks sufficient rigor. In light of the discussions and the current state of the submission, the AC recommends rejection. \n \n Since the final scores of the reviewers might suggest otherwise, please let me explain my recommendation. \n \n The main contribution of this paper seems to be essentially a fast alternative to the method proposed in 'Adversarial Manipulation of Deep Representations', by Sabour et al, ICLR'16, although the lack of rigor and clarity in the presentation of section 3 makes this assessment uncertain. The most likely 'interpretation' of Eq (3) suggests that eta(x_o, x_t) = nabla_{x_o}( || f^(l)_w(x_t) - f^(l)_w(x_o) ||^2), which is simply one step of gradient descent of the method described in Sabour et al. One reviewer actually asked for clarification on this point on Dec. 26th, but the problem seems to be still present in the current manuscript. \n \n More generally, visualization and adversarial methods based on backpropagation of some form of distance measured in feature space towards the pixel space are not new; they can be traced back to Simoncelli & Portilla '99. \n Fast approximations based on simply stopping the gradient descent after one iteration do not constitute enough novelty. \n \n Another instance of lack of clarity that has also been pointed out in this discussion but apparently not addressed in the final version is the so-called PASS measure. It is not defined anywhere in the text, and the authors should not expect the reader to know its definition beforehand. \n \n Besides these issues, the paper does not contribute to the state-of-the-art of adversarial training nor feature visualization, mostly because its experiments are limited to mnist and face datasets. Since the main contribution of the paper is empirical, more emphasis should be made to present experiments on larger, more numerous datasets."
  },
  {
    "people": [
      "Alex"
    ],
    "review": "Hi,\n\nI have a question about eq. 3 and 4.\n\nAs far as I understood from the beginning of section 3, f_{w}^{l} (x) is essentially activations at layer l, which means that value of f_{w}^{l} (x) is in R^{d_l} space where d_l is dimensionality of output of layer l.\nIn eq. (3) and (4) you \\eta compute gradient of f_{w}^{l} (x) over input x, which I would expect to be Jacobian matrix with size d_l*n (where n - dimensionality of input x). So looking at eq (3) and (4) I would expect that \\eta and x has different dimensionality. At the same time in section 4 you add s*\\eta to x.\nCould you explain this discrepancy in dimensionality and how \\eta should be calculated?\nMaybe you meant that numerator of eq (3) and (4) contain norm of f_{w}^{l} instead of it's value?\n\nThanks,\nAlex"
  },
  {
    "people": [
      "Sabour",
      "Sabour"
    ],
    "review": "This paper studies the effects of modifying intermediate representations arising in deep convolutional networks, with the purpose of visualizing the role of specific neurons, and also to construct adversarial examples. The paper presents experiments on MNIST as well as faces. \n \n The reviewers agreed that, while this contribution presents an interesting framework, it lacks comparisons with existing methods, and the description of the method lacks sufficient rigor. In light of the discussions and the current state of the submission, the AC recommends rejection. \n \n Since the final scores of the reviewers might suggest otherwise, please let me explain my recommendation. \n \n The main contribution of this paper seems to be essentially a fast alternative to the method proposed in 'Adversarial Manipulation of Deep Representations', by Sabour et al, ICLR'16, although the lack of rigor and clarity in the presentation of section 3 makes this assessment uncertain. The most likely 'interpretation' of Eq (3) suggests that eta(x_o, x_t) = nabla_{x_o}( || f^(l)_w(x_t) - f^(l)_w(x_o) ||^2), which is simply one step of gradient descent of the method described in Sabour et al. One reviewer actually asked for clarification on this point on Dec. 26th, but the problem seems to be still present in the current manuscript. \n \n More generally, visualization and adversarial methods based on backpropagation of some form of distance measured in feature space towards the pixel space are not new; they can be traced back to Simoncelli & Portilla '99. \n Fast approximations based on simply stopping the gradient descent after one iteration do not constitute enough novelty. \n \n Another instance of lack of clarity that has also been pointed out in this discussion but apparently not addressed in the final version is the so-called PASS measure. It is not defined anywhere in the text, and the authors should not expect the reader to know its definition beforehand. \n \n Besides these issues, the paper does not contribute to the state-of-the-art of adversarial training nor feature visualization, mostly because its experiments are limited to mnist and face datasets. Since the main contribution of the paper is empirical, more emphasis should be made to present experiments on larger, more numerous datasets."
  },
  {
    "people": [
      "Alex"
    ],
    "review": "Hi,\n\nI have a question about eq. 3 and 4.\n\nAs far as I understood from the beginning of section 3, f_{w}^{l} (x) is essentially activations at layer l, which means that value of f_{w}^{l} (x) is in R^{d_l} space where d_l is dimensionality of output of layer l.\nIn eq. (3) and (4) you \\eta compute gradient of f_{w}^{l} (x) over input x, which I would expect to be Jacobian matrix with size d_l*n (where n - dimensionality of input x). So looking at eq (3) and (4) I would expect that \\eta and x has different dimensionality. At the same time in section 4 you add s*\\eta to x.\nCould you explain this discrepancy in dimensionality and how \\eta should be calculated?\nMaybe you meant that numerator of eq (3) and (4) contain norm of f_{w}^{l} instead of it's value?\n\nThanks,\nAlex"
  },
  {
    "people": [
      "Neelakantan"
    ],
    "review": "This paper introduces the concept of fuzzy paraphrases to aid in the learning of distributed word representations from a corpus augmented by a lexicon or ontology. Sometimes polysemy is context-dependent, but prior approaches have neglected this fact when incorporating external paraphrase information during learning. The main idea is to introduce a function that essentially judges the context-sensitivity of paraphrase candidates, down-weighting those candidates that depend strongly on context. This function is inferred from bilingual translation agreement.\n\nThe main argumentation leading to the model selection is intuitive, and I believe that the inclusion of good paraphrases and the elimination of bad paraphrases during training should in principle improve word representation quality. However, the main questions are how well the proposed method achieves this goal, and, even if it achieves it well, whether it makes much difference in practical terms.\n\nRegarding the first question, I am not entirely convinced that the parameterization of the control function f(x_ij) is optimal. It would have been nice to see some experiments investigating different choices, in particular some baselines where the effect of f is diminished (so that it reduces to f=1 in the limit) would have been interesting. I also feel like there would be a lot to gain from having f be a function of the nearby word embeddings, though this would obvious incur a significant slowdown. (See for example 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space' by Neelakantan et al, which should probably be cited.) As it stands, the experimental results do not clearly distinguish the fuzzy paraphrase approach from prior work, i.e. tables 3 and 4 do not show major trends one way or the other.\n\nRegarding the second question, it is hard to draw many conclusions from analogy tasks alone, especially when effects unrelated to good/bad paraphrasing such as corpus size/content, window size, vocabulary size, etc., can have an outsize effect on performance. \n\nOverall, I think this is a good paper presenting a sensible idea, but I am not convinced by the experiments that the specific approach is achieving its goal. With some improved experiments and analysis, I would wholeheartedly recommend this paper for acceptance; as it stands, I am on the fence."
  },
  {
    "people": [
      "Linzen"
    ],
    "review": "This paper tries to leverage an external lexicon / knowledge base to improve corpus-based word representations by determining (in a fuzzy way) which potential paraphrase is the most appropriate in a particular context.\n\nI think this paper is a bit lost in translation. The grammatical and storytelling styles made it really difficult for me to concentrate, and even unintelligible at times. One of the most important criteria in a conference paper is to communicate one's ideas clearly; unfortunately, I do not feel that this paper meets that standard.\n\nIn addition, the evaluation is rather lacking. There are many ways to evaluate word representations, and Google's analogy dataset has many issues (see, for example, Linzen's paper from RepEval 2016, as well as Drozd et al., COLING 2016).\n\nFinally, this work does not provide any qualitative result or motivation. Why does this method work better? Where does it fail? What have we learned about word representations / lexicons / corpus-based methods in general?"
  },
  {
    "people": [
      "Neelakantan"
    ],
    "review": "This paper introduces the concept of fuzzy paraphrases to aid in the learning of distributed word representations from a corpus augmented by a lexicon or ontology. Sometimes polysemy is context-dependent, but prior approaches have neglected this fact when incorporating external paraphrase information during learning. The main idea is to introduce a function that essentially judges the context-sensitivity of paraphrase candidates, down-weighting those candidates that depend strongly on context. This function is inferred from bilingual translation agreement.\n\nThe main argumentation leading to the model selection is intuitive, and I believe that the inclusion of good paraphrases and the elimination of bad paraphrases during training should in principle improve word representation quality. However, the main questions are how well the proposed method achieves this goal, and, even if it achieves it well, whether it makes much difference in practical terms.\n\nRegarding the first question, I am not entirely convinced that the parameterization of the control function f(x_ij) is optimal. It would have been nice to see some experiments investigating different choices, in particular some baselines where the effect of f is diminished (so that it reduces to f=1 in the limit) would have been interesting. I also feel like there would be a lot to gain from having f be a function of the nearby word embeddings, though this would obvious incur a significant slowdown. (See for example 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space' by Neelakantan et al, which should probably be cited.) As it stands, the experimental results do not clearly distinguish the fuzzy paraphrase approach from prior work, i.e. tables 3 and 4 do not show major trends one way or the other.\n\nRegarding the second question, it is hard to draw many conclusions from analogy tasks alone, especially when effects unrelated to good/bad paraphrasing such as corpus size/content, window size, vocabulary size, etc., can have an outsize effect on performance. \n\nOverall, I think this is a good paper presenting a sensible idea, but I am not convinced by the experiments that the specific approach is achieving its goal. With some improved experiments and analysis, I would wholeheartedly recommend this paper for acceptance; as it stands, I am on the fence."
  },
  {
    "people": [
      "Neelakantan"
    ],
    "review": "This paper introduces the concept of fuzzy paraphrases to aid in the learning of distributed word representations from a corpus augmented by a lexicon or ontology. Sometimes polysemy is context-dependent, but prior approaches have neglected this fact when incorporating external paraphrase information during learning. The main idea is to introduce a function that essentially judges the context-sensitivity of paraphrase candidates, down-weighting those candidates that depend strongly on context. This function is inferred from bilingual translation agreement.\n\nThe main argumentation leading to the model selection is intuitive, and I believe that the inclusion of good paraphrases and the elimination of bad paraphrases during training should in principle improve word representation quality. However, the main questions are how well the proposed method achieves this goal, and, even if it achieves it well, whether it makes much difference in practical terms.\n\nRegarding the first question, I am not entirely convinced that the parameterization of the control function f(x_ij) is optimal. It would have been nice to see some experiments investigating different choices, in particular some baselines where the effect of f is diminished (so that it reduces to f=1 in the limit) would have been interesting. I also feel like there would be a lot to gain from having f be a function of the nearby word embeddings, though this would obvious incur a significant slowdown. (See for example 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space' by Neelakantan et al, which should probably be cited.) As it stands, the experimental results do not clearly distinguish the fuzzy paraphrase approach from prior work, i.e. tables 3 and 4 do not show major trends one way or the other.\n\nRegarding the second question, it is hard to draw many conclusions from analogy tasks alone, especially when effects unrelated to good/bad paraphrasing such as corpus size/content, window size, vocabulary size, etc., can have an outsize effect on performance. \n\nOverall, I think this is a good paper presenting a sensible idea, but I am not convinced by the experiments that the specific approach is achieving its goal. With some improved experiments and analysis, I would wholeheartedly recommend this paper for acceptance; as it stands, I am on the fence."
  },
  {
    "people": [
      "Linzen"
    ],
    "review": "This paper tries to leverage an external lexicon / knowledge base to improve corpus-based word representations by determining (in a fuzzy way) which potential paraphrase is the most appropriate in a particular context.\n\nI think this paper is a bit lost in translation. The grammatical and storytelling styles made it really difficult for me to concentrate, and even unintelligible at times. One of the most important criteria in a conference paper is to communicate one's ideas clearly; unfortunately, I do not feel that this paper meets that standard.\n\nIn addition, the evaluation is rather lacking. There are many ways to evaluate word representations, and Google's analogy dataset has many issues (see, for example, Linzen's paper from RepEval 2016, as well as Drozd et al., COLING 2016).\n\nFinally, this work does not provide any qualitative result or motivation. Why does this method work better? Where does it fail? What have we learned about word representations / lexicons / corpus-based methods in general?"
  },
  {
    "people": [
      "Neelakantan"
    ],
    "review": "This paper introduces the concept of fuzzy paraphrases to aid in the learning of distributed word representations from a corpus augmented by a lexicon or ontology. Sometimes polysemy is context-dependent, but prior approaches have neglected this fact when incorporating external paraphrase information during learning. The main idea is to introduce a function that essentially judges the context-sensitivity of paraphrase candidates, down-weighting those candidates that depend strongly on context. This function is inferred from bilingual translation agreement.\n\nThe main argumentation leading to the model selection is intuitive, and I believe that the inclusion of good paraphrases and the elimination of bad paraphrases during training should in principle improve word representation quality. However, the main questions are how well the proposed method achieves this goal, and, even if it achieves it well, whether it makes much difference in practical terms.\n\nRegarding the first question, I am not entirely convinced that the parameterization of the control function f(x_ij) is optimal. It would have been nice to see some experiments investigating different choices, in particular some baselines where the effect of f is diminished (so that it reduces to f=1 in the limit) would have been interesting. I also feel like there would be a lot to gain from having f be a function of the nearby word embeddings, though this would obvious incur a significant slowdown. (See for example 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space' by Neelakantan et al, which should probably be cited.) As it stands, the experimental results do not clearly distinguish the fuzzy paraphrase approach from prior work, i.e. tables 3 and 4 do not show major trends one way or the other.\n\nRegarding the second question, it is hard to draw many conclusions from analogy tasks alone, especially when effects unrelated to good/bad paraphrasing such as corpus size/content, window size, vocabulary size, etc., can have an outsize effect on performance. \n\nOverall, I think this is a good paper presenting a sensible idea, but I am not convinced by the experiments that the specific approach is achieving its goal. With some improved experiments and analysis, I would wholeheartedly recommend this paper for acceptance; as it stands, I am on the fence."
  },
  {
    "people": [
      "Heess"
    ],
    "review": "I like the setting presented in this paper but I have several criticism/questions:\n\n(1) What are the failure model of this work? As richness of behaviors get complex, I expected this approach to have issues with the diversity of skills that could be discovered.\n\n(2) Looking at Sec 5.3 -- \" let X be a random variable denoting the grid in which the agent is currently situated\" -- is the space discretized? And if so why and what happens if it isn't. \n\n(3) Expanding on the first point, does the approach work with more complicated embodiment? Say a 5-link swimmer instead of 2? I think this is important to assess the generality of this approach\n\n(4) Authors claim that \"Recently, Heess et al. (2016) have independently proposed to learn a range of skills in a pre-training environment that will be useful for the downstream tasks, which is similar to our framework. However, their pre-training setup requires a set of goals to be specified. In comparison, we use intrinsic rewards as the only signal to the agent during the pre-training phase, the construction of which only requires very minimal domain knowledge.\"\n\nI don't entirely agree with this. The rewards that this paper proposes are also quite hand-crafted and specific to a seemingly limited set of control tasks."
  },
  {
    "people": [
      "Heess",
      "Heess",
      "Duan"
    ],
    "review": "Interesting work on hierarchical control, similar to the work of Heess et al. \nExperiments are strong and manage to complete benchmarks that previous work could not. Analysis of the experiments is a bit on the weaker side.\n\n(1) Like other reviewers, I find the use of the term \u2018intrinsic\u2019 motivation somewhat inappropriate (mostly because of its current meaning in RL).  Pre-training robots with locomotion by rewarding speed (or rewarding grasping for a manipulating arm) is very geared towards the tasks they will later accomplish. The pre-training tasks from Heess et al., while not identical, are similar. \n\n(2) The Mutual Information regularization is elegant and works generally well, but does not seem to help in the more complex mazes 1,2 and 3. The authors note this - is there any interpretation or analysis for this result?\n\n(3) The factorization between S_agent and S_rest should be clearly detailed in the paper. Duan et al specify S_agent, but for replicability, S_rest should be clearly specified as well - did I miss it?\n\n(4) It would be interesting to provide some analysis of the switching behavior of the agent. More generally, some further analysis of the policies (failure modes, effects of switching time on performance) would have been welcome."
  },
  {
    "people": [
      "Heess"
    ],
    "review": "I like the setting presented in this paper but I have several criticism/questions:\n\n(1) What are the failure model of this work? As richness of behaviors get complex, I expected this approach to have issues with the diversity of skills that could be discovered.\n\n(2) Looking at Sec 5.3 -- \" let X be a random variable denoting the grid in which the agent is currently situated\" -- is the space discretized? And if so why and what happens if it isn't. \n\n(3) Expanding on the first point, does the approach work with more complicated embodiment? Say a 5-link swimmer instead of 2? I think this is important to assess the generality of this approach\n\n(4) Authors claim that \"Recently, Heess et al. (2016) have independently proposed to learn a range of skills in a pre-training environment that will be useful for the downstream tasks, which is similar to our framework. However, their pre-training setup requires a set of goals to be specified. In comparison, we use intrinsic rewards as the only signal to the agent during the pre-training phase, the construction of which only requires very minimal domain knowledge.\"\n\nI don't entirely agree with this. The rewards that this paper proposes are also quite hand-crafted and specific to a seemingly limited set of control tasks. "
  },
  {
    "people": [
      "Heess"
    ],
    "review": "I like the setting presented in this paper but I have several criticism/questions:\n\n(1) What are the failure model of this work? As richness of behaviors get complex, I expected this approach to have issues with the diversity of skills that could be discovered.\n\n(2) Looking at Sec 5.3 -- \" let X be a random variable denoting the grid in which the agent is currently situated\" -- is the space discretized? And if so why and what happens if it isn't. \n\n(3) Expanding on the first point, does the approach work with more complicated embodiment? Say a 5-link swimmer instead of 2? I think this is important to assess the generality of this approach\n\n(4) Authors claim that \"Recently, Heess et al. (2016) have independently proposed to learn a range of skills in a pre-training environment that will be useful for the downstream tasks, which is similar to our framework. However, their pre-training setup requires a set of goals to be specified. In comparison, we use intrinsic rewards as the only signal to the agent during the pre-training phase, the construction of which only requires very minimal domain knowledge.\"\n\nI don't entirely agree with this. The rewards that this paper proposes are also quite hand-crafted and specific to a seemingly limited set of control tasks."
  },
  {
    "people": [
      "Heess",
      "Heess",
      "Duan"
    ],
    "review": "Interesting work on hierarchical control, similar to the work of Heess et al. \nExperiments are strong and manage to complete benchmarks that previous work could not. Analysis of the experiments is a bit on the weaker side.\n\n(1) Like other reviewers, I find the use of the term \u2018intrinsic\u2019 motivation somewhat inappropriate (mostly because of its current meaning in RL).  Pre-training robots with locomotion by rewarding speed (or rewarding grasping for a manipulating arm) is very geared towards the tasks they will later accomplish. The pre-training tasks from Heess et al., while not identical, are similar. \n\n(2) The Mutual Information regularization is elegant and works generally well, but does not seem to help in the more complex mazes 1,2 and 3. The authors note this - is there any interpretation or analysis for this result?\n\n(3) The factorization between S_agent and S_rest should be clearly detailed in the paper. Duan et al specify S_agent, but for replicability, S_rest should be clearly specified as well - did I miss it?\n\n(4) It would be interesting to provide some analysis of the switching behavior of the agent. More generally, some further analysis of the policies (failure modes, effects of switching time on performance) would have been welcome."
  },
  {
    "people": [
      "Heess"
    ],
    "review": "I like the setting presented in this paper but I have several criticism/questions:\n\n(1) What are the failure model of this work? As richness of behaviors get complex, I expected this approach to have issues with the diversity of skills that could be discovered.\n\n(2) Looking at Sec 5.3 -- \" let X be a random variable denoting the grid in which the agent is currently situated\" -- is the space discretized? And if so why and what happens if it isn't. \n\n(3) Expanding on the first point, does the approach work with more complicated embodiment? Say a 5-link swimmer instead of 2? I think this is important to assess the generality of this approach\n\n(4) Authors claim that \"Recently, Heess et al. (2016) have independently proposed to learn a range of skills in a pre-training environment that will be useful for the downstream tasks, which is similar to our framework. However, their pre-training setup requires a set of goals to be specified. In comparison, we use intrinsic rewards as the only signal to the agent during the pre-training phase, the construction of which only requires very minimal domain knowledge.\"\n\nI don't entirely agree with this. The rewards that this paper proposes are also quite hand-crafted and specific to a seemingly limited set of control tasks. "
  },
  {
    "people": [
      "Choromanska",
      "Choromanska"
    ],
    "review": "\nThis paper extend the Spin Glass analysis of Choromanska et al. (2015a) to Res Nets which yield the novel dynamic ensemble results for Res Nets and the connection to Batch Normalization and the analysis of their loss surface of Res Nets.\n\nThe paper is well-written with many insightful explanation of results. Although the technical contributions extend the Spin Glass model analysis of the ones by Choromanska et al. (2015a), the updated version could eliminate one of the unrealistic assumptions and the analysis further provides novel dynamic ensemble results and the connection to Batch Normalization that gives more insightful results about the structure of Res Nets. \n\nIt is essential to show this dynamic behaviour in a regime without batch normalization to untangle the normalization effect on ensemble feature. Hence authors claim that steady increase in the L_2 norm of the weights will maintain the this feature but setting for Figure 1 is restrictive to empirically support the claim. At least results on CIFAR 10 without batch normalization for showing effect of L_2 norm increase and results that support claims about Theorem 4 would strengthen the paper.\n\nThis work provides an initial rigorous framework to analyze better the inherent structure of the current state of art Res Net architectures and its variants which can stimulate potentially more significant results towards careful understanding of current state of art models (Rather than always to attempting to improve the performance of Res Nets by applying intuitive incremental heuristics, it is important to progress on some solid understanding too)."
  },
  {
    "people": [
      "A. Choromanska",
      "G. Ben Arous"
    ],
    "review": "-  The issue regarding unrealistic assumptions of spin glass analysis for landscape of neural networks was posed as an open problem in COLT 2015.[1] Can you discuss the effect of this problem for your analysis?\n\n- Regarding assumption that minimal of (12) should hold, is this assumption is realistic or not?\n\n-Choromanska(AISTATS 2015) supports the claims based on theoretical results with many empirical results however you do not provide such analysis. Can you support your claims with reasonable empirical results?\n\n- What is the empirical setup for Figure 1?\n\n-Fractal net paper on arxiv claim that residuals are incidental. Can you elaborate on that based on your framework? What about densely connected conv networks(huang,2016 )? \n\n[1] A. Choromanska, Y. LeCun, G. Ben Arous, Open Problem: The landscape of the loss surfaces of multilayer networks, in the Conference on Learning Theory (COLT), Open Problems, 2015\n"
  },
  {
    "people": [
      "Choromanska",
      "Choromanska"
    ],
    "review": "\nThis paper extend the Spin Glass analysis of Choromanska et al. (2015a) to Res Nets which yield the novel dynamic ensemble results for Res Nets and the connection to Batch Normalization and the analysis of their loss surface of Res Nets.\n\nThe paper is well-written with many insightful explanation of results. Although the technical contributions extend the Spin Glass model analysis of the ones by Choromanska et al. (2015a), the updated version could eliminate one of the unrealistic assumptions and the analysis further provides novel dynamic ensemble results and the connection to Batch Normalization that gives more insightful results about the structure of Res Nets. \n\nIt is essential to show this dynamic behaviour in a regime without batch normalization to untangle the normalization effect on ensemble feature. Hence authors claim that steady increase in the L_2 norm of the weights will maintain the this feature but setting for Figure 1 is restrictive to empirically support the claim. At least results on CIFAR 10 without batch normalization for showing effect of L_2 norm increase and results that support claims about Theorem 4 would strengthen the paper.\n\nThis work provides an initial rigorous framework to analyze better the inherent structure of the current state of art Res Net architectures and its variants which can stimulate potentially more significant results towards careful understanding of current state of art models (Rather than always to attempting to improve the performance of Res Nets by applying intuitive incremental heuristics, it is important to progress on some solid understanding too)."
  },
  {
    "people": [
      "A. Choromanska",
      "G. Ben Arous"
    ],
    "review": "-  The issue regarding unrealistic assumptions of spin glass analysis for landscape of neural networks was posed as an open problem in COLT 2015.[1] Can you discuss the effect of this problem for your analysis?\n\n- Regarding assumption that minimal of (12) should hold, is this assumption is realistic or not?\n\n-Choromanska(AISTATS 2015) supports the claims based on theoretical results with many empirical results however you do not provide such analysis. Can you support your claims with reasonable empirical results?\n\n- What is the empirical setup for Figure 1?\n\n-Fractal net paper on arxiv claim that residuals are incidental. Can you elaborate on that based on your framework? What about densely connected conv networks(huang,2016 )? \n\n[1] A. Choromanska, Y. LeCun, G. Ben Arous, Open Problem: The landscape of the loss surfaces of multilayer networks, in the Conference on Learning Theory (COLT), Open Problems, 2015\n"
  },
  {
    "people": [
      "Zaremba"
    ],
    "review": "This paper presents a thorough analysis of different methods to do curriculum learning. The major issue I have with it is that the dataset used seems very specific and does not necessarily justified, as mentioned by AnonReviewer3. It would have been great to see experiments on more standard tasks. Also, I really can't understand how the performance of FFNN models can be so good, please elaborate on this (see last comment).\nHowever, the paper is well written, the comparisons of the described methods are interesting and would probably apply to some other datasets as well.\n\nThe paper is way too long (18 pages!). Please reduce it or move some of the results to an appendix section.\n\nThe method described is extremely similar to the one described in Reinforcement learning neural turing machines (Zaremba et al., 2016,"
  },
  {
    "people": [
      "Bengio",
      "Ranzato"
    ],
    "review": "First up, I want to point out that this paper is really long. Like 17 pages long -- without any supplementary material. While ICLR does not have an official page limit, it would be nice if authors put themselves in the reviewer's shoes and did not take undue advantage of this rule. Having 1 or 2 pages in addition to the conventional 8 page limit is ok, but more than doubling the pages is quite unfair. \n\nNow for the review: The paper proposes a new artificial dataset for sequence learning. I call it artificial because it was artificially generated from the original MNIST dataset which is a smallish dataset of real images of handwritten digits. In addition to the dataset, the authors propose to train recurrent networks using a schedule over the length of the sequence, which they call \"incremental learning\". The experiments show that their proposed schedule is better than not having any schedule on this data set. Furthermore, they also show that their proposed schedule is better than a few other intuitive schedules. The authors verify this by doing some ablation studies over the model on the proposed dataset. \n\nI have following issues with this paper: \n\n-- I did not find anything novel in this paper. The proposed incremental learning schedule is nothing new and is a natural thing to try when learning sequences. Similar idea have already been tried by a number of authors, including Bengio 2015, and Ranzato 2015. The only new piece of work is the ablation studies which the authors conduct to tease out and verify that indeed the improvement in performance is due to the curriculum used. \n\n-- Furthermore, the authors only test their hypothesis on a single dataset which they propose and is artificially generated. Why not use it on a real sequential dataset, such as, language modeling. Does the technique not work in that scenario? In fact I am quite positive that for language modeling where the vocabulary size is huge, the performance gains will be no where close to the 74% reported in the paper.\n\n-- I'm not convinced about the value of having this artificial dataset. Already there are so many real world sequential dataset available, including in text, speech, finance and other areas. What exactly does this dataset bring to the table is not super clear to me. While having another dataset may not be a bad thing in itself, I almost felt that this dataset was created for the sole purpose of making the proposed ideas work. It would have been so much better had the authors shown experiments on other datasets. \n\n-- As I said, the paper is way too long. A significant part of the length of the paper is due to a collection of experiments which are completely un-related to the main message of the paper. For instance, the experiment in Section 6.2 is completely unrelated to the story of the paper. Same is true with the transfer learning experiments of Section 6.4.\n"
  },
  {
    "people": [
      "Zaremba"
    ],
    "review": "This paper presents a thorough analysis of different methods to do curriculum learning. The major issue I have with it is that the dataset used seems very specific and does not necessarily justified, as mentioned by AnonReviewer3. It would have been great to see experiments on more standard tasks. Also, I really can't understand how the performance of FFNN models can be so good, please elaborate on this (see last comment).\nHowever, the paper is well written, the comparisons of the described methods are interesting and would probably apply to some other datasets as well.\n\nThe paper is way too long (18 pages!). Please reduce it or move some of the results to an appendix section.\n\nThe method described is extremely similar to the one described in Reinforcement learning neural turing machines (Zaremba et al., 2016, "
  },
  {
    "people": [
      "Zaremba"
    ],
    "review": "This paper presents a thorough analysis of different methods to do curriculum learning. The major issue I have with it is that the dataset used seems very specific and does not necessarily justified, as mentioned by AnonReviewer3. It would have been great to see experiments on more standard tasks. Also, I really can't understand how the performance of FFNN models can be so good, please elaborate on this (see last comment).\nHowever, the paper is well written, the comparisons of the described methods are interesting and would probably apply to some other datasets as well.\n\nThe paper is way too long (18 pages!). Please reduce it or move some of the results to an appendix section.\n\nThe method described is extremely similar to the one described in Reinforcement learning neural turing machines (Zaremba et al., 2016,"
  },
  {
    "people": [
      "Bengio",
      "Ranzato"
    ],
    "review": "First up, I want to point out that this paper is really long. Like 17 pages long -- without any supplementary material. While ICLR does not have an official page limit, it would be nice if authors put themselves in the reviewer's shoes and did not take undue advantage of this rule. Having 1 or 2 pages in addition to the conventional 8 page limit is ok, but more than doubling the pages is quite unfair. \n\nNow for the review: The paper proposes a new artificial dataset for sequence learning. I call it artificial because it was artificially generated from the original MNIST dataset which is a smallish dataset of real images of handwritten digits. In addition to the dataset, the authors propose to train recurrent networks using a schedule over the length of the sequence, which they call \"incremental learning\". The experiments show that their proposed schedule is better than not having any schedule on this data set. Furthermore, they also show that their proposed schedule is better than a few other intuitive schedules. The authors verify this by doing some ablation studies over the model on the proposed dataset. \n\nI have following issues with this paper: \n\n-- I did not find anything novel in this paper. The proposed incremental learning schedule is nothing new and is a natural thing to try when learning sequences. Similar idea have already been tried by a number of authors, including Bengio 2015, and Ranzato 2015. The only new piece of work is the ablation studies which the authors conduct to tease out and verify that indeed the improvement in performance is due to the curriculum used. \n\n-- Furthermore, the authors only test their hypothesis on a single dataset which they propose and is artificially generated. Why not use it on a real sequential dataset, such as, language modeling. Does the technique not work in that scenario? In fact I am quite positive that for language modeling where the vocabulary size is huge, the performance gains will be no where close to the 74% reported in the paper.\n\n-- I'm not convinced about the value of having this artificial dataset. Already there are so many real world sequential dataset available, including in text, speech, finance and other areas. What exactly does this dataset bring to the table is not super clear to me. While having another dataset may not be a bad thing in itself, I almost felt that this dataset was created for the sole purpose of making the proposed ideas work. It would have been so much better had the authors shown experiments on other datasets. \n\n-- As I said, the paper is way too long. A significant part of the length of the paper is due to a collection of experiments which are completely un-related to the main message of the paper. For instance, the experiment in Section 6.2 is completely unrelated to the story of the paper. Same is true with the transfer learning experiments of Section 6.4.\n"
  },
  {
    "people": [
      "Zaremba"
    ],
    "review": "This paper presents a thorough analysis of different methods to do curriculum learning. The major issue I have with it is that the dataset used seems very specific and does not necessarily justified, as mentioned by AnonReviewer3. It would have been great to see experiments on more standard tasks. Also, I really can't understand how the performance of FFNN models can be so good, please elaborate on this (see last comment).\nHowever, the paper is well written, the comparisons of the described methods are interesting and would probably apply to some other datasets as well.\n\nThe paper is way too long (18 pages!). Please reduce it or move some of the results to an appendix section.\n\nThe method described is extremely similar to the one described in Reinforcement learning neural turing machines (Zaremba et al., 2016, "
  },
  {
    "people": [
      "Precup",
      "Munos"
    ],
    "review": "This paper formalizes the problem setting of having only a subset of available MDPs for which one has access to a reward. The authors name this setting \"semi-supervised reinforcement learning\" (SSRL), as a reference to semi-supervised learning (where one only has access to labels for a subset of the dataset). They provide an approach for solving SSRL named semi-supervised skill generalization (S3G), which builds on the framework of maximum entropy control. The whole approach is straightforward and amounts to an EM algorithm with partial labels (: they alternate iteratively between estimating a reward function (parametrized) and fitting a control policy using this reward function. They provide experiments on 4 tasks (obstacle, 2-link reacher, 2-link reacher with vision, half-cheetah) in MuJoCo.\n\nThe paper is well-written, and is overall clear. The appendix provides some more context, I think a few implementation details are missing to be able to fully reproduce the experiments from the paper, but they will provide the code.\n\nThe link to inverse reinforcement learning seems to be done correctly. However, there is no reference to off-policy policy learning, and, for instance, it seems to me that the \\tau \\in D_{samp} term of equation (3) could benefit from variance reduction as in e.g. TB(\\lambda) [Precup et al. 2000] or Retrace(\\lambda) [Munos et al. 2016].\n\nThe experimental section is convincing, but I would appreciate a precision (and small discussion) of this sentence \"To extensively test the generalization capabilities of the policies learned with each method, we measure performance on a wide range of settings that is a superset of the unlabeled and labeled MDPs\" with numbers for the different scenarios (or the replacement of superset by \"union\" if this is the case). It may explain better the poor results of \"oracle\" on \"obstacle\" and \"2-link reacher\", and reinforce* the further sentences \"In the obstacle task, the true reward function is not sufficiently shaped for learning in the unlabeled MDPs. Hence, the reward regression and oracle methods perform poorly\".\n\nCorrection on page 4: \"5-tuple M_i = (S, A, T, R)\" is a 4-tuple.\n\nOverall, I think that this is a good and sound paper. I am personally unsure as to if all the parallels and/or references to previous work are complete, thus my confidence score of 3.\n\n(* pun intended)"
  },
  {
    "people": [
      "Precup",
      "Munos"
    ],
    "review": "This paper formalizes the problem setting of having only a subset of available MDPs for which one has access to a reward. The authors name this setting \"semi-supervised reinforcement learning\" (SSRL), as a reference to semi-supervised learning (where one only has access to labels for a subset of the dataset). They provide an approach for solving SSRL named semi-supervised skill generalization (S3G), which builds on the framework of maximum entropy control. The whole approach is straightforward and amounts to an EM algorithm with partial labels (: they alternate iteratively between estimating a reward function (parametrized) and fitting a control policy using this reward function. They provide experiments on 4 tasks (obstacle, 2-link reacher, 2-link reacher with vision, half-cheetah) in MuJoCo.\n\nThe paper is well-written, and is overall clear. The appendix provides some more context, I think a few implementation details are missing to be able to fully reproduce the experiments from the paper, but they will provide the code.\n\nThe link to inverse reinforcement learning seems to be done correctly. However, there is no reference to off-policy policy learning, and, for instance, it seems to me that the \\tau \\in D_{samp} term of equation (3) could benefit from variance reduction as in e.g. TB(\\lambda) [Precup et al. 2000] or Retrace(\\lambda) [Munos et al. 2016].\n\nThe experimental section is convincing, but I would appreciate a precision (and small discussion) of this sentence \"To extensively test the generalization capabilities of the policies learned with each method, we measure performance on a wide range of settings that is a superset of the unlabeled and labeled MDPs\" with numbers for the different scenarios (or the replacement of superset by \"union\" if this is the case). It may explain better the poor results of \"oracle\" on \"obstacle\" and \"2-link reacher\", and reinforce* the further sentences \"In the obstacle task, the true reward function is not sufficiently shaped for learning in the unlabeled MDPs. Hence, the reward regression and oracle methods perform poorly\".\n\nCorrection on page 4: \"5-tuple M_i = (S, A, T, R)\" is a 4-tuple.\n\nOverall, I think that this is a good and sound paper. I am personally unsure as to if all the parallels and/or references to previous work are complete, thus my confidence score of 3.\n\n(* pun intended)"
  },
  {
    "people": [
      "Wu Y",
      "Zhang Y",
      "Bengio Y"
    ],
    "review": "* Brief Summary: \n\nThis paper explores an extension of multiplicative RNNs to the LSTM type of models. The resulting proposal is very similar to [1]. Authors show experimental results on character-level language modeling tasks. In general, I think the paper is well-written and the explanations are quite clear.\n\n* Criticisms:\n\n- In terms of contributions, the paper is weak. The motivation makes sense, however, very similar work has been done in [1] and already an extension over [2]. Because of that this paper mainly stands as an application paper.\n- The results are encouraging. On the other hand, they are still behind the state of art without using dynamic evaluation. \n- There are some non-standard choices on modifications on the standard algorithms, such as \"l\" parameter of RMSProp and multiplying output gate before the nonlinearity.\n- The experimental results are only limited to character-level language modeling only. \n\n* An Overview of the Review:\n\nPros:\n- A simple modification that seems to reasonably well in practice.\n- Well-written.\n\nCons:\n- Lack of good enough experimental results.\n- Not enough contributions (almost trivial extension over existing algorithms).\n- Non-standard modifications over the existing algorithms.\n\n[1] Wu Y, Zhang S, Zhang Y, Bengio Y, Salakhutdinov RR. On multiplicative integration with recurrent neural networks. InAdvances in Neural Information Processing Systems 2016 (pp. 2856-2864).\n[2] Sutskever I, Martens J, Hinton GE. Generating text with recurrent neural networks. InProceedings of the 28th International Conference on Machine Learning (ICML-11) 2011 (pp. 1017-1024)."
  },
  {
    "people": [
      "Wu Y",
      "Zhang Y",
      "Bengio Y"
    ],
    "review": "* Brief Summary: \n\nThis paper explores an extension of multiplicative RNNs to the LSTM type of models. The resulting proposal is very similar to [1]. Authors show experimental results on character-level language modeling tasks. In general, I think the paper is well-written and the explanations are quite clear.\n\n* Criticisms:\n\n- In terms of contributions, the paper is weak. The motivation makes sense, however, very similar work has been done in [1] and already an extension over [2]. Because of that this paper mainly stands as an application paper.\n- The results are encouraging. On the other hand, they are still behind the state of art without using dynamic evaluation. \n- There are some non-standard choices on modifications on the standard algorithms, such as \"l\" parameter of RMSProp and multiplying output gate before the nonlinearity.\n- The experimental results are only limited to character-level language modeling only. \n\n* An Overview of the Review:\n\nPros:\n- A simple modification that seems to reasonably well in practice.\n- Well-written.\n\nCons:\n- Lack of good enough experimental results.\n- Not enough contributions (almost trivial extension over existing algorithms).\n- Non-standard modifications over the existing algorithms.\n\n[1] Wu Y, Zhang S, Zhang Y, Bengio Y, Salakhutdinov RR. On multiplicative integration with recurrent neural networks. InAdvances in Neural Information Processing Systems 2016 (pp. 2856-2864).\n[2] Sutskever I, Martens J, Hinton GE. Generating text with recurrent neural networks. InProceedings of the 28th International Conference on Machine Learning (ICML-11) 2011 (pp. 1017-1024)."
  },
  {
    "people": [
      "Tan"
    ],
    "review": "This paper proposes a neural architecture for answering non-factoid questions. The author's model improves over previous neural models for answer sentence selection. Experiments are conducted on a Japanese love advice corpus; the coolest part of the paper for me was that the model was actually rolled out to the public and its answers were rated twice as good as actual human contributors! \n\nIt was hard for me to determine the novelty of the contribution. The authors mention that their model \"fills the gap\nbetween answer selection and generation\"; however, no generation is actually performed by the model! Instead, the model appears to be very similar to the QA-LSTM of Tan et al., 2015 except that there are additional terms in the objective to handle conclusion and supplementary sentences. The structure of the answer is fixed to a predefined template (e.g., conclusion --> supplementary), so the model is not really learning how to order the sentences. The other contribution is the \"word embedding with semantics\" portion described in sec 4.1, which is essentially just the paragraph vector model except with \"titles\" and \"categories\" instead of paragraphs. \n\nWhile the result of the paper is a model that has actually demonstrated real-life usefulness, the technical contributions do not strike me as novel enough for publication at ICLR.\n\nOther comments:\n- One major issue with the reliance of the model on the template is that you can't evaluate on commonly-used non-factoid QA datasets such as InsuranceQA. If the template were not fixed beforehand (but possibly learned by the model), you could conceivably evaluate on different datasets. \n- The examples in Table 4 don't show a clear edge in answer quality to your model; QA-LSTM seems to choose good answers as well.\n- Doesn't the construction model have an advantage over the vanilla QA-LSTM in that it knows which sentences are conclusions and which are supplementary? Or does QA-LSTM also get this distinction?"
  },
  {
    "people": [
      "Tan",
      "Rao",
      "Su",
      "Tan",
      "Tan",
      "Tan"
    ],
    "review": "Summary: The paper presents an approach \u2013 Neural Answer Construction Model for the task of answering non-factoid questions, in particular, love-advice questions. The two main features of the proposed model are the following \u2013 1) it incorporates the biases of semantics behind questions into word embeddings, 2) in addition to optimizing for closeness between questions and answers, it also optimizes for optimum combination of sentences in the predicted answer. The proposed model is evaluated using the dataset from a Japanese online QA service and is shown to outperform the baseline model (Tan et al. 2015) by 20% relatively (6% absolutely). The paper also experiments with few other baseline models (ablations of the proposed model).\n\nStrengths:\n\n1. The two motivations behind the proposed approach \u2013 need to understand the ambiguous use of words depending on context, and need to generate new answers rather than just selecting from answers held by QA sites \u2013 are reasonable.\n\n2. The novelty in the paper involves the following \u2013 1) incorporating biases of semantics behind questions into word embeddings using paragraph2vec like model, modified to take as inputs - words from questions, question title token and question category token, 2) modelling optimum combination of sentences (conclusion and supplement sentences) in the predicted answer, 3) designing abstract scenario for answers, inspired by automated web-service composition framework (Rao & Su (2005)), and 4) extracting important topics in conclusion sentence and emphasizing them in supplemental sentence using attention mechanism (attention mechanism is similar to Tan et al. 2016).\n\n3. The proposed method is shown to outperform the current best method (Tan et al. 2015) by 20% relatively (6% absolutely) which seems to be significant improvement. \n\n4. The paper presents few ablations studies that provide insights on how much different components of the model (such as incorporating biases into word embeddings, incorporating attention from conclusion to supplement) are helping towards performance improvement.\n\nWeaknesses/Suggestions/Questions:\n\n1. How are the abstract patterns determined, i,e., how did the authors determine that the answers to love-advice questions generally constitute of sympathy, conclusion, supplement for conclusion and encouragement? How much is the improvement in performance when using abstract patterns compared to the case when not using these patters, i.e. when candidate answers are picked from union of all corpuses rather than picking from respective corpuses (corpuses for sympathy, conclusion etc.).\n\n2. It seems that the abstract patterns are specific to the type of questions. So, the abstract patterns for love-advice will be different from those for business advice. Thus, it seems like the abstract patterns need to be hand-coded for different types and hence one model cannot generalize across different types.\n\n3. The paper should present explicit analysis of how much combinational optimization between sentences help \u2013 comparison between model performance with and without combinational optimization keeping rest of the model architecture same. The authors could also plot the accuracy of the model as a function of the combinational optimization scores. This will provide insights into how significant are the combinational optimization scores towards overall model accuracy.\n\n\n4. Paper says that current systems designed for non-factoid QA cannot generalize to questions outside those stored in QA sites and claims that this is one of the contributions of this paper. In order to ground that claim, the paper should show experimentally how well the proposed method generalized to such out-of-domain questions. Although the questions asked by human experts in the human evaluation were not from the evaluation datasets, the paper should analyze how different those questions were compared to the questions present in the evaluation datasets.\n\n5. For human evaluation, were the outputs of the proposed model and that of the QA-LSTM model judged each judged by both the human experts OR one of the human experts judged the outputs of one system and the other human expert judged the outputs of the other system? If both the sets of outputs were each judged by both the human experts, how were the ratings of the two experts combined for every questions? \n\n6. I wonder why the authors did not do a human evaluation where they just ask human workers (not experts) to compare the output of the proposed model with that of the QA-LSTM model \u2013 which of the two outputs they would like to hear when asking for advice. Such an evaluation would not get biased by whether each sentence is good or not, whether the combination is good or not. Looking at the qualitative examples in Table 4, I personally like the output of the QA-LSTM more than that of the proposed model because they seem to provide a direct answer to the question (e.g., for the first example the output of the QA-LSTM says \u201cYou should wait until you feel excited\u201d, whereas the output of the proposed model says \u201cIt is better to concentrate on how to confess your love to her\u201d which seems a bit indirect to the question asked.)\n\n7. Given a question, is the ground-truth answer different in the two tasks -- answer selection and answer construction?\n\n8. The paper mentions that Attentive LSTM (Tan et al. 2016) is evaluated as the current best answer selection method (section 5.2). So, why is its accuracy lower than that of QA-LSTM in table 1. The authors explain this by pointing out the issue of questions being very long compared to answers and hence the attention being noisy. But, did these issues not exist in the dataset used by Tan et al. 2016?\n\n9. The paper says the proposed method achieves 20% gain over current best (in Conclusion section) where they refer to QA-LSTM as the current best method. However, in the description of Attentive LSTM (section 5.2), the paper mentions that Attention LSTM is the current best method. So, could authors please clarify the discrepancy?\n\n10. Minor correction: remove space between 20 and % in abstract.\n\nReview Summary: The problem of non-factoid QA being dealt with in the paper is an interesting and useful problem to solve. The motivations presented in the paper behind the proposed approach are reasonable. The experiments show that the proposed model outperforms the baseline model. However, the use of abstract patterns to determine the answer seems like hand-designing and hence it seems like these abstract patterns need to be designed for every other type of non-factoid question and hence the proposed approach is not generalizable to other types. Also, the paper needs more analysis of the results to provide insights into the contribution of different model components."
  },
  {
    "people": [
      "Tan",
      "Tan",
      "Tan"
    ],
    "review": "This paper extends mostly on top of the work of QA-biLSTM and QA-biLSTM with attentions, as proposed in Tan et al. 2015 and Tan et al. 2016, in the following 2 ways:\n\n1. It trains a topic-specific word embedding using an approach similar to Paragraph2vec by leveraging the topic and title information provided in the data.\n\n2. It considers the multiple-unit answer selection problem (e.g., one sentence selected from answer section, and another selected from supplemental section) vs. the single answer selection problem as studied in Tan et al 2015 and 2016. The mechanism used to retain the coherence between different parts of the answers is inspired by the attention mechanism introduced by Tan et al. 2016.\n\nWhile the practical results presented in the paper is interesting, the main innovations of this paper are rather limited. "
  },
  {
    "people": [
      "Tan"
    ],
    "review": "This paper proposes a neural architecture for answering non-factoid questions. The author's model improves over previous neural models for answer sentence selection. Experiments are conducted on a Japanese love advice corpus; the coolest part of the paper for me was that the model was actually rolled out to the public and its answers were rated twice as good as actual human contributors! \n\nIt was hard for me to determine the novelty of the contribution. The authors mention that their model \"fills the gap\nbetween answer selection and generation\"; however, no generation is actually performed by the model! Instead, the model appears to be very similar to the QA-LSTM of Tan et al., 2015 except that there are additional terms in the objective to handle conclusion and supplementary sentences. The structure of the answer is fixed to a predefined template (e.g., conclusion --> supplementary), so the model is not really learning how to order the sentences. The other contribution is the \"word embedding with semantics\" portion described in sec 4.1, which is essentially just the paragraph vector model except with \"titles\" and \"categories\" instead of paragraphs. \n\nWhile the result of the paper is a model that has actually demonstrated real-life usefulness, the technical contributions do not strike me as novel enough for publication at ICLR.\n\nOther comments:\n- One major issue with the reliance of the model on the template is that you can't evaluate on commonly-used non-factoid QA datasets such as InsuranceQA. If the template were not fixed beforehand (but possibly learned by the model), you could conceivably evaluate on different datasets. \n- The examples in Table 4 don't show a clear edge in answer quality to your model; QA-LSTM seems to choose good answers as well.\n- Doesn't the construction model have an advantage over the vanilla QA-LSTM in that it knows which sentences are conclusions and which are supplementary? Or does QA-LSTM also get this distinction?"
  },
  {
    "people": [
      "Tan"
    ],
    "review": "This paper proposes a neural architecture for answering non-factoid questions. The author's model improves over previous neural models for answer sentence selection. Experiments are conducted on a Japanese love advice corpus; the coolest part of the paper for me was that the model was actually rolled out to the public and its answers were rated twice as good as actual human contributors! \n\nIt was hard for me to determine the novelty of the contribution. The authors mention that their model \"fills the gap\nbetween answer selection and generation\"; however, no generation is actually performed by the model! Instead, the model appears to be very similar to the QA-LSTM of Tan et al., 2015 except that there are additional terms in the objective to handle conclusion and supplementary sentences. The structure of the answer is fixed to a predefined template (e.g., conclusion --> supplementary), so the model is not really learning how to order the sentences. The other contribution is the \"word embedding with semantics\" portion described in sec 4.1, which is essentially just the paragraph vector model except with \"titles\" and \"categories\" instead of paragraphs. \n\nWhile the result of the paper is a model that has actually demonstrated real-life usefulness, the technical contributions do not strike me as novel enough for publication at ICLR.\n\nOther comments:\n- One major issue with the reliance of the model on the template is that you can't evaluate on commonly-used non-factoid QA datasets such as InsuranceQA. If the template were not fixed beforehand (but possibly learned by the model), you could conceivably evaluate on different datasets. \n- The examples in Table 4 don't show a clear edge in answer quality to your model; QA-LSTM seems to choose good answers as well.\n- Doesn't the construction model have an advantage over the vanilla QA-LSTM in that it knows which sentences are conclusions and which are supplementary? Or does QA-LSTM also get this distinction?"
  },
  {
    "people": [
      "Tan",
      "Rao",
      "Su",
      "Tan",
      "Tan",
      "Tan"
    ],
    "review": "Summary: The paper presents an approach \u2013 Neural Answer Construction Model for the task of answering non-factoid questions, in particular, love-advice questions. The two main features of the proposed model are the following \u2013 1) it incorporates the biases of semantics behind questions into word embeddings, 2) in addition to optimizing for closeness between questions and answers, it also optimizes for optimum combination of sentences in the predicted answer. The proposed model is evaluated using the dataset from a Japanese online QA service and is shown to outperform the baseline model (Tan et al. 2015) by 20% relatively (6% absolutely). The paper also experiments with few other baseline models (ablations of the proposed model).\n\nStrengths:\n\n1. The two motivations behind the proposed approach \u2013 need to understand the ambiguous use of words depending on context, and need to generate new answers rather than just selecting from answers held by QA sites \u2013 are reasonable.\n\n2. The novelty in the paper involves the following \u2013 1) incorporating biases of semantics behind questions into word embeddings using paragraph2vec like model, modified to take as inputs - words from questions, question title token and question category token, 2) modelling optimum combination of sentences (conclusion and supplement sentences) in the predicted answer, 3) designing abstract scenario for answers, inspired by automated web-service composition framework (Rao & Su (2005)), and 4) extracting important topics in conclusion sentence and emphasizing them in supplemental sentence using attention mechanism (attention mechanism is similar to Tan et al. 2016).\n\n3. The proposed method is shown to outperform the current best method (Tan et al. 2015) by 20% relatively (6% absolutely) which seems to be significant improvement. \n\n4. The paper presents few ablations studies that provide insights on how much different components of the model (such as incorporating biases into word embeddings, incorporating attention from conclusion to supplement) are helping towards performance improvement.\n\nWeaknesses/Suggestions/Questions:\n\n1. How are the abstract patterns determined, i,e., how did the authors determine that the answers to love-advice questions generally constitute of sympathy, conclusion, supplement for conclusion and encouragement? How much is the improvement in performance when using abstract patterns compared to the case when not using these patters, i.e. when candidate answers are picked from union of all corpuses rather than picking from respective corpuses (corpuses for sympathy, conclusion etc.).\n\n2. It seems that the abstract patterns are specific to the type of questions. So, the abstract patterns for love-advice will be different from those for business advice. Thus, it seems like the abstract patterns need to be hand-coded for different types and hence one model cannot generalize across different types.\n\n3. The paper should present explicit analysis of how much combinational optimization between sentences help \u2013 comparison between model performance with and without combinational optimization keeping rest of the model architecture same. The authors could also plot the accuracy of the model as a function of the combinational optimization scores. This will provide insights into how significant are the combinational optimization scores towards overall model accuracy.\n\n\n4. Paper says that current systems designed for non-factoid QA cannot generalize to questions outside those stored in QA sites and claims that this is one of the contributions of this paper. In order to ground that claim, the paper should show experimentally how well the proposed method generalized to such out-of-domain questions. Although the questions asked by human experts in the human evaluation were not from the evaluation datasets, the paper should analyze how different those questions were compared to the questions present in the evaluation datasets.\n\n5. For human evaluation, were the outputs of the proposed model and that of the QA-LSTM model judged each judged by both the human experts OR one of the human experts judged the outputs of one system and the other human expert judged the outputs of the other system? If both the sets of outputs were each judged by both the human experts, how were the ratings of the two experts combined for every questions? \n\n6. I wonder why the authors did not do a human evaluation where they just ask human workers (not experts) to compare the output of the proposed model with that of the QA-LSTM model \u2013 which of the two outputs they would like to hear when asking for advice. Such an evaluation would not get biased by whether each sentence is good or not, whether the combination is good or not. Looking at the qualitative examples in Table 4, I personally like the output of the QA-LSTM more than that of the proposed model because they seem to provide a direct answer to the question (e.g., for the first example the output of the QA-LSTM says \u201cYou should wait until you feel excited\u201d, whereas the output of the proposed model says \u201cIt is better to concentrate on how to confess your love to her\u201d which seems a bit indirect to the question asked.)\n\n7. Given a question, is the ground-truth answer different in the two tasks -- answer selection and answer construction?\n\n8. The paper mentions that Attentive LSTM (Tan et al. 2016) is evaluated as the current best answer selection method (section 5.2). So, why is its accuracy lower than that of QA-LSTM in table 1. The authors explain this by pointing out the issue of questions being very long compared to answers and hence the attention being noisy. But, did these issues not exist in the dataset used by Tan et al. 2016?\n\n9. The paper says the proposed method achieves 20% gain over current best (in Conclusion section) where they refer to QA-LSTM as the current best method. However, in the description of Attentive LSTM (section 5.2), the paper mentions that Attention LSTM is the current best method. So, could authors please clarify the discrepancy?\n\n10. Minor correction: remove space between 20 and % in abstract.\n\nReview Summary: The problem of non-factoid QA being dealt with in the paper is an interesting and useful problem to solve. The motivations presented in the paper behind the proposed approach are reasonable. The experiments show that the proposed model outperforms the baseline model. However, the use of abstract patterns to determine the answer seems like hand-designing and hence it seems like these abstract patterns need to be designed for every other type of non-factoid question and hence the proposed approach is not generalizable to other types. Also, the paper needs more analysis of the results to provide insights into the contribution of different model components."
  },
  {
    "people": [
      "Tan",
      "Tan",
      "Tan"
    ],
    "review": "This paper extends mostly on top of the work of QA-biLSTM and QA-biLSTM with attentions, as proposed in Tan et al. 2015 and Tan et al. 2016, in the following 2 ways:\n\n1. It trains a topic-specific word embedding using an approach similar to Paragraph2vec by leveraging the topic and title information provided in the data.\n\n2. It considers the multiple-unit answer selection problem (e.g., one sentence selected from answer section, and another selected from supplemental section) vs. the single answer selection problem as studied in Tan et al 2015 and 2016. The mechanism used to retain the coherence between different parts of the answers is inspired by the attention mechanism introduced by Tan et al. 2016.\n\nWhile the practical results presented in the paper is interesting, the main innovations of this paper are rather limited. "
  },
  {
    "people": [
      "Tan"
    ],
    "review": "This paper proposes a neural architecture for answering non-factoid questions. The author's model improves over previous neural models for answer sentence selection. Experiments are conducted on a Japanese love advice corpus; the coolest part of the paper for me was that the model was actually rolled out to the public and its answers were rated twice as good as actual human contributors! \n\nIt was hard for me to determine the novelty of the contribution. The authors mention that their model \"fills the gap\nbetween answer selection and generation\"; however, no generation is actually performed by the model! Instead, the model appears to be very similar to the QA-LSTM of Tan et al., 2015 except that there are additional terms in the objective to handle conclusion and supplementary sentences. The structure of the answer is fixed to a predefined template (e.g., conclusion --> supplementary), so the model is not really learning how to order the sentences. The other contribution is the \"word embedding with semantics\" portion described in sec 4.1, which is essentially just the paragraph vector model except with \"titles\" and \"categories\" instead of paragraphs. \n\nWhile the result of the paper is a model that has actually demonstrated real-life usefulness, the technical contributions do not strike me as novel enough for publication at ICLR.\n\nOther comments:\n- One major issue with the reliance of the model on the template is that you can't evaluate on commonly-used non-factoid QA datasets such as InsuranceQA. If the template were not fixed beforehand (but possibly learned by the model), you could conceivably evaluate on different datasets. \n- The examples in Table 4 don't show a clear edge in answer quality to your model; QA-LSTM seems to choose good answers as well.\n- Doesn't the construction model have an advantage over the vanilla QA-LSTM in that it knows which sentences are conclusions and which are supplementary? Or does QA-LSTM also get this distinction?"
  },
  {
    "people": [
      "Graves",
      "Jaitly"
    ],
    "review": "The paper proposes an approach to sequence transduction for the case when a monotonic alignment between the input and the output is plausible. It is assumed that the alignment can be provided as a part of training data, with Chinese Restaurant process being used in the actual experiments. \n\nThe idea makes sense, although its applicability is limited to the domains where a monotonic alignment is available. But as discussed during the pre-review period, there has been a lot of strongly overlapping related work, such as probabilistic models with hard-alignment (Sequence Transduction With Recurrent Neural Network, Graves et al, 2012) and also attempts to use external alignments in end-to-end models (A Neural Transducer, Jaitly et al, 2015). That said, I do not think the approach is sufficiently novel. \n\nI also have a concern regarding the evaluation. I do not think it is fair to compare the proposed model that depends on external alignment with the vanilla soft-attention model that learns alignments from scratch. In a control experiment soft-attention could be trained to match the external alignment. Such a pretraining could reduce overfitting on the small dataset, the one on which the proposed approach brings the most improvement. On a larger dataset, especially SIGMORPHON, the improvements are not very big and are only obtained for a certain class of languages.\n\nTo sum up, two main issues are (a) lack of novelty (b) the comparison of a model trained with external alignment and one without it."
  },
  {
    "people": [
      "Jailty el"
    ],
    "review": "This paper proposes a sequence transduction model that first uses a traditional statistical alignment methods to provide alignments for an encoder-decoder type model. The paper provides experiments on a number of morphological inflection generation datasets. They shows an improvement over other models, although they have much smaller improvements over a soft attention model on some of their tasks. \n\nI found this paper to be well-written and to have very thorough experiments/analysis, but I have concerns that this work isn't particularly different from previous approaches and thus has a more focused contribution that is limited to its application on this type of shorter input (the authors \"suggest\" that their approach is sufficient for shorter sequences, but don't compare against the approach of Chorowski et al. 2015 or Jailty el at 2016).\n\nIn summary, I found this paper to be well-executed/well-written, but it's novelty and scope too small. That said, I feel this work would make a very good short paper. "
  },
  {
    "people": [
      "Graves",
      "Jaitly"
    ],
    "review": "The paper proposes an approach to sequence transduction for the case when a monotonic alignment between the input and the output is plausible. It is assumed that the alignment can be provided as a part of training data, with Chinese Restaurant process being used in the actual experiments. \n\nThe idea makes sense, although its applicability is limited to the domains where a monotonic alignment is available. But as discussed during the pre-review period, there has been a lot of strongly overlapping related work, such as probabilistic models with hard-alignment (Sequence Transduction With Recurrent Neural Network, Graves et al, 2012) and also attempts to use external alignments in end-to-end models (A Neural Transducer, Jaitly et al, 2015). That said, I do not think the approach is sufficiently novel. \n\nI also have a concern regarding the evaluation. I do not think it is fair to compare the proposed model that depends on external alignment with the vanilla soft-attention model that learns alignments from scratch. In a control experiment soft-attention could be trained to match the external alignment. Such a pretraining could reduce overfitting on the small dataset, the one on which the proposed approach brings the most improvement. On a larger dataset, especially SIGMORPHON, the improvements are not very big and are only obtained for a certain class of languages.\n\nTo sum up, two main issues are (a) lack of novelty (b) the comparison of a model trained with external alignment and one without it. \n"
  },
  {
    "people": [
      "Graves",
      "Jaitly"
    ],
    "review": "The paper proposes an approach to sequence transduction for the case when a monotonic alignment between the input and the output is plausible. It is assumed that the alignment can be provided as a part of training data, with Chinese Restaurant process being used in the actual experiments. \n\nThe idea makes sense, although its applicability is limited to the domains where a monotonic alignment is available. But as discussed during the pre-review period, there has been a lot of strongly overlapping related work, such as probabilistic models with hard-alignment (Sequence Transduction With Recurrent Neural Network, Graves et al, 2012) and also attempts to use external alignments in end-to-end models (A Neural Transducer, Jaitly et al, 2015). That said, I do not think the approach is sufficiently novel. \n\nI also have a concern regarding the evaluation. I do not think it is fair to compare the proposed model that depends on external alignment with the vanilla soft-attention model that learns alignments from scratch. In a control experiment soft-attention could be trained to match the external alignment. Such a pretraining could reduce overfitting on the small dataset, the one on which the proposed approach brings the most improvement. On a larger dataset, especially SIGMORPHON, the improvements are not very big and are only obtained for a certain class of languages.\n\nTo sum up, two main issues are (a) lack of novelty (b) the comparison of a model trained with external alignment and one without it."
  },
  {
    "people": [
      "Jailty el"
    ],
    "review": "This paper proposes a sequence transduction model that first uses a traditional statistical alignment methods to provide alignments for an encoder-decoder type model. The paper provides experiments on a number of morphological inflection generation datasets. They shows an improvement over other models, although they have much smaller improvements over a soft attention model on some of their tasks. \n\nI found this paper to be well-written and to have very thorough experiments/analysis, but I have concerns that this work isn't particularly different from previous approaches and thus has a more focused contribution that is limited to its application on this type of shorter input (the authors \"suggest\" that their approach is sufficient for shorter sequences, but don't compare against the approach of Chorowski et al. 2015 or Jailty el at 2016).\n\nIn summary, I found this paper to be well-executed/well-written, but it's novelty and scope too small. That said, I feel this work would make a very good short paper. "
  },
  {
    "people": [
      "Graves",
      "Jaitly"
    ],
    "review": "The paper proposes an approach to sequence transduction for the case when a monotonic alignment between the input and the output is plausible. It is assumed that the alignment can be provided as a part of training data, with Chinese Restaurant process being used in the actual experiments. \n\nThe idea makes sense, although its applicability is limited to the domains where a monotonic alignment is available. But as discussed during the pre-review period, there has been a lot of strongly overlapping related work, such as probabilistic models with hard-alignment (Sequence Transduction With Recurrent Neural Network, Graves et al, 2012) and also attempts to use external alignments in end-to-end models (A Neural Transducer, Jaitly et al, 2015). That said, I do not think the approach is sufficiently novel. \n\nI also have a concern regarding the evaluation. I do not think it is fair to compare the proposed model that depends on external alignment with the vanilla soft-attention model that learns alignments from scratch. In a control experiment soft-attention could be trained to match the external alignment. Such a pretraining could reduce overfitting on the small dataset, the one on which the proposed approach brings the most improvement. On a larger dataset, especially SIGMORPHON, the improvements are not very big and are only obtained for a certain class of languages.\n\nTo sum up, two main issues are (a) lack of novelty (b) the comparison of a model trained with external alignment and one without it. \n"
  },
  {
    "people": [
      "Shi",
      "Shi"
    ],
    "review": "The authors address the problem of modeling temporally-changing signal on a graph, where the signal at one node changes as a function of the inputs and the hidden states of its neighborhood, the size of which is a hyperparameter. The approach follows closely that of Shi et al. 2015, but it is generalized to arbitrary graph structures rather than a fixed grid by using graph convolutions of Defferrard et al. 2016. This is not a strict generalization because the graph formulation treats all edges equally, while the conv kernels in Shi et al. have a built in directionality. The authors show results on a moving MNIST and on the Penn Tree Bank Language Modeling task.\n\nThe paper, model and experiments are decent but I have some concerns:\n\n1. The proposed model is not exceptionally novel from a technical perspective. I usually don't mind if this is the case provided that the authors make up for the deficiency with thorough experimental evaluation, clear write up, and interesting insights into the pros/cons of the approach with respect to previous models. In this case I lean towards this not being the case.\n\n2. The experiment results section is rather terse and light on interpretation. I'm not fully up to date on the latest of Penn Tree Bank language modeling results but I do know that it is a hotly contested and well-known dataset. I am surprised to see a comparison only to Zaremba et al 2014 where I would expect to see multiple other results.\n\n3. The writing is not very clear and the authors don't make sufficiently strong attempt to compare the models or provide insight or comparisons into why the proposed model works better. In particular, unless I'm mistaken the word probabilities are a function of the neighborhood in the graph. What is the width of this graph? For example, suppose I sample a word in one part of the graph, doesn't this information have to propagate to the other parts of the graph along the edges? Also, it's not clear to me how the model can achieve reasonable results on moving MNIST when it cannot distinguish the direction of the moving edges. The authors state this but do not provide satisfying insight into how this can work. How does a pixel know that it should turn on in the next frame? I wish the authors thought about this more and presented it more clearly.\n\nIn summary, the paper has somewhat weak technical contribution, the experiments section is not very thorough, and the insights are sparse."
  },
  {
    "people": [
      "Shi"
    ],
    "review": "The paper proposes to combine graph convolution with RNNs to solve problems in which inputs are graphs. The two key ideas are: (i) a graph convolutional layer is used to extract features which are then fed in an RNN, and (ii) matrix multiplications are replaced by graph convolution operations. (i) is applied to language modelling, yielding lower perplexity on Penn Treebank (PTB) compared with LSTM. (ii) outperformed LSTM + CNN on the moving-MNIST.\n\nBoth two models/ideas are actually trivial and in line with the current trend of combining different architectures. For instance, the idea of replacing matrix multiplications by graph convolution is a small extension for Shi et al.\n\nRegarding to the experiment on PTB (section 5.2), I'm skeptical about the way the experiment carried out. The reason is that, instead of using the given development set to tune the models, the authors blindly used an available configuration which is for a different model.\n\nPros: \n- good experimental results\n\nCons:\n- ideas are quite trivial \n- the experiment on PTB was carried out improperly "
  },
  {
    "people": [
      "Shi",
      "Defferrad",
      "Shi",
      "Kalchbrenner",
      "Zaremba",
      "Zaremba",
      "Zaremba",
      "Shi",
      "Defferrad"
    ],
    "review": "This paper investigates the  modeling of graph sequences . Authors propose Graph Convolutional Recurrent Networks (GRCN)  that extends convLSTM  (Shi et al. 2015) for data having an unregular graph structure at each timestep. They replace the 2D convolution with a graph convolutional operator from (Defferrad et al., 2016).\nAuthors propose two variations of the GRCN model. In Model 1,  the graph convolution is only applied on the input data. In Model 2, the graph convolution  is applied on both  input data and the previous hidden states. They evaluate their approaches on two different tasks, video generation using the movingMNIST dataset and world-level language modelling using Penntreebank.\n\nOn movingMNIST authors show that their GRCN 2 improves upon convLSTM. However, they evaluate only with one-layer convLSTM, while Shi et al. report better results with 3 layers (also not as good as  GRCN) . It would be nice to evaluate GCRCN in that setting as well.\nWhile the authors show an improvement of GRCN relatively to convLSTM, GRCN on this task seems relatively weak compared to recent works such as the Video Pixel Networks (Kalchbrenner et al., 2016). It contradicts the claim that \"Model 2 has shown good performance in the case of video prediction\" in the conclusion.\n\nFor the Penntreebank experiments, author compares  their model 1 with FC-LSTM, with or without dropout. However, the results in (Zaremba et al., 2014) still seems different than the one reported here. In (Zaremba et al., 2014), they  reports a test perplexity of 78.4 for the large regularized LSTM in their table 1 which outperforms the score of the GRCN. Also, following works such as variational dropout or zoneout have since improve upon Zaremba results. Is there some differences in the experimental setting?  It would be nice to have results that are directly comparable to previous work.\n\n\nPros:\n- Interesting model, \nCons:\n- Overall, the proposed contribution is relatively incremental compared to (Shi et al. 2015) and (Defferrad et al., 2016). \n- Weak results of GRCN relatively to previous works in the experiments, that do not convince of the GRCN advantages.\n"
  },
  {
    "people": [
      "Shi",
      "Shi"
    ],
    "review": "The authors address the problem of modeling temporally-changing signal on a graph, where the signal at one node changes as a function of the inputs and the hidden states of its neighborhood, the size of which is a hyperparameter. The approach follows closely that of Shi et al. 2015, but it is generalized to arbitrary graph structures rather than a fixed grid by using graph convolutions of Defferrard et al. 2016. This is not a strict generalization because the graph formulation treats all edges equally, while the conv kernels in Shi et al. have a built in directionality. The authors show results on a moving MNIST and on the Penn Tree Bank Language Modeling task.\n\nThe paper, model and experiments are decent but I have some concerns:\n\n1. The proposed model is not exceptionally novel from a technical perspective. I usually don't mind if this is the case provided that the authors make up for the deficiency with thorough experimental evaluation, clear write up, and interesting insights into the pros/cons of the approach with respect to previous models. In this case I lean towards this not being the case.\n\n2. The experiment results section is rather terse and light on interpretation. I'm not fully up to date on the latest of Penn Tree Bank language modeling results but I do know that it is a hotly contested and well-known dataset. I am surprised to see a comparison only to Zaremba et al 2014 where I would expect to see multiple other results.\n\n3. The writing is not very clear and the authors don't make sufficiently strong attempt to compare the models or provide insight or comparisons into why the proposed model works better. In particular, unless I'm mistaken the word probabilities are a function of the neighborhood in the graph. What is the width of this graph? For example, suppose I sample a word in one part of the graph, doesn't this information have to propagate to the other parts of the graph along the edges? Also, it's not clear to me how the model can achieve reasonable results on moving MNIST when it cannot distinguish the direction of the moving edges. The authors state this but do not provide satisfying insight into how this can work. How does a pixel know that it should turn on in the next frame? I wish the authors thought about this more and presented it more clearly.\n\nIn summary, the paper has somewhat weak technical contribution, the experiments section is not very thorough, and the insights are sparse.\n"
  },
  {
    "people": [
      "Shi",
      "Shi"
    ],
    "review": "The authors address the problem of modeling temporally-changing signal on a graph, where the signal at one node changes as a function of the inputs and the hidden states of its neighborhood, the size of which is a hyperparameter. The approach follows closely that of Shi et al. 2015, but it is generalized to arbitrary graph structures rather than a fixed grid by using graph convolutions of Defferrard et al. 2016. This is not a strict generalization because the graph formulation treats all edges equally, while the conv kernels in Shi et al. have a built in directionality. The authors show results on a moving MNIST and on the Penn Tree Bank Language Modeling task.\n\nThe paper, model and experiments are decent but I have some concerns:\n\n1. The proposed model is not exceptionally novel from a technical perspective. I usually don't mind if this is the case provided that the authors make up for the deficiency with thorough experimental evaluation, clear write up, and interesting insights into the pros/cons of the approach with respect to previous models. In this case I lean towards this not being the case.\n\n2. The experiment results section is rather terse and light on interpretation. I'm not fully up to date on the latest of Penn Tree Bank language modeling results but I do know that it is a hotly contested and well-known dataset. I am surprised to see a comparison only to Zaremba et al 2014 where I would expect to see multiple other results.\n\n3. The writing is not very clear and the authors don't make sufficiently strong attempt to compare the models or provide insight or comparisons into why the proposed model works better. In particular, unless I'm mistaken the word probabilities are a function of the neighborhood in the graph. What is the width of this graph? For example, suppose I sample a word in one part of the graph, doesn't this information have to propagate to the other parts of the graph along the edges? Also, it's not clear to me how the model can achieve reasonable results on moving MNIST when it cannot distinguish the direction of the moving edges. The authors state this but do not provide satisfying insight into how this can work. How does a pixel know that it should turn on in the next frame? I wish the authors thought about this more and presented it more clearly.\n\nIn summary, the paper has somewhat weak technical contribution, the experiments section is not very thorough, and the insights are sparse."
  },
  {
    "people": [
      "Shi"
    ],
    "review": "The paper proposes to combine graph convolution with RNNs to solve problems in which inputs are graphs. The two key ideas are: (i) a graph convolutional layer is used to extract features which are then fed in an RNN, and (ii) matrix multiplications are replaced by graph convolution operations. (i) is applied to language modelling, yielding lower perplexity on Penn Treebank (PTB) compared with LSTM. (ii) outperformed LSTM + CNN on the moving-MNIST.\n\nBoth two models/ideas are actually trivial and in line with the current trend of combining different architectures. For instance, the idea of replacing matrix multiplications by graph convolution is a small extension for Shi et al.\n\nRegarding to the experiment on PTB (section 5.2), I'm skeptical about the way the experiment carried out. The reason is that, instead of using the given development set to tune the models, the authors blindly used an available configuration which is for a different model.\n\nPros: \n- good experimental results\n\nCons:\n- ideas are quite trivial \n- the experiment on PTB was carried out improperly "
  },
  {
    "people": [
      "Shi",
      "Defferrad",
      "Shi",
      "Kalchbrenner",
      "Zaremba",
      "Zaremba",
      "Zaremba",
      "Shi",
      "Defferrad"
    ],
    "review": "This paper investigates the  modeling of graph sequences . Authors propose Graph Convolutional Recurrent Networks (GRCN)  that extends convLSTM  (Shi et al. 2015) for data having an unregular graph structure at each timestep. They replace the 2D convolution with a graph convolutional operator from (Defferrad et al., 2016).\nAuthors propose two variations of the GRCN model. In Model 1,  the graph convolution is only applied on the input data. In Model 2, the graph convolution  is applied on both  input data and the previous hidden states. They evaluate their approaches on two different tasks, video generation using the movingMNIST dataset and world-level language modelling using Penntreebank.\n\nOn movingMNIST authors show that their GRCN 2 improves upon convLSTM. However, they evaluate only with one-layer convLSTM, while Shi et al. report better results with 3 layers (also not as good as  GRCN) . It would be nice to evaluate GCRCN in that setting as well.\nWhile the authors show an improvement of GRCN relatively to convLSTM, GRCN on this task seems relatively weak compared to recent works such as the Video Pixel Networks (Kalchbrenner et al., 2016). It contradicts the claim that \"Model 2 has shown good performance in the case of video prediction\" in the conclusion.\n\nFor the Penntreebank experiments, author compares  their model 1 with FC-LSTM, with or without dropout. However, the results in (Zaremba et al., 2014) still seems different than the one reported here. In (Zaremba et al., 2014), they  reports a test perplexity of 78.4 for the large regularized LSTM in their table 1 which outperforms the score of the GRCN. Also, following works such as variational dropout or zoneout have since improve upon Zaremba results. Is there some differences in the experimental setting?  It would be nice to have results that are directly comparable to previous work.\n\n\nPros:\n- Interesting model, \nCons:\n- Overall, the proposed contribution is relatively incremental compared to (Shi et al. 2015) and (Defferrad et al., 2016). \n- Weak results of GRCN relatively to previous works in the experiments, that do not convince of the GRCN advantages.\n"
  },
  {
    "people": [
      "Shi",
      "Shi"
    ],
    "review": "The authors address the problem of modeling temporally-changing signal on a graph, where the signal at one node changes as a function of the inputs and the hidden states of its neighborhood, the size of which is a hyperparameter. The approach follows closely that of Shi et al. 2015, but it is generalized to arbitrary graph structures rather than a fixed grid by using graph convolutions of Defferrard et al. 2016. This is not a strict generalization because the graph formulation treats all edges equally, while the conv kernels in Shi et al. have a built in directionality. The authors show results on a moving MNIST and on the Penn Tree Bank Language Modeling task.\n\nThe paper, model and experiments are decent but I have some concerns:\n\n1. The proposed model is not exceptionally novel from a technical perspective. I usually don't mind if this is the case provided that the authors make up for the deficiency with thorough experimental evaluation, clear write up, and interesting insights into the pros/cons of the approach with respect to previous models. In this case I lean towards this not being the case.\n\n2. The experiment results section is rather terse and light on interpretation. I'm not fully up to date on the latest of Penn Tree Bank language modeling results but I do know that it is a hotly contested and well-known dataset. I am surprised to see a comparison only to Zaremba et al 2014 where I would expect to see multiple other results.\n\n3. The writing is not very clear and the authors don't make sufficiently strong attempt to compare the models or provide insight or comparisons into why the proposed model works better. In particular, unless I'm mistaken the word probabilities are a function of the neighborhood in the graph. What is the width of this graph? For example, suppose I sample a word in one part of the graph, doesn't this information have to propagate to the other parts of the graph along the edges? Also, it's not clear to me how the model can achieve reasonable results on moving MNIST when it cannot distinguish the direction of the moving edges. The authors state this but do not provide satisfying insight into how this can work. How does a pixel know that it should turn on in the next frame? I wish the authors thought about this more and presented it more clearly.\n\nIn summary, the paper has somewhat weak technical contribution, the experiments section is not very thorough, and the insights are sparse.\n"
  },
  {
    "people": [
      "Salimans",
      "Salimans"
    ],
    "review": "This is a great paper and I really enjoy reading it. One thing I would like to see is more quantitative results on CIFAR 10 dataset. For example, is there a table of pairwise GMAM metrics on CIFAR, similar to the one on MNIST? Is it possible to show the Inception Scores on CIFAR? In the paper it's said that: \"Salimans et al. (2016) recommend an Inception score, however, it assumes labels exist for the dataset.\". To my knowledge it's not true, in Salimans et al. (2016) they also compute Inception Scores for models without using label information (See Table 3)."
  },
  {
    "people": [
      "Salimans",
      "Salimans"
    ],
    "review": "This is a great paper and I really enjoy reading it. One thing I would like to see is more quantitative results on CIFAR 10 dataset. For example, is there a table of pairwise GMAM metrics on CIFAR, similar to the one on MNIST? Is it possible to show the Inception Scores on CIFAR? In the paper it's said that: \"Salimans et al. (2016) recommend an Inception score, however, it assumes labels exist for the dataset.\". To my knowledge it's not true, in Salimans et al. (2016) they also compute Inception Scores for models without using label information (See Table 3)."
  },
  {
    "people": [
      "Gregor"
    ],
    "review": "This paper presents a generative model of video sequence data where the frames are assumed to be generated by a static background with a 2d sprite composited onto it at each timestep.  The sprite itself is allowed to dynamically change its appearance and location within the image from frame to frame.  This paper follows the VAE (Variational Autoencoder) approach, where a recognition/inference network allows them to recover the latent state at each timestep.\n\nSome results are presented on simple synthetic data (such as a moving rectangle on a black background or the \u201cMoving MNIST\u201d data.  However, the results are preliminary and I suspect that the assumptions used in the paper are far too strong too be useful in real videos.  On the Moving MNIST data, the numerical results are not competitive to state of the art numbers.\n\nThe model itself is also not particularly novel and the work currently misses some relevant citations.  The form of the forward model, for example, could be viewed as a variation on the DRAW paper by Gregor et al (ICML 2014).  Efficient Inference in Occlusion-Aware Generative Models of Images by Huang & Murphy (ICLR) is another relevant work, which used a variational auto-encoder with a spatial transformer and an RNN-like sequence model to model the appearance of multiple sprites on a background.\n\nFinally, the exposition in this paper is short on many details and I don\u2019t believe that the paper is reproducible from the text alone.  For example, it is not clear what the form of the recognition model is\u2026  Low-level details (which are very important) are also not presented, such as initialization strategy."
  },
  {
    "people": [
      "Gregor"
    ],
    "review": "This paper presents a generative model of video sequence data where the frames are assumed to be generated by a static background with a 2d sprite composited onto it at each timestep.  The sprite itself is allowed to dynamically change its appearance and location within the image from frame to frame.  This paper follows the VAE (Variational Autoencoder) approach, where a recognition/inference network allows them to recover the latent state at each timestep.\n\nSome results are presented on simple synthetic data (such as a moving rectangle on a black background or the \u201cMoving MNIST\u201d data.  However, the results are preliminary and I suspect that the assumptions used in the paper are far too strong too be useful in real videos.  On the Moving MNIST data, the numerical results are not competitive to state of the art numbers.\n\nThe model itself is also not particularly novel and the work currently misses some relevant citations.  The form of the forward model, for example, could be viewed as a variation on the DRAW paper by Gregor et al (ICML 2014).  Efficient Inference in Occlusion-Aware Generative Models of Images by Huang & Murphy (ICLR) is another relevant work, which used a variational auto-encoder with a spatial transformer and an RNN-like sequence model to model the appearance of multiple sprites on a background.\n\nFinally, the exposition in this paper is short on many details and I don\u2019t believe that the paper is reproducible from the text alone.  For example, it is not clear what the form of the recognition model is\u2026  Low-level details (which are very important) are also not presented, such as initialization strategy."
  },
  {
    "people": [
      "Gregor"
    ],
    "review": "This paper presents a generative model of video sequence data where the frames are assumed to be generated by a static background with a 2d sprite composited onto it at each timestep.  The sprite itself is allowed to dynamically change its appearance and location within the image from frame to frame.  This paper follows the VAE (Variational Autoencoder) approach, where a recognition/inference network allows them to recover the latent state at each timestep.\n\nSome results are presented on simple synthetic data (such as a moving rectangle on a black background or the \u201cMoving MNIST\u201d data.  However, the results are preliminary and I suspect that the assumptions used in the paper are far too strong too be useful in real videos.  On the Moving MNIST data, the numerical results are not competitive to state of the art numbers.\n\nThe model itself is also not particularly novel and the work currently misses some relevant citations.  The form of the forward model, for example, could be viewed as a variation on the DRAW paper by Gregor et al (ICML 2014).  Efficient Inference in Occlusion-Aware Generative Models of Images by Huang & Murphy (ICLR) is another relevant work, which used a variational auto-encoder with a spatial transformer and an RNN-like sequence model to model the appearance of multiple sprites on a background.\n\nFinally, the exposition in this paper is short on many details and I don\u2019t believe that the paper is reproducible from the text alone.  For example, it is not clear what the form of the recognition model is\u2026  Low-level details (which are very important) are also not presented, such as initialization strategy."
  },
  {
    "people": [
      "Gregor"
    ],
    "review": "This paper presents a generative model of video sequence data where the frames are assumed to be generated by a static background with a 2d sprite composited onto it at each timestep.  The sprite itself is allowed to dynamically change its appearance and location within the image from frame to frame.  This paper follows the VAE (Variational Autoencoder) approach, where a recognition/inference network allows them to recover the latent state at each timestep.\n\nSome results are presented on simple synthetic data (such as a moving rectangle on a black background or the \u201cMoving MNIST\u201d data.  However, the results are preliminary and I suspect that the assumptions used in the paper are far too strong too be useful in real videos.  On the Moving MNIST data, the numerical results are not competitive to state of the art numbers.\n\nThe model itself is also not particularly novel and the work currently misses some relevant citations.  The form of the forward model, for example, could be viewed as a variation on the DRAW paper by Gregor et al (ICML 2014).  Efficient Inference in Occlusion-Aware Generative Models of Images by Huang & Murphy (ICLR) is another relevant work, which used a variational auto-encoder with a spatial transformer and an RNN-like sequence model to model the appearance of multiple sprites on a background.\n\nFinally, the exposition in this paper is short on many details and I don\u2019t believe that the paper is reproducible from the text alone.  For example, it is not clear what the form of the recognition model is\u2026  Low-level details (which are very important) are also not presented, such as initialization strategy."
  },
  {
    "people": [
      "Bach"
    ],
    "review": "This paper proposes an interesting framework (as a follow-up work of the author's previous paper) to learn compositional rules used to compose better music. The system consists of two components, a generative component (student) and a discriminative component (teacher). The generative component is a Probabilistic Graphical Models, generating the music following learned rules. The teacher compares the generated music with the empirical distribution of exemplar music (e.g, Bach\u2019s chorales) and propose new rules for the student to learn so that it could improve.\n\nThe framework is different from GANs that the both the generative and discriminative components are interpretable. From the paper, it seems that the system can indeed learn sensible rules from the composed music and apply them in the next iteration, if trained in a curriculum manner. However, there is no comparison between the proposed system and its previous version, nor comparison between the proposed system and other simple baselines, e.g., an LSTM generative model. This might pose a concern here. \n\nI found this paper a bit hard to read, partly due to (1) lots of music terms (e.g, Tbl. 1 does not make sense to me) that hinders understanding of how the system performs, and (2) over-complicated math symbols and concept. For example, In Page 4, the concept of raw/high-level feature, Feature-Induced Partition and Conceptual Hierarchy, all means a non-overlapping hierarchical clustering on the 4-dimensional feature space. Also, there seems to be no hierarchy in Informational Hierarchy, but a list of rules. It would be much clearer if the authors write the paper in a plain way. \n\nOverall, the paper proposes a working system that seems to be interesting. But I am not confident enough to give strong conclusions."
  },
  {
    "people": [
      "Bach"
    ],
    "review": "Summary: \nThe paper presents an advanced self-learning model that extracts compositional rules from Bach's chorales, which extends their previous work in: 1) the rule hierarchy in both conceptual and informational dimensions; 2) adaptive 2-D memory selection which assumes the features follow Dirichlet Distribution. Sonority (column of 4 MIDI numbers) acts as word in language model: unigram statistics have been used to learn the fundamental rules in music theory, while n-grams with higher order help characterize part writing. Sonorities have been clustered together based on feature functions through iterations. The partition induced by the features is recognized as a rule if it is sufficiently significant. As a result, two sample syllabi with different difficulty strides and \"satisfactory gaps\" have been generated in terms of sets of learned rules. \n\n1. Quality:\n a) Strengths: In the paper, the exploration of hierarchies in two dimensions makes the learning process more cognitive and interpretable. The authors also demonstrate an effective memory selection to speed up the learning.\n\n b) Flaws: The paper only discussed N<=5, which might limit the learning and interpretation capacities of the proposed model, failing to capture long-distance dependence of music. (In the replies to questions, the authors mentioned they had experimented with max N=10, but I'm not sure why related results were not included in the paper). Besides the elaborated interpretation of results, a survey seeking the opinions of students in a music department might make the evaluation of system performance more persuasive.\n\n2. Clarity:\n a) Pros: The paper clearly delivers an improved automatic theorist system which learns and represents music concepts as well as thoroughly interprets and compares the learned rules with music theory. Proper analogies and examples help the reader perceive the ideas more easily.\n\n b) Cons: Although detailed definitions can be found in the authors' previous MUS-ROVER I papers, it would be great if they had described the optimization more clearly (in Figure 1. and related parts).  The \"(Conceptual-Hierarchy Filter)\" row in equations (3): the prime symbol should appear in the subscript.\n\n3. Originality:\nThe representation of music concepts and rules is still an open area, the paper investigate the topic in a novel way. It illustrates an alternative besides other interpretable feature learning methods such as autoencoders, GAN, etc. \n\n4. Significance:\nIt is good to see some corresponding interpretations for the learned rules from music theory. The authors mentioned students in music could and should be involved in the self-learning loop to interact, which is very interesting. I hope their advantages can be combined in the practice of music theory teaching and learning.\n"
  },
  {
    "people": [
      "Bach"
    ],
    "review": "This paper proposes an interesting framework (as a follow-up work of the author's previous paper) to learn compositional rules used to compose better music. The system consists of two components, a generative component (student) and a discriminative component (teacher). The generative component is a Probabilistic Graphical Models, generating the music following learned rules. The teacher compares the generated music with the empirical distribution of exemplar music (e.g, Bach\u2019s chorales) and propose new rules for the student to learn so that it could improve.\n\nThe framework is different from GANs that the both the generative and discriminative components are interpretable. From the paper, it seems that the system can indeed learn sensible rules from the composed music and apply them in the next iteration, if trained in a curriculum manner. However, there is no comparison between the proposed system and its previous version, nor comparison between the proposed system and other simple baselines, e.g., an LSTM generative model. This might pose a concern here. \n\nI found this paper a bit hard to read, partly due to (1) lots of music terms (e.g, Tbl. 1 does not make sense to me) that hinders understanding of how the system performs, and (2) over-complicated math symbols and concept. For example, In Page 4, the concept of raw/high-level feature, Feature-Induced Partition and Conceptual Hierarchy, all means a non-overlapping hierarchical clustering on the 4-dimensional feature space. Also, there seems to be no hierarchy in Informational Hierarchy, but a list of rules. It would be much clearer if the authors write the paper in a plain way. \n\nOverall, the paper proposes a working system that seems to be interesting. But I am not confident enough to give strong conclusions."
  },
  {
    "people": [
      "Bach"
    ],
    "review": "I am curious how well the described approach might scale. Certain aspects (seem to me to) depend on exhaustive searches/enumerations, but this was not entirely clear. Suppose, for example, that rather than Bach chorales, the system was optimized to help students learn orchestration, where there can easily be 10-20 instruments. In this case, the family of windows in Eq(1) would therefore correspond to a significantly larger power set. Furthermore, the pieces are much longer, so counting occurrences of arbitrary compositions of atomic operators could be combinatorially non-trivial, etc. Yet, conceptually organizing the rules of orchestration hierarchically might actually be quite a helpful application. Any comment on this would be appreciated."
  },
  {
    "people": [
      "Bach"
    ],
    "review": "This paper proposes an interesting framework (as a follow-up work of the author's previous paper) to learn compositional rules used to compose better music. The system consists of two components, a generative component (student) and a discriminative component (teacher). The generative component is a Probabilistic Graphical Models, generating the music following learned rules. The teacher compares the generated music with the empirical distribution of exemplar music (e.g, Bach\u2019s chorales) and propose new rules for the student to learn so that it could improve.\n\nThe framework is different from GANs that the both the generative and discriminative components are interpretable. From the paper, it seems that the system can indeed learn sensible rules from the composed music and apply them in the next iteration, if trained in a curriculum manner. However, there is no comparison between the proposed system and its previous version, nor comparison between the proposed system and other simple baselines, e.g., an LSTM generative model. This might pose a concern here. \n\nI found this paper a bit hard to read, partly due to (1) lots of music terms (e.g, Tbl. 1 does not make sense to me) that hinders understanding of how the system performs, and (2) over-complicated math symbols and concept. For example, In Page 4, the concept of raw/high-level feature, Feature-Induced Partition and Conceptual Hierarchy, all means a non-overlapping hierarchical clustering on the 4-dimensional feature space. Also, there seems to be no hierarchy in Informational Hierarchy, but a list of rules. It would be much clearer if the authors write the paper in a plain way. \n\nOverall, the paper proposes a working system that seems to be interesting. But I am not confident enough to give strong conclusions."
  },
  {
    "people": [
      "Bach"
    ],
    "review": "Summary: \nThe paper presents an advanced self-learning model that extracts compositional rules from Bach's chorales, which extends their previous work in: 1) the rule hierarchy in both conceptual and informational dimensions; 2) adaptive 2-D memory selection which assumes the features follow Dirichlet Distribution. Sonority (column of 4 MIDI numbers) acts as word in language model: unigram statistics have been used to learn the fundamental rules in music theory, while n-grams with higher order help characterize part writing. Sonorities have been clustered together based on feature functions through iterations. The partition induced by the features is recognized as a rule if it is sufficiently significant. As a result, two sample syllabi with different difficulty strides and \"satisfactory gaps\" have been generated in terms of sets of learned rules. \n\n1. Quality:\n a) Strengths: In the paper, the exploration of hierarchies in two dimensions makes the learning process more cognitive and interpretable. The authors also demonstrate an effective memory selection to speed up the learning.\n\n b) Flaws: The paper only discussed N<=5, which might limit the learning and interpretation capacities of the proposed model, failing to capture long-distance dependence of music. (In the replies to questions, the authors mentioned they had experimented with max N=10, but I'm not sure why related results were not included in the paper). Besides the elaborated interpretation of results, a survey seeking the opinions of students in a music department might make the evaluation of system performance more persuasive.\n\n2. Clarity:\n a) Pros: The paper clearly delivers an improved automatic theorist system which learns and represents music concepts as well as thoroughly interprets and compares the learned rules with music theory. Proper analogies and examples help the reader perceive the ideas more easily.\n\n b) Cons: Although detailed definitions can be found in the authors' previous MUS-ROVER I papers, it would be great if they had described the optimization more clearly (in Figure 1. and related parts).  The \"(Conceptual-Hierarchy Filter)\" row in equations (3): the prime symbol should appear in the subscript.\n\n3. Originality:\nThe representation of music concepts and rules is still an open area, the paper investigate the topic in a novel way. It illustrates an alternative besides other interpretable feature learning methods such as autoencoders, GAN, etc. \n\n4. Significance:\nIt is good to see some corresponding interpretations for the learned rules from music theory. The authors mentioned students in music could and should be involved in the self-learning loop to interact, which is very interesting. I hope their advantages can be combined in the practice of music theory teaching and learning.\n"
  },
  {
    "people": [
      "Bach"
    ],
    "review": "This paper proposes an interesting framework (as a follow-up work of the author's previous paper) to learn compositional rules used to compose better music. The system consists of two components, a generative component (student) and a discriminative component (teacher). The generative component is a Probabilistic Graphical Models, generating the music following learned rules. The teacher compares the generated music with the empirical distribution of exemplar music (e.g, Bach\u2019s chorales) and propose new rules for the student to learn so that it could improve.\n\nThe framework is different from GANs that the both the generative and discriminative components are interpretable. From the paper, it seems that the system can indeed learn sensible rules from the composed music and apply them in the next iteration, if trained in a curriculum manner. However, there is no comparison between the proposed system and its previous version, nor comparison between the proposed system and other simple baselines, e.g., an LSTM generative model. This might pose a concern here. \n\nI found this paper a bit hard to read, partly due to (1) lots of music terms (e.g, Tbl. 1 does not make sense to me) that hinders understanding of how the system performs, and (2) over-complicated math symbols and concept. For example, In Page 4, the concept of raw/high-level feature, Feature-Induced Partition and Conceptual Hierarchy, all means a non-overlapping hierarchical clustering on the 4-dimensional feature space. Also, there seems to be no hierarchy in Informational Hierarchy, but a list of rules. It would be much clearer if the authors write the paper in a plain way. \n\nOverall, the paper proposes a working system that seems to be interesting. But I am not confident enough to give strong conclusions."
  },
  {
    "people": [
      "Bach"
    ],
    "review": "I am curious how well the described approach might scale. Certain aspects (seem to me to) depend on exhaustive searches/enumerations, but this was not entirely clear. Suppose, for example, that rather than Bach chorales, the system was optimized to help students learn orchestration, where there can easily be 10-20 instruments. In this case, the family of windows in Eq(1) would therefore correspond to a significantly larger power set. Furthermore, the pieces are much longer, so counting occurrences of arbitrary compositions of atomic operators could be combinatorially non-trivial, etc. Yet, conceptually organizing the rules of orchestration hierarchically might actually be quite a helpful application. Any comment on this would be appreciated."
  },
  {
    "people": [
      "De Brabandere"
    ],
    "review": "I sincerely apologize for the late review!\n\nThe first part has a strong emphasis on the technical part. It could benefit from some high level arguments on what the method aims to achieve, what limitation is there to overcome. I may have misunderstood the contribution (in which case please correct me) that the main novel part of the paper is the suggestion to learn the group parameterizations instead of pre-fixing them. So instead of applying it to common spatial filters as in De Brabandere et al., it is applied to Steerable Frames?\n\nThe first contribution suggests that \"general frame bases are better suited to represent sensory input data than the commonly used pixel basis.\". The experiments on Cifar10+ indicate that this is not true in general. Considering the basis as a hyper-parameter, expensive search has to be conducted to find that the Gauss-Frame gives better results. I assume this does not suggest that the Gauss-Frame is always better, at least there is weak evidence on a single network presented. Maybe the first contribution has to be re-stated. Further is the \"Pixel\" network representation corrected for the larger number of parameters. As someone who is interested in using this, what are the runtime considerations? \n\nI would strongly suggest to improve Fig.3. The Figure uses \"w\" several times in different notations and depictions. It mixes boxes, single symbols and illustrative figures. It took some time to decipher the Figure and its flow. \n\n\nSummary: The paper is sufficiently clear, technical at many places and readability can be improved. E.g., the introduction of frames in the beginning lacks motivation and is rather unclear to someone new to this concept. The work falls in the general category of methods that impose knowledge about filter transformations into the network architecture. For me that has always two sides, the algorithmic and technical part (there are several ways to do this) and the practical side (should I do it)? This is a possible approach to this problem but after the paper I was a bit wondering what I have learned, I am certainly not inspired based on the content of the paper to integrate or build on this work. I am lacking insights into transformational parameters that are relevant for a problem. While the spatial transformer network paper was weaker on the technical elegance side, it provided exactly this: an insight into the feature transformation learned by the algorithm. I am missing this here, e.g., from Table 2  I learn that among four choices one works empirically better. What is destroyed by the x^py^p and Hermite frames that the ResNet is *not* able to recover from? You can construct network architectures that are the superset of both, so that inferior performance could be avoided. \n\nThe algorithm is clear but it is similar to the Dynamic Filter Networks paper. And I am unfortunately not convinced about the usefulness of this particular formulation. I'd expect a stronger paper with more insights into transformations and comparisons to standard techniques, a clear delineation of when this is advised. "
  },
  {
    "people": [
      "De Brabandere"
    ],
    "review": "I sincerely apologize for the late review!\n\nThe first part has a strong emphasis on the technical part. It could benefit from some high level arguments on what the method aims to achieve, what limitation is there to overcome. I may have misunderstood the contribution (in which case please correct me) that the main novel part of the paper is the suggestion to learn the group parameterizations instead of pre-fixing them. So instead of applying it to common spatial filters as in De Brabandere et al., it is applied to Steerable Frames?\n\nThe first contribution suggests that \"general frame bases are better suited to represent sensory input data than the commonly used pixel basis.\". The experiments on Cifar10+ indicate that this is not true in general. Considering the basis as a hyper-parameter, expensive search has to be conducted to find that the Gauss-Frame gives better results. I assume this does not suggest that the Gauss-Frame is always better, at least there is weak evidence on a single network presented. Maybe the first contribution has to be re-stated. Further is the \"Pixel\" network representation corrected for the larger number of parameters. As someone who is interested in using this, what are the runtime considerations? \n\nI would strongly suggest to improve Fig.3. The Figure uses \"w\" several times in different notations and depictions. It mixes boxes, single symbols and illustrative figures. It took some time to decipher the Figure and its flow. \n\n\nSummary: The paper is sufficiently clear, technical at many places and readability can be improved. E.g., the introduction of frames in the beginning lacks motivation and is rather unclear to someone new to this concept. The work falls in the general category of methods that impose knowledge about filter transformations into the network architecture. For me that has always two sides, the algorithmic and technical part (there are several ways to do this) and the practical side (should I do it)? This is a possible approach to this problem but after the paper I was a bit wondering what I have learned, I am certainly not inspired based on the content of the paper to integrate or build on this work. I am lacking insights into transformational parameters that are relevant for a problem. While the spatial transformer network paper was weaker on the technical elegance side, it provided exactly this: an insight into the feature transformation learned by the algorithm. I am missing this here, e.g., from Table 2  I learn that among four choices one works empirically better. What is destroyed by the x^py^p and Hermite frames that the ResNet is *not* able to recover from? You can construct network architectures that are the superset of both, so that inferior performance could be avoided. \n\nThe algorithm is clear but it is similar to the Dynamic Filter Networks paper. And I am unfortunately not convinced about the usefulness of this particular formulation. I'd expect a stronger paper with more insights into transformations and comparisons to standard techniques, a clear delineation of when this is advised. "
  },
  {
    "people": [
      "Edward"
    ],
    "review": "There was general agreement from the reviewers that this looks like an important development in the area of probabilistic programming. Some reviewers felt the impact of the work could be very significant. The quality of the work and the paper were perceived as being quite high. The main weakness highlighted by the most negative reviewer (who felt the work was marginally below threshold) is the level of empirical evaluation given within the submitted manuscript. The authors did submit a revision and they outline the reviewer\u00d5s points that they have addressed. It appears that if accepted this manuscript would constitute the first peer-reviewed paper on the subject of this new software package (Edward). Based on both the numeric scores, the quality and potential significance of this work I recommend acceptance."
  },
  {
    "people": [
      "Edward",
      "Edward",
      "Edward"
    ],
    "review": "Thank you to all the reviewers for their insightful feedback. As the reviewers agree, we think Edward \"has the potential to transform the way we work in the probabilistic modelling community, allowing us to perform rapid-prototyping to iterate through ideas quickly\" (Reviewer 4). We make general comments here; specific comments are made on the reviewer's thread. We have also uploaded a revised submission following the feedback.\n\nA major contribution in the paper is in real-world application: \"the Edward library provides an extremely impressive collection of modern probabilistic inference methods in an easily usable form\" (Reviewer 1). This enables researchers to build on top of current methods and also easily compare to them as baselines.\n\nA second contribution in the paper is in methodology. We proposed new compositional representations, \"putt[ing] focus on compositional inference\" (Reviewer 3). This \"allow[s] the users to select the inference method to suit their needs and constraints\" (Reviewer 3), such as for designing rich variational models and generative adversarial networks. We also show how to integrate such a language into computational graph frameworks; this provides significant speedups with distributing training, parallelism, vectorisation, and GPU support.\n\nFinally, we note that Edward is growing with active contributions from the community ("
  },
  {
    "people": [
      "Edward",
      "Edward",
      "Edward"
    ],
    "review": "The paper introduces Edward, a probabilistic programming language\nbuilt over TensorFlow and Python, and supporting a broad range of most\npopular contemporary methods in probabilistic machine learning.\n\n\nQuality:\n\nThe Edward library provides an extremely impressive collection of\nmodern probabilistic inference methods in an easily usable form.\nThe paper provides a brief review of the most important techniques\nespecially from a representation learning perspective, combined with\ntwo experiments on implementing various modern variational inference\nmethods and GPU-accelerated HMC.\n\nThe first experiment (variational inference) would be more valuable if\nthere was a clear link to complete code to reproduce the results\nprovided. The HMC experiment looks OK, except the characterising Stan\nas a hand-optimised implementation seems unfair as the code is clearly\nnot hand-optimised for this specific model and hardware configuration.\nI do not think anyone doubts the quality of your implementation, so\nplease do not ruin the picture by unsubstantiated sensationalist\nclaims. Instead of current drama, I would suggest comparing\nhead-to-head against Stan on single core and separately reporting the\nextra speedups you gain from parallelisation and GPU. These numbers\nwould also help the readers to estimate the performance of the method\nfor other hardware configurations.\n\n\nClarity:\n\nThe paper is in general clearly written and easy to read. The numerous\ncode examples are helpful, but also difficult as it is sometimes\nunclear what is missing. It would be very helpful if the authors could\nprovide and clearly link to a machine-readable companion (a Jupyter\nnotebook would be great, but even text or HTML would be easier to\ncopy-paste from than a pdf like the paper) with complete runnable code\nfor all the examples.\n\n\nOriginality:\n\nThe Edward library is clearly a unique collection of probabilistic\ninference methods. In terms of the paper, the main threat to novelty\ncomes from previous publications of the same group. The main paper\nrefers to Tran et al. (2016a) which covers a lot of similar material,\nalthough from a different perspective. It is unclear if the other\npaper has been published or submitted somewhere and if so, where.\n\n\nSignificance:\n\nIt seems very likely Edward will have a profound impact on the field\nof Bayesian machine learning and deep learning.\n\n\nOther comments:\n\nIn Sec. 2 you draw a clear distinction between specialised languages\n(including Stan) and Turing-complete languages such as Edward. This\nseems unfair as I believe Stan is also Turing complete. Additionally\nno proof is provided to support the Turing-completeness of Edward.\n"
  },
  {
    "people": [
      "Edward"
    ],
    "review": "There was general agreement from the reviewers that this looks like an important development in the area of probabilistic programming. Some reviewers felt the impact of the work could be very significant. The quality of the work and the paper were perceived as being quite high. The main weakness highlighted by the most negative reviewer (who felt the work was marginally below threshold) is the level of empirical evaluation given within the submitted manuscript. The authors did submit a revision and they outline the reviewer\u00d5s points that they have addressed. It appears that if accepted this manuscript would constitute the first peer-reviewed paper on the subject of this new software package (Edward). Based on both the numeric scores, the quality and potential significance of this work I recommend acceptance."
  },
  {
    "people": [
      "Edward",
      "Edward",
      "Edward"
    ],
    "review": "Thank you to all the reviewers for their insightful feedback. As the reviewers agree, we think Edward \"has the potential to transform the way we work in the probabilistic modelling community, allowing us to perform rapid-prototyping to iterate through ideas quickly\" (Reviewer 4). We make general comments here; specific comments are made on the reviewer's thread. We have also uploaded a revised submission following the feedback.\n\nA major contribution in the paper is in real-world application: \"the Edward library provides an extremely impressive collection of modern probabilistic inference methods in an easily usable form\" (Reviewer 1). This enables researchers to build on top of current methods and also easily compare to them as baselines.\n\nA second contribution in the paper is in methodology. We proposed new compositional representations, \"putt[ing] focus on compositional inference\" (Reviewer 3). This \"allow[s] the users to select the inference method to suit their needs and constraints\" (Reviewer 3), such as for designing rich variational models and generative adversarial networks. We also show how to integrate such a language into computational graph frameworks; this provides significant speedups with distributing training, parallelism, vectorisation, and GPU support.\n\nFinally, we note that Edward is growing with active contributions from the community ("
  },
  {
    "people": [
      "Edward",
      "Edward",
      "Edward"
    ],
    "review": "The paper introduces Edward, a probabilistic programming language\nbuilt over TensorFlow and Python, and supporting a broad range of most\npopular contemporary methods in probabilistic machine learning.\n\n\nQuality:\n\nThe Edward library provides an extremely impressive collection of\nmodern probabilistic inference methods in an easily usable form.\nThe paper provides a brief review of the most important techniques\nespecially from a representation learning perspective, combined with\ntwo experiments on implementing various modern variational inference\nmethods and GPU-accelerated HMC.\n\nThe first experiment (variational inference) would be more valuable if\nthere was a clear link to complete code to reproduce the results\nprovided. The HMC experiment looks OK, except the characterising Stan\nas a hand-optimised implementation seems unfair as the code is clearly\nnot hand-optimised for this specific model and hardware configuration.\nI do not think anyone doubts the quality of your implementation, so\nplease do not ruin the picture by unsubstantiated sensationalist\nclaims. Instead of current drama, I would suggest comparing\nhead-to-head against Stan on single core and separately reporting the\nextra speedups you gain from parallelisation and GPU. These numbers\nwould also help the readers to estimate the performance of the method\nfor other hardware configurations.\n\n\nClarity:\n\nThe paper is in general clearly written and easy to read. The numerous\ncode examples are helpful, but also difficult as it is sometimes\nunclear what is missing. It would be very helpful if the authors could\nprovide and clearly link to a machine-readable companion (a Jupyter\nnotebook would be great, but even text or HTML would be easier to\ncopy-paste from than a pdf like the paper) with complete runnable code\nfor all the examples.\n\n\nOriginality:\n\nThe Edward library is clearly a unique collection of probabilistic\ninference methods. In terms of the paper, the main threat to novelty\ncomes from previous publications of the same group. The main paper\nrefers to Tran et al. (2016a) which covers a lot of similar material,\nalthough from a different perspective. It is unclear if the other\npaper has been published or submitted somewhere and if so, where.\n\n\nSignificance:\n\nIt seems very likely Edward will have a profound impact on the field\nof Bayesian machine learning and deep learning.\n\n\nOther comments:\n\nIn Sec. 2 you draw a clear distinction between specialised languages\n(including Stan) and Turing-complete languages such as Edward. This\nseems unfair as I believe Stan is also Turing complete. Additionally\nno proof is provided to support the Turing-completeness of Edward.\n"
  },
  {
    "people": [
      "Serban IV",
      "Bengio Y"
    ],
    "review": "\n* Summary: This paper proposes a neural machine translation model that translates the source and the target texts in an end to end manner from characters to characters. The model can learn morphology in the encoder and in the decoder the authors use a hierarchical decoder. Authors provide very compelling results on various bilingual corpora for different language pairs. The paper is well-written, the results are competitive compared to other baselines in the literature.\n\n\n* Review:\n     - I think the paper is very well written, I like the analysis presented in this paper. It is clean and precise. \n     - The idea of using hierarchical decoders have been explored before, e.g. [1]. Can you cite those papers?\n     - This paper is mainly an application paper and it is mainly the application of several existing components on the character-level NMT tasks. In this sense, it is good that authors made their codes available online. However, the contributions from the general ML point of view is still limited.\n                   \n* Some Requests:\n -Can you add the size of the models to the Table 1? \n- Can you add some of the failure cases of your model, where the model failed to translate correctly?\n\n* An Overview of the Review:\n\nPros:\n    - The paper is well written\n    - Extensive analysis of the model on various language pairs\n    - Convincing experimental results.    \n    \nCons:\n    - The model is complicated.\n    - Mainly an architecture engineering/application paper(bringing together various well-known techniques), not much novelty.\n    - The proposed model is potentially slower than the regular models since it needs to operate over the characters instead of the words and uses several RNNs.\n\n[1] Serban IV, Sordoni A, Bengio Y, Courville A, Pineau J. Hierarchical neural network generative models for movie dialogues. arXiv preprint arXiv:1507.04808. 2015 Jul 17.\n"
  },
  {
    "people": [
      "Luong",
      "Manning",
      "Minh-Thang Luong",
      "Christopher D. Manning"
    ],
    "review": "Update after reading the authors' responses & the paper revision dated Dec 21:\nI have removed the comment \"insufficient comparison to past work\" in the title & update the score from 3 -> 5.\nThe main reason for the score is on novelty. The proposal of HGRU & the use of the R matrix are basically just to achieve the effect of \"whether to continue from character-level states or using word-level states\". It seems that these solutions are specific to symbolic frameworks like Theano (which the authors used) and TensorFlow. This, however, is not a problem for languages like Matlab (which Luong & Manning used) or Torch.\n\n-----\n\nThis is a well-written paper with good analysis in which I especially like Figure 5. However I think there is little novelty in this work. The title is about learning morphology but there is nothing specifically enforced in the model to learn morphemes or subword units. For example, maybe some constraints can be put on the weights in w_i in Figure 1 to detect morpheme boundaries or some additional objective like MDL can be used (though it's not clear how these constraints can be incorporated cleanly). \n\nMoreover, I'm very surprised that litte comparison (only a brief mention) was given to the work of (Luong & Manning, 2016) [1], which trains deep 8-layer word-character models and achieves much better results on English-Czech, e.g., 19.6 BLEU compared to 17.0 BLEU achieved in the paper. I think the HGRU thing is over-complicated in terms of presentation. If I read correctly, what HGRU does is basically either continue the character decoder or reset using word-level states at boundaries, which is what was done in [1]. Luong & Manning (2016) even make it more efficient by not having to decode all target words at the morpheme level & it would be good to know the speed of the model proposed in this ICLR submission. What end up new in this paper are perhaps different analyses on what a character-based model learns & adding an additional RNN layer in the encoder.\n\nOne minor comment: annotate h_t in Figure 1.\n\n[1] Minh-Thang Luong and Christopher D. Manning. 2016. Achieving Open Vocabulary Neural Machine Translation\nwith Hybrid Word-Character Models. ACL. "
  },
  {
    "people": [
      "Serban IV",
      "Bengio Y"
    ],
    "review": "\n* Summary: This paper proposes a neural machine translation model that translates the source and the target texts in an end to end manner from characters to characters. The model can learn morphology in the encoder and in the decoder the authors use a hierarchical decoder. Authors provide very compelling results on various bilingual corpora for different language pairs. The paper is well-written, the results are competitive compared to other baselines in the literature.\n\n\n* Review:\n     - I think the paper is very well written, I like the analysis presented in this paper. It is clean and precise. \n     - The idea of using hierarchical decoders have been explored before, e.g. [1]. Can you cite those papers?\n     - This paper is mainly an application paper and it is mainly the application of several existing components on the character-level NMT tasks. In this sense, it is good that authors made their codes available online. However, the contributions from the general ML point of view is still limited.\n                   \n* Some Requests:\n -Can you add the size of the models to the Table 1? \n- Can you add some of the failure cases of your model, where the model failed to translate correctly?\n\n* An Overview of the Review:\n\nPros:\n    - The paper is well written\n    - Extensive analysis of the model on various language pairs\n    - Convincing experimental results.    \n    \nCons:\n    - The model is complicated.\n    - Mainly an architecture engineering/application paper(bringing together various well-known techniques), not much novelty.\n    - The proposed model is potentially slower than the regular models since it needs to operate over the characters instead of the words and uses several RNNs.\n\n[1] Serban IV, Sordoni A, Bengio Y, Courville A, Pineau J. Hierarchical neural network generative models for movie dialogues. arXiv preprint arXiv:1507.04808. 2015 Jul 17.\n"
  },
  {
    "people": [
      "Luong",
      "Manning",
      "Minh-Thang Luong",
      "Christopher D. Manning"
    ],
    "review": "Update after reading the authors' responses & the paper revision dated Dec 21:\nI have removed the comment \"insufficient comparison to past work\" in the title & update the score from 3 -> 5.\nThe main reason for the score is on novelty. The proposal of HGRU & the use of the R matrix are basically just to achieve the effect of \"whether to continue from character-level states or using word-level states\". It seems that these solutions are specific to symbolic frameworks like Theano (which the authors used) and TensorFlow. This, however, is not a problem for languages like Matlab (which Luong & Manning used) or Torch.\n\n-----\n\nThis is a well-written paper with good analysis in which I especially like Figure 5. However I think there is little novelty in this work. The title is about learning morphology but there is nothing specifically enforced in the model to learn morphemes or subword units. For example, maybe some constraints can be put on the weights in w_i in Figure 1 to detect morpheme boundaries or some additional objective like MDL can be used (though it's not clear how these constraints can be incorporated cleanly). \n\nMoreover, I'm very surprised that litte comparison (only a brief mention) was given to the work of (Luong & Manning, 2016) [1], which trains deep 8-layer word-character models and achieves much better results on English-Czech, e.g., 19.6 BLEU compared to 17.0 BLEU achieved in the paper. I think the HGRU thing is over-complicated in terms of presentation. If I read correctly, what HGRU does is basically either continue the character decoder or reset using word-level states at boundaries, which is what was done in [1]. Luong & Manning (2016) even make it more efficient by not having to decode all target words at the morpheme level & it would be good to know the speed of the model proposed in this ICLR submission. What end up new in this paper are perhaps different analyses on what a character-based model learns & adding an additional RNN layer in the encoder.\n\nOne minor comment: annotate h_t in Figure 1.\n\n[1] Minh-Thang Luong and Christopher D. Manning. 2016. Achieving Open Vocabulary Neural Machine Translation\nwith Hybrid Word-Character Models. ACL. "
  },
  {
    "people": [
      "Schmit",
      "Duvenaud",
      "Lloyd",
      "Grosse",
      "Tenenbaum",
      "Ghahramani"
    ],
    "review": "This paper proposes using functions such as sin and cos as basis functions, then training a neural network with L1 regularization to obtain a simple estimate of functions that can extrapolate under some circumstances.\n \n Pros:\n - the paper has a wide-ranging discussion connecting extrapolation in regression problems to adjacent fields of system identification and causal learning.\n - the method is sensible enough, and should probably be a baseline in the time-series literature. It also seems like an advance on the hard-to-optimize Eureqa method.\n \n Cons:\n I agree with the authors that Reviewer 5's comments aren't very helpful, but this paper really does ignore or dismiss a lot of recent progress and related methods. Specifically:\n - The authors claim that cross-validation can't be used to choose the model, since it wouldn't encourage extrapolation - but why not partition the data in contiguous chunks, as is done in time-series methods?\n - The authors introduce an annealing trick to help with the L1 objective, but there is a rich literature on gradient-based optimization methods with L1 regularization that address exactly this problem.\n - The authors mostly consider toy data, limiting the potential impact of their method.\n - The authors don't compare against closely related methods developed to address the exact same setting. Namely, Schmit + Lipson's Eureqa method, and the Gaussian process methods of Duvenaud, Lloyd, Grosse, Tenenbaum and Ghahramani.\n - The authors invent their own ad-hoc model-selection procedure, again ignoring a massive literature.\n \n Given the many \"cons\", it is recommended that this paper not be presented at the conference track, but be featured at the workshop track."
  },
  {
    "people": [
      "Schmit",
      "Duvenaud",
      "Lloyd",
      "Grosse",
      "Tenenbaum",
      "Ghahramani"
    ],
    "review": "This paper proposes using functions such as sin and cos as basis functions, then training a neural network with L1 regularization to obtain a simple estimate of functions that can extrapolate under some circumstances.\n \n Pros:\n - the paper has a wide-ranging discussion connecting extrapolation in regression problems to adjacent fields of system identification and causal learning.\n - the method is sensible enough, and should probably be a baseline in the time-series literature. It also seems like an advance on the hard-to-optimize Eureqa method.\n \n Cons:\n I agree with the authors that Reviewer 5's comments aren't very helpful, but this paper really does ignore or dismiss a lot of recent progress and related methods. Specifically:\n - The authors claim that cross-validation can't be used to choose the model, since it wouldn't encourage extrapolation - but why not partition the data in contiguous chunks, as is done in time-series methods?\n - The authors introduce an annealing trick to help with the L1 objective, but there is a rich literature on gradient-based optimization methods with L1 regularization that address exactly this problem.\n - The authors mostly consider toy data, limiting the potential impact of their method.\n - The authors don't compare against closely related methods developed to address the exact same setting. Namely, Schmit + Lipson's Eureqa method, and the Gaussian process methods of Duvenaud, Lloyd, Grosse, Tenenbaum and Ghahramani.\n - The authors invent their own ad-hoc model-selection procedure, again ignoring a massive literature.\n \n Given the many \"cons\", it is recommended that this paper not be presented at the conference track, but be featured at the workshop track."
  },
  {
    "people": [
      "Bengio",
      "Daume",
      "Ross",
      "Srivastava",
      "Goodfellow"
    ],
    "review": "This manuscript tries to tackle neural network regularization by blending the target distribution with predictions of the model itself. In this sense it is similar in spirit to scheduled sampling (Bengio et al) and SEARN (Daume et al) DAgger (Ross et al) which consider a \"roll-in\" mixture of the target and model distributions during training. It was clarified in the pre-review questions that these targets are generated on-line rather than from a lagged distribution, which I think makes the algorithm pseudocode somewhat misleading if I understand it correctly.\n\nThis is an incremental improvement on the idea of label softening/smoothing that has recently been revived, and so the novelty is not that high. The author points out that co-label similarity is better preserved by this method but it doesn't follow that this is causal re: regularization; a natural baseline would be a fixed, soft label distribution, as well as one where the softening/temperature of the label distribution is gradually reduced (as one would expect for this method to do as the model gets closer and closer to reproducing the target distribution).\n\nIt's an interesting and somewhat appealing idea but the case is not clearly made that this is all that useful. The dropout baselines for MNIST seem quite far from results already in the literature (Srivastava et al 2014 achieves 1.06% with a 3x1024 MLP with dropout and a simple max norm constraint; the dropout baselines here fail to break 1.3% which is rather high by contemporary standards on the permutation-invariant task), and results for CIFAR10 are quite far from the current state of the art, making it difficult to judge the contribution in light of other innovations. The largest benchmark considered is SVHN where the reported accuracies are quite bad indeed; SOTA for single net performance has been less than half the reported error rates for 3-4 years now. It's unclear what conclusions can be drawn about how this would help (or even hurt) in a better-tuned setting.\n\nI have remaining reservations about data hygiene, namely reporting minimum test loss/maximum test accuracy rather than an unbiased method for model selection (minimum validation set error, for example). Relatedly, the regularization potential of early stopping on a validation set is not considered. See, e.g. the protocol in Goodfellow et al (2013)."
  },
  {
    "people": [
      "Hinton",
      "Hinton"
    ],
    "review": "The paper introduced a regularization scheme through soft-target that are produced by mixing between the true hard label and the current model prediction. Very similar method was proposed in Section 6 from (Hinton et al. 2016, Distilling the Knowledge in a Neural Network). \n\nPros: \n+ Comprehensive analysis on the co-label similarity.\n\nCons:\n- Weak baselines. I am not sure the authors have found the best hyper-parameters in their experiments. I just trained a 5 layer fully connected MNIST model with 512 hidden units without any regularizer and achieved 0.986 acc. using Adam and He initialization, where the paper reported 0.981 for such architecture. \n- The authors failed to bring the novel idea. It is very similar to (Hinton et al. 2016). This is probably not enough for ICLR."
  },
  {
    "people": [
      "Hinton"
    ],
    "review": "Inspired by the analysis on the effect of the co-label similarity (Hinton et al., 2015), this paper proposes a soft-target regularization that iteratively trains the network using weighted average of the exponential moving average of past labels and hard labels as target argument of loss. They claim that this prevents the disappearing of co-label similarity after early training and  yields a competitive regularization to dropout without sacrificing network capacity.\n\nIn order to make a fair comparison to dropout,  the dropout should be tuned carefully. Showing that it performs better than dropout regularization for some particular values of dropout (Table 2) does not demonstrate a convincing advantage. It is possible that dropout performs better after a reasonable tuning with cross-validation.\n\nThe baseline architectures used in the experiments do not belong the recent state of art methods thus yielding significantly lower accuracy. It seems also that experiment setup does not involve any data augmentation, the results can also change with augmentation. It is not clear why number of epochs are set to a small number like 100 without putting some convergence tests.. Therefore the significance of the method is not convincingly demonstrated in empirical study.\n\nCo-label similarities could be calculated using softmax results at final layer rather than using predicted labels.  The advantage over dropout is not clear in Figure 4, the dropout is set to 0.2 without any cross-validation.  \n\n\nRegularizing by enforcing the training steps to keep co-label similarities is interesting idea but not very novel and the results are not significant.\n\nPros : \n- provides an investigation of regularization on co-label similarity during training\n\nCons:\n-The empirical results do not support the intuitive claims regarding proposed procedure\nIterative version can be unstable in practice\n\n\n"
  },
  {
    "people": [
      "Bengio",
      "Daume",
      "Ross",
      "Srivastava",
      "Goodfellow"
    ],
    "review": "This manuscript tries to tackle neural network regularization by blending the target distribution with predictions of the model itself. In this sense it is similar in spirit to scheduled sampling (Bengio et al) and SEARN (Daume et al) DAgger (Ross et al) which consider a \"roll-in\" mixture of the target and model distributions during training. It was clarified in the pre-review questions that these targets are generated on-line rather than from a lagged distribution, which I think makes the algorithm pseudocode somewhat misleading if I understand it correctly.\n\nThis is an incremental improvement on the idea of label softening/smoothing that has recently been revived, and so the novelty is not that high. The author points out that co-label similarity is better preserved by this method but it doesn't follow that this is causal re: regularization; a natural baseline would be a fixed, soft label distribution, as well as one where the softening/temperature of the label distribution is gradually reduced (as one would expect for this method to do as the model gets closer and closer to reproducing the target distribution).\n\nIt's an interesting and somewhat appealing idea but the case is not clearly made that this is all that useful. The dropout baselines for MNIST seem quite far from results already in the literature (Srivastava et al 2014 achieves 1.06% with a 3x1024 MLP with dropout and a simple max norm constraint; the dropout baselines here fail to break 1.3% which is rather high by contemporary standards on the permutation-invariant task), and results for CIFAR10 are quite far from the current state of the art, making it difficult to judge the contribution in light of other innovations. The largest benchmark considered is SVHN where the reported accuracies are quite bad indeed; SOTA for single net performance has been less than half the reported error rates for 3-4 years now. It's unclear what conclusions can be drawn about how this would help (or even hurt) in a better-tuned setting.\n\nI have remaining reservations about data hygiene, namely reporting minimum test loss/maximum test accuracy rather than an unbiased method for model selection (minimum validation set error, for example). Relatedly, the regularization potential of early stopping on a validation set is not considered. See, e.g. the protocol in Goodfellow et al (2013)."
  },
  {
    "people": [
      "Bergstra",
      "Bengio",
      "Snoek"
    ],
    "review": "\n- You definitely need to report misclassification error results on test data for obvious reasons related to losses and final test misclassification error. Currently comparisons are not conclusive.\n\n-  Can you explain better the reason for using the particular updates in (3) and (4) better? Why don't you do for example totally corrective update, e.g. take convex combination of all \\cal{F}'s (or some portion) up to current iteration in (3)? Therefore \\beta and \\gamma should be tuned reasonably well to see whether (3) and (4) is really helping or not and the range for cross validation should be reported.\n\n- The reason to set n_t n_b is not satisfactory.  It is crucial to cross-validate such parameters. Isn't  n_t = {1,2} unreasonably small number that can cause unstable results? why all n_b and n_t are equal?Are there results on other n_b and n_t's that were tried?\n\n- It is stated that colabel similarities disappear when network starts to overfit. However distillation ( Hinton et.al. ,2015 ) captures colabel similarities after training a model and using distillation. This method seems an iterative extension of distillation without using a bigger teacher model. Does proposed method gives better results then a two step version of distillation ?  \n\n- How do you tune \\lambda for weight decay? \n\n- From paper: \"We considered a frozen set of hyper-parameters for the SoftTarget regularization to show that SoftTarget regularization can still work without a having to conduct a large grid search\". This argument is not valid in ML, maybe if you did a reasonable search, you would get worse results (since you should not look test error until you finish the cross-validation).   Why a common hyper parameter tuning procedure is not used e.g. random search (Bergstra and Bengio, JMLR 2012) or Bayesian optimization (Snoek et al ,NIPS 2012) ?  Setting the hyper parameters to some numbers without searching a range or set can dramatically ruin fair comparison. "
  },
  {
    "people": [
      "Bengio",
      "Daume",
      "Ross",
      "Srivastava",
      "Goodfellow"
    ],
    "review": "This manuscript tries to tackle neural network regularization by blending the target distribution with predictions of the model itself. In this sense it is similar in spirit to scheduled sampling (Bengio et al) and SEARN (Daume et al) DAgger (Ross et al) which consider a \"roll-in\" mixture of the target and model distributions during training. It was clarified in the pre-review questions that these targets are generated on-line rather than from a lagged distribution, which I think makes the algorithm pseudocode somewhat misleading if I understand it correctly.\n\nThis is an incremental improvement on the idea of label softening/smoothing that has recently been revived, and so the novelty is not that high. The author points out that co-label similarity is better preserved by this method but it doesn't follow that this is causal re: regularization; a natural baseline would be a fixed, soft label distribution, as well as one where the softening/temperature of the label distribution is gradually reduced (as one would expect for this method to do as the model gets closer and closer to reproducing the target distribution).\n\nIt's an interesting and somewhat appealing idea but the case is not clearly made that this is all that useful. The dropout baselines for MNIST seem quite far from results already in the literature (Srivastava et al 2014 achieves 1.06% with a 3x1024 MLP with dropout and a simple max norm constraint; the dropout baselines here fail to break 1.3% which is rather high by contemporary standards on the permutation-invariant task), and results for CIFAR10 are quite far from the current state of the art, making it difficult to judge the contribution in light of other innovations. The largest benchmark considered is SVHN where the reported accuracies are quite bad indeed; SOTA for single net performance has been less than half the reported error rates for 3-4 years now. It's unclear what conclusions can be drawn about how this would help (or even hurt) in a better-tuned setting.\n\nI have remaining reservations about data hygiene, namely reporting minimum test loss/maximum test accuracy rather than an unbiased method for model selection (minimum validation set error, for example). Relatedly, the regularization potential of early stopping on a validation set is not considered. See, e.g. the protocol in Goodfellow et al (2013)."
  },
  {
    "people": [
      "Hinton",
      "Hinton"
    ],
    "review": "The paper introduced a regularization scheme through soft-target that are produced by mixing between the true hard label and the current model prediction. Very similar method was proposed in Section 6 from (Hinton et al. 2016, Distilling the Knowledge in a Neural Network). \n\nPros: \n+ Comprehensive analysis on the co-label similarity.\n\nCons:\n- Weak baselines. I am not sure the authors have found the best hyper-parameters in their experiments. I just trained a 5 layer fully connected MNIST model with 512 hidden units without any regularizer and achieved 0.986 acc. using Adam and He initialization, where the paper reported 0.981 for such architecture. \n- The authors failed to bring the novel idea. It is very similar to (Hinton et al. 2016). This is probably not enough for ICLR."
  },
  {
    "people": [
      "Hinton"
    ],
    "review": "Inspired by the analysis on the effect of the co-label similarity (Hinton et al., 2015), this paper proposes a soft-target regularization that iteratively trains the network using weighted average of the exponential moving average of past labels and hard labels as target argument of loss. They claim that this prevents the disappearing of co-label similarity after early training and  yields a competitive regularization to dropout without sacrificing network capacity.\n\nIn order to make a fair comparison to dropout,  the dropout should be tuned carefully. Showing that it performs better than dropout regularization for some particular values of dropout (Table 2) does not demonstrate a convincing advantage. It is possible that dropout performs better after a reasonable tuning with cross-validation.\n\nThe baseline architectures used in the experiments do not belong the recent state of art methods thus yielding significantly lower accuracy. It seems also that experiment setup does not involve any data augmentation, the results can also change with augmentation. It is not clear why number of epochs are set to a small number like 100 without putting some convergence tests.. Therefore the significance of the method is not convincingly demonstrated in empirical study.\n\nCo-label similarities could be calculated using softmax results at final layer rather than using predicted labels.  The advantage over dropout is not clear in Figure 4, the dropout is set to 0.2 without any cross-validation.  \n\n\nRegularizing by enforcing the training steps to keep co-label similarities is interesting idea but not very novel and the results are not significant.\n\nPros : \n- provides an investigation of regularization on co-label similarity during training\n\nCons:\n-The empirical results do not support the intuitive claims regarding proposed procedure\nIterative version can be unstable in practice\n\n\n"
  },
  {
    "people": [
      "Bengio",
      "Daume",
      "Ross",
      "Srivastava",
      "Goodfellow"
    ],
    "review": "This manuscript tries to tackle neural network regularization by blending the target distribution with predictions of the model itself. In this sense it is similar in spirit to scheduled sampling (Bengio et al) and SEARN (Daume et al) DAgger (Ross et al) which consider a \"roll-in\" mixture of the target and model distributions during training. It was clarified in the pre-review questions that these targets are generated on-line rather than from a lagged distribution, which I think makes the algorithm pseudocode somewhat misleading if I understand it correctly.\n\nThis is an incremental improvement on the idea of label softening/smoothing that has recently been revived, and so the novelty is not that high. The author points out that co-label similarity is better preserved by this method but it doesn't follow that this is causal re: regularization; a natural baseline would be a fixed, soft label distribution, as well as one where the softening/temperature of the label distribution is gradually reduced (as one would expect for this method to do as the model gets closer and closer to reproducing the target distribution).\n\nIt's an interesting and somewhat appealing idea but the case is not clearly made that this is all that useful. The dropout baselines for MNIST seem quite far from results already in the literature (Srivastava et al 2014 achieves 1.06% with a 3x1024 MLP with dropout and a simple max norm constraint; the dropout baselines here fail to break 1.3% which is rather high by contemporary standards on the permutation-invariant task), and results for CIFAR10 are quite far from the current state of the art, making it difficult to judge the contribution in light of other innovations. The largest benchmark considered is SVHN where the reported accuracies are quite bad indeed; SOTA for single net performance has been less than half the reported error rates for 3-4 years now. It's unclear what conclusions can be drawn about how this would help (or even hurt) in a better-tuned setting.\n\nI have remaining reservations about data hygiene, namely reporting minimum test loss/maximum test accuracy rather than an unbiased method for model selection (minimum validation set error, for example). Relatedly, the regularization potential of early stopping on a validation set is not considered. See, e.g. the protocol in Goodfellow et al (2013)."
  },
  {
    "people": [
      "Bergstra",
      "Bengio",
      "Snoek"
    ],
    "review": "\n- You definitely need to report misclassification error results on test data for obvious reasons related to losses and final test misclassification error. Currently comparisons are not conclusive.\n\n-  Can you explain better the reason for using the particular updates in (3) and (4) better? Why don't you do for example totally corrective update, e.g. take convex combination of all \\cal{F}'s (or some portion) up to current iteration in (3)? Therefore \\beta and \\gamma should be tuned reasonably well to see whether (3) and (4) is really helping or not and the range for cross validation should be reported.\n\n- The reason to set n_t n_b is not satisfactory.  It is crucial to cross-validate such parameters. Isn't  n_t = {1,2} unreasonably small number that can cause unstable results? why all n_b and n_t are equal?Are there results on other n_b and n_t's that were tried?\n\n- It is stated that colabel similarities disappear when network starts to overfit. However distillation ( Hinton et.al. ,2015 ) captures colabel similarities after training a model and using distillation. This method seems an iterative extension of distillation without using a bigger teacher model. Does proposed method gives better results then a two step version of distillation ?  \n\n- How do you tune \\lambda for weight decay? \n\n- From paper: \"We considered a frozen set of hyper-parameters for the SoftTarget regularization to show that SoftTarget regularization can still work without a having to conduct a large grid search\". This argument is not valid in ML, maybe if you did a reasonable search, you would get worse results (since you should not look test error until you finish the cross-validation).   Why a common hyper parameter tuning procedure is not used e.g. random search (Bergstra and Bengio, JMLR 2012) or Bayesian optimization (Snoek et al ,NIPS 2012) ?  Setting the hyper parameters to some numbers without searching a range or set can dramatically ruin fair comparison. "
  },
  {
    "people": [
      "Maxout"
    ],
    "review": "Update: raised the score, because I think the arguments about adversarial examples are compelling.  I think that the paper convincingly proves that this method acts as a decent regularizer, but I'm not convinced that it's a competitive regularizer.  For example, I don't believe that there is sufficient evidence that it gives a better regularizer than dropout/normalization/etc.  I also think that it will be much harder to tune than these other methods (discussed in my rebuttal reply).  \n\n----\n\nSummary: If I understand correctly, this paper proposes to take the \"bottleneck\" term from variational autoencoders which pulls the latent variable towards a noise prior (like N(0,1)) and apply it in a supervised learning context where the reconstruction term log(p(x|z)) is replaced with the usual supervised cross-entropy objective.  \n\nThe argument is that this is an effective regularizer and increases robustness to adversarial attacks.  \n\nPros: \n\n-The presentation is quite good and the paper is easy to follow.  \n\n-The idea is reasonable and the relationship to previous work is well described.  \n\n-The robustness to adversarial examples experiment seems convincing, though I'm not an expert in this area.  Is there any way to compare to an external quantitative baseline on robustness to adversarial examples?  This would help a lot, since I'm not sure how the method here compares with other regularizers in terms of combatting adversarial examples.  For example, if one uses a very high dropout rate, does this confer a comparable robustness to adversarial examples (perhaps at the expense of accuracy)?  \n\nCons: \n\n-MNIST accuracy results don't seem very strong, unless I'm missing something.  The Maxout paper from ICML 2013 listed many permutation invariant MNIST results with error rates below 1%.  So the 1.13% error rate listed here doesn't necessarily prove that the method is a competitive regularizer.  I also suspect that tuning this method to make it work well is harder than other regularizers like dropout.  \n\n-There are many distinct architectural choices with this method, particularly in how many hidden layers come before and after z.  For example, the output could directly follow z, or there could be several layers between z and the output.  As far as I can tell the paper says that p(y | z) is a simple logistic regression (i.e. one weight matrix followed by softmax), but it's not obvious why this choice was made.  Did it work best empirically?  \n\nOther: \n\n-I wonder what would happen if you \"trained against\" the discovered adversarial examples while also using the method from this paper.  Would it learn to have a higher variance p(z | x) when presented with an adversarial example?  "
  },
  {
    "people": [
      "Louizos"
    ],
    "review": "Summary:\nThe paper \u201cDeep Variational Information Bottleneck\u201d explores the optimization of neural networks for variational approximations of the information bottleneck (IB; Tishby et al., 1999). On the example of MNIST, the authors show that this may be used for regularization or to improve robustness against adversarial attacks.\n\nReview:\nThe IB is potentially very useful for important applications (regularization, adversarial robustness, and privacy are mentioned in the paper). Combining the IB with recent advances in deep learning to make it more widely applicable is an excellent idea. But given that the theoretical contribution is a fairly straight-forward application of well-known ideas, I would have liked to see a stronger experimental section.\n\nSince the proposed approach allows us to scale IB, a better demonstration of this would have been on a larger problem than MNIST. It is also not clear whether the proposed approach will still work well to regularize more interesting networks with many layers.\n\nWhy is dropout not included in the quantitative comparison of robustness to adversarial examples (Figure 4)?\n\nHow was the number of samples (12) chosen?\n\nWhat are the error bars in Figure 1 (a)?\n\nOn page 7 the authors claim \u201cthe posterior covariance becomes larger\u201d as beta \u201cdecreases\u201d (increases?). Is this really the case? It\u2019s hard to judge based on Figure 1, since the figures are differently scaled.\n\nIt might be worth comparing to variational fair autoencoders (Louizos et al., 2016), which also try to learn representations minimizing the information shared with an aspect of the input.\n\nThe paper is well written and easy to follow."
  },
  {
    "people": [
      "Gal Chechik",
      "Amir Globerson",
      "Naftali Tishby",
      "Yair Weiss"
    ],
    "review": "This is an interesting paper. \n\nIt is known that IB for Gaussian variables is equivalent to CCA in certain sense\nGal Chechik and Amir Globerson and Naftali Tishby and Yair Weiss. Information Bottleneck for Gaussian variables. JMLR 2005.\n\nAnd although the motivations and approximations used are somewhat different, DVIB and our deep variational CCA paper (also in submission to ICLR, "
  },
  {
    "people": [
      "Maxout"
    ],
    "review": "Update: raised the score, because I think the arguments about adversarial examples are compelling.  I think that the paper convincingly proves that this method acts as a decent regularizer, but I'm not convinced that it's a competitive regularizer.  For example, I don't believe that there is sufficient evidence that it gives a better regularizer than dropout/normalization/etc.  I also think that it will be much harder to tune than these other methods (discussed in my rebuttal reply).  \n\n----\n\nSummary: If I understand correctly, this paper proposes to take the \"bottleneck\" term from variational autoencoders which pulls the latent variable towards a noise prior (like N(0,1)) and apply it in a supervised learning context where the reconstruction term log(p(x|z)) is replaced with the usual supervised cross-entropy objective.  \n\nThe argument is that this is an effective regularizer and increases robustness to adversarial attacks.  \n\nPros: \n\n-The presentation is quite good and the paper is easy to follow.  \n\n-The idea is reasonable and the relationship to previous work is well described.  \n\n-The robustness to adversarial examples experiment seems convincing, though I'm not an expert in this area.  Is there any way to compare to an external quantitative baseline on robustness to adversarial examples?  This would help a lot, since I'm not sure how the method here compares with other regularizers in terms of combatting adversarial examples.  For example, if one uses a very high dropout rate, does this confer a comparable robustness to adversarial examples (perhaps at the expense of accuracy)?  \n\nCons: \n\n-MNIST accuracy results don't seem very strong, unless I'm missing something.  The Maxout paper from ICML 2013 listed many permutation invariant MNIST results with error rates below 1%.  So the 1.13% error rate listed here doesn't necessarily prove that the method is a competitive regularizer.  I also suspect that tuning this method to make it work well is harder than other regularizers like dropout.  \n\n-There are many distinct architectural choices with this method, particularly in how many hidden layers come before and after z.  For example, the output could directly follow z, or there could be several layers between z and the output.  As far as I can tell the paper says that p(y | z) is a simple logistic regression (i.e. one weight matrix followed by softmax), but it's not obvious why this choice was made.  Did it work best empirically?  \n\nOther: \n\n-I wonder what would happen if you \"trained against\" the discovered adversarial examples while also using the method from this paper.  Would it learn to have a higher variance p(z | x) when presented with an adversarial example?  "
  },
  {
    "people": [
      "Louizos"
    ],
    "review": "Summary:\nThe paper \u201cDeep Variational Information Bottleneck\u201d explores the optimization of neural networks for variational approximations of the information bottleneck (IB; Tishby et al., 1999). On the example of MNIST, the authors show that this may be used for regularization or to improve robustness against adversarial attacks.\n\nReview:\nThe IB is potentially very useful for important applications (regularization, adversarial robustness, and privacy are mentioned in the paper). Combining the IB with recent advances in deep learning to make it more widely applicable is an excellent idea. But given that the theoretical contribution is a fairly straight-forward application of well-known ideas, I would have liked to see a stronger experimental section.\n\nSince the proposed approach allows us to scale IB, a better demonstration of this would have been on a larger problem than MNIST. It is also not clear whether the proposed approach will still work well to regularize more interesting networks with many layers.\n\nWhy is dropout not included in the quantitative comparison of robustness to adversarial examples (Figure 4)?\n\nHow was the number of samples (12) chosen?\n\nWhat are the error bars in Figure 1 (a)?\n\nOn page 7 the authors claim \u201cthe posterior covariance becomes larger\u201d as beta \u201cdecreases\u201d (increases?). Is this really the case? It\u2019s hard to judge based on Figure 1, since the figures are differently scaled.\n\nIt might be worth comparing to variational fair autoencoders (Louizos et al., 2016), which also try to learn representations minimizing the information shared with an aspect of the input.\n\nThe paper is well written and easy to follow."
  },
  {
    "people": [
      "Gal Chechik",
      "Amir Globerson",
      "Naftali Tishby",
      "Yair Weiss"
    ],
    "review": "This is an interesting paper. \n\nIt is known that IB for Gaussian variables is equivalent to CCA in certain sense\nGal Chechik and Amir Globerson and Naftali Tishby and Yair Weiss. Information Bottleneck for Gaussian variables. JMLR 2005.\n\nAnd although the motivations and approximations used are somewhat different, DVIB and our deep variational CCA paper (also in submission to ICLR, "
  },
  {
    "people": [
      "Jonathan Chang",
      "Jordan Boyd-Graber",
      "Sean Gerrish",
      "David M. Blei"
    ],
    "review": "First, let me praise the authors for generating and releasing an NLP data set: a socially useful task.\n\nThe authors use an algorithm to generate a 500-cluster-per-language data set in semantic similarity. This brings up a few points.\n\n1. If the point of using the algorithm is to be scalable, why release such a small data set? It's roughly the same order of magnitude as the data sets released in the SemEval tasks over the recent years. I would have expected something orders of magnitude larger.\n\n2. The authors hand checked a small subset of the clusters: they found one where it was ambiguous, and should probably have been removed. Mechanical Turk can scale pretty well -- why not post-facto filter all of the clusters using MT? This is (in effect) how ImageNet was created, and it has millions of items.\n\n3. Evaluating data set papers is an tricky issue. What makes a data set \"good\" or publishable? There are a number of medium-sized NLP data sets released every year (e.g., through SemEval). Those are designed to address tasks in NLP that people find interesting. I don't know of a data set that exactly addresses the task that the authors propose: the task is trying to address the idea of semantic similarity, which has had multiple data sets thrown at it since SemEval 2012. I wish that the paper had included comparisons to show that the particular task / data combination is better suited for analyzing semantic similarity than other existing data sets.\n\nTwo final notes:\n\nA. This paper doesn't seem very well-suited to ICLR.  New NLP data sets may be indirectly useful for evaluating word embeddings (and hence representations). But, I didn't learn much from the paper: GloVe is empirically less good for semantic similarity than other embeddings? If true, why? That would be interesting.\n\nB. The first proposal for the \"put a word into a cluster and see if it stands out\" task (in the context of human evaluation of topic models), is\n\nJonathan Chang, Jordan Boyd-Graber, Chong Wang, Sean Gerrish, and David M. Blei. Reading Tea Leaves: HowHumans Interpret Topic Models. Neural Information Processing Systems, 2009\n\nwhich deserves a citation, I think."
  },
  {
    "people": [
      "Camacho-Collados & Navigli"
    ],
    "review": "This paper introduces a new dataset to evaluate word representations. The task considered in the paper, called outlier detection (also known as word intrusion), is to identify which word does not belong to a set of semantically related words. The task was proposed by Camacho-Collados & Navigli (2016) as an evaluation of word representations. The main contribution of this paper is to introduce a new dataset for this task, covering 5 languages. The dataset was generated automatically from the Wikidata hierarchy.\nEntities which are instances of the same category are considered as belonging to the same cluster, and outliers are sampled at various distances in the tree. Several heuristics are then proposed to exclude uninteresting clusters from the dataset.\n\nDeveloping good ressources to evaluate word representations is an important task. The new dataset introduced in this paper might be an interesting addition to the existing ones (however, it is hard to say by only reviewing the paper). I am a bit concerned by the lack of discussion and comparison with existing approaches (besides word similarity datasets). In particular, I believe it would be interesting to discuss the advantages of this evaluation/dataset, compared to existing ones such as word analogies. The proposed evaluation also seems highly related to entity typing, which is not discussed in the paper.\n\nOverall, I believe that introducing ressources for evaluating word representations is very important for the community. However, I am a bit ambivalent about this submission. I am not entirely convinced that the proposed dataset have clear advantages over existing ressources. It also seems that existing tasks, such as entity typing, already capture similar properties of word representations. Finally, it might be more relevant to submit this paper to LREC than to ICLR."
  },
  {
    "people": [
      "Jonathan Chang",
      "Jordan Boyd-Graber",
      "Sean Gerrish",
      "David M. Blei"
    ],
    "review": "First, let me praise the authors for generating and releasing an NLP data set: a socially useful task.\n\nThe authors use an algorithm to generate a 500-cluster-per-language data set in semantic similarity. This brings up a few points.\n\n1. If the point of using the algorithm is to be scalable, why release such a small data set? It's roughly the same order of magnitude as the data sets released in the SemEval tasks over the recent years. I would have expected something orders of magnitude larger.\n\n2. The authors hand checked a small subset of the clusters: they found one where it was ambiguous, and should probably have been removed. Mechanical Turk can scale pretty well -- why not post-facto filter all of the clusters using MT? This is (in effect) how ImageNet was created, and it has millions of items.\n\n3. Evaluating data set papers is an tricky issue. What makes a data set \"good\" or publishable? There are a number of medium-sized NLP data sets released every year (e.g., through SemEval). Those are designed to address tasks in NLP that people find interesting. I don't know of a data set that exactly addresses the task that the authors propose: the task is trying to address the idea of semantic similarity, which has had multiple data sets thrown at it since SemEval 2012. I wish that the paper had included comparisons to show that the particular task / data combination is better suited for analyzing semantic similarity than other existing data sets.\n\nTwo final notes:\n\nA. This paper doesn't seem very well-suited to ICLR.  New NLP data sets may be indirectly useful for evaluating word embeddings (and hence representations). But, I didn't learn much from the paper: GloVe is empirically less good for semantic similarity than other embeddings? If true, why? That would be interesting.\n\nB. The first proposal for the \"put a word into a cluster and see if it stands out\" task (in the context of human evaluation of topic models), is\n\nJonathan Chang, Jordan Boyd-Graber, Chong Wang, Sean Gerrish, and David M. Blei. Reading Tea Leaves: HowHumans Interpret Topic Models. Neural Information Processing Systems, 2009\n\nwhich deserves a citation, I think.\n\n"
  },
  {
    "people": [
      "Jonathan Chang",
      "Jordan Boyd-Graber",
      "Sean Gerrish",
      "David M. Blei"
    ],
    "review": "First, let me praise the authors for generating and releasing an NLP data set: a socially useful task.\n\nThe authors use an algorithm to generate a 500-cluster-per-language data set in semantic similarity. This brings up a few points.\n\n1. If the point of using the algorithm is to be scalable, why release such a small data set? It's roughly the same order of magnitude as the data sets released in the SemEval tasks over the recent years. I would have expected something orders of magnitude larger.\n\n2. The authors hand checked a small subset of the clusters: they found one where it was ambiguous, and should probably have been removed. Mechanical Turk can scale pretty well -- why not post-facto filter all of the clusters using MT? This is (in effect) how ImageNet was created, and it has millions of items.\n\n3. Evaluating data set papers is an tricky issue. What makes a data set \"good\" or publishable? There are a number of medium-sized NLP data sets released every year (e.g., through SemEval). Those are designed to address tasks in NLP that people find interesting. I don't know of a data set that exactly addresses the task that the authors propose: the task is trying to address the idea of semantic similarity, which has had multiple data sets thrown at it since SemEval 2012. I wish that the paper had included comparisons to show that the particular task / data combination is better suited for analyzing semantic similarity than other existing data sets.\n\nTwo final notes:\n\nA. This paper doesn't seem very well-suited to ICLR.  New NLP data sets may be indirectly useful for evaluating word embeddings (and hence representations). But, I didn't learn much from the paper: GloVe is empirically less good for semantic similarity than other embeddings? If true, why? That would be interesting.\n\nB. The first proposal for the \"put a word into a cluster and see if it stands out\" task (in the context of human evaluation of topic models), is\n\nJonathan Chang, Jordan Boyd-Graber, Chong Wang, Sean Gerrish, and David M. Blei. Reading Tea Leaves: HowHumans Interpret Topic Models. Neural Information Processing Systems, 2009\n\nwhich deserves a citation, I think."
  },
  {
    "people": [
      "Camacho-Collados & Navigli"
    ],
    "review": "This paper introduces a new dataset to evaluate word representations. The task considered in the paper, called outlier detection (also known as word intrusion), is to identify which word does not belong to a set of semantically related words. The task was proposed by Camacho-Collados & Navigli (2016) as an evaluation of word representations. The main contribution of this paper is to introduce a new dataset for this task, covering 5 languages. The dataset was generated automatically from the Wikidata hierarchy.\nEntities which are instances of the same category are considered as belonging to the same cluster, and outliers are sampled at various distances in the tree. Several heuristics are then proposed to exclude uninteresting clusters from the dataset.\n\nDeveloping good ressources to evaluate word representations is an important task. The new dataset introduced in this paper might be an interesting addition to the existing ones (however, it is hard to say by only reviewing the paper). I am a bit concerned by the lack of discussion and comparison with existing approaches (besides word similarity datasets). In particular, I believe it would be interesting to discuss the advantages of this evaluation/dataset, compared to existing ones such as word analogies. The proposed evaluation also seems highly related to entity typing, which is not discussed in the paper.\n\nOverall, I believe that introducing ressources for evaluating word representations is very important for the community. However, I am a bit ambivalent about this submission. I am not entirely convinced that the proposed dataset have clear advantages over existing ressources. It also seems that existing tasks, such as entity typing, already capture similar properties of word representations. Finally, it might be more relevant to submit this paper to LREC than to ICLR."
  },
  {
    "people": [
      "Jonathan Chang",
      "Jordan Boyd-Graber",
      "Sean Gerrish",
      "David M. Blei"
    ],
    "review": "First, let me praise the authors for generating and releasing an NLP data set: a socially useful task.\n\nThe authors use an algorithm to generate a 500-cluster-per-language data set in semantic similarity. This brings up a few points.\n\n1. If the point of using the algorithm is to be scalable, why release such a small data set? It's roughly the same order of magnitude as the data sets released in the SemEval tasks over the recent years. I would have expected something orders of magnitude larger.\n\n2. The authors hand checked a small subset of the clusters: they found one where it was ambiguous, and should probably have been removed. Mechanical Turk can scale pretty well -- why not post-facto filter all of the clusters using MT? This is (in effect) how ImageNet was created, and it has millions of items.\n\n3. Evaluating data set papers is an tricky issue. What makes a data set \"good\" or publishable? There are a number of medium-sized NLP data sets released every year (e.g., through SemEval). Those are designed to address tasks in NLP that people find interesting. I don't know of a data set that exactly addresses the task that the authors propose: the task is trying to address the idea of semantic similarity, which has had multiple data sets thrown at it since SemEval 2012. I wish that the paper had included comparisons to show that the particular task / data combination is better suited for analyzing semantic similarity than other existing data sets.\n\nTwo final notes:\n\nA. This paper doesn't seem very well-suited to ICLR.  New NLP data sets may be indirectly useful for evaluating word embeddings (and hence representations). But, I didn't learn much from the paper: GloVe is empirically less good for semantic similarity than other embeddings? If true, why? That would be interesting.\n\nB. The first proposal for the \"put a word into a cluster and see if it stands out\" task (in the context of human evaluation of topic models), is\n\nJonathan Chang, Jordan Boyd-Graber, Chong Wang, Sean Gerrish, and David M. Blei. Reading Tea Leaves: HowHumans Interpret Topic Models. Neural Information Processing Systems, 2009\n\nwhich deserves a citation, I think.\n\n"
  },
  {
    "people": [
      "Chen"
    ],
    "review": "The paper aims to consolidate some recent literature in simple types of \"reading comprehension\" tasks involving matching questions to answers to be found in a passage, and then to explore the types of structure learned by these models and propose modifications. These reading comprehension datasets such as CNN/Daily Mail are on the simpler side because they do not generally involve chains of reasoning over multiple pieces of supporting evidence as can be found in datasets like MCTest. Many models have been proposed for this task, and the paper breaks down these models into \"aggregation readers\" and \"explicit reference readers.\" The authors show that the aggregation readers organize their hidden states into a predicate structure which allows them to mimic the explicit reference readers. The authors then experiment with adding linguistic features, including reference features, to the existing models to improve performance.\n\nI appreciate the re-naming and re-writing of the paper to make it more clear that the aggregation readers are specifically learning a predicate structure, as well as the inclusion of results about dimensionality of the symbol space. Further, I think the effort to organize and categorize several different reading comprehension models into broader classes is useful, as the field has been producing many such models and the landscape is unclear. \n\nThe concerns with this paper are that the predicate structure demonstrated is fairly simple, and it is not clear that it provides insight towards the development of better models in the future, since the \"explicit reference readers\" need not learn it, and the CNN/Daily Mail dataset has very little headroom left as demonstrated by Chen et al. 2016. The desire for \"dramatic improvements in performance\" mentioned in the discussion section probably cannot be achieved on these datasets. More complex datasets would probably involve multi-hop inference which this paper does not discuss. Further, the message of the paper is a bit scattered and hard to parse, and could benefit from a bit more focus.\n\nI think that with the explosion of various competing neural network models for NLP tasks, contributions like this one which attempt to organize and analyze the landscape are valuable, but that this paper might be better suited for an NLP conference or journal such as TACL.\n"
  },
  {
    "people": [
      "Chen"
    ],
    "review": "This paper aims to provide an insightful and analytic survey over the recent literature on reading comprehension with the distinct goal of investigating whether logical structure (or predication, as the authors rephrased in their response) arises in many of the recent models. I really like the spirit of the paper and appreciate the efforts to organize rather chaotic recent literature into two unified themes: \"aggregation readers\" and \"explicit reference models\u201d. Overall the quality of writing is great and section 3 was especially nice to read. I\u2019m also happy with the proposed rewording from \"logical structure\" to \u201cpredication\", and the clarification by the authors was detailed and helpful.\n\nI think I still have slight mixed feelings about the contribution of the work. First, I wonder whether the choice of the dataset was ideal in the first place to accomplish the desired goal of the paper. There have been concerns about CNN/DailyMail dataset (Chen et al. ACL\u201916) and it is not clear to me whether the dataset supports investigation on logical structure of interesting kinds. Maybe it is bound to be rather about lack of logical structure.\n\nSecond, I wish the discussion on predication sheds more practical insights into dataset design or model design to better tackle reading comprehension challenges. In that sense, it may have been more helpful if the authors could make more precise analysis on different types of reading comprehension challenges, what types of logical structure are lacking in various existing models and datasets, and point to specific directions where the community needs to focus more.\n\n"
  },
  {
    "people": [
      "Chen"
    ],
    "review": "The paper aims to consolidate some recent literature in simple types of \"reading comprehension\" tasks involving matching questions to answers to be found in a passage, and then to explore the types of structure learned by these models and propose modifications. These reading comprehension datasets such as CNN/Daily Mail are on the simpler side because they do not generally involve chains of reasoning over multiple pieces of supporting evidence as can be found in datasets like MCTest. Many models have been proposed for this task, and the paper breaks down these models into \"aggregation readers\" and \"explicit reference readers.\" The authors show that the aggregation readers organize their hidden states into a predicate structure which allows them to mimic the explicit reference readers. The authors then experiment with adding linguistic features, including reference features, to the existing models to improve performance.\n\nI appreciate the re-naming and re-writing of the paper to make it more clear that the aggregation readers are specifically learning a predicate structure, as well as the inclusion of results about dimensionality of the symbol space. Further, I think the effort to organize and categorize several different reading comprehension models into broader classes is useful, as the field has been producing many such models and the landscape is unclear. \n\nThe concerns with this paper are that the predicate structure demonstrated is fairly simple, and it is not clear that it provides insight towards the development of better models in the future, since the \"explicit reference readers\" need not learn it, and the CNN/Daily Mail dataset has very little headroom left as demonstrated by Chen et al. 2016. The desire for \"dramatic improvements in performance\" mentioned in the discussion section probably cannot be achieved on these datasets. More complex datasets would probably involve multi-hop inference which this paper does not discuss. Further, the message of the paper is a bit scattered and hard to parse, and could benefit from a bit more focus.\n\nI think that with the explosion of various competing neural network models for NLP tasks, contributions like this one which attempt to organize and analyze the landscape are valuable, but that this paper might be better suited for an NLP conference or journal such as TACL.\n"
  },
  {
    "people": [
      "Chen"
    ],
    "review": "This paper aims to provide an insightful and analytic survey over the recent literature on reading comprehension with the distinct goal of investigating whether logical structure (or predication, as the authors rephrased in their response) arises in many of the recent models. I really like the spirit of the paper and appreciate the efforts to organize rather chaotic recent literature into two unified themes: \"aggregation readers\" and \"explicit reference models\u201d. Overall the quality of writing is great and section 3 was especially nice to read. I\u2019m also happy with the proposed rewording from \"logical structure\" to \u201cpredication\", and the clarification by the authors was detailed and helpful.\n\nI think I still have slight mixed feelings about the contribution of the work. First, I wonder whether the choice of the dataset was ideal in the first place to accomplish the desired goal of the paper. There have been concerns about CNN/DailyMail dataset (Chen et al. ACL\u201916) and it is not clear to me whether the dataset supports investigation on logical structure of interesting kinds. Maybe it is bound to be rather about lack of logical structure.\n\nSecond, I wish the discussion on predication sheds more practical insights into dataset design or model design to better tackle reading comprehension challenges. In that sense, it may have been more helpful if the authors could make more precise analysis on different types of reading comprehension challenges, what types of logical structure are lacking in various existing models and datasets, and point to specific directions where the community needs to focus more.\n\n"
  },
  {
    "people": [
      "Salimans"
    ],
    "review": "Improved GAN (Salimans et al., 2016) also adds gaussian noise to inputs of discriminators in their implementation ("
  },
  {
    "people": [
      "Salimans"
    ],
    "review": "Improved GAN (Salimans et al., 2016) also adds gaussian noise to inputs of discriminators in their implementation ("
  },
  {
    "people": [
      "Samek"
    ],
    "review": "We thank the reviewers for a detailed review.  The rebuttal below addresses some of the mentioned concerns.\n\nRegarding \u201cfar too long\u201d and \u201cunnecessarily grandiose name for literally, a scaled image\u201d: \n\nWe\u2019d agree that the paper is long for the ideas in it. The length stems from the difficulty of not having a crisp evaluation technique for feature importance. So we try to resort to qualitative discussions together with images. But we  can definitely try to tighten the writing. We are open to changing the title of the paper to \u201cInterior Gradients\u201d or something like it, though it is worth noting that while scaling intensities seems natural for images, analogous scaling for Text or Drug Discovery models results in inputs that are more obviously fake, i.e., counterfactual.\n\nRegarding \u201chow the proposed scheme for feature importance ranking is useful\u201d: \n\nWhile debugging deep networks is hard in general, examining feature importance scores offers a limited but useful insight into the operation of the network on a particular input. For us, the experience with the Drug Discovery network where we found, via our attributions, that the bond features were severely underused (see Section 3.1) was a concrete instance of how feature importance analysis could help debug and improve networks. As we discussed in section 2.7, we do mention the limitations of our technique in understanding what the network does. The same pros and cons would seem to apply to other feature importance techniques (see Section 2.8). The key difference is that ours is much easier to implement--- as simple as computing a gradient.\n\nRegarding \u201cThe quantitative evidence is quite limited and most of the paper is spent on qualitative results\u201d: \n\nWe address with the following multipart response; apologies for the lengthy response.\n\nFirst, we do plan to produce a comparison with side by sides for LRP and our method for the MNIST data set over the next few weeks as a sanity check.\n\nHowever, we don\u2019t think that that there is a strong metric to compare different feature importance techniques. This is acknowledged by Samek et al in their 2015 ICML Visualization Workshop work. We elaborate on this further at the end of this rebuttal.  \n\nMethods like DeepLift and Layer-wise Relevance Propagation (LRP) break a fundamental axiom in our mind: the attributions depend on the implementation, i.e. two networks that implement identical input-output  relationships can have different attributions. This seems odd---see Section 2.4 and Figure 14.  Perhaps, we did not emphasize this enough in the paper. \n\nThe main focus for us in the evaluation conducted so far has been to ensure that our output was sensible. In Section 2.5, we discuss a combination of approaches that we used to assess the attributions, including eyeballing, localization, and ablations. We welcome you to visualize more attributions at: "
  },
  {
    "people": [
      "Samek"
    ],
    "review": "We thank the reviewers for a detailed review.  The rebuttal below addresses some of the mentioned concerns.\n\nRegarding \u201cfar too long\u201d and \u201cunnecessarily grandiose name for literally, a scaled image\u201d: \n\nWe\u2019d agree that the paper is long for the ideas in it. The length stems from the difficulty of not having a crisp evaluation technique for feature importance. So we try to resort to qualitative discussions together with images. But we  can definitely try to tighten the writing. We are open to changing the title of the paper to \u201cInterior Gradients\u201d or something like it, though it is worth noting that while scaling intensities seems natural for images, analogous scaling for Text or Drug Discovery models results in inputs that are more obviously fake, i.e., counterfactual.\n\nRegarding \u201chow the proposed scheme for feature importance ranking is useful\u201d: \n\nWhile debugging deep networks is hard in general, examining feature importance scores offers a limited but useful insight into the operation of the network on a particular input. For us, the experience with the Drug Discovery network where we found, via our attributions, that the bond features were severely underused (see Section 3.1) was a concrete instance of how feature importance analysis could help debug and improve networks. As we discussed in section 2.7, we do mention the limitations of our technique in understanding what the network does. The same pros and cons would seem to apply to other feature importance techniques (see Section 2.8). The key difference is that ours is much easier to implement--- as simple as computing a gradient.\n\nRegarding \u201cThe quantitative evidence is quite limited and most of the paper is spent on qualitative results\u201d: \n\nWe address with the following multipart response; apologies for the lengthy response.\n\nFirst, we do plan to produce a comparison with side by sides for LRP and our method for the MNIST data set over the next few weeks as a sanity check.\n\nHowever, we don\u2019t think that that there is a strong metric to compare different feature importance techniques. This is acknowledged by Samek et al in their 2015 ICML Visualization Workshop work. We elaborate on this further at the end of this rebuttal.  \n\nMethods like DeepLift and Layer-wise Relevance Propagation (LRP) break a fundamental axiom in our mind: the attributions depend on the implementation, i.e. two networks that implement identical input-output  relationships can have different attributions. This seems odd---see Section 2.4 and Figure 14.  Perhaps, we did not emphasize this enough in the paper. \n\nThe main focus for us in the evaluation conducted so far has been to ensure that our output was sensible. In Section 2.5, we discuss a combination of approaches that we used to assess the attributions, including eyeballing, localization, and ablations. We welcome you to visualize more attributions at: "
  },
  {
    "people": [
      "Weston"
    ],
    "review": "SUMMARY.\n\nThe paper proposes a machine reading approach for cloze-style question answering.\nThe proposed system first encodes the query and the document using a bidirectional gru. These two representations are combined together using a Gated Attention (GA).\nGA calculates the compatibility of each word in the document and the query as a probability distribution.\nFor each word in the document a gate is calculated weighting the query representation according to the word compatibility.\nUltimately, the gate is applied to the gru-encoded document word.\nThe resulting word vectors are re-encoded with a bidirectional GRU.\nThis process is performed for multiple hops. After k hops, the probability of a word to be part of the answer is calculated by a log-linear model that take as input the last word representations, and the concatenation of the last query representation before and after the cloze token.\nThe probability of a candidate being the answer to the question is given by a linear combination of the single word probabilities.\n\nThe proposed model is tested on 4 different dataset. \nThe authors shown that the proposed model works well (state-of-the-art performance) for 3 out of 4 benchmarks.\n\n\n----------\n\nOVERALL JUDGMENT\nThe main contribution of the paper is the gated attention mechanism, that in my opinion, is a simple and interesting idea.\nThe paper is well thought, and the ablation study on the benefits given by the gated attention are convincing.\nThe GA reader as whole model outperforms previous state-of-the-art models on 3 benchmarks and seems very promising also on the CBT dataset.\nI would have liked to see some discussion on why the model works less well on the CBT dataset, though.\n\n\n----------\n\nDETAILED COMMENTS\n\nminor. In the introduction, Weston et al., 2014 do not use any attention mechanism."
  },
  {
    "people": [
      "Cloze"
    ],
    "review": "This paper presents an interesting idea for iteratively re-weighting the word representations in a document (hence the GRU-coded doc representation as well) with a simple multiplication operation. As the authors correctly pointed out, such an operation serves as a \"filter\" to reduce the attentions to less relevant parts in the document, hence leading to better performance of the modeling.\n\nThe results are or close to the state-of-the-art for a few Cloze-style QA tasks. This paper would deserve an even higher score, if the following limitations could be addressed better:\n\n1. While interesting and conceptually simple (though with significant increased computational overheads), the architecture proposed in the paper is for a very specific task. \n\n2. The improvement of the main idea of this paper (gated attention) is less significant, comparing GA Reader-- vs. GA Reader, while the latter includes a number of engineering tricks such as adding character embedding and using a word embedding trained from larger corpus (GloVe), as well as some small improvements on the modeling by using token-specific attention in (5).\n\n3. I also wish the authors can shed more lights on what a role the K (number of hops) plays, both intuitively and empirically. I feel more insights could be obtained if we do more deeper analysis of K's impacts to different types of questions for example."
  },
  {
    "people": [
      "Weston"
    ],
    "review": "SUMMARY.\n\nThe paper proposes a machine reading approach for cloze-style question answering.\nThe proposed system first encodes the query and the document using a bidirectional gru. These two representations are combined together using a Gated Attention (GA).\nGA calculates the compatibility of each word in the document and the query as a probability distribution.\nFor each word in the document a gate is calculated weighting the query representation according to the word compatibility.\nUltimately, the gate is applied to the gru-encoded document word.\nThe resulting word vectors are re-encoded with a bidirectional GRU.\nThis process is performed for multiple hops. After k hops, the probability of a word to be part of the answer is calculated by a log-linear model that take as input the last word representations, and the concatenation of the last query representation before and after the cloze token.\nThe probability of a candidate being the answer to the question is given by a linear combination of the single word probabilities.\n\nThe proposed model is tested on 4 different dataset. \nThe authors shown that the proposed model works well (state-of-the-art performance) for 3 out of 4 benchmarks.\n\n\n----------\n\nOVERALL JUDGMENT\nThe main contribution of the paper is the gated attention mechanism, that in my opinion, is a simple and interesting idea.\nThe paper is well thought, and the ablation study on the benefits given by the gated attention are convincing.\nThe GA reader as whole model outperforms previous state-of-the-art models on 3 benchmarks and seems very promising also on the CBT dataset.\nI would have liked to see some discussion on why the model works less well on the CBT dataset, though.\n\n\n----------\n\nDETAILED COMMENTS\n\nminor. In the introduction, Weston et al., 2014 do not use any attention mechanism."
  },
  {
    "people": [
      "Kyunghyun Cho's"
    ],
    "review": "Hi, for CNN/DailyMail, did you obtain the data via DeepMind's extraction script, or did you use Kyunghyun Cho's preprocessed data?"
  },
  {
    "people": [
      "Weston"
    ],
    "review": "SUMMARY.\n\nThe paper proposes a machine reading approach for cloze-style question answering.\nThe proposed system first encodes the query and the document using a bidirectional gru. These two representations are combined together using a Gated Attention (GA).\nGA calculates the compatibility of each word in the document and the query as a probability distribution.\nFor each word in the document a gate is calculated weighting the query representation according to the word compatibility.\nUltimately, the gate is applied to the gru-encoded document word.\nThe resulting word vectors are re-encoded with a bidirectional GRU.\nThis process is performed for multiple hops. After k hops, the probability of a word to be part of the answer is calculated by a log-linear model that take as input the last word representations, and the concatenation of the last query representation before and after the cloze token.\nThe probability of a candidate being the answer to the question is given by a linear combination of the single word probabilities.\n\nThe proposed model is tested on 4 different dataset. \nThe authors shown that the proposed model works well (state-of-the-art performance) for 3 out of 4 benchmarks.\n\n\n----------\n\nOVERALL JUDGMENT\nThe main contribution of the paper is the gated attention mechanism, that in my opinion, is a simple and interesting idea.\nThe paper is well thought, and the ablation study on the benefits given by the gated attention are convincing.\nThe GA reader as whole model outperforms previous state-of-the-art models on 3 benchmarks and seems very promising also on the CBT dataset.\nI would have liked to see some discussion on why the model works less well on the CBT dataset, though.\n\n\n----------\n\nDETAILED COMMENTS\n\nminor. In the introduction, Weston et al., 2014 do not use any attention mechanism."
  },
  {
    "people": [
      "Cloze"
    ],
    "review": "This paper presents an interesting idea for iteratively re-weighting the word representations in a document (hence the GRU-coded doc representation as well) with a simple multiplication operation. As the authors correctly pointed out, such an operation serves as a \"filter\" to reduce the attentions to less relevant parts in the document, hence leading to better performance of the modeling.\n\nThe results are or close to the state-of-the-art for a few Cloze-style QA tasks. This paper would deserve an even higher score, if the following limitations could be addressed better:\n\n1. While interesting and conceptually simple (though with significant increased computational overheads), the architecture proposed in the paper is for a very specific task. \n\n2. The improvement of the main idea of this paper (gated attention) is less significant, comparing GA Reader-- vs. GA Reader, while the latter includes a number of engineering tricks such as adding character embedding and using a word embedding trained from larger corpus (GloVe), as well as some small improvements on the modeling by using token-specific attention in (5).\n\n3. I also wish the authors can shed more lights on what a role the K (number of hops) plays, both intuitively and empirically. I feel more insights could be obtained if we do more deeper analysis of K's impacts to different types of questions for example."
  },
  {
    "people": [
      "Weston"
    ],
    "review": "SUMMARY.\n\nThe paper proposes a machine reading approach for cloze-style question answering.\nThe proposed system first encodes the query and the document using a bidirectional gru. These two representations are combined together using a Gated Attention (GA).\nGA calculates the compatibility of each word in the document and the query as a probability distribution.\nFor each word in the document a gate is calculated weighting the query representation according to the word compatibility.\nUltimately, the gate is applied to the gru-encoded document word.\nThe resulting word vectors are re-encoded with a bidirectional GRU.\nThis process is performed for multiple hops. After k hops, the probability of a word to be part of the answer is calculated by a log-linear model that take as input the last word representations, and the concatenation of the last query representation before and after the cloze token.\nThe probability of a candidate being the answer to the question is given by a linear combination of the single word probabilities.\n\nThe proposed model is tested on 4 different dataset. \nThe authors shown that the proposed model works well (state-of-the-art performance) for 3 out of 4 benchmarks.\n\n\n----------\n\nOVERALL JUDGMENT\nThe main contribution of the paper is the gated attention mechanism, that in my opinion, is a simple and interesting idea.\nThe paper is well thought, and the ablation study on the benefits given by the gated attention are convincing.\nThe GA reader as whole model outperforms previous state-of-the-art models on 3 benchmarks and seems very promising also on the CBT dataset.\nI would have liked to see some discussion on why the model works less well on the CBT dataset, though.\n\n\n----------\n\nDETAILED COMMENTS\n\nminor. In the introduction, Weston et al., 2014 do not use any attention mechanism."
  },
  {
    "people": [
      "Kyunghyun Cho's"
    ],
    "review": "Hi, for CNN/DailyMail, did you obtain the data via DeepMind's extraction script, or did you use Kyunghyun Cho's preprocessed data?"
  },
  {
    "people": [
      "Ian",
      "Ian"
    ],
    "review": "This paper is a well written paper. This paper can be divided into 2 parts:\n1.Adversary training on ImageNet \n2.Empirical study of label leak, single/multiple step attack, transferability and importance of model capacity\n\nFor part [1], I don\u2019t think training without clean example will not make reasonable ImageNet level model. Ian\u2019s experiment in \u201cExplaining and Harnessing Adversarial Examples\u201d didn't use BatchNorm, which may be important for training large scale model. This part looks like an extension to Ian\u2019s work with Inception-V3 model. I suggest to add an experiment of training without clean samples.\n\nFor part [2], The experiments cover most variables in adversary training, yet lack technical depth.  The depth, model capacity experiments can be explained by regularizer effect of adv training;  Label leaking is novel; In transferability experiment with FGSM, if we do careful observe on some special MNIST FGSM example, we can find augmentation effect on numbers, which makes grey part on image to make the number look more like the other numbers. Although this effect is hard to be observed with complex data such as CIFAR-10 or ImageNet, they may be related to the authors' observation \"FGSM examples are most transferable\".  \n\nIn this part the authors raise many interesting problems or guess, but lack theoretical explanations. \n\nOverall I think these empirical observations are useful for future work."
  },
  {
    "people": [
      "Ian",
      "Ian"
    ],
    "review": "This paper is a well written paper. This paper can be divided into 2 parts:\n1.Adversary training on ImageNet \n2.Empirical study of label leak, single/multiple step attack, transferability and importance of model capacity\n\nFor part [1], I don\u2019t think training without clean example will not make reasonable ImageNet level model. Ian\u2019s experiment in \u201cExplaining and Harnessing Adversarial Examples\u201d didn't use BatchNorm, which may be important for training large scale model. This part looks like an extension to Ian\u2019s work with Inception-V3 model. I suggest to add an experiment of training without clean samples.\n\nFor part [2], The experiments cover most variables in adversary training, yet lack technical depth.  The depth, model capacity experiments can be explained by regularizer effect of adv training;  Label leaking is novel; In transferability experiment with FGSM, if we do careful observe on some special MNIST FGSM example, we can find augmentation effect on numbers, which makes grey part on image to make the number look more like the other numbers. Although this effect is hard to be observed with complex data such as CIFAR-10 or ImageNet, they may be related to the authors' observation \"FGSM examples are most transferable\".  \n\nIn this part the authors raise many interesting problems or guess, but lack theoretical explanations. \n\nOverall I think these empirical observations are useful for future work. \n\n"
  },
  {
    "people": [
      "Ian",
      "Ian"
    ],
    "review": "This paper is a well written paper. This paper can be divided into 2 parts:\n1.Adversary training on ImageNet \n2.Empirical study of label leak, single/multiple step attack, transferability and importance of model capacity\n\nFor part [1], I don\u2019t think training without clean example will not make reasonable ImageNet level model. Ian\u2019s experiment in \u201cExplaining and Harnessing Adversarial Examples\u201d didn't use BatchNorm, which may be important for training large scale model. This part looks like an extension to Ian\u2019s work with Inception-V3 model. I suggest to add an experiment of training without clean samples.\n\nFor part [2], The experiments cover most variables in adversary training, yet lack technical depth.  The depth, model capacity experiments can be explained by regularizer effect of adv training;  Label leaking is novel; In transferability experiment with FGSM, if we do careful observe on some special MNIST FGSM example, we can find augmentation effect on numbers, which makes grey part on image to make the number look more like the other numbers. Although this effect is hard to be observed with complex data such as CIFAR-10 or ImageNet, they may be related to the authors' observation \"FGSM examples are most transferable\".  \n\nIn this part the authors raise many interesting problems or guess, but lack theoretical explanations. \n\nOverall I think these empirical observations are useful for future work."
  },
  {
    "people": [
      "Ian",
      "Ian"
    ],
    "review": "This paper is a well written paper. This paper can be divided into 2 parts:\n1.Adversary training on ImageNet \n2.Empirical study of label leak, single/multiple step attack, transferability and importance of model capacity\n\nFor part [1], I don\u2019t think training without clean example will not make reasonable ImageNet level model. Ian\u2019s experiment in \u201cExplaining and Harnessing Adversarial Examples\u201d didn't use BatchNorm, which may be important for training large scale model. This part looks like an extension to Ian\u2019s work with Inception-V3 model. I suggest to add an experiment of training without clean samples.\n\nFor part [2], The experiments cover most variables in adversary training, yet lack technical depth.  The depth, model capacity experiments can be explained by regularizer effect of adv training;  Label leaking is novel; In transferability experiment with FGSM, if we do careful observe on some special MNIST FGSM example, we can find augmentation effect on numbers, which makes grey part on image to make the number look more like the other numbers. Although this effect is hard to be observed with complex data such as CIFAR-10 or ImageNet, they may be related to the authors' observation \"FGSM examples are most transferable\".  \n\nIn this part the authors raise many interesting problems or guess, but lack theoretical explanations. \n\nOverall I think these empirical observations are useful for future work. \n\n"
  },
  {
    "people": [
      "Kershenbaum",
      "van Gael",
      "Wang",
      "Paisley",
      "Blei",
      "Miller",
      "Harrison",
      "Johnson",
      "Duvenaud",
      "Wiltschko",
      "Datta",
      "Adams"
    ],
    "review": "The paper is a novel application for the sticky HDP-HMM, focused on correctly identifying the number of components in bird and whale song across a variety of datasets. It's nice to see the model applied to an interesting dataset. My main issues with the paper have to do with structure and the choice of representation used in the model. Namely:\n\nThe organization of the paper could be significantly improved. There is a lot of repetitive introduction that adds little to the paper. The first and last two sentences of the abstract could be cut. Many other parts of the abstract basically repeat the introduction. The second paragraph of section 2.3 also repeats your introduction - by now we know what you're doing. I think most people reading this will have no idea what Kershenbaum (2014) is. The description of the data should go in the experiments section. \"Different hypotheses for the songs were emitted\" in the introduction is odd phrasing. Figure 4 should be the first figure and go in the introduction. Figure 5 should be in the methods section. A summary of Table 1 should be in the experiments section. Generally the writing could be tightened quite a bit, which would make space for these figures. The description of the HDP-HMM, which mostly follows the existing literature, is well done.\n\nSome general questions about the methods used:\n\nIf you're interested in scalable inference, why use Gibbs sampling? Why not the beam sampler (van Gael 2008), which at least recently was the state of the art for MCMC inference in the HDP-HMM? More generally, why use MCMC at all? For very large datasets, most of the Bayesian ML community has converged on stochastic variational inference as the most practical method (eg Wang, Paisley and Blei 2011).\n\nIf your interest is mainly in the number of clusters, how would you address the fact that DP mixture models are known not to be consistent for estimating the true number of clusters (Miller and Harrison 2013)?\n\nMFCC features are calibrated to the human auditory system, not bird or whale auditory systems. In your data, do you calibrate the MFCC scale to be closer to the auditory systems of the animals that generated the song?\n\nAnd a final suggestion for future work, which could use the results presented here as a baseline:\n\nGiven the success of LSTMs in speech recognition in recent years, it may be the case that deep learned representations are superior to linear features (like the means of each cluster in an HDP-HMM) for animal song as well. Have you considered a hybrid model, similar to recent work combining autoencoders and graphical models (Johnson, Duvenaud, Wiltschko, Datta and Adams 2016)?"
  },
  {
    "people": [
      "Kershenbaum",
      "van Gael",
      "Wang",
      "Paisley",
      "Blei",
      "Miller",
      "Harrison",
      "Johnson",
      "Duvenaud",
      "Wiltschko",
      "Datta",
      "Adams"
    ],
    "review": "The paper is a novel application for the sticky HDP-HMM, focused on correctly identifying the number of components in bird and whale song across a variety of datasets. It's nice to see the model applied to an interesting dataset. My main issues with the paper have to do with structure and the choice of representation used in the model. Namely:\n\nThe organization of the paper could be significantly improved. There is a lot of repetitive introduction that adds little to the paper. The first and last two sentences of the abstract could be cut. Many other parts of the abstract basically repeat the introduction. The second paragraph of section 2.3 also repeats your introduction - by now we know what you're doing. I think most people reading this will have no idea what Kershenbaum (2014) is. The description of the data should go in the experiments section. \"Different hypotheses for the songs were emitted\" in the introduction is odd phrasing. Figure 4 should be the first figure and go in the introduction. Figure 5 should be in the methods section. A summary of Table 1 should be in the experiments section. Generally the writing could be tightened quite a bit, which would make space for these figures. The description of the HDP-HMM, which mostly follows the existing literature, is well done.\n\nSome general questions about the methods used:\n\nIf you're interested in scalable inference, why use Gibbs sampling? Why not the beam sampler (van Gael 2008), which at least recently was the state of the art for MCMC inference in the HDP-HMM? More generally, why use MCMC at all? For very large datasets, most of the Bayesian ML community has converged on stochastic variational inference as the most practical method (eg Wang, Paisley and Blei 2011).\n\nIf your interest is mainly in the number of clusters, how would you address the fact that DP mixture models are known not to be consistent for estimating the true number of clusters (Miller and Harrison 2013)?\n\nMFCC features are calibrated to the human auditory system, not bird or whale auditory systems. In your data, do you calibrate the MFCC scale to be closer to the auditory systems of the animals that generated the song?\n\nAnd a final suggestion for future work, which could use the results presented here as a baseline:\n\nGiven the success of LSTMs in speech recognition in recent years, it may be the case that deep learned representations are superior to linear features (like the means of each cluster in an HDP-HMM) for animal song as well. Have you considered a hybrid model, similar to recent work combining autoencoders and graphical models (Johnson, Duvenaud, Wiltschko, Datta and Adams 2016)?"
  },
  {
    "people": [
      "Kershenbaum",
      "van Gael",
      "Wang",
      "Paisley",
      "Blei",
      "Miller",
      "Harrison",
      "Johnson",
      "Duvenaud",
      "Wiltschko",
      "Datta",
      "Adams"
    ],
    "review": "The paper is a novel application for the sticky HDP-HMM, focused on correctly identifying the number of components in bird and whale song across a variety of datasets. It's nice to see the model applied to an interesting dataset. My main issues with the paper have to do with structure and the choice of representation used in the model. Namely:\n\nThe organization of the paper could be significantly improved. There is a lot of repetitive introduction that adds little to the paper. The first and last two sentences of the abstract could be cut. Many other parts of the abstract basically repeat the introduction. The second paragraph of section 2.3 also repeats your introduction - by now we know what you're doing. I think most people reading this will have no idea what Kershenbaum (2014) is. The description of the data should go in the experiments section. \"Different hypotheses for the songs were emitted\" in the introduction is odd phrasing. Figure 4 should be the first figure and go in the introduction. Figure 5 should be in the methods section. A summary of Table 1 should be in the experiments section. Generally the writing could be tightened quite a bit, which would make space for these figures. The description of the HDP-HMM, which mostly follows the existing literature, is well done.\n\nSome general questions about the methods used:\n\nIf you're interested in scalable inference, why use Gibbs sampling? Why not the beam sampler (van Gael 2008), which at least recently was the state of the art for MCMC inference in the HDP-HMM? More generally, why use MCMC at all? For very large datasets, most of the Bayesian ML community has converged on stochastic variational inference as the most practical method (eg Wang, Paisley and Blei 2011).\n\nIf your interest is mainly in the number of clusters, how would you address the fact that DP mixture models are known not to be consistent for estimating the true number of clusters (Miller and Harrison 2013)?\n\nMFCC features are calibrated to the human auditory system, not bird or whale auditory systems. In your data, do you calibrate the MFCC scale to be closer to the auditory systems of the animals that generated the song?\n\nAnd a final suggestion for future work, which could use the results presented here as a baseline:\n\nGiven the success of LSTMs in speech recognition in recent years, it may be the case that deep learned representations are superior to linear features (like the means of each cluster in an HDP-HMM) for animal song as well. Have you considered a hybrid model, similar to recent work combining autoencoders and graphical models (Johnson, Duvenaud, Wiltschko, Datta and Adams 2016)?"
  },
  {
    "people": [
      "Kershenbaum",
      "van Gael",
      "Wang",
      "Paisley",
      "Blei",
      "Miller",
      "Harrison",
      "Johnson",
      "Duvenaud",
      "Wiltschko",
      "Datta",
      "Adams"
    ],
    "review": "The paper is a novel application for the sticky HDP-HMM, focused on correctly identifying the number of components in bird and whale song across a variety of datasets. It's nice to see the model applied to an interesting dataset. My main issues with the paper have to do with structure and the choice of representation used in the model. Namely:\n\nThe organization of the paper could be significantly improved. There is a lot of repetitive introduction that adds little to the paper. The first and last two sentences of the abstract could be cut. Many other parts of the abstract basically repeat the introduction. The second paragraph of section 2.3 also repeats your introduction - by now we know what you're doing. I think most people reading this will have no idea what Kershenbaum (2014) is. The description of the data should go in the experiments section. \"Different hypotheses for the songs were emitted\" in the introduction is odd phrasing. Figure 4 should be the first figure and go in the introduction. Figure 5 should be in the methods section. A summary of Table 1 should be in the experiments section. Generally the writing could be tightened quite a bit, which would make space for these figures. The description of the HDP-HMM, which mostly follows the existing literature, is well done.\n\nSome general questions about the methods used:\n\nIf you're interested in scalable inference, why use Gibbs sampling? Why not the beam sampler (van Gael 2008), which at least recently was the state of the art for MCMC inference in the HDP-HMM? More generally, why use MCMC at all? For very large datasets, most of the Bayesian ML community has converged on stochastic variational inference as the most practical method (eg Wang, Paisley and Blei 2011).\n\nIf your interest is mainly in the number of clusters, how would you address the fact that DP mixture models are known not to be consistent for estimating the true number of clusters (Miller and Harrison 2013)?\n\nMFCC features are calibrated to the human auditory system, not bird or whale auditory systems. In your data, do you calibrate the MFCC scale to be closer to the auditory systems of the animals that generated the song?\n\nAnd a final suggestion for future work, which could use the results presented here as a baseline:\n\nGiven the success of LSTMs in speech recognition in recent years, it may be the case that deep learned representations are superior to linear features (like the means of each cluster in an HDP-HMM) for animal song as well. Have you considered a hybrid model, similar to recent work combining autoencoders and graphical models (Johnson, Duvenaud, Wiltschko, Datta and Adams 2016)?"
  },
  {
    "people": [
      "Dempster-Shaffer"
    ],
    "review": "This work proposes to use basic probability assignment to improve deep transfer learning. A particular re-weighting scheme inspired by Dempster-Shaffer and exploiting the confusion matrix of the source task is introduced. The authors also suggest learning the convolutional filters separately to break non-convexity. \n\nThe main problem with this paper is the writing. There are many typos, and the presentation is not clear. For example, the way the training set for weak classifiers are constructed remains unclear to me despite the author's previous answer. I do not buy the explanation about the use of both training and validation sets to compute BPA. Also, I am not convinced non-convexity is a problem here and the author does not provide an ablation study to validate the necessity of separately learning the filters. One last question is CIFAR has three channels and MNIST only one: How it this handled when pairing the datasets in the second set of experiments?\n\nOverall, I believe the proposed idea of reweighing is interesting, but the work can be globally improved/clarified. \n\nI suggest a reject. "
  },
  {
    "people": [
      "Dempster-Shaffer"
    ],
    "review": "This work proposes to use basic probability assignment to improve deep transfer learning. A particular re-weighting scheme inspired by Dempster-Shaffer and exploiting the confusion matrix of the source task is introduced. The authors also suggest learning the convolutional filters separately to break non-convexity. \n\nThe main problem with this paper is the writing. There are many typos, and the presentation is not clear. For example, the way the training set for weak classifiers are constructed remains unclear to me despite the author's previous answer. I do not buy the explanation about the use of both training and validation sets to compute BPA. Also, I am not convinced non-convexity is a problem here and the author does not provide an ablation study to validate the necessity of separately learning the filters. One last question is CIFAR has three channels and MNIST only one: How it this handled when pairing the datasets in the second set of experiments?\n\nOverall, I believe the proposed idea of reweighing is interesting, but the work can be globally improved/clarified. \n\nI suggest a reject. "
  },
  {
    "people": [
      "Kingma",
      "Kingma",
      "Kingma",
      "Kingma",
      "Kingma"
    ],
    "review": "This paper introduces a variant of the semi-supervised variational auto-encoder (VAE) framework. The authors present a way of introducing structure (observed variables) inside the recognition network.\n\nI find that the presentation of the inference with auxiliary variables could be avoided, as it actually makes the presentation unnecessarily complicated. Specifically, the expressions with auxiliary variables are helpful for devising a unified implementation, but modeling-wise one can get the same model without these auxiliary variables and recover a minimal extension of VAE where part of the generating space is actually observed. The observed variables mean that the posterior needs to also condition on those, so as to incorporate the information they convey. The way this is done in this paper is actually not very different from Kingma et al. 2014, and I am surprised that the experiments show a large deviation in these two methods' results. Given the similarity of the models, it'd be useful if the authors could give a possible explanation on the superiority of their method compared to Kingma et al. 2014. By the way, I was wondering if the experimental setup is the same as in Kingma et al. 2014 for the results of Fig. 5 (bottom) - the authors mention that they use CNNs for feature extraction but from the paper it's not clear if Kingma et al. do the same. \n\nOn a related note, I was wondering the same for the comparison with Jampani et al. 2015. In particular, is that model also using the same rate of supervision for a fair comparison?\n\nThe experiment in section 4.3 is interesting and demonstrates a useful property of the approach.\n\nThe discussion of the supervision rate (and the pre-review answer) is helpful in giving some insight about what is a successful training protocol to use in semi-supervised learning.\n\nOverall, the paper is interesting but the title and introduction made me expect something more from it. From the title I expected a method for interpreting general deep generative models, instead the described approach was about a semi-supervised variant of VAE - naturally including labelled examples disentangles the latent space, but this is a general property of any semi-supervised probabilistic model and not unique to the approach described here. Moreover, from the intro I expected to see a more general approximation scheme for the variational posterior (similar to Ranganath et al. 2015  which trully allows very flexible distributions), however this is not the case here.\n\nGiven the above, the contributions of this paper are in defining a slight variant of the semi-supervised VAE, and (perhaps more importantly) formulating it in a way that is amendable to easier automation in terms of software. But methodologically there is not much contribution to the current literature. The authors mention that they plan to extend the framework in the probabilistic programming setting. It seems indeed that this would be a very promising and useful extension. \n\nMinor note: three of Kingma's papers are all cited in the main text as Kingma et al. 2014, causing confusion. I suggest using Kingma et al. 2014a etc."
  },
  {
    "people": [
      "Diederik P Kingma",
      "Shakir Mohamed",
      "Danilo Jimenez Rezende",
      "Max Welling"
    ],
    "review": "We thank the reviewers for their comments and suggestions and are encouraged by\nthe positive feedback regarding its value, interest, relevance, and clarity.\n\nIn this top-level response, we address the two central issues raised by the\nreviewers.\n\n- The title and the semantics of 'structure'\n\n  The reviewers found the title somewhat confusing and perhaps overly general in\n  addition to potential confusion due the many meanings of the term 'structure'.\n  To ameliorate this issue, we have done the following:\n\n  a. Title change\n\n     We changed the title to\n     \t``Learning Disentangled Representations in Deep Generative Models''\n     to better reflect the contributions of the submission.\n\n  b. Clarification for 'structure'\n\n     We realise that the term 'structure' can have multiple meanings, even\n     within the confines of graphical and generative models.\n\n     Our use of the term is intended to refer to the (arbitrary) dependencies\n     one would like to employ in the recognition model, particularly in regard\n     to there being consistent 'interpretable' semantics of what the variables\n     in the model represent.\n\n     We have updated the abstract and the introduction (Section 1, end) in the\n     manuscript making this clarification and removing extraneous instances.\n\n- Relation to Kingma.et.al 2014 [1]\n\n  We would like to note here that we do not simply reformulate the\n  semi-supervised work in [1]. Our formulation is not a trivial extension of\n  [1], rather, it allows us to extend semi-supervised learning to a broader\n  class of latent-variable models.\n  We list below the important distinctions:\n\n  a. Continuous-domain semi-supervision\n\n     We can handle partial labels for continuous random variables, not just\n     discrete ones. In this case, the factorisation in Eqn 4 corresponds to a\n     *regressor* instead of a classifier. The work in [1] requires\n     marginalization over the partially-observed variable\u2019s support in the\n     unsupervised case, which means that latent variables must in practice be\n     discrete.\n\n     Indeed we make use of continuous latent variables in the Faces experiment\n     (Section 4.2), where the lighting is a partially supervised 3-dimensional\n     Gaussian random variable.\n\n  b. Scaling the classifier term\n\n     The formulation in Eqn 4 naturally incorporates the classifier term, as\n     opposed to a separate fixed hyper-parameter (alpha, in Eqn 9 in [1]) that\n     controls the contribution of the classifier term in the objective. \n\n     This makes the formulation more flexible and general purpose for different\n     factorisations of the variational approximations used.\n\n  c. Framework for  implementation of models\n\n     As pointed out by the reviewers, our formulation allows for easy automated\n     implementation of a wide variety of models. This is in the same spirit as a\n     number of approaches such as Automatic Differentiation (AD) and\n     Probabilistic Program inference, where the choice of a particular means of\n     representation enables ease of automation for a great variety of different\n     cases.\n\n     The ease with which one can describe even minimally complex models, such as\n     that employed for the Multi-MNIST experiments, is shown in the Appendix.\n\n  d. Semi-supervised learning on varying subsets of variables\n\n     A particular benefit of the automation mentioned above, is that we derive\n     the ability to partially supervise any subset of variables, regardless of\n     type. Indeed we even have the ability to supervise *different* latent\n     variables for different data points by virtue of our formulation\n     automatically factorising into labelled and unlabelled terms, on a\n     per-data-point (or minibatch) basis. This is a particularly desirable\n     characteristic to have in dealing with missing-data issues often\n     encountered in large datasets.\n\nWe have updated the Framework and Formulation section (Section 3, after Eqn 4)\nin the manuscript with a description of these differences.\n\nWe address the remainder of the comments by the reviewers in the corresponding\nresponses to the reviews.\n\n[1] Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling.\n    Semi-supervised learning with deep generative models. In Advances in Neural\n    Information Processing Systems, pp. 3581\u20133589, 2014\n"
  },
  {
    "people": [
      "Kingma"
    ],
    "review": "This paper investigates deep generative models with multiple stochastic nodes and gives them meaning by semi-supervision. From a methodological point of view, there is nothing fundamentally novel (it is very similar to the semi-supervised work of Kingma et al; although this work has sometimes more than two latent nodes, it is not a complex extension). There is a fairly classical auxiliary variable trick used to make sure the inference network for y is trained over all data points (by supposing y is in fact is a latent variable with an observation \\tilde y; the observation is y if y is observed, or uninformative for unobserved y). Alternatively, one can separate the inference used to learn the generative model (which throws out inference over y if it is observed), from an inference used to 'exercise' the model (approximate the complex p(y|x) in the model by a simpler q(y|x) - effectively inferring the target p(y|x) for the data where only x is collected). Results are strong, although on simple datasets. Overall this is a well written, interesting paper, but lacking in terms of methodological advances.\n\nMinor:\n- I feel the title is a bit too general for the content of the paper. I personally don't agree with the strong contrast made between deep generative models and graphical models (deep generative models are graphical models, but they are more typically learned and un-interpretable than classical graphical models; and having multiple stochastic variables is not exclusive to graphical models, see DRAW, Deep Kalman Filter, Recurrent VAE, etc.). The word 'structure' is a bit problematic; here, the paper seems more concerned with disentangling and semanticizing the latent representation of a generative model by supervision. It is debatable whether the models themselves have structure."
  },
  {
    "people": [
      "Kingma"
    ],
    "review": "This paper proposed a variant of the semi-supervised VAE model which leads to a unified objective for supervised and unsupervised VAE.  This variant gives software implementation of these VAE models more flexibility in specifying which variables are supervised and which are not.\n\nThis development introduces a few extra terms compared to the original semi-supervised VAE formulation proposed by Kingma et al., 2014.  From the experiment results it seems that these terms do not do much as the new formulation and the performance difference between the proposed method and Kingma et al. 2014 are not very significant (Figure 5).  Therefore the benefit of the new formulation is likely to be just software engineering flexibility and convenience.\n\nThis flexibility and convenience is nice to have, but it is better to demonstrate a few situations where the proposed method can be applied while for other previous methods it is non-trivial to do.\n\nThe paper's title and the way it is written make me expect a lot more than what is currently in the paper.  I was expecting to see, for example, structured hidden variable model for the posterior (page 4, top), or really \"structured interpretation\" of the generative model (title), but I didn't see any of these.  The main contribution of this paper (a variant of the semi-supervised VAE model) is quite far from these.\n\nAside from these, the plug-in estimation for discrete variables only works when the function h(x,y) is a continuous function of y.  If however, h(x, y) is not continuous in y, for example h takes one form when y=1 and another form when y=2, then the approach of using Expectation[y] to replace y will not work.  Therefore the \"plug-in\" estimation has its limitations.\n"
  },
  {
    "people": [
      "Kingma",
      "Kingma",
      "Kingma",
      "Kingma",
      "Kingma"
    ],
    "review": "This paper introduces a variant of the semi-supervised variational auto-encoder (VAE) framework. The authors present a way of introducing structure (observed variables) inside the recognition network.\n\nI find that the presentation of the inference with auxiliary variables could be avoided, as it actually makes the presentation unnecessarily complicated. Specifically, the expressions with auxiliary variables are helpful for devising a unified implementation, but modeling-wise one can get the same model without these auxiliary variables and recover a minimal extension of VAE where part of the generating space is actually observed. The observed variables mean that the posterior needs to also condition on those, so as to incorporate the information they convey. The way this is done in this paper is actually not very different from Kingma et al. 2014, and I am surprised that the experiments show a large deviation in these two methods' results. Given the similarity of the models, it'd be useful if the authors could give a possible explanation on the superiority of their method compared to Kingma et al. 2014. By the way, I was wondering if the experimental setup is the same as in Kingma et al. 2014 for the results of Fig. 5 (bottom) - the authors mention that they use CNNs for feature extraction but from the paper it's not clear if Kingma et al. do the same. \n\nOn a related note, I was wondering the same for the comparison with Jampani et al. 2015. In particular, is that model also using the same rate of supervision for a fair comparison?\n\nThe experiment in section 4.3 is interesting and demonstrates a useful property of the approach.\n\nThe discussion of the supervision rate (and the pre-review answer) is helpful in giving some insight about what is a successful training protocol to use in semi-supervised learning.\n\nOverall, the paper is interesting but the title and introduction made me expect something more from it. From the title I expected a method for interpreting general deep generative models, instead the described approach was about a semi-supervised variant of VAE - naturally including labelled examples disentangles the latent space, but this is a general property of any semi-supervised probabilistic model and not unique to the approach described here. Moreover, from the intro I expected to see a more general approximation scheme for the variational posterior (similar to Ranganath et al. 2015  which trully allows very flexible distributions), however this is not the case here.\n\nGiven the above, the contributions of this paper are in defining a slight variant of the semi-supervised VAE, and (perhaps more importantly) formulating it in a way that is amendable to easier automation in terms of software. But methodologically there is not much contribution to the current literature. The authors mention that they plan to extend the framework in the probabilistic programming setting. It seems indeed that this would be a very promising and useful extension. \n\nMinor note: three of Kingma's papers are all cited in the main text as Kingma et al. 2014, causing confusion. I suggest using Kingma et al. 2014a etc.\n"
  },
  {
    "people": [
      "Kingma",
      "Kingma",
      "Kingma",
      "Kingma",
      "Kingma"
    ],
    "review": "This paper introduces a variant of the semi-supervised variational auto-encoder (VAE) framework. The authors present a way of introducing structure (observed variables) inside the recognition network.\n\nI find that the presentation of the inference with auxiliary variables could be avoided, as it actually makes the presentation unnecessarily complicated. Specifically, the expressions with auxiliary variables are helpful for devising a unified implementation, but modeling-wise one can get the same model without these auxiliary variables and recover a minimal extension of VAE where part of the generating space is actually observed. The observed variables mean that the posterior needs to also condition on those, so as to incorporate the information they convey. The way this is done in this paper is actually not very different from Kingma et al. 2014, and I am surprised that the experiments show a large deviation in these two methods' results. Given the similarity of the models, it'd be useful if the authors could give a possible explanation on the superiority of their method compared to Kingma et al. 2014. By the way, I was wondering if the experimental setup is the same as in Kingma et al. 2014 for the results of Fig. 5 (bottom) - the authors mention that they use CNNs for feature extraction but from the paper it's not clear if Kingma et al. do the same. \n\nOn a related note, I was wondering the same for the comparison with Jampani et al. 2015. In particular, is that model also using the same rate of supervision for a fair comparison?\n\nThe experiment in section 4.3 is interesting and demonstrates a useful property of the approach.\n\nThe discussion of the supervision rate (and the pre-review answer) is helpful in giving some insight about what is a successful training protocol to use in semi-supervised learning.\n\nOverall, the paper is interesting but the title and introduction made me expect something more from it. From the title I expected a method for interpreting general deep generative models, instead the described approach was about a semi-supervised variant of VAE - naturally including labelled examples disentangles the latent space, but this is a general property of any semi-supervised probabilistic model and not unique to the approach described here. Moreover, from the intro I expected to see a more general approximation scheme for the variational posterior (similar to Ranganath et al. 2015  which trully allows very flexible distributions), however this is not the case here.\n\nGiven the above, the contributions of this paper are in defining a slight variant of the semi-supervised VAE, and (perhaps more importantly) formulating it in a way that is amendable to easier automation in terms of software. But methodologically there is not much contribution to the current literature. The authors mention that they plan to extend the framework in the probabilistic programming setting. It seems indeed that this would be a very promising and useful extension. \n\nMinor note: three of Kingma's papers are all cited in the main text as Kingma et al. 2014, causing confusion. I suggest using Kingma et al. 2014a etc."
  },
  {
    "people": [
      "Diederik P Kingma",
      "Shakir Mohamed",
      "Danilo Jimenez Rezende",
      "Max Welling"
    ],
    "review": "We thank the reviewers for their comments and suggestions and are encouraged by\nthe positive feedback regarding its value, interest, relevance, and clarity.\n\nIn this top-level response, we address the two central issues raised by the\nreviewers.\n\n- The title and the semantics of 'structure'\n\n  The reviewers found the title somewhat confusing and perhaps overly general in\n  addition to potential confusion due the many meanings of the term 'structure'.\n  To ameliorate this issue, we have done the following:\n\n  a. Title change\n\n     We changed the title to\n     \t``Learning Disentangled Representations in Deep Generative Models''\n     to better reflect the contributions of the submission.\n\n  b. Clarification for 'structure'\n\n     We realise that the term 'structure' can have multiple meanings, even\n     within the confines of graphical and generative models.\n\n     Our use of the term is intended to refer to the (arbitrary) dependencies\n     one would like to employ in the recognition model, particularly in regard\n     to there being consistent 'interpretable' semantics of what the variables\n     in the model represent.\n\n     We have updated the abstract and the introduction (Section 1, end) in the\n     manuscript making this clarification and removing extraneous instances.\n\n- Relation to Kingma.et.al 2014 [1]\n\n  We would like to note here that we do not simply reformulate the\n  semi-supervised work in [1]. Our formulation is not a trivial extension of\n  [1], rather, it allows us to extend semi-supervised learning to a broader\n  class of latent-variable models.\n  We list below the important distinctions:\n\n  a. Continuous-domain semi-supervision\n\n     We can handle partial labels for continuous random variables, not just\n     discrete ones. In this case, the factorisation in Eqn 4 corresponds to a\n     *regressor* instead of a classifier. The work in [1] requires\n     marginalization over the partially-observed variable\u2019s support in the\n     unsupervised case, which means that latent variables must in practice be\n     discrete.\n\n     Indeed we make use of continuous latent variables in the Faces experiment\n     (Section 4.2), where the lighting is a partially supervised 3-dimensional\n     Gaussian random variable.\n\n  b. Scaling the classifier term\n\n     The formulation in Eqn 4 naturally incorporates the classifier term, as\n     opposed to a separate fixed hyper-parameter (alpha, in Eqn 9 in [1]) that\n     controls the contribution of the classifier term in the objective. \n\n     This makes the formulation more flexible and general purpose for different\n     factorisations of the variational approximations used.\n\n  c. Framework for  implementation of models\n\n     As pointed out by the reviewers, our formulation allows for easy automated\n     implementation of a wide variety of models. This is in the same spirit as a\n     number of approaches such as Automatic Differentiation (AD) and\n     Probabilistic Program inference, where the choice of a particular means of\n     representation enables ease of automation for a great variety of different\n     cases.\n\n     The ease with which one can describe even minimally complex models, such as\n     that employed for the Multi-MNIST experiments, is shown in the Appendix.\n\n  d. Semi-supervised learning on varying subsets of variables\n\n     A particular benefit of the automation mentioned above, is that we derive\n     the ability to partially supervise any subset of variables, regardless of\n     type. Indeed we even have the ability to supervise *different* latent\n     variables for different data points by virtue of our formulation\n     automatically factorising into labelled and unlabelled terms, on a\n     per-data-point (or minibatch) basis. This is a particularly desirable\n     characteristic to have in dealing with missing-data issues often\n     encountered in large datasets.\n\nWe have updated the Framework and Formulation section (Section 3, after Eqn 4)\nin the manuscript with a description of these differences.\n\nWe address the remainder of the comments by the reviewers in the corresponding\nresponses to the reviews.\n\n[1] Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling.\n    Semi-supervised learning with deep generative models. In Advances in Neural\n    Information Processing Systems, pp. 3581\u20133589, 2014\n"
  },
  {
    "people": [
      "Kingma"
    ],
    "review": "This paper investigates deep generative models with multiple stochastic nodes and gives them meaning by semi-supervision. From a methodological point of view, there is nothing fundamentally novel (it is very similar to the semi-supervised work of Kingma et al; although this work has sometimes more than two latent nodes, it is not a complex extension). There is a fairly classical auxiliary variable trick used to make sure the inference network for y is trained over all data points (by supposing y is in fact is a latent variable with an observation \\tilde y; the observation is y if y is observed, or uninformative for unobserved y). Alternatively, one can separate the inference used to learn the generative model (which throws out inference over y if it is observed), from an inference used to 'exercise' the model (approximate the complex p(y|x) in the model by a simpler q(y|x) - effectively inferring the target p(y|x) for the data where only x is collected). Results are strong, although on simple datasets. Overall this is a well written, interesting paper, but lacking in terms of methodological advances.\n\nMinor:\n- I feel the title is a bit too general for the content of the paper. I personally don't agree with the strong contrast made between deep generative models and graphical models (deep generative models are graphical models, but they are more typically learned and un-interpretable than classical graphical models; and having multiple stochastic variables is not exclusive to graphical models, see DRAW, Deep Kalman Filter, Recurrent VAE, etc.). The word 'structure' is a bit problematic; here, the paper seems more concerned with disentangling and semanticizing the latent representation of a generative model by supervision. It is debatable whether the models themselves have structure."
  },
  {
    "people": [
      "Kingma"
    ],
    "review": "This paper proposed a variant of the semi-supervised VAE model which leads to a unified objective for supervised and unsupervised VAE.  This variant gives software implementation of these VAE models more flexibility in specifying which variables are supervised and which are not.\n\nThis development introduces a few extra terms compared to the original semi-supervised VAE formulation proposed by Kingma et al., 2014.  From the experiment results it seems that these terms do not do much as the new formulation and the performance difference between the proposed method and Kingma et al. 2014 are not very significant (Figure 5).  Therefore the benefit of the new formulation is likely to be just software engineering flexibility and convenience.\n\nThis flexibility and convenience is nice to have, but it is better to demonstrate a few situations where the proposed method can be applied while for other previous methods it is non-trivial to do.\n\nThe paper's title and the way it is written make me expect a lot more than what is currently in the paper.  I was expecting to see, for example, structured hidden variable model for the posterior (page 4, top), or really \"structured interpretation\" of the generative model (title), but I didn't see any of these.  The main contribution of this paper (a variant of the semi-supervised VAE model) is quite far from these.\n\nAside from these, the plug-in estimation for discrete variables only works when the function h(x,y) is a continuous function of y.  If however, h(x, y) is not continuous in y, for example h takes one form when y=1 and another form when y=2, then the approach of using Expectation[y] to replace y will not work.  Therefore the \"plug-in\" estimation has its limitations.\n"
  },
  {
    "people": [
      "Kingma",
      "Kingma",
      "Kingma",
      "Kingma",
      "Kingma"
    ],
    "review": "This paper introduces a variant of the semi-supervised variational auto-encoder (VAE) framework. The authors present a way of introducing structure (observed variables) inside the recognition network.\n\nI find that the presentation of the inference with auxiliary variables could be avoided, as it actually makes the presentation unnecessarily complicated. Specifically, the expressions with auxiliary variables are helpful for devising a unified implementation, but modeling-wise one can get the same model without these auxiliary variables and recover a minimal extension of VAE where part of the generating space is actually observed. The observed variables mean that the posterior needs to also condition on those, so as to incorporate the information they convey. The way this is done in this paper is actually not very different from Kingma et al. 2014, and I am surprised that the experiments show a large deviation in these two methods' results. Given the similarity of the models, it'd be useful if the authors could give a possible explanation on the superiority of their method compared to Kingma et al. 2014. By the way, I was wondering if the experimental setup is the same as in Kingma et al. 2014 for the results of Fig. 5 (bottom) - the authors mention that they use CNNs for feature extraction but from the paper it's not clear if Kingma et al. do the same. \n\nOn a related note, I was wondering the same for the comparison with Jampani et al. 2015. In particular, is that model also using the same rate of supervision for a fair comparison?\n\nThe experiment in section 4.3 is interesting and demonstrates a useful property of the approach.\n\nThe discussion of the supervision rate (and the pre-review answer) is helpful in giving some insight about what is a successful training protocol to use in semi-supervised learning.\n\nOverall, the paper is interesting but the title and introduction made me expect something more from it. From the title I expected a method for interpreting general deep generative models, instead the described approach was about a semi-supervised variant of VAE - naturally including labelled examples disentangles the latent space, but this is a general property of any semi-supervised probabilistic model and not unique to the approach described here. Moreover, from the intro I expected to see a more general approximation scheme for the variational posterior (similar to Ranganath et al. 2015  which trully allows very flexible distributions), however this is not the case here.\n\nGiven the above, the contributions of this paper are in defining a slight variant of the semi-supervised VAE, and (perhaps more importantly) formulating it in a way that is amendable to easier automation in terms of software. But methodologically there is not much contribution to the current literature. The authors mention that they plan to extend the framework in the probabilistic programming setting. It seems indeed that this would be a very promising and useful extension. \n\nMinor note: three of Kingma's papers are all cited in the main text as Kingma et al. 2014, causing confusion. I suggest using Kingma et al. 2014a etc.\n"
  },
  {
    "people": [
      "Graves",
      "Wayne",
      "Danihelka",
      "Zaremba",
      "Mikolov",
      "Joulin",
      "Fergus",
      "Andrychowicz",
      "Kurach",
      "Riedel",
      "Bo\u0161njak",
      "Rockt\u00e4schel",
      "Tian",
      "Zitnick",
      "Kurach",
      "Andrychowicz",
      "Sutskever"
    ],
    "review": "Thank you to all the reviewers for their time and comments. To summarize, the reviews say this is an interesting and well-written paper. The biggest issue raised in the reviews is that the scale of programs is said to be small (we disagree), and there are questions about how the approach will scale to more complicated programs (this is a big open problem, which--as discussed in Sec 7--we agree we don't solve, but we believe that DeepCoder lays a solid foundation upon which to scale up from.)\n\nWe would like to respond to these issues in a bit more detail:\n\n1. DeepCoder is solving problems that are significantly more complex than those tackled in machine learning research (e.g., as can be found in [1-6]), and it can significantly speed up strong methods from the programming languages community. DeepCoder scales up better than any method we are aware of for solving simple programming competition style problems from I/O examples.\n\n2. Inducing large programs from weak supervision (in this case input-output examples) is a major open problem in computer science. We don't claim to solve this problem, and it would be a major breakthrough if we did. There clearly need to be many significant research contributions to get there. DeepCoder develops two ideas that are generally useful: 1. learning bottom-up cues to guide search over program space, and 2. building a system that sits on top of and improves the strongest search-based techniques. The benefit is that as new search techniques are developed, the DeepCoder framework can be added on top, and it will likely provide significant additional improvements (we have now shown large improvements over 3 different search techniques: 2 from the original paper plus the \\lambda^2 system requested in reviewer comments).\n\n3. There is a question of whether there is enough information in input-output examples to induce large programs, and we agree that there is probably not enough information using strictly the formulation in this paper (see Discussion). However, it is straightforward to extend DeepCoder: First, by using natural language as an additional input to the encoder, we can learn to extract more information about what the program is expected to do, and how it is expected to do it. Second, generating data from a trained generative model of source code will cause the system to favor generating programs that are likely under the trained model, which will help with the ranking problem. Third, the definition of attributes can be expanded to be richer (e.g., ranking the different instructions per position in the program, or combinations of instructions), to enable the neural network component to solve more of the problem (and thus rely less on the search). Together, we believe these directions represent a clear path towards scaling up program synthesis even further by leveraging the key ideas in DeepCoder.\n\n\n[1] Graves, A., Wayne, G. and Danihelka, I., 2014. Neural turing machines. arXiv preprint arXiv:1410.5401.\n\n[2] Zaremba, W., Mikolov, T., Joulin, A. and Fergus, R., 2015. Learning simple algorithms from examples. arXiv preprint arXiv:1511.07275.\n\n[3] Andrychowicz, M. and Kurach, K., 2016. Learning Efficient Algorithms with Hierarchical Attentive Memory. arXiv preprint arXiv:1602.03218.\n\n[4] Riedel, S., Bo\u0161njak, M. and Rockt\u00e4schel, T., 2016. Programming with a Differentiable Forth Interpreter. arXiv preprint arXiv:1605.06640.\n\n[5] Gong, Q, Tian, Y, Zitnick, C. L., Unsupervised Program Induction with Hierarchical Generative Convolutional Neural Networks. ICLR 2017 Submission.\n\n[6] Kurach, K., Andrychowicz, M. and Sutskever, I., 2015. Neural random-access machines. arXiv preprint arXiv:1511.06392."
  },
  {
    "people": [
      "Ling",
      "Grefenstette"
    ],
    "review": "Just had a quick scan over this paper. It's very cool to see program induction models that provide interpretable programs rather than a set of weights. Looking forward to reading this in more depth.\n\nTwo relevant pieces of work you may wish to read/refer to here (full disclosure: they are from our group so this is a shameless plug):\n\n1) Program generation from text with efficient marginalisation over multiple forms of attention:\nLatent Predictor Networks for Code Generation, Ling et al, ACL 2016.\n\n2) Learning pushdown automata and other data structures, similar to the Joulin & Mikolov (NIPS 2015) paper you cite:\nLearning to transduce with unbounded memory, Grefenstette et al. (NIPS 2015)\n"
  },
  {
    "people": [
      "Graves",
      "Wayne",
      "Danihelka",
      "Zaremba",
      "Mikolov",
      "Joulin",
      "Fergus",
      "Andrychowicz",
      "Kurach",
      "Riedel",
      "Bo\u0161njak",
      "Rockt\u00e4schel",
      "Tian",
      "Zitnick",
      "Kurach",
      "Andrychowicz",
      "Sutskever"
    ],
    "review": "Thank you to all the reviewers for their time and comments. To summarize, the reviews say this is an interesting and well-written paper. The biggest issue raised in the reviews is that the scale of programs is said to be small (we disagree), and there are questions about how the approach will scale to more complicated programs (this is a big open problem, which--as discussed in Sec 7--we agree we don't solve, but we believe that DeepCoder lays a solid foundation upon which to scale up from.)\n\nWe would like to respond to these issues in a bit more detail:\n\n1. DeepCoder is solving problems that are significantly more complex than those tackled in machine learning research (e.g., as can be found in [1-6]), and it can significantly speed up strong methods from the programming languages community. DeepCoder scales up better than any method we are aware of for solving simple programming competition style problems from I/O examples.\n\n2. Inducing large programs from weak supervision (in this case input-output examples) is a major open problem in computer science. We don't claim to solve this problem, and it would be a major breakthrough if we did. There clearly need to be many significant research contributions to get there. DeepCoder develops two ideas that are generally useful: 1. learning bottom-up cues to guide search over program space, and 2. building a system that sits on top of and improves the strongest search-based techniques. The benefit is that as new search techniques are developed, the DeepCoder framework can be added on top, and it will likely provide significant additional improvements (we have now shown large improvements over 3 different search techniques: 2 from the original paper plus the \\lambda^2 system requested in reviewer comments).\n\n3. There is a question of whether there is enough information in input-output examples to induce large programs, and we agree that there is probably not enough information using strictly the formulation in this paper (see Discussion). However, it is straightforward to extend DeepCoder: First, by using natural language as an additional input to the encoder, we can learn to extract more information about what the program is expected to do, and how it is expected to do it. Second, generating data from a trained generative model of source code will cause the system to favor generating programs that are likely under the trained model, which will help with the ranking problem. Third, the definition of attributes can be expanded to be richer (e.g., ranking the different instructions per position in the program, or combinations of instructions), to enable the neural network component to solve more of the problem (and thus rely less on the search). Together, we believe these directions represent a clear path towards scaling up program synthesis even further by leveraging the key ideas in DeepCoder.\n\n\n[1] Graves, A., Wayne, G. and Danihelka, I., 2014. Neural turing machines. arXiv preprint arXiv:1410.5401.\n\n[2] Zaremba, W., Mikolov, T., Joulin, A. and Fergus, R., 2015. Learning simple algorithms from examples. arXiv preprint arXiv:1511.07275.\n\n[3] Andrychowicz, M. and Kurach, K., 2016. Learning Efficient Algorithms with Hierarchical Attentive Memory. arXiv preprint arXiv:1602.03218.\n\n[4] Riedel, S., Bo\u0161njak, M. and Rockt\u00e4schel, T., 2016. Programming with a Differentiable Forth Interpreter. arXiv preprint arXiv:1605.06640.\n\n[5] Gong, Q, Tian, Y, Zitnick, C. L., Unsupervised Program Induction with Hierarchical Generative Convolutional Neural Networks. ICLR 2017 Submission.\n\n[6] Kurach, K., Andrychowicz, M. and Sutskever, I., 2015. Neural random-access machines. arXiv preprint arXiv:1511.06392."
  },
  {
    "people": [
      "Ling",
      "Grefenstette"
    ],
    "review": "Just had a quick scan over this paper. It's very cool to see program induction models that provide interpretable programs rather than a set of weights. Looking forward to reading this in more depth.\n\nTwo relevant pieces of work you may wish to read/refer to here (full disclosure: they are from our group so this is a shameless plug):\n\n1) Program generation from text with efficient marginalisation over multiple forms of attention:\nLatent Predictor Networks for Code Generation, Ling et al, ACL 2016.\n\n2) Learning pushdown automata and other data structures, similar to the Joulin & Mikolov (NIPS 2015) paper you cite:\nLearning to transduce with unbounded memory, Grefenstette et al. (NIPS 2015)\n"
  },
  {
    "people": [
      "Mairal",
      "Ramirez",
      "Ignacio",
      "Guillermo Sapiro"
    ],
    "review": "I'd like to thank the authors for their detailed response and clarifications.\n\nThis work proposes new training scheme for online sparse dictionary learning. The model assumes a non-stationary flow of the incoming data. The goal (and the challenge) is to learn a model in an online manner in a way that is capable of  adjusting to the new incoming data without forgetting how to represent previously seen data. The proposed approach deals with this problem by incorporating a mechanism for adding or deleting atoms in the dictionary. This procedure is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus. \n\nThe paper has two main innovations over the baseline approach (Mairal et al): (i) \u201cneuronal birth\u201d which represents an adaptive way of increasing the number of atoms in the dictionary (ii) \"neuronal death\", which corresponds to removing \u201cuseless\u201d dictionary atoms.\n\nNeural death is implemented by including an group-sparsity regularization to the dictionary atoms themselves (the group corresponds to a column of the dictionary). This promotes to shrink to zero atoms that are not very useful, keeping controlled the increase of the dictionary size.\n\nI believe that the strong side of the paper is its connections with the adult neurogenesis phenomenon, which is, in my opinion a very nice feature.\nThe paper is very well written and easy to follow.\n\nOn the other hand, the overall technique is not very novel. Although not exactly equivalent, similar ideas have been explored. While the neural death is implemente elegantly with a sparsity-promoting regularization term, the neural birth is performed by relying on heuristics that measure how well the dictionary can represent new incoming data. Which depending on the \"level\" of non-stationarity in the incoming data (or presence of outliers) could be difficult to set. Still, having adaptive dictionary size is very interesting.\n\nThe authors could also cite some references in model selection literature. In particular, some ideas such as MDL have been used for automatically selecting the dictionary size (I believe this work does not address the online setting, but still its a relevant reference to have). For instance,\n\nRamirez, Ignacio, and Guillermo Sapiro. \"An MDL framework for sparse coding and dictionary learning.\" IEEE Transactions on Signal Processing 60.6 (2012): 2913-2927."
  },
  {
    "people": [
      "Bengio"
    ],
    "review": "We would like to thank all reviewers for their insightful comments that helped us to improve the paper; to address those comments, we modified the paper as follows:\n\n1.\tAdded a subsection 3.1 in section 3, describing various details of the proposed algorithm, as suggested by Reviewer1.\n\n2.\tAdded a section (Appendix, B9, Figures 21-26) demonstrating stability of our results (our method consistently outperforming the non-adaptive dictionary learning)  while varying all of the algorithm\u2019s parameters, one at a time, over a wide range of values.  \n\n3.\tAdded a section (Appendix, B7, Fig. 19) with new empirical results to address the concern of Reviewer3 regarding comparison with a version of the baseline method which involved re-initialization of ``dead\u2019\u2019 elements; we did not observe any significance difference in the performance of such augmented method versus the original baseline (the number of dead elements appearing in the baseline method was quite small); on the contrary, our method employing group-sparsity was removing larger number of ``weak\u2019\u2019 (low l2-norm) elements and yielded better performance due to such explicit regularization.\n\n4.\tAdded a section (Appendix, B8, Fig. 20) with new results evaluating our method under different ordering of the input data, to address the question of the Reviewer1 regarding the potential sensitivity of our results to the ordering of the training domains  (e.g., from easier to harder). No significant changes  were observed due to permutation of the input datasets (our method was still outperforming the baseline), i.e. buildings images were not necessarily \u2018simpler\u2019 than natural ones.\n\n5.\tA summary of our contributions is added at the end of the intro section. Our main point is that the contribution of this paper is truly novel and nontrivial. Indeed,\n\n    a.\tWe are the first to propose an online model-selection approach to dictionary learning (DL) based on dynamic addition and deletion of the elements (hidden units), which leads to significant performance improvements over the state-of-art online DL, especially in non-stationary settings. None of the reviewers have provided a reference that would contradict the above statement.\n\n    b.\tWhile some prior work (e.g., Bengio et al, NIPS 2009) involved deletion of dictionary elements, no addition was involved in that approach and, more importantly, their approach only concerned an offline dictionary learning, not online. Similarly, approaches to neural nets involving cascade correlations and other methods adding hidden units are lacking regularization (deletion) that we achieve via group sparsity constraint.    Finally, Reviewer3 mentions prior work on off-line MDL-based model selection in dictionary learning, but the major difference is, again, that our setting is online learning, while their approach is off-line.  \n\n    c.\tFurthermore, none of the prior approaches explored the interplay between addition and deletion, neither theoretically nor empirically, while we performed such evaluation and identified regimes when our online model-selection would outperform the non-adaptive baseline   (Section 5).\n\n\nThus, to the best of our knowledge, our proposed method is completely novel, and provides a considerable improvement over the state-of-art, especially when the input is nonstationary.\n  \n\nOur detailed replies to each review are put as separate comments below.\n"
  },
  {
    "people": [
      "Mairal",
      "Ramirez",
      "Ignacio",
      "Guillermo Sapiro"
    ],
    "review": "\nI'd like to thank the authors for their detailed response and clarifications.\n\nThis work proposes new training scheme for online sparse dictionary learning. The model assumes a non-stationary flow of the incoming data. The goal (and the challenge) is to learn a model in an online manner in a way that is capable of  adjusting to the new incoming data without forgetting how to represent previously seen data. The proposed approach deals with this problem by incorporating a mechanism for adding or deleting atoms in the dictionary. This procedure is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus. \n\nThe paper has two main innovations over the baseline approach (Mairal et al): (i) \u201cneuronal birth\u201d which represents an adaptive way of increasing the number of atoms in the dictionary (ii) \"neuronal death\", which corresponds to removing \u201cuseless\u201d dictionary atoms.\n\nNeural death is implemented by including an group-sparsity regularization to the dictionary atoms themselves (the group corresponds to a column of the dictionary). This promotes to shrink to zero atoms that are not very useful, keeping controlled the increase of the dictionary size.\n\nI believe that the strong side of the paper is its connections with the adult neurogenesis phenomenon, which is, in my opinion a very nice feature.\nThe paper is very well written and easy to follow.\n\nOn the other hand, the overall technique is not very novel. Although not exactly equivalent, similar ideas have been explored. While the neural death is implemente elegantly with a sparsity-promoting regularization term, the neural birth is performed by relying on heuristics that measure how well the dictionary can represent new incoming data. Which depending on the \"level\" of non-stationarity in the incoming data (or presence of outliers) could be difficult to set. Still, having adaptive dictionary size is very interesting.\n\nThe authors could also cite some references in model selection literature. In particular, some ideas such as MDL have been used for automatically selecting the dictionary size (I believe this work does not address the online setting, but still its a relevant reference to have). For instance,\n\nRamirez, Ignacio, and Guillermo Sapiro. \"An MDL framework for sparse coding and dictionary learning.\" IEEE Transactions on Signal Processing 60.6 (2012): 2913-2927.\n"
  },
  {
    "people": [
      "Mairal",
      "Ramirez",
      "Ignacio",
      "Guillermo Sapiro"
    ],
    "review": "I'd like to thank the authors for their detailed response and clarifications.\n\nThis work proposes new training scheme for online sparse dictionary learning. The model assumes a non-stationary flow of the incoming data. The goal (and the challenge) is to learn a model in an online manner in a way that is capable of  adjusting to the new incoming data without forgetting how to represent previously seen data. The proposed approach deals with this problem by incorporating a mechanism for adding or deleting atoms in the dictionary. This procedure is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus. \n\nThe paper has two main innovations over the baseline approach (Mairal et al): (i) \u201cneuronal birth\u201d which represents an adaptive way of increasing the number of atoms in the dictionary (ii) \"neuronal death\", which corresponds to removing \u201cuseless\u201d dictionary atoms.\n\nNeural death is implemented by including an group-sparsity regularization to the dictionary atoms themselves (the group corresponds to a column of the dictionary). This promotes to shrink to zero atoms that are not very useful, keeping controlled the increase of the dictionary size.\n\nI believe that the strong side of the paper is its connections with the adult neurogenesis phenomenon, which is, in my opinion a very nice feature.\nThe paper is very well written and easy to follow.\n\nOn the other hand, the overall technique is not very novel. Although not exactly equivalent, similar ideas have been explored. While the neural death is implemente elegantly with a sparsity-promoting regularization term, the neural birth is performed by relying on heuristics that measure how well the dictionary can represent new incoming data. Which depending on the \"level\" of non-stationarity in the incoming data (or presence of outliers) could be difficult to set. Still, having adaptive dictionary size is very interesting.\n\nThe authors could also cite some references in model selection literature. In particular, some ideas such as MDL have been used for automatically selecting the dictionary size (I believe this work does not address the online setting, but still its a relevant reference to have). For instance,\n\nRamirez, Ignacio, and Guillermo Sapiro. \"An MDL framework for sparse coding and dictionary learning.\" IEEE Transactions on Signal Processing 60.6 (2012): 2913-2927."
  },
  {
    "people": [
      "Bengio"
    ],
    "review": "We would like to thank all reviewers for their insightful comments that helped us to improve the paper; to address those comments, we modified the paper as follows:\n\n1.\tAdded a subsection 3.1 in section 3, describing various details of the proposed algorithm, as suggested by Reviewer1.\n\n2.\tAdded a section (Appendix, B9, Figures 21-26) demonstrating stability of our results (our method consistently outperforming the non-adaptive dictionary learning)  while varying all of the algorithm\u2019s parameters, one at a time, over a wide range of values.  \n\n3.\tAdded a section (Appendix, B7, Fig. 19) with new empirical results to address the concern of Reviewer3 regarding comparison with a version of the baseline method which involved re-initialization of ``dead\u2019\u2019 elements; we did not observe any significance difference in the performance of such augmented method versus the original baseline (the number of dead elements appearing in the baseline method was quite small); on the contrary, our method employing group-sparsity was removing larger number of ``weak\u2019\u2019 (low l2-norm) elements and yielded better performance due to such explicit regularization.\n\n4.\tAdded a section (Appendix, B8, Fig. 20) with new results evaluating our method under different ordering of the input data, to address the question of the Reviewer1 regarding the potential sensitivity of our results to the ordering of the training domains  (e.g., from easier to harder). No significant changes  were observed due to permutation of the input datasets (our method was still outperforming the baseline), i.e. buildings images were not necessarily \u2018simpler\u2019 than natural ones.\n\n5.\tA summary of our contributions is added at the end of the intro section. Our main point is that the contribution of this paper is truly novel and nontrivial. Indeed,\n\n    a.\tWe are the first to propose an online model-selection approach to dictionary learning (DL) based on dynamic addition and deletion of the elements (hidden units), which leads to significant performance improvements over the state-of-art online DL, especially in non-stationary settings. None of the reviewers have provided a reference that would contradict the above statement.\n\n    b.\tWhile some prior work (e.g., Bengio et al, NIPS 2009) involved deletion of dictionary elements, no addition was involved in that approach and, more importantly, their approach only concerned an offline dictionary learning, not online. Similarly, approaches to neural nets involving cascade correlations and other methods adding hidden units are lacking regularization (deletion) that we achieve via group sparsity constraint.    Finally, Reviewer3 mentions prior work on off-line MDL-based model selection in dictionary learning, but the major difference is, again, that our setting is online learning, while their approach is off-line.  \n\n    c.\tFurthermore, none of the prior approaches explored the interplay between addition and deletion, neither theoretically nor empirically, while we performed such evaluation and identified regimes when our online model-selection would outperform the non-adaptive baseline   (Section 5).\n\n\nThus, to the best of our knowledge, our proposed method is completely novel, and provides a considerable improvement over the state-of-art, especially when the input is nonstationary.\n  \n\nOur detailed replies to each review are put as separate comments below.\n"
  },
  {
    "people": [
      "Mairal",
      "Ramirez",
      "Ignacio",
      "Guillermo Sapiro"
    ],
    "review": "\nI'd like to thank the authors for their detailed response and clarifications.\n\nThis work proposes new training scheme for online sparse dictionary learning. The model assumes a non-stationary flow of the incoming data. The goal (and the challenge) is to learn a model in an online manner in a way that is capable of  adjusting to the new incoming data without forgetting how to represent previously seen data. The proposed approach deals with this problem by incorporating a mechanism for adding or deleting atoms in the dictionary. This procedure is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus. \n\nThe paper has two main innovations over the baseline approach (Mairal et al): (i) \u201cneuronal birth\u201d which represents an adaptive way of increasing the number of atoms in the dictionary (ii) \"neuronal death\", which corresponds to removing \u201cuseless\u201d dictionary atoms.\n\nNeural death is implemented by including an group-sparsity regularization to the dictionary atoms themselves (the group corresponds to a column of the dictionary). This promotes to shrink to zero atoms that are not very useful, keeping controlled the increase of the dictionary size.\n\nI believe that the strong side of the paper is its connections with the adult neurogenesis phenomenon, which is, in my opinion a very nice feature.\nThe paper is very well written and easy to follow.\n\nOn the other hand, the overall technique is not very novel. Although not exactly equivalent, similar ideas have been explored. While the neural death is implemente elegantly with a sparsity-promoting regularization term, the neural birth is performed by relying on heuristics that measure how well the dictionary can represent new incoming data. Which depending on the \"level\" of non-stationarity in the incoming data (or presence of outliers) could be difficult to set. Still, having adaptive dictionary size is very interesting.\n\nThe authors could also cite some references in model selection literature. In particular, some ideas such as MDL have been used for automatically selecting the dictionary size (I believe this work does not address the online setting, but still its a relevant reference to have). For instance,\n\nRamirez, Ignacio, and Guillermo Sapiro. \"An MDL framework for sparse coding and dictionary learning.\" IEEE Transactions on Signal Processing 60.6 (2012): 2913-2927.\n"
  },
  {
    "people": [
      "Shie Mannor",
      "Sergey"
    ],
    "review": "This paper addresses an important and timely topic in a creative way. I consider it to have three flaws (and one good idea).\n\n1) insufficient context of what is known and had been studied before (in shallow RL), for example within the field of \u201crobust RL\u201d. A good place to start might be with the work of Shie Mannor.\n\n2) an ill-defined general problem setup. Does it make sense to do post-hoc labeling of certain actions as \u201ccatastrophic\u201d if the agent is not informed about that metric during learning? Training a system to do one thing (maximize reward), but then evaluating it with a different metric is misleading. On the training metric, it could even be that the baseline outperforms the new algorithm? So I\u2019d want to see plots for \u201caverage reward\u201d in fig 3 as well. Also, what would the baseline learn if it was given large negative rewards for entering these otherwise invisible \u201cdanger states\u201d?\n\n3) a somewhat ad-hoc solution, that introduces new domain-specific hyperparameters (k_r, k_lambda and lambda) a second deep network and and two additional replay memories. In terms of results, I\u2019m also unsure whether I can trust the results, given the long-standing track-record of cart-pole being fully solved by many methods: is DQN an outlier here? Or is the convnet not an appropriate function-approximator? Actually: which exact variant \u201cstate-of-the-art\u201d variant of DQN are you using?\n\nThe good idea that I encourage the authors to pursue further is D_d, this set of rare but dangerous states, that should be kept around in some form. I see it as an ingredient for continual learning that most typical methods lack -- it is also one of the big differences between RL and supervised learning, where such states would generally be discarded as outliers.\n\nGiven my comment a couple of weeks ago, and the prompt response (\u201cwe implemented expected SARSA\u201d), I would have expected that the paper had been revised with the new results by now? In any case, I\u2019m open to discussing all these points and revising my opinion based on an updated version of the paper.\n\nMinor comment: the bibliography is done sloppily, with missing years, conference venues and missing/misspelled author lists, e.g. \u201cSergey et al. Levine\u201d. I also think it is good form to cite the actual conference publications instead of arXiv where applicable."
  },
  {
    "people": [
      "Jaderberg",
      "Max",
      "Blundell",
      "Charles",
      "Narasimhan"
    ],
    "review": "- The topic of keeping around highly rewarding or dangerous states is important and has been studied extensively in the RL literature. After the pre-review comments, authors do mention that they compared against expected SARSA but I would really like to see these and other extensive baselines before accepting this paper. \n\n- There is also an increasing amount of literature of using reward replay buffers in deep RL agents (c.f. Jaderberg, Max, et al. \"Reinforcement learning with unsupervised auxiliary tasks.\", Blundell, Charles, et al. \"Model-free episodic control.\" , Narasimhan et al. \"Language understanding for text-based games using deep reinforcement learning\"), which could perhaps reinforce the agent to avoid revisiting catastrophic states. \n\n- Overall, the approach presented is not very principled. For instance, why isn't catastrophe directly provided as a signal to the learner instead of a separate model? "
  },
  {
    "people": [
      "Shie Mannor",
      "Sergey"
    ],
    "review": "This paper addresses an important and timely topic in a creative way. I consider it to have three flaws (and one good idea).\n\n1) insufficient context of what is known and had been studied before (in shallow RL), for example within the field of \u201crobust RL\u201d. A good place to start might be with the work of Shie Mannor.\n\n2) an ill-defined general problem setup. Does it make sense to do post-hoc labeling of certain actions as \u201ccatastrophic\u201d if the agent is not informed about that metric during learning? Training a system to do one thing (maximize reward), but then evaluating it with a different metric is misleading. On the training metric, it could even be that the baseline outperforms the new algorithm? So I\u2019d want to see plots for \u201caverage reward\u201d in fig 3 as well. Also, what would the baseline learn if it was given large negative rewards for entering these otherwise invisible \u201cdanger states\u201d?\n\n3) a somewhat ad-hoc solution, that introduces new domain-specific hyperparameters (k_r, k_lambda and lambda) a second deep network and and two additional replay memories. In terms of results, I\u2019m also unsure whether I can trust the results, given the long-standing track-record of cart-pole being fully solved by many methods: is DQN an outlier here? Or is the convnet not an appropriate function-approximator? Actually: which exact variant \u201cstate-of-the-art\u201d variant of DQN are you using?\n\nThe good idea that I encourage the authors to pursue further is D_d, this set of rare but dangerous states, that should be kept around in some form. I see it as an ingredient for continual learning that most typical methods lack -- it is also one of the big differences between RL and supervised learning, where such states would generally be discarded as outliers.\n\nGiven my comment a couple of weeks ago, and the prompt response (\u201cwe implemented expected SARSA\u201d), I would have expected that the paper had been revised with the new results by now? In any case, I\u2019m open to discussing all these points and revising my opinion based on an updated version of the paper.\n\nMinor comment: the bibliography is done sloppily, with missing years, conference venues and missing/misspelled author lists, e.g. \u201cSergey et al. Levine\u201d. I also think it is good form to cite the actual conference publications instead of arXiv where applicable."
  },
  {
    "people": [
      "Shie Mannor",
      "Sergey"
    ],
    "review": "This paper addresses an important and timely topic in a creative way. I consider it to have three flaws (and one good idea).\n\n1) insufficient context of what is known and had been studied before (in shallow RL), for example within the field of \u201crobust RL\u201d. A good place to start might be with the work of Shie Mannor.\n\n2) an ill-defined general problem setup. Does it make sense to do post-hoc labeling of certain actions as \u201ccatastrophic\u201d if the agent is not informed about that metric during learning? Training a system to do one thing (maximize reward), but then evaluating it with a different metric is misleading. On the training metric, it could even be that the baseline outperforms the new algorithm? So I\u2019d want to see plots for \u201caverage reward\u201d in fig 3 as well. Also, what would the baseline learn if it was given large negative rewards for entering these otherwise invisible \u201cdanger states\u201d?\n\n3) a somewhat ad-hoc solution, that introduces new domain-specific hyperparameters (k_r, k_lambda and lambda) a second deep network and and two additional replay memories. In terms of results, I\u2019m also unsure whether I can trust the results, given the long-standing track-record of cart-pole being fully solved by many methods: is DQN an outlier here? Or is the convnet not an appropriate function-approximator? Actually: which exact variant \u201cstate-of-the-art\u201d variant of DQN are you using?\n\nThe good idea that I encourage the authors to pursue further is D_d, this set of rare but dangerous states, that should be kept around in some form. I see it as an ingredient for continual learning that most typical methods lack -- it is also one of the big differences between RL and supervised learning, where such states would generally be discarded as outliers.\n\nGiven my comment a couple of weeks ago, and the prompt response (\u201cwe implemented expected SARSA\u201d), I would have expected that the paper had been revised with the new results by now? In any case, I\u2019m open to discussing all these points and revising my opinion based on an updated version of the paper.\n\nMinor comment: the bibliography is done sloppily, with missing years, conference venues and missing/misspelled author lists, e.g. \u201cSergey et al. Levine\u201d. I also think it is good form to cite the actual conference publications instead of arXiv where applicable."
  },
  {
    "people": [
      "Jaderberg",
      "Max",
      "Blundell",
      "Charles",
      "Narasimhan"
    ],
    "review": "- The topic of keeping around highly rewarding or dangerous states is important and has been studied extensively in the RL literature. After the pre-review comments, authors do mention that they compared against expected SARSA but I would really like to see these and other extensive baselines before accepting this paper. \n\n- There is also an increasing amount of literature of using reward replay buffers in deep RL agents (c.f. Jaderberg, Max, et al. \"Reinforcement learning with unsupervised auxiliary tasks.\", Blundell, Charles, et al. \"Model-free episodic control.\" , Narasimhan et al. \"Language understanding for text-based games using deep reinforcement learning\"), which could perhaps reinforce the agent to avoid revisiting catastrophic states. \n\n- Overall, the approach presented is not very principled. For instance, why isn't catastrophe directly provided as a signal to the learner instead of a separate model? "
  },
  {
    "people": [
      "Shie Mannor",
      "Sergey"
    ],
    "review": "This paper addresses an important and timely topic in a creative way. I consider it to have three flaws (and one good idea).\n\n1) insufficient context of what is known and had been studied before (in shallow RL), for example within the field of \u201crobust RL\u201d. A good place to start might be with the work of Shie Mannor.\n\n2) an ill-defined general problem setup. Does it make sense to do post-hoc labeling of certain actions as \u201ccatastrophic\u201d if the agent is not informed about that metric during learning? Training a system to do one thing (maximize reward), but then evaluating it with a different metric is misleading. On the training metric, it could even be that the baseline outperforms the new algorithm? So I\u2019d want to see plots for \u201caverage reward\u201d in fig 3 as well. Also, what would the baseline learn if it was given large negative rewards for entering these otherwise invisible \u201cdanger states\u201d?\n\n3) a somewhat ad-hoc solution, that introduces new domain-specific hyperparameters (k_r, k_lambda and lambda) a second deep network and and two additional replay memories. In terms of results, I\u2019m also unsure whether I can trust the results, given the long-standing track-record of cart-pole being fully solved by many methods: is DQN an outlier here? Or is the convnet not an appropriate function-approximator? Actually: which exact variant \u201cstate-of-the-art\u201d variant of DQN are you using?\n\nThe good idea that I encourage the authors to pursue further is D_d, this set of rare but dangerous states, that should be kept around in some form. I see it as an ingredient for continual learning that most typical methods lack -- it is also one of the big differences between RL and supervised learning, where such states would generally be discarded as outliers.\n\nGiven my comment a couple of weeks ago, and the prompt response (\u201cwe implemented expected SARSA\u201d), I would have expected that the paper had been revised with the new results by now? In any case, I\u2019m open to discussing all these points and revising my opinion based on an updated version of the paper.\n\nMinor comment: the bibliography is done sloppily, with missing years, conference venues and missing/misspelled author lists, e.g. \u201cSergey et al. Levine\u201d. I also think it is good form to cite the actual conference publications instead of arXiv where applicable."
  },
  {
    "people": [
      "Vinyals"
    ],
    "review": "The paper propose DRNN as a neural decoder for tree structures. I like the model architecture since it has two clear improvements over traditional approaches \u2014 (1) the information flows in two directions, both from the parent and from siblings, which is desirable in tree structures (2) the model use a probability distribution to model the tree boundary (i.e. the last sibling or the leaf). This avoids the use of special ending symbols which is larger in size and putting more things to learn for the parameters (shared with other symbols).\n\nThe authors test the DRNN using the tasks of recovering the synthetic trees and recovering functional programs. The model did better than traditional methods like seq2seq models.\n\nI think the recovering synthetic tree task is not very satisfying for two reasons \u2014 (1) the surface form itself already containing some of the topological information which makes the task easier than it should be (2) as we can see from figure 3, when the number of nodes grows (even to a number not very large), the performance of the model drops dramatically, I am not sure if a simple baseline only captures the topological information in the surface string would be much worse than this. And DRNN in this case, seems can\u2019t show its full potentials since the length of the information flow in the model won\u2019t be very long.\n\nI think the experiments are interesting. But I think there are some other tasks which are more difficult and the tree structure information are more important in such tasks. For example, we have the seq2seq parsing model (Vinyals et al, 2014), is it possible to use the DRNN proposed here on the decoder side? I think tasks like this can show more potentials of the DRNN and can be very convincing that model architectures like this are better than traditional alternatives."
  },
  {
    "people": [
      "Vinyals"
    ],
    "review": "The paper propose DRNN as a neural decoder for tree structures. I like the model architecture since it has two clear improvements over traditional approaches \u2014 (1) the information flows in two directions, both from the parent and from siblings, which is desirable in tree structures (2) the model use a probability distribution to model the tree boundary (i.e. the last sibling or the leaf). This avoids the use of special ending symbols which is larger in size and putting more things to learn for the parameters (shared with other symbols).\n\nThe authors test the DRNN using the tasks of recovering the synthetic trees and recovering functional programs. The model did better than traditional methods like seq2seq models.\n\nI think the recovering synthetic tree task is not very satisfying for two reasons \u2014 (1) the surface form itself already containing some of the topological information which makes the task easier than it should be (2) as we can see from figure 3, when the number of nodes grows (even to a number not very large), the performance of the model drops dramatically, I am not sure if a simple baseline only captures the topological information in the surface string would be much worse than this. And DRNN in this case, seems can\u2019t show its full potentials since the length of the information flow in the model won\u2019t be very long.\n\nI think the experiments are interesting. But I think there are some other tasks which are more difficult and the tree structure information are more important in such tasks. For example, we have the seq2seq parsing model (Vinyals et al, 2014), is it possible to use the DRNN proposed here on the decoder side? I think tasks like this can show more potentials of the DRNN and can be very convincing that model architectures like this are better than traditional alternatives."
  },
  {
    "people": [
      "Han",
      "Han",
      "Han",
      "Han",
      "Jeff Pool",
      "John Tran",
      "William J. Dally",
      "Yunchao Gong",
      "Liu Liu",
      "Ming Yang",
      "Lubomir Bourdev"
    ],
    "review": "Thanks to all the reviewers for constructive suggestions and comments. We are really excited that the novelty of our paper has been well recognized.\n\nIn this updated version, we carefully considered all reviewers\u2019 suggestions to improve the paper. Generally, we performed four aspects of works: (1) the result comparison of pruning + quantization between our method and Han et al.\u2019s method [1] was incorporated into the paper (please see Section 3.4 for details); (2) the result comparison of weight quantization between our method and vector quantization [2] was also incorporated into the paper (please see Section 3.4 for details); (3) we tried our best to improve the clarifications of our encoding method for weight quantization, definition of bit-width, detailed experimental settings and so on, and several rounds of proof-reading and revising were also conducted; (4) more experimental results (including the statistical analyses on the distribution of weights after quantization and our latest progress on developing INQ for deep CNNs with low-precision weights and low-precision activations) that reviewers may be interested were added to the paper as the supplementary materials.\n \nMoreover, to make our work fully reproducible, the code (along with an instruction manual) will be released to public as we promised in the paper submission.\n\n(1) To reviewer 1:\n\nQuestion1: \u201cAlso, the description of the pruning-inspired partitioning strategy could be clarified somewhat... e.g., the chosen splitting ratio of 50% only seems to be referenced in a figure caption and not the main text.\u201d\n\nFollowing your suggestion, we added detailed parameter settings (such as splitting ratio and etc.) to the respective sets of experiments described in Section 3 accordingly.\n\nQuestion 2: \u201cThe paper could use another second pass for writing style and grammar.\u201d\n\nFollowing your suggestion, we tried our best to do a much better work on revising and proof-reading, with the helps from the native colleagues in USA.\n\n(2) To reviewer 2:\n\nThanks for your recognition of the novelty of our method. We believe that our responses posted on Dec. 16, 2016 should well address your concern on the result comparison of pruning + quantization between our method and Han et al.\u2019s method [1]. Furthermore, detailed result comparisons can be found in Section 3.4 of the paper. It can be clearly seen that our method outperforms Han et al.\u2019s method [1] with significant margins.\n\n(3) To reviewer 3:\n\nQuestion 1: \u201c1) It would be good to incorporate some of the answers into the paper, mainly the results with pruning + this method as that can be compared fairly to Han et al. and outperforms it.\u201d\n\nFollowing your suggestion, we incorporated related results into the paper (please see Section 3.4 for details).\n\nQuestion 2: \u201cIt would be good to better explain the encoding method (my question 4) as it is not that clear from the paper (e.g. made me make a mistake in question 5 for the computation of n2).\u201d\n\nFollowing your suggestion, we revised related parts, especially the clarification of our encoding method based on our previous responses to your questions accordingly (please see Section 2.1 for details).\n\nQuestion 3: \"The \"5 bits\" is misleading as in fact what is used is variable length encoding (which is on average close to 5 bits) where: - 0 is represented with 1 bit, e.g. 0 - other values are represented with 5 bits, where the first bit is needed to distinguish from 0, and the remaining 4 bits represent the 16 different values for the powers of 2.\u201d\n\nFollowing your suggestion, we made a clear clarification on the definition of bit-width accordingly.\n\nReferences:\nSong Han, Jeff Pool, John Tran, and William J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. ICLR, 2016.\nYunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Compressing deep convolutional networks using vector quantization. arXiv preprint arXiv:1412.6115v1, 2014."
  },
  {
    "people": [
      "Han"
    ],
    "review": "The idea of this paper is reasonable - gradually go from original weights to compressed weights by compressing a part of them and fine-tuning the rest. Everything seems fine, results look good, and my questions have been addressed.\n\nTo improve the paper:\n\n1) It would be good to incorporate some of the answers into the paper, mainly the results with pruning + this method as that can be compared fairly to Han et al. and outperforms it.\n\n2) It would be good to better explain the encoding method (my question 4) as it is not that clear from the paper (e.g. made me make a mistake in question 5 for the computation of n2). The \"5 bits\" is misleading as in fact what is used is variable length encoding (which is on average close to 5 bits) where:\n- 0 is represented with 1 bit, e.g. 0\n- other values are represented with 5 bits, where the first bit is needed to distinguish from 0, and the remaining 4 bits represent the 16 different values for the powers of 2.\n"
  },
  {
    "people": [
      "Han",
      "Han",
      "Han",
      "Han",
      "Jeff Pool",
      "John Tran",
      "William J. Dally",
      "Yunchao Gong",
      "Liu Liu",
      "Ming Yang",
      "Lubomir Bourdev"
    ],
    "review": "Thanks to all the reviewers for constructive suggestions and comments. We are really excited that the novelty of our paper has been well recognized.\n\nIn this updated version, we carefully considered all reviewers\u2019 suggestions to improve the paper. Generally, we performed four aspects of works: (1) the result comparison of pruning + quantization between our method and Han et al.\u2019s method [1] was incorporated into the paper (please see Section 3.4 for details); (2) the result comparison of weight quantization between our method and vector quantization [2] was also incorporated into the paper (please see Section 3.4 for details); (3) we tried our best to improve the clarifications of our encoding method for weight quantization, definition of bit-width, detailed experimental settings and so on, and several rounds of proof-reading and revising were also conducted; (4) more experimental results (including the statistical analyses on the distribution of weights after quantization and our latest progress on developing INQ for deep CNNs with low-precision weights and low-precision activations) that reviewers may be interested were added to the paper as the supplementary materials.\n \nMoreover, to make our work fully reproducible, the code (along with an instruction manual) will be released to public as we promised in the paper submission.\n\n(1) To reviewer 1:\n\nQuestion1: \u201cAlso, the description of the pruning-inspired partitioning strategy could be clarified somewhat... e.g., the chosen splitting ratio of 50% only seems to be referenced in a figure caption and not the main text.\u201d\n\nFollowing your suggestion, we added detailed parameter settings (such as splitting ratio and etc.) to the respective sets of experiments described in Section 3 accordingly.\n\nQuestion 2: \u201cThe paper could use another second pass for writing style and grammar.\u201d\n\nFollowing your suggestion, we tried our best to do a much better work on revising and proof-reading, with the helps from the native colleagues in USA.\n\n(2) To reviewer 2:\n\nThanks for your recognition of the novelty of our method. We believe that our responses posted on Dec. 16, 2016 should well address your concern on the result comparison of pruning + quantization between our method and Han et al.\u2019s method [1]. Furthermore, detailed result comparisons can be found in Section 3.4 of the paper. It can be clearly seen that our method outperforms Han et al.\u2019s method [1] with significant margins.\n\n(3) To reviewer 3:\n\nQuestion 1: \u201c1) It would be good to incorporate some of the answers into the paper, mainly the results with pruning + this method as that can be compared fairly to Han et al. and outperforms it.\u201d\n\nFollowing your suggestion, we incorporated related results into the paper (please see Section 3.4 for details).\n\nQuestion 2: \u201cIt would be good to better explain the encoding method (my question 4) as it is not that clear from the paper (e.g. made me make a mistake in question 5 for the computation of n2).\u201d\n\nFollowing your suggestion, we revised related parts, especially the clarification of our encoding method based on our previous responses to your questions accordingly (please see Section 2.1 for details).\n\nQuestion 3: \"The \"5 bits\" is misleading as in fact what is used is variable length encoding (which is on average close to 5 bits) where: - 0 is represented with 1 bit, e.g. 0 - other values are represented with 5 bits, where the first bit is needed to distinguish from 0, and the remaining 4 bits represent the 16 different values for the powers of 2.\u201d\n\nFollowing your suggestion, we made a clear clarification on the definition of bit-width accordingly.\n\nReferences:\nSong Han, Jeff Pool, John Tran, and William J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. ICLR, 2016.\nYunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Compressing deep convolutional networks using vector quantization. arXiv preprint arXiv:1412.6115v1, 2014."
  },
  {
    "people": [
      "Han"
    ],
    "review": "The idea of this paper is reasonable - gradually go from original weights to compressed weights by compressing a part of them and fine-tuning the rest. Everything seems fine, results look good, and my questions have been addressed.\n\nTo improve the paper:\n\n1) It would be good to incorporate some of the answers into the paper, mainly the results with pruning + this method as that can be compared fairly to Han et al. and outperforms it.\n\n2) It would be good to better explain the encoding method (my question 4) as it is not that clear from the paper (e.g. made me make a mistake in question 5 for the computation of n2). The \"5 bits\" is misleading as in fact what is used is variable length encoding (which is on average close to 5 bits) where:\n- 0 is represented with 1 bit, e.g. 0\n- other values are represented with 5 bits, where the first bit is needed to distinguish from 0, and the remaining 4 bits represent the 16 different values for the powers of 2.\n"
  },
  {
    "people": [
      "Choi",
      "Choi"
    ],
    "review": "In light of the detailed author responses and further updates to the manuscript, I am raising my score to an 8 and reiterating my support for this paper. I think it will be among the strongest non-traditional applied deep learning work at ICLR and will receive a great deal of interest and attention from attendees.\n\n-----\n\nThis paper describes modern deep learning approach to the problem of predicting the medications taken by a patient during a period of time based solely upon the sequence of ICD-9 codes assigned to the patient during that same time period. This problem is formulated as a multilabel sequence classification (in contrast to language modeling, which is multiclass classification). They propose to use standard LSTM and GRU architectures with embedding layers to handle the sparse categorical inputs, similar to that described in related work by Choi, et al. In experiments using a cohort of ~610K patient records, they find that RNN models outperform strong baselines including an MLP and a random forest, as well as a common sense baseline. The differences in performance between the recurrent models and the MLP appear to be large enough to be significant, given the size of the test set.\n\nStrengths:\n- Very important problem. As the authors point out, two the value propositions of EHRs -- which have been widely adopted throughout the US due to a combination of legislation and billions of dollars in incentives from the federal government -- included more accurate records and fewer medication mistakes. These two benefits have largely failed to materialize. This seems like a major opportunity for data mining and machine learning.\n- Paper is well-written with lucid introduction and motivation, thorough discussion of related work, clear description of experiments and metrics, and interesting qualitative analysis of results.\n- Empirical results are solid with a strong win for RNNs over convincing baselines. This is in contrast to some recent related papers, including Lipton & Kale et al, ICLR 2016, where the gap between the RNN and MLP was relatively small, and Choi et al, MLHC 2016, which omitted many obvious baselines.\n- Discussion is thorough and thoughtful. The authors are right about the kidney code embedding results: this is a very promising result.\n\nWeaknesses:\n- The authors make several unintuitive decisions related to data preprocessing and experimental design, foremost among them the choice NOT to use full patient sequences but instead only truncated patient sequences that each ends at randomly chosen time point. This does not necessarily invalidate their results, but it is somewhat unnatural and the explanation is difficult to follow, reducing the paper's potential impact. It is also reduces the RNN's potential advantage.\n- The chosen metrics seem appropriate, but non-experts may have trouble interpreting the absolute and relative performances (beyond the superficial, e.g., RNN score 0.01 more than NN!). The authors should invest some space in explaining (1) what level of performance -- for each metric -- would be necessary for the model to be useful in a real clinical setting and (2) whether the gaps between the various models are \"significant\" (even in an informal sense).\n- The paper proposes nothing novel in terms of methods, which is a serious weakness for a methods conference like ICLR. I think it is strong enough empirically (and sufficiently interesting in application) to warrant acceptance regardless, but there may be things the authors can do to make it more competitive. For example, one potential hypothesis is that higher capacity models are more prone to overfitting noisy targets. Is there some way to investigate this, perhaps by looking at the kinds of errors each model makes?\n\nI have a final comment: as a piece of clinical work, the paper has a huge weakness: the lack of ground truth labels for missing medications. Models are both trained and tested on data with noisy labels. For training, the authors are right that this shouldn't be a huge problem, provided the label noise is random (even class conditional isn't too big of a problem). For testing, though, this seems like it could skew metrics. Further, the assumption that the label noise is not systemic seems very unlikely given that these data are recorded by human clinicians. The cases shown in Appendix C lend some credence to this assertion: for Case 1, 7/26 actual medications received probabilities < 0.5. My hunch is that clinical reviewers would view the paper with great skepticism. The authors will need to get creative about evaluation -- or invest a lot of time/money in labeling data -- to really prove that this works.\n\nFor what it is worth, I hope that this paper is accepted as I think it will be of great interest to the ICLR community. However, I am borderline about whether I'd be willing to fight for its acceptance. If the authors can address the reviewers' critiques -- and in particular, dive into the question of overfitting the imperfect labels and provide some insights -- I might be willing to raise my score and lobby for acceptance."
  },
  {
    "people": [
      "Choi",
      "Choi"
    ],
    "review": "In light of the detailed author responses and further updates to the manuscript, I am raising my score to an 8 and reiterating my support for this paper. I think it will be among the strongest non-traditional applied deep learning work at ICLR and will receive a great deal of interest and attention from attendees.\n\n-----\n\nThis paper describes modern deep learning approach to the problem of predicting the medications taken by a patient during a period of time based solely upon the sequence of ICD-9 codes assigned to the patient during that same time period. This problem is formulated as a multilabel sequence classification (in contrast to language modeling, which is multiclass classification). They propose to use standard LSTM and GRU architectures with embedding layers to handle the sparse categorical inputs, similar to that described in related work by Choi, et al. In experiments using a cohort of ~610K patient records, they find that RNN models outperform strong baselines including an MLP and a random forest, as well as a common sense baseline. The differences in performance between the recurrent models and the MLP appear to be large enough to be significant, given the size of the test set.\n\nStrengths:\n- Very important problem. As the authors point out, two the value propositions of EHRs -- which have been widely adopted throughout the US due to a combination of legislation and billions of dollars in incentives from the federal government -- included more accurate records and fewer medication mistakes. These two benefits have largely failed to materialize. This seems like a major opportunity for data mining and machine learning.\n- Paper is well-written with lucid introduction and motivation, thorough discussion of related work, clear description of experiments and metrics, and interesting qualitative analysis of results.\n- Empirical results are solid with a strong win for RNNs over convincing baselines. This is in contrast to some recent related papers, including Lipton & Kale et al, ICLR 2016, where the gap between the RNN and MLP was relatively small, and Choi et al, MLHC 2016, which omitted many obvious baselines.\n- Discussion is thorough and thoughtful. The authors are right about the kidney code embedding results: this is a very promising result.\n\nWeaknesses:\n- The authors make several unintuitive decisions related to data preprocessing and experimental design, foremost among them the choice NOT to use full patient sequences but instead only truncated patient sequences that each ends at randomly chosen time point. This does not necessarily invalidate their results, but it is somewhat unnatural and the explanation is difficult to follow, reducing the paper's potential impact. It is also reduces the RNN's potential advantage.\n- The chosen metrics seem appropriate, but non-experts may have trouble interpreting the absolute and relative performances (beyond the superficial, e.g., RNN score 0.01 more than NN!). The authors should invest some space in explaining (1) what level of performance -- for each metric -- would be necessary for the model to be useful in a real clinical setting and (2) whether the gaps between the various models are \"significant\" (even in an informal sense).\n- The paper proposes nothing novel in terms of methods, which is a serious weakness for a methods conference like ICLR. I think it is strong enough empirically (and sufficiently interesting in application) to warrant acceptance regardless, but there may be things the authors can do to make it more competitive. For example, one potential hypothesis is that higher capacity models are more prone to overfitting noisy targets. Is there some way to investigate this, perhaps by looking at the kinds of errors each model makes?\n\nI have a final comment: as a piece of clinical work, the paper has a huge weakness: the lack of ground truth labels for missing medications. Models are both trained and tested on data with noisy labels. For training, the authors are right that this shouldn't be a huge problem, provided the label noise is random (even class conditional isn't too big of a problem). For testing, though, this seems like it could skew metrics. Further, the assumption that the label noise is not systemic seems very unlikely given that these data are recorded by human clinicians. The cases shown in Appendix C lend some credence to this assertion: for Case 1, 7/26 actual medications received probabilities < 0.5. My hunch is that clinical reviewers would view the paper with great skepticism. The authors will need to get creative about evaluation -- or invest a lot of time/money in labeling data -- to really prove that this works.\n\nFor what it is worth, I hope that this paper is accepted as I think it will be of great interest to the ICLR community. However, I am borderline about whether I'd be willing to fight for its acceptance. If the authors can address the reviewers' critiques -- and in particular, dive into the question of overfitting the imperfect labels and provide some insights -- I might be willing to raise my score and lobby for acceptance."
  },
  {
    "people": [
      "Parikh",
      "Luong",
      "Bahdanau",
      "Lili Mou"
    ],
    "review": "The paper presents a general approach to modeling for natural language understanding problems with two distinct textual inputs (such as a question and a source text) that can be aligned in some way. In the approach, soft attention is first used to derive alignments between the tokens of the two texts, then a comparison function uses the resulting alignments (represented as pairs of attention queries and attention results) to derive a representations that are aggregated by CNN into a single vector from which an output can be computed. The paper both presents this as an overall modeling strategy that can be made to work quite well, and offers a detailed empirical analysis of the comparison component of the model.\n\nThis work is timely. Language understanding problems of this kind are a major open issue in NLP, and are just at the threshold of being addressable with representation learning methods. The work presents a general approach which is straightforward and reasonable, and shows that it can yield good results. The work borders on incremental (relative to their earlier work or that of Parikh et al.), but it contributes in enough substantial ways that I'd strongly recommend acceptance.\n\nDetail: \n- The model, at least as implemented for the problems with longer sequences (everything but SNLI), is not sensitive to word order. It is empirically competitive, but this insensitivity places a strong upper bound on its performance. The paper does make this clear, but it seems salient enough to warrant a brief mention in the introduction or discussion sections.\n- If I understand correctly, your attention strategy is based more closely on the general/bilinear strategy of Luong et al. '15 than it is on the earlier Bahdanau work. You should probably cite the former (or some other more directly relevant reference for that strategy).\n- Since the NTN risks overfitting because of its large number of parameters, did you try using a version with input dimension l and a smaller output dimension m (so an l*l*m tensor)?\n- You should probably note that SubMultNN looks a lot like the strategy for *sentence*-level matching in the Lili Mou paper you cite.\n- Is there a reason you use the same parameters for preprocessing the question and answer in (1)? These could require different things to be weighted highly."
  },
  {
    "people": [
      "Parikh",
      "Luong",
      "Bahdanau",
      "Lili Mou"
    ],
    "review": "The paper presents a general approach to modeling for natural language understanding problems with two distinct textual inputs (such as a question and a source text) that can be aligned in some way. In the approach, soft attention is first used to derive alignments between the tokens of the two texts, then a comparison function uses the resulting alignments (represented as pairs of attention queries and attention results) to derive a representations that are aggregated by CNN into a single vector from which an output can be computed. The paper both presents this as an overall modeling strategy that can be made to work quite well, and offers a detailed empirical analysis of the comparison component of the model.\n\nThis work is timely. Language understanding problems of this kind are a major open issue in NLP, and are just at the threshold of being addressable with representation learning methods. The work presents a general approach which is straightforward and reasonable, and shows that it can yield good results. The work borders on incremental (relative to their earlier work or that of Parikh et al.), but it contributes in enough substantial ways that I'd strongly recommend acceptance.\n\nDetail: \n- The model, at least as implemented for the problems with longer sequences (everything but SNLI), is not sensitive to word order. It is empirically competitive, but this insensitivity places a strong upper bound on its performance. The paper does make this clear, but it seems salient enough to warrant a brief mention in the introduction or discussion sections.\n- If I understand correctly, your attention strategy is based more closely on the general/bilinear strategy of Luong et al. '15 than it is on the earlier Bahdanau work. You should probably cite the former (or some other more directly relevant reference for that strategy).\n- Since the NTN risks overfitting because of its large number of parameters, did you try using a version with input dimension l and a smaller output dimension m (so an l*l*m tensor)?\n- You should probably note that SubMultNN looks a lot like the strategy for *sentence*-level matching in the Lili Mou paper you cite.\n- Is there a reason you use the same parameters for preprocessing the question and answer in (1)? These could require different things to be weighted highly.\n"
  },
  {
    "people": [
      "Parikh",
      "Luong",
      "Bahdanau",
      "Lili Mou"
    ],
    "review": "The paper presents a general approach to modeling for natural language understanding problems with two distinct textual inputs (such as a question and a source text) that can be aligned in some way. In the approach, soft attention is first used to derive alignments between the tokens of the two texts, then a comparison function uses the resulting alignments (represented as pairs of attention queries and attention results) to derive a representations that are aggregated by CNN into a single vector from which an output can be computed. The paper both presents this as an overall modeling strategy that can be made to work quite well, and offers a detailed empirical analysis of the comparison component of the model.\n\nThis work is timely. Language understanding problems of this kind are a major open issue in NLP, and are just at the threshold of being addressable with representation learning methods. The work presents a general approach which is straightforward and reasonable, and shows that it can yield good results. The work borders on incremental (relative to their earlier work or that of Parikh et al.), but it contributes in enough substantial ways that I'd strongly recommend acceptance.\n\nDetail: \n- The model, at least as implemented for the problems with longer sequences (everything but SNLI), is not sensitive to word order. It is empirically competitive, but this insensitivity places a strong upper bound on its performance. The paper does make this clear, but it seems salient enough to warrant a brief mention in the introduction or discussion sections.\n- If I understand correctly, your attention strategy is based more closely on the general/bilinear strategy of Luong et al. '15 than it is on the earlier Bahdanau work. You should probably cite the former (or some other more directly relevant reference for that strategy).\n- Since the NTN risks overfitting because of its large number of parameters, did you try using a version with input dimension l and a smaller output dimension m (so an l*l*m tensor)?\n- You should probably note that SubMultNN looks a lot like the strategy for *sentence*-level matching in the Lili Mou paper you cite.\n- Is there a reason you use the same parameters for preprocessing the question and answer in (1)? These could require different things to be weighted highly."
  },
  {
    "people": [
      "Parikh",
      "Luong",
      "Bahdanau",
      "Lili Mou"
    ],
    "review": "The paper presents a general approach to modeling for natural language understanding problems with two distinct textual inputs (such as a question and a source text) that can be aligned in some way. In the approach, soft attention is first used to derive alignments between the tokens of the two texts, then a comparison function uses the resulting alignments (represented as pairs of attention queries and attention results) to derive a representations that are aggregated by CNN into a single vector from which an output can be computed. The paper both presents this as an overall modeling strategy that can be made to work quite well, and offers a detailed empirical analysis of the comparison component of the model.\n\nThis work is timely. Language understanding problems of this kind are a major open issue in NLP, and are just at the threshold of being addressable with representation learning methods. The work presents a general approach which is straightforward and reasonable, and shows that it can yield good results. The work borders on incremental (relative to their earlier work or that of Parikh et al.), but it contributes in enough substantial ways that I'd strongly recommend acceptance.\n\nDetail: \n- The model, at least as implemented for the problems with longer sequences (everything but SNLI), is not sensitive to word order. It is empirically competitive, but this insensitivity places a strong upper bound on its performance. The paper does make this clear, but it seems salient enough to warrant a brief mention in the introduction or discussion sections.\n- If I understand correctly, your attention strategy is based more closely on the general/bilinear strategy of Luong et al. '15 than it is on the earlier Bahdanau work. You should probably cite the former (or some other more directly relevant reference for that strategy).\n- Since the NTN risks overfitting because of its large number of parameters, did you try using a version with input dimension l and a smaller output dimension m (so an l*l*m tensor)?\n- You should probably note that SubMultNN looks a lot like the strategy for *sentence*-level matching in the Lili Mou paper you cite.\n- Is there a reason you use the same parameters for preprocessing the question and answer in (1)? These could require different things to be weighted highly.\n"
  },
  {
    "people": [
      "Diederik P. Kingma",
      "Tim Salimans",
      "Max Welling"
    ],
    "review": "This paper presents a theoretical underpinning of dropout, and uses this derivation to both characterize its properties and to extend the method. A solid contribution. I am surprised that none of the reviewers mentioned that this work is closely related to the uncited 2015 paper \"Variational Dropout and the Local Reparameterization Trick\" by Diederik P. Kingma, Tim Salimans, Max Welling."
  },
  {
    "people": [
      "Diederik P. Kingma",
      "Tim Salimans",
      "Max Welling"
    ],
    "review": "This paper presents a theoretical underpinning of dropout, and uses this derivation to both characterize its properties and to extend the method. A solid contribution. I am surprised that none of the reviewers mentioned that this work is closely related to the uncited 2015 paper \"Variational Dropout and the Local Reparameterization Trick\" by Diederik P. Kingma, Tim Salimans, Max Welling."
  },
  {
    "people": [
      "Degris"
    ],
    "review": "First of all, thanks for this excellent work.\n\nMy question is about eq. 4. In Degris et al (2012) the policy gradient is computed as the expectation under the off-policy behavior of \\rho(s_t, a_t) \\psi(s_t, a_t) (R_t^\\lambda - V(s_t))\nWith \\rho(s_t,a_t) = \\pi(a_t | s_t) / \\mu(a_t | s_t) and \\psi(s_t, a_t) = \\grad_\\theta ( log \\pi (a_t | s_t) ) /  \\pi (a_t | s_t)\nThe last division by \\pi (a_t | s_t) is missing in equation (4).\n\nAm I mistaken or is the reference wrong?\nThanks for your time."
  },
  {
    "people": [
      "Xi Chen"
    ],
    "review": "Dear reviewers, we would really appreciate it if you can take a look at the paper again in light of our replies, the updated paper, and the comments from Xi Chen. Thanks very much for your time!"
  },
  {
    "people": [
      "Schaul"
    ],
    "review": "This paper introduces an actor-critic deep RL approach with experience replay, which combines truncated importance sampling and trust region policy optimization. The paper also proposes a new method called stochastic duelling networks to estimate the critic for continuous action spaces. The method is applied to Atari games and continuous control problems, where it yields performance comparable to state-of-the-art methods.\n\nAs mentioned in the beginning of the paper, the main contributions of this work lies in combining 1) truncated importance sampling with retrace, 2) trust region policy optimization, and 3) stochastic duelling networks. These improvements work well and may be beneficial to future work in RL.\n\nHowever, each improvement appears to be quite incremental. Moreover, the ACER framework seems much more complex and fragile to implement compared to the standard deep q-learning with prioritized replay (which appears to perform just as well on Atari games). So for the Atari domain, I would still put my money on prioritized replay due to its simplicity. Thirdly, improving sample efficiency for deep RL is a laudable goal, but really this goal should be pursued in a problem setting where sample efficiency is important. Unfortunately, the paper only evaluates sample efficiency in the Atari and continuous control tasks domain; two domains where sample efficiency is not important. Thus, it is not clear that the proposed method ACER will generalize to problems where we really care about sample efficiency.\n\nSome technical aspects which need clarifications:\n- For Retrace, I assume that you compute recursively $Q^{ret}$ starting from the end of each trajectory? Please comment on this.\n- It's not clear to me how to derive eq. (7). Is an approximation (double tilde) sign missing?\n- In section 3.1 the paper argued that $Q^{ret}$ gives a lower-variance estimate of the action-value function. Then why not use it in eq. (8) for the bias correction term?\n- The paper states that it uses a replay memory of 50000 frames, so that across threads it is comparable in size to previous work. However, for each thread this is much smaller compared to earlier experiments on Atari games. For example, one million experience replay transitions were used in the paper \"Prioritized Experience Replay\" by Schaul et al. This may have a huge impact on performance of the models (both for ACER and for the competing models). In order to properly assess the improvements of ACER over previous work, the authors need to also experiment with larger experience replay memories.\n\n\nOther comments:\n- Please move Section 7 to the appendix.\n- \"Moreover, when using small values of lambda to reduce variance, occasional large importance weights can still cause instability\": I think what is meant is using *large* values of lambda.\n- Above eq. (6) mention that the squared error is used.\n- Missing a \"t\" subscript at the beginning of eq. (9)?\n- It was hard to understand the stochastic duelling networks. Please rephrase this part.\n- Please clarify this sentence \"To compare different agents, we adopt as our metric the median of the human normalized score over all 57 games.\"\n- Figure 2 (Bottom): Please add label to vertical axes."
  },
  {
    "people": [
      "Degris"
    ],
    "review": "First of all, thanks for this excellent work.\n\nMy question is about eq. 4. In Degris et al (2012) the policy gradient is computed as the expectation under the off-policy behavior of \\rho(s_t, a_t) \\psi(s_t, a_t) (R_t^\\lambda - V(s_t))\nWith \\rho(s_t,a_t) = \\pi(a_t | s_t) / \\mu(a_t | s_t) and \\psi(s_t, a_t) = \\grad_\\theta ( log \\pi (a_t | s_t) ) /  \\pi (a_t | s_t)\nThe last division by \\pi (a_t | s_t) is missing in equation (4).\n\nAm I mistaken or is the reference wrong?\nThanks for your time."
  },
  {
    "people": [
      "Xi Chen"
    ],
    "review": "Dear reviewers, we would really appreciate it if you can take a look at the paper again in light of our replies, the updated paper, and the comments from Xi Chen. Thanks very much for your time!"
  },
  {
    "people": [
      "Schaul"
    ],
    "review": "This paper introduces an actor-critic deep RL approach with experience replay, which combines truncated importance sampling and trust region policy optimization. The paper also proposes a new method called stochastic duelling networks to estimate the critic for continuous action spaces. The method is applied to Atari games and continuous control problems, where it yields performance comparable to state-of-the-art methods.\n\nAs mentioned in the beginning of the paper, the main contributions of this work lies in combining 1) truncated importance sampling with retrace, 2) trust region policy optimization, and 3) stochastic duelling networks. These improvements work well and may be beneficial to future work in RL.\n\nHowever, each improvement appears to be quite incremental. Moreover, the ACER framework seems much more complex and fragile to implement compared to the standard deep q-learning with prioritized replay (which appears to perform just as well on Atari games). So for the Atari domain, I would still put my money on prioritized replay due to its simplicity. Thirdly, improving sample efficiency for deep RL is a laudable goal, but really this goal should be pursued in a problem setting where sample efficiency is important. Unfortunately, the paper only evaluates sample efficiency in the Atari and continuous control tasks domain; two domains where sample efficiency is not important. Thus, it is not clear that the proposed method ACER will generalize to problems where we really care about sample efficiency.\n\nSome technical aspects which need clarifications:\n- For Retrace, I assume that you compute recursively $Q^{ret}$ starting from the end of each trajectory? Please comment on this.\n- It's not clear to me how to derive eq. (7). Is an approximation (double tilde) sign missing?\n- In section 3.1 the paper argued that $Q^{ret}$ gives a lower-variance estimate of the action-value function. Then why not use it in eq. (8) for the bias correction term?\n- The paper states that it uses a replay memory of 50000 frames, so that across threads it is comparable in size to previous work. However, for each thread this is much smaller compared to earlier experiments on Atari games. For example, one million experience replay transitions were used in the paper \"Prioritized Experience Replay\" by Schaul et al. This may have a huge impact on performance of the models (both for ACER and for the competing models). In order to properly assess the improvements of ACER over previous work, the authors need to also experiment with larger experience replay memories.\n\n\nOther comments:\n- Please move Section 7 to the appendix.\n- \"Moreover, when using small values of lambda to reduce variance, occasional large importance weights can still cause instability\": I think what is meant is using *large* values of lambda.\n- Above eq. (6) mention that the squared error is used.\n- Missing a \"t\" subscript at the beginning of eq. (9)?\n- It was hard to understand the stochastic duelling networks. Please rephrase this part.\n- Please clarify this sentence \"To compare different agents, we adopt as our metric the median of the human normalized score over all 57 games.\"\n- Figure 2 (Bottom): Please add label to vertical axes."
  },
  {
    "people": [
      "Hochreiter"
    ],
    "review": "The paper presents a repurposing of rectified factor networks proposed\nearlier by the same authors to biclustering. The method seems\npotentially quite interesting but the paper has serious problems in\nthe presentation.\n\n\nQuality:\n\nThe method relies mainly on techniques presented in a NIPS 2015 paper\nby (mostly) the same authors. The experimental procedure should be\nclarified further. The results (especially Table 2) seem to depend\ncritically upon the sparsity of the reported clusters, but the authors\ndo not explain in sufficient detail how the sparsity hyperparameter is\ndetermined.\n\n\nClarity:\n\nThe style of writing is terrible and completely unacceptable as a\nscientific publication. The text looks more like an industry white\npaper or advertisement, not an objective scientific paper. A complete\nrewrite would be needed before the paper can be considered for\npublication. Specifically, all references to companies using your\nmethods must be deleted.\n\nAdditionally, Table 1 is essentially unreadable. I would recommend\nusing a figure or cleaning up the table by removing all engineering\nnotation and reporting numbers per 1000 so that e.g. \"0.475 +/- 9e-4\"\nwould become \"475 +/- 0.9\". In general figures would be preferred as a\nprimary means for presenting the results in text while tables can be\nincluded as supplementary information.\n\n\nOriginality:\n\nThe novelty of the work appears limited: the method is mostly based on\na NIPS 2015 paper by the same authors. The experimental evaluation\nappears at least partially novel, but for example the IBD detection is\nvery similar to Hochreiter (2013) but without any comparison.\n\n\nSignificance:\n\nThe authors' strongest claim is based on strong empirical performance\nin their own benchmark problems. It is however unclear how useful this\nwould be to others as there is no code available and the details of\nthe implementation are less than complete. Furthermore, the method\ndepends on many specific tuning parameters whose tuning method is not\nfully defined, leaving it unclear how to guarantee the generalisation\nof the good performance.\n"
  },
  {
    "people": [
      "Hochreiter"
    ],
    "review": "The paper presents a repurposing of rectified factor networks proposed\nearlier by the same authors to biclustering. The method seems\npotentially quite interesting but the paper has serious problems in\nthe presentation.\n\n\nQuality:\n\nThe method relies mainly on techniques presented in a NIPS 2015 paper\nby (mostly) the same authors. The experimental procedure should be\nclarified further. The results (especially Table 2) seem to depend\ncritically upon the sparsity of the reported clusters, but the authors\ndo not explain in sufficient detail how the sparsity hyperparameter is\ndetermined.\n\n\nClarity:\n\nThe style of writing is terrible and completely unacceptable as a\nscientific publication. The text looks more like an industry white\npaper or advertisement, not an objective scientific paper. A complete\nrewrite would be needed before the paper can be considered for\npublication. Specifically, all references to companies using your\nmethods must be deleted.\n\nAdditionally, Table 1 is essentially unreadable. I would recommend\nusing a figure or cleaning up the table by removing all engineering\nnotation and reporting numbers per 1000 so that e.g. \"0.475 +/- 9e-4\"\nwould become \"475 +/- 0.9\". In general figures would be preferred as a\nprimary means for presenting the results in text while tables can be\nincluded as supplementary information.\n\n\nOriginality:\n\nThe novelty of the work appears limited: the method is mostly based on\na NIPS 2015 paper by the same authors. The experimental evaluation\nappears at least partially novel, but for example the IBD detection is\nvery similar to Hochreiter (2013) but without any comparison.\n\n\nSignificance:\n\nThe authors' strongest claim is based on strong empirical performance\nin their own benchmark problems. It is however unclear how useful this\nwould be to others as there is no code available and the details of\nthe implementation are less than complete. Furthermore, the method\ndepends on many specific tuning parameters whose tuning method is not\nfully defined, leaving it unclear how to guarantee the generalisation\nof the good performance.\n"
  },
  {
    "people": [
      "Blendenpick",
      "Avron",
      "Michael Mahoney",
      "Petros Drineas"
    ],
    "review": "The reviewers largely agree that this paper is well written and presents an interesting, novel approach to parallelizing Stochastic Gradient Descent. However, the current formulation is restricted to linear regression models and requires sketching techniques to handle large number of features, although it is striking that very aggressive sketching (k~10) still works well. In this setting though, there are specialized randomized solvers such as Blendenpick (Avron et al) which sketch the data upfront to construct a high quality pre-conditioner for use with iterative methods. \n The authors are encouraged to either compare with state of the art parallel randomized least squares solvers developed in the numerical linear algebra community (see papers by Michael Mahoney, Petros Drineas and others), or broaden the scope of the proposed methods to include models of current interest (e.g., DNNs). The latter would of course be of specific interest to the ICLR community."
  },
  {
    "people": [
      "Blendenpick",
      "Avron",
      "Michael Mahoney",
      "Petros Drineas"
    ],
    "review": "The reviewers largely agree that this paper is well written and presents an interesting, novel approach to parallelizing Stochastic Gradient Descent. However, the current formulation is restricted to linear regression models and requires sketching techniques to handle large number of features, although it is striking that very aggressive sketching (k~10) still works well. In this setting though, there are specialized randomized solvers such as Blendenpick (Avron et al) which sketch the data upfront to construct a high quality pre-conditioner for use with iterative methods. \n The authors are encouraged to either compare with state of the art parallel randomized least squares solvers developed in the numerical linear algebra community (see papers by Michael Mahoney, Petros Drineas and others), or broaden the scope of the proposed methods to include models of current interest (e.g., DNNs). The latter would of course be of specific interest to the ICLR community."
  },
  {
    "people": [
      "Stein"
    ],
    "review": "This paper considers the energy-based model interpretation of GAN, where the discriminator is an unnormalized model for the likelihood of a generative model p(x|theta) and the generator is a directed model that approximates this distribution. The generator is used to draw approximate negative phase samples that are used in stochastic maximum likelihood / contrastive divergence learning of the EBM / discriminator.\n\nThe main idea in the paper is to fit the generator by following the Stein variational gradient. In practice this gradient consists of the usual gradient provided by the discriminator with an added term that provides a repulsive force between the sampled data points to increase sample diversity.\n\nThe idea of using a kernel to push apart the sampled points is interesting, and will work in low dimensions, but it is hard to see how it can work in full scale images. For high dimensional samples x, the proposed kernel is unlikely to provide a useful distance measure between points. There are no convincing experiments in the paper that show otherwise. Specifically:\n\n- There is no experiment that compares between standard GAN and GAN + repulsion, using the same architecture. (please address this in the rebuttal)\n- If the Stein variational idea is taken literally, the right thing to do would be to fully optimize the generator at every step, and then taking a single optimization step on the discriminator. Instead, each is updated in turn, and the learning rates of both steps are adjusted to keep the two \"in line\".\n- The kernel used to fit the generator is defined in the auto-encoder space of the discriminator, and thus depends on the discriminator parameters. The objective that is used to fit the generator thus changes at every step, and the procedure can no longer be interpreted as stochastic gradient descent with respect to any single well defined objective.\n\nThe authors obtain good results: The generated images clearly look better than those generated by DCGAN. However, their approach has a number of changes compared to DCGAN, so it is not clear where the improvement comes from. In addition, by now the DCGAN is no longer a very strong baseline, as various other techniques have been proposed.\n\nNote: The use of phi for both the \"particle gradient direction\" and energy function is confusing"
  },
  {
    "people": [
      "Viterbi"
    ],
    "review": "We highly appreciate the time and feedback from all the reviewers, all of which we will take into serious consideration in our revision. We will particularly strengthen and clarify the empirical experiments. Below we address some of the major points: \n\n[Testing Accuracy Score]\nWe agree with the reviewers' point on the \"testing accuracy\" score, but think that it still provides some valuable insight about the dataset. Its blindness to the background can be a good thing in that it captures more information about the \"effective amount\" of objects the dataset contains.  The problem is that it is very difficult to obtain a *perfect* score, and reporting more than one metrics (in an objective fashion) can help to gain more comprehensive understandings. \n\n[Repulsive Term in High Dimension]\nOur repulsive force works due to two tricks: 1) scaling the bandwidth with the data diversity using the median trick, which alleviates the exponential decay of RBF kernel. 2) define kernel on the feature space instead of the raw pixels of the images, which allows us to respect the manifold structure of the images. The framework of SVGD allows us to use any positive definite kernels and change it adaptively during iterations, because the kernel only defines the \"tangent space\" for improvement. \n\nSteinGAN without kernel corresponds to Viterbi training of the energy model and we find it work well with careful tuning of parameters, but tend to converge to a small number of bad-looking images after running a large number of iterations; adding the kernel under the same setting helps prevent this problem. Our current results on CIFAR10 shows that SteinGAN without kernel gives an inception score of 6.34, while that SteinGAN with kernel gives 6.76. \n\n[Amortized is slower than non-amortized]\nAlthough the amortized algorithm has the overhead of updating $\\xi$, it stores the information in a generative network, and allows us to simulate as many images as we need. By using the one-step gradient update we proposed, the update of $\\xi$ is the same as standard backpropagation except replacing the Dlogp with the SVGD gradient. "
  },
  {
    "people": [
      "Li",
      "Li",
      "Li",
      "Swersky",
      "Zemel",
      "R."
    ],
    "review": "The authors propose amortized SVGD, an amortized form of prior work on SVGD, which is a particle variational method that maximally decreases the KL divergence at each update. \"amortized SVGD\" is done by training a neural network to learn this dynamic. They then apply this idea to train energy-based models, which admit a tractable unnormalized density.\n\nIn SVGD, the main difference from just MAP is the addition of a \"repulsive force\" that prevents degeneracy by encouraging probability mass to be spread to locations outside the mode. How this is able to still act as a strong enough entropy-like term in high dimensions is curious. From my understanding of their previous work, this was not a problem as the only experiments were on toy and UCI data sets.\n\nIn the experimental results here, they apply the kernel on the hidden representation of an autoencoder, which seems key, similar to Li et al. (2015) where their kernel approach for MMD would not work as well otherwise. However, unlike Li et al. (2015) the autoencoder is part of the model itself and not fixed. This breaks much of the authors' proposed motivation and criticisms of prior work, if they must autoencode onto some low-dimensional space (putting most effort then on the autoencoder, which changes per iteration) before then applying their method.\n\nUnlike previous literature which uses inference networks, their amortized SVGD approach seems in fact slower than the non-amortized approach. This is because they must make the actual update on xi before then regressing to perform the update on eta (in previous approaches, this would be like having to perform local inferences before then updating inference network parameters, or at least partially performing the local inference). This seems quite costly during training.\n\nI recommend the paper be rejected, and that the authors provide more comprehensive experimental results, expecially around the influence of the autoencoder, the incremental updates versus full updates, and the training time of amortized vs non-amortized approaches. The current results are promising but unclear why given the many knobs that the authors are playing with.\n\nReferences\n\nLi, Y., Swersky, K., & Zemel, R. (2015). Generative Moment Matching Networks. Presented at the International Conference on Machine Learning."
  },
  {
    "people": [
      "Stein"
    ],
    "review": "This paper considers the energy-based model interpretation of GAN, where the discriminator is an unnormalized model for the likelihood of a generative model p(x|theta) and the generator is a directed model that approximates this distribution. The generator is used to draw approximate negative phase samples that are used in stochastic maximum likelihood / contrastive divergence learning of the EBM / discriminator.\n\nThe main idea in the paper is to fit the generator by following the Stein variational gradient. In practice this gradient consists of the usual gradient provided by the discriminator with an added term that provides a repulsive force between the sampled data points to increase sample diversity.\n\nThe idea of using a kernel to push apart the sampled points is interesting, and will work in low dimensions, but it is hard to see how it can work in full scale images. For high dimensional samples x, the proposed kernel is unlikely to provide a useful distance measure between points. There are no convincing experiments in the paper that show otherwise. Specifically:\n\n- There is no experiment that compares between standard GAN and GAN + repulsion, using the same architecture. (please address this in the rebuttal)\n- If the Stein variational idea is taken literally, the right thing to do would be to fully optimize the generator at every step, and then taking a single optimization step on the discriminator. Instead, each is updated in turn, and the learning rates of both steps are adjusted to keep the two \"in line\".\n- The kernel used to fit the generator is defined in the auto-encoder space of the discriminator, and thus depends on the discriminator parameters. The objective that is used to fit the generator thus changes at every step, and the procedure can no longer be interpreted as stochastic gradient descent with respect to any single well defined objective.\n\nThe authors obtain good results: The generated images clearly look better than those generated by DCGAN. However, their approach has a number of changes compared to DCGAN, so it is not clear where the improvement comes from. In addition, by now the DCGAN is no longer a very strong baseline, as various other techniques have been proposed.\n\nNote: The use of phi for both the \"particle gradient direction\" and energy function is confusing"
  },
  {
    "people": [
      "Stein"
    ],
    "review": "This paper considers the energy-based model interpretation of GAN, where the discriminator is an unnormalized model for the likelihood of a generative model p(x|theta) and the generator is a directed model that approximates this distribution. The generator is used to draw approximate negative phase samples that are used in stochastic maximum likelihood / contrastive divergence learning of the EBM / discriminator.\n\nThe main idea in the paper is to fit the generator by following the Stein variational gradient. In practice this gradient consists of the usual gradient provided by the discriminator with an added term that provides a repulsive force between the sampled data points to increase sample diversity.\n\nThe idea of using a kernel to push apart the sampled points is interesting, and will work in low dimensions, but it is hard to see how it can work in full scale images. For high dimensional samples x, the proposed kernel is unlikely to provide a useful distance measure between points. There are no convincing experiments in the paper that show otherwise. Specifically:\n\n- There is no experiment that compares between standard GAN and GAN + repulsion, using the same architecture. (please address this in the rebuttal)\n- If the Stein variational idea is taken literally, the right thing to do would be to fully optimize the generator at every step, and then taking a single optimization step on the discriminator. Instead, each is updated in turn, and the learning rates of both steps are adjusted to keep the two \"in line\".\n- The kernel used to fit the generator is defined in the auto-encoder space of the discriminator, and thus depends on the discriminator parameters. The objective that is used to fit the generator thus changes at every step, and the procedure can no longer be interpreted as stochastic gradient descent with respect to any single well defined objective.\n\nThe authors obtain good results: The generated images clearly look better than those generated by DCGAN. However, their approach has a number of changes compared to DCGAN, so it is not clear where the improvement comes from. In addition, by now the DCGAN is no longer a very strong baseline, as various other techniques have been proposed.\n\nNote: The use of phi for both the \"particle gradient direction\" and energy function is confusing"
  },
  {
    "people": [
      "Viterbi"
    ],
    "review": "We highly appreciate the time and feedback from all the reviewers, all of which we will take into serious consideration in our revision. We will particularly strengthen and clarify the empirical experiments. Below we address some of the major points: \n\n[Testing Accuracy Score]\nWe agree with the reviewers' point on the \"testing accuracy\" score, but think that it still provides some valuable insight about the dataset. Its blindness to the background can be a good thing in that it captures more information about the \"effective amount\" of objects the dataset contains.  The problem is that it is very difficult to obtain a *perfect* score, and reporting more than one metrics (in an objective fashion) can help to gain more comprehensive understandings. \n\n[Repulsive Term in High Dimension]\nOur repulsive force works due to two tricks: 1) scaling the bandwidth with the data diversity using the median trick, which alleviates the exponential decay of RBF kernel. 2) define kernel on the feature space instead of the raw pixels of the images, which allows us to respect the manifold structure of the images. The framework of SVGD allows us to use any positive definite kernels and change it adaptively during iterations, because the kernel only defines the \"tangent space\" for improvement. \n\nSteinGAN without kernel corresponds to Viterbi training of the energy model and we find it work well with careful tuning of parameters, but tend to converge to a small number of bad-looking images after running a large number of iterations; adding the kernel under the same setting helps prevent this problem. Our current results on CIFAR10 shows that SteinGAN without kernel gives an inception score of 6.34, while that SteinGAN with kernel gives 6.76. \n\n[Amortized is slower than non-amortized]\nAlthough the amortized algorithm has the overhead of updating $\\xi$, it stores the information in a generative network, and allows us to simulate as many images as we need. By using the one-step gradient update we proposed, the update of $\\xi$ is the same as standard backpropagation except replacing the Dlogp with the SVGD gradient. "
  },
  {
    "people": [
      "Li",
      "Li",
      "Li",
      "Swersky",
      "Zemel",
      "R."
    ],
    "review": "The authors propose amortized SVGD, an amortized form of prior work on SVGD, which is a particle variational method that maximally decreases the KL divergence at each update. \"amortized SVGD\" is done by training a neural network to learn this dynamic. They then apply this idea to train energy-based models, which admit a tractable unnormalized density.\n\nIn SVGD, the main difference from just MAP is the addition of a \"repulsive force\" that prevents degeneracy by encouraging probability mass to be spread to locations outside the mode. How this is able to still act as a strong enough entropy-like term in high dimensions is curious. From my understanding of their previous work, this was not a problem as the only experiments were on toy and UCI data sets.\n\nIn the experimental results here, they apply the kernel on the hidden representation of an autoencoder, which seems key, similar to Li et al. (2015) where their kernel approach for MMD would not work as well otherwise. However, unlike Li et al. (2015) the autoencoder is part of the model itself and not fixed. This breaks much of the authors' proposed motivation and criticisms of prior work, if they must autoencode onto some low-dimensional space (putting most effort then on the autoencoder, which changes per iteration) before then applying their method.\n\nUnlike previous literature which uses inference networks, their amortized SVGD approach seems in fact slower than the non-amortized approach. This is because they must make the actual update on xi before then regressing to perform the update on eta (in previous approaches, this would be like having to perform local inferences before then updating inference network parameters, or at least partially performing the local inference). This seems quite costly during training.\n\nI recommend the paper be rejected, and that the authors provide more comprehensive experimental results, expecially around the influence of the autoencoder, the incremental updates versus full updates, and the training time of amortized vs non-amortized approaches. The current results are promising but unclear why given the many knobs that the authors are playing with.\n\nReferences\n\nLi, Y., Swersky, K., & Zemel, R. (2015). Generative Moment Matching Networks. Presented at the International Conference on Machine Learning."
  },
  {
    "people": [
      "Stein"
    ],
    "review": "This paper considers the energy-based model interpretation of GAN, where the discriminator is an unnormalized model for the likelihood of a generative model p(x|theta) and the generator is a directed model that approximates this distribution. The generator is used to draw approximate negative phase samples that are used in stochastic maximum likelihood / contrastive divergence learning of the EBM / discriminator.\n\nThe main idea in the paper is to fit the generator by following the Stein variational gradient. In practice this gradient consists of the usual gradient provided by the discriminator with an added term that provides a repulsive force between the sampled data points to increase sample diversity.\n\nThe idea of using a kernel to push apart the sampled points is interesting, and will work in low dimensions, but it is hard to see how it can work in full scale images. For high dimensional samples x, the proposed kernel is unlikely to provide a useful distance measure between points. There are no convincing experiments in the paper that show otherwise. Specifically:\n\n- There is no experiment that compares between standard GAN and GAN + repulsion, using the same architecture. (please address this in the rebuttal)\n- If the Stein variational idea is taken literally, the right thing to do would be to fully optimize the generator at every step, and then taking a single optimization step on the discriminator. Instead, each is updated in turn, and the learning rates of both steps are adjusted to keep the two \"in line\".\n- The kernel used to fit the generator is defined in the auto-encoder space of the discriminator, and thus depends on the discriminator parameters. The objective that is used to fit the generator thus changes at every step, and the procedure can no longer be interpreted as stochastic gradient descent with respect to any single well defined objective.\n\nThe authors obtain good results: The generated images clearly look better than those generated by DCGAN. However, their approach has a number of changes compared to DCGAN, so it is not clear where the improvement comes from. In addition, by now the DCGAN is no longer a very strong baseline, as various other techniques have been proposed.\n\nNote: The use of phi for both the \"particle gradient direction\" and energy function is confusing"
  },
  {
    "people": [
      "Jonschkowski",
      "Brock",
      "Lange"
    ],
    "review": "We would like to thank all reviewers for their thorough and helpful comments!\n\n1) Before we turn to the individual questions raised the reviewers, we would like to address the main issue that all reviewers raised, namely the relationship of our method to multi-task learning:\n\n\u201cThe authors state that the proposed method is orthogonal to multi-task learning though the end goal of learning to solve multiple tasks is the same.\u201d (AnonReviewer1)\n\u201cThe argument did not support the lack of comparison to multi-task joint-learning.\u201d (AnonReviewer2)\n\u201cLimiting the multi-task learning to be different to individual tasks rather than sharing and transferring knowledge between tasks\u201d (AnonReviewer 3)\n\nThe intro has been rewritten to clarify our motivation and how our work compares to multi-task learning. We completely agree that successful RL will require multi-task learning to share knowledge that generalizes over multiple tasks. \n\nBut there are sets of tasks that require multiple dedicated skills without sharing knowledge. For instance, in a video game, an agent have to achieve several subgoals (fight an enemy, avoid obstacles,...), each of these can be seen as individual task. Learning multiple, (sub-)policies dedicated to *different* tasks is a problem of its own right, as it faces significant theoretical issues, such as \u201ccatastrophic forgetting\u201d. We have elaborated on this argument in the introduction.\n\nSince there is few work approaching this problem in RL, our paper studies the question of how to learn fully independent policies for different tasks. We fully agree that future work will need to combine learning shared and separate representations but we regard our work on the  independent-policy multi-task RL problem as a contribution in itself. \n\nWe now reply to the individual comments raised by the reviewers.\n\n----\n\nAnonReviewer1\n\n1) \u201cReferences to other multi-task learning works, e.g. policy distillation and actor-mimic (both ICLR \u201916), may be appropriate as well.\u201d\n\nThank you for the pointers, we have integrated the two suggested papers in the related work of the paper.\n\n2) \u201cThe approach does not necessarily share information across tasks for better learning, and requires learning a different policy for each task\u201d\n\nThis is the very idea of the method proposed, we updated the introduction to clarify the reasons we focused on this approach.\n\n3) \u201cIn the second navigation scenario, only the state representation is qualitatively shown, not the resulting control policy nor any other learned state representations for comparison. Since the multi-task state representation learning approach is only useful if you can also learn control better, the paper should also evaluate on control, with the same comparisons as in the first experiment. Without this evaluation, the experiment is incomplete.\u201d\n\nWe agree and as mentioned before this was a preliminary and incomplete experiment, and we decided to remove it from the paper. \n\n4) \u201cLastly, to be on par with publications at a venue like ICLR, the method should be evaluated more thoroughly, on a wider range of set-ups [...]\u201d\n\nWe agree that it is beneficial to apply a method to a wider range of tasks. Yet, we chose to invest into rigorously evaluating the performance of the method on the chosen task, and provide a thorough argument why and how the method works. We believe that it will scale to a wider range of tasks, but we will have to address this in future work.\n\n5) \u201cCould this approach be combined with other state representation learning approaches? e.g. approaches that use an autoencoder.\u201d\n\nYes, in principle it would be possible to use other state representation learning objectives.  Note, however, that in the slot car racing scenario a PCA/auto-encoder loss will not perform as well as LRP, as it has will try to explain all variations in the observation, in particular the second slot car. This has been shown in our previous work (Jonschkowski & Brock 2015) and is also reflected in the performance of PCA in the slot-car experiment.\n\n6) \u201dOne additional useful comparison would be to evaluate performance in the single-task setting (e.g. only controlling the red car), as an upper bound on how well the policy should be able to perform. Does the learned multi-task policy reach the same level of performance? This upper bound will be tighter than the \u201cknown car position\u201d baseline (which is also useful in its own right).\u201d\n\nThank you for this suggestion; in our experiment, however, the performance of the car in the single-task setting is identical to the performance we see in the multi-task setting. The reason is that the task detector module has a very high accuracy (greater than 99%) for the slot car tasks, and in consequence, a separate policy for each slot car is learned.\n\n7) \u201cDoes the \u201cobservations\u201d baseline eventually reach the performance of the LRP approach? It would be useful to know if this approach simply speeds up learning (significantly) or if it enables better performance.\u201d\n\nOur experiments and previous work (Jonschkowski & Brock 2015) suggest that it will eventually reach the same performance with enough data, but for now, even in our largest experiments, we did not see it happening.\n\n8) \u201cIf there are aliasing issues with the images, why not just use higher resolution images?\u201d\n\nMainly computational reasons: we wanted to evaluate a wide variety of parameter settings and study their influence on our algorithm, yet we did not have the computational power to do this exhaustively on higher resolutions.\n\n---\n\nAnonReviewer2\n\n1) \u201cThe author simply stated again what they did for the modeling without counter the comparison to hard-gating, but it's probably less an issue compared to Question 1.\u201d\n\nWe are sorry that our answer in the pre-review phase did not address your question. We were trying to explain that our method is technically a soft gating, but effectively learns to perform hard gating. We are not sure how whether and how using a hard would influence the conclusion of the paper, and we are not aware of a way to implement a differentiable hard gate (if it is not differentiable, we cannot train it using backpropagation).\n\n---\n\nAnonReviewer3\n\n2) \u201cParameters choice is arbitrary (w parameters)\u201d\nThe weights w for the different methods are chosen as described in Jonschkowski & Brock 2015, by monitoring the gradient on a small part of the training set. The goal is to have gradients of the same magnitude for the different terms in the loss, so only relative weighting matters. Small changes to the parameters do not affect the method and there is no need for careful tuning.\n\n3) \u201cThe experiments could have been conducted using a standardized simulation tool such as OpenAI Gym to make it easy to compare.\u201d\nWe agree that evaluating experiments on a standardized tool such as OpenAI gym is a great idea. We want to point out, though, that the slot car racing task considered in the paper is a well-known task that has been evaluated in previous work, too, e.g. (Lange et al., 2012). Moreover, it is the simplest task that has the properties we are interested in this paper (non-overlapping tasks).\nBut we agree that that open simulation tools such as OpenAI gym are great and we will apply our method to these tasks in future work."
  },
  {
    "people": [
      "Jonschkowski",
      "Brock"
    ],
    "review": "This paper builds upon the method of Jonschkowski & Brock to learn state representations for multiple tasks, rather than a single task. The research direction of learning representations for multiple tasks is an interesting one, and largely unexplored. The approach in the paper is to learn a different representation for each task, and a different policy for each task, where the task is detected automatically and built into the neural network.\n\nThe authors state that the proposed method is orthogonal to multi-task learning, though the end goal of learning to solve multiple tasks is the same. It would be interesting and helpful to see more discussion on this point in the paper, as discussed in the pre-review question phase. References to other multi-task learning works, e.g. policy distillation and actor-mimic (both ICLR \u201916), may be appropriate as well.\n\nThe method proposes to jointly learn a task classifier with a state representation learner, by using a differentiable gating mechanism to control the flow of information. The paper proposes a task coherence prior for this gating mechanism to ensure that the learned task classifier is temporally coherent. Introducing this structure is what enables the method to improve performance over the standard, non-multitask approach.\n\nThe evaluation involves two toy experimental scenarios. The first involves controlling one of two cars to drive around a track. In this task, detecting the \u201ctask\u201d is very easy, and the learned state representation is linear in the observation. The paper evaluates the performance of the policies learned with the proposed approach, and shows sufficient comparisons to demonstrate the usefulness of the approach over a standard non-multitask set-up.\n\nIn the second navigation scenario, only the state representation is qualitatively shown, not the resulting control policy nor any other learned state representations for comparison. Since the multi-task state representation learning approach is only useful if you can also learn control better, the paper should also evaluate on control, with the same comparisons as in the first experiment. Without this evaluation, the experiment is incomplete.\n\nLastly, to be on par with publications at a venue like ICLR, the method should be evaluated more thoroughly, on a wider range of set-ups, to demonstrate the generality of the approach and show that the method applies to more complex tasks. While in theory, the method should scale, the experiments do not demonstrate that it can handle more realistic scenarios, such as scaling beyond MNIST-level images, to 3D or real images, or higher-dimensional control tasks. Evaluating the method in this more complex scenario is important, because unexpected issues can come up when trying to scale. If scaling-up is straight-forward, then running this experiment (and including it in the paper) should be straight-forward.\n\nIn summary, here are the pros and cons of this paper:\nCons\n- The approach does not necessarily share information across tasks for better learning, and requires learning a different policy for each task\n- Only one experimental set-up that evaluates learned policy with multi-task state representation\n- No experiments on more realistic scenarios, such 3D environments or high-dimensional control problems\nPros: \n- This approach enables using the same network for multiple tasks, which is often not true for transfer and multi-task learning approaches\n- Novel way to learn a single policy for multiple tasks, including a task coherence prior which ensures that the task classification is meaningful\n- Experimentally validated on two toy tasks. One task shows improvement over baseline approaches\n\nThus, my rating would be higher if the paper included an evaluation of the control policy for navigation and included another more challenging and compelling scenario.\n\n\nLastly, here are some minor comments/questions on how I think the paper could be improved, but are not as important as the above:\n\nApproach:\nCould this approach be combined with other state representation learning approaches? e.g. approaches that use an autoencoder.\n\nExperiments:\nOne additional useful comparison would be to evaluate performance in the single-task setting (e.g. only controlling the red car), as an upper bound on how well the policy should be able to perform. Does the learned multi-task policy reach the same level of performance? This upper bound will be tighter than the \u201cknown car position\u201d baseline (which is also useful in its own right).\n\nDoes the \u201cobservations\u201d baseline eventually reach the performance of the LRP approach? It would be useful to know if this approach simply speeds up learning (significantly) or if it enables better performance.\n\nIf there are aliasing issues with the images, why not just use higher resolution images?"
  },
  {
    "people": [
      "Jonschkowski",
      "Brock",
      "Lange"
    ],
    "review": "We would like to thank all reviewers for their thorough and helpful comments!\n\n1) Before we turn to the individual questions raised the reviewers, we would like to address the main issue that all reviewers raised, namely the relationship of our method to multi-task learning:\n\n\u201cThe authors state that the proposed method is orthogonal to multi-task learning though the end goal of learning to solve multiple tasks is the same.\u201d (AnonReviewer1)\n\u201cThe argument did not support the lack of comparison to multi-task joint-learning.\u201d (AnonReviewer2)\n\u201cLimiting the multi-task learning to be different to individual tasks rather than sharing and transferring knowledge between tasks\u201d (AnonReviewer 3)\n\nThe intro has been rewritten to clarify our motivation and how our work compares to multi-task learning. We completely agree that successful RL will require multi-task learning to share knowledge that generalizes over multiple tasks. \n\nBut there are sets of tasks that require multiple dedicated skills without sharing knowledge. For instance, in a video game, an agent have to achieve several subgoals (fight an enemy, avoid obstacles,...), each of these can be seen as individual task. Learning multiple, (sub-)policies dedicated to *different* tasks is a problem of its own right, as it faces significant theoretical issues, such as \u201ccatastrophic forgetting\u201d. We have elaborated on this argument in the introduction.\n\nSince there is few work approaching this problem in RL, our paper studies the question of how to learn fully independent policies for different tasks. We fully agree that future work will need to combine learning shared and separate representations but we regard our work on the  independent-policy multi-task RL problem as a contribution in itself. \n\nWe now reply to the individual comments raised by the reviewers.\n\n----\n\nAnonReviewer1\n\n1) \u201cReferences to other multi-task learning works, e.g. policy distillation and actor-mimic (both ICLR \u201916), may be appropriate as well.\u201d\n\nThank you for the pointers, we have integrated the two suggested papers in the related work of the paper.\n\n2) \u201cThe approach does not necessarily share information across tasks for better learning, and requires learning a different policy for each task\u201d\n\nThis is the very idea of the method proposed, we updated the introduction to clarify the reasons we focused on this approach.\n\n3) \u201cIn the second navigation scenario, only the state representation is qualitatively shown, not the resulting control policy nor any other learned state representations for comparison. Since the multi-task state representation learning approach is only useful if you can also learn control better, the paper should also evaluate on control, with the same comparisons as in the first experiment. Without this evaluation, the experiment is incomplete.\u201d\n\nWe agree and as mentioned before this was a preliminary and incomplete experiment, and we decided to remove it from the paper. \n\n4) \u201cLastly, to be on par with publications at a venue like ICLR, the method should be evaluated more thoroughly, on a wider range of set-ups [...]\u201d\n\nWe agree that it is beneficial to apply a method to a wider range of tasks. Yet, we chose to invest into rigorously evaluating the performance of the method on the chosen task, and provide a thorough argument why and how the method works. We believe that it will scale to a wider range of tasks, but we will have to address this in future work.\n\n5) \u201cCould this approach be combined with other state representation learning approaches? e.g. approaches that use an autoencoder.\u201d\n\nYes, in principle it would be possible to use other state representation learning objectives.  Note, however, that in the slot car racing scenario a PCA/auto-encoder loss will not perform as well as LRP, as it has will try to explain all variations in the observation, in particular the second slot car. This has been shown in our previous work (Jonschkowski & Brock 2015) and is also reflected in the performance of PCA in the slot-car experiment.\n\n6) \u201dOne additional useful comparison would be to evaluate performance in the single-task setting (e.g. only controlling the red car), as an upper bound on how well the policy should be able to perform. Does the learned multi-task policy reach the same level of performance? This upper bound will be tighter than the \u201cknown car position\u201d baseline (which is also useful in its own right).\u201d\n\nThank you for this suggestion; in our experiment, however, the performance of the car in the single-task setting is identical to the performance we see in the multi-task setting. The reason is that the task detector module has a very high accuracy (greater than 99%) for the slot car tasks, and in consequence, a separate policy for each slot car is learned.\n\n7) \u201cDoes the \u201cobservations\u201d baseline eventually reach the performance of the LRP approach? It would be useful to know if this approach simply speeds up learning (significantly) or if it enables better performance.\u201d\n\nOur experiments and previous work (Jonschkowski & Brock 2015) suggest that it will eventually reach the same performance with enough data, but for now, even in our largest experiments, we did not see it happening.\n\n8) \u201cIf there are aliasing issues with the images, why not just use higher resolution images?\u201d\n\nMainly computational reasons: we wanted to evaluate a wide variety of parameter settings and study their influence on our algorithm, yet we did not have the computational power to do this exhaustively on higher resolutions.\n\n---\n\nAnonReviewer2\n\n1) \u201cThe author simply stated again what they did for the modeling without counter the comparison to hard-gating, but it's probably less an issue compared to Question 1.\u201d\n\nWe are sorry that our answer in the pre-review phase did not address your question. We were trying to explain that our method is technically a soft gating, but effectively learns to perform hard gating. We are not sure how whether and how using a hard would influence the conclusion of the paper, and we are not aware of a way to implement a differentiable hard gate (if it is not differentiable, we cannot train it using backpropagation).\n\n---\n\nAnonReviewer3\n\n2) \u201cParameters choice is arbitrary (w parameters)\u201d\nThe weights w for the different methods are chosen as described in Jonschkowski & Brock 2015, by monitoring the gradient on a small part of the training set. The goal is to have gradients of the same magnitude for the different terms in the loss, so only relative weighting matters. Small changes to the parameters do not affect the method and there is no need for careful tuning.\n\n3) \u201cThe experiments could have been conducted using a standardized simulation tool such as OpenAI Gym to make it easy to compare.\u201d\nWe agree that evaluating experiments on a standardized tool such as OpenAI gym is a great idea. We want to point out, though, that the slot car racing task considered in the paper is a well-known task that has been evaluated in previous work, too, e.g. (Lange et al., 2012). Moreover, it is the simplest task that has the properties we are interested in this paper (non-overlapping tasks).\nBut we agree that that open simulation tools such as OpenAI gym are great and we will apply our method to these tasks in future work."
  },
  {
    "people": [
      "Jonschkowski",
      "Brock"
    ],
    "review": "This paper builds upon the method of Jonschkowski & Brock to learn state representations for multiple tasks, rather than a single task. The research direction of learning representations for multiple tasks is an interesting one, and largely unexplored. The approach in the paper is to learn a different representation for each task, and a different policy for each task, where the task is detected automatically and built into the neural network.\n\nThe authors state that the proposed method is orthogonal to multi-task learning, though the end goal of learning to solve multiple tasks is the same. It would be interesting and helpful to see more discussion on this point in the paper, as discussed in the pre-review question phase. References to other multi-task learning works, e.g. policy distillation and actor-mimic (both ICLR \u201916), may be appropriate as well.\n\nThe method proposes to jointly learn a task classifier with a state representation learner, by using a differentiable gating mechanism to control the flow of information. The paper proposes a task coherence prior for this gating mechanism to ensure that the learned task classifier is temporally coherent. Introducing this structure is what enables the method to improve performance over the standard, non-multitask approach.\n\nThe evaluation involves two toy experimental scenarios. The first involves controlling one of two cars to drive around a track. In this task, detecting the \u201ctask\u201d is very easy, and the learned state representation is linear in the observation. The paper evaluates the performance of the policies learned with the proposed approach, and shows sufficient comparisons to demonstrate the usefulness of the approach over a standard non-multitask set-up.\n\nIn the second navigation scenario, only the state representation is qualitatively shown, not the resulting control policy nor any other learned state representations for comparison. Since the multi-task state representation learning approach is only useful if you can also learn control better, the paper should also evaluate on control, with the same comparisons as in the first experiment. Without this evaluation, the experiment is incomplete.\n\nLastly, to be on par with publications at a venue like ICLR, the method should be evaluated more thoroughly, on a wider range of set-ups, to demonstrate the generality of the approach and show that the method applies to more complex tasks. While in theory, the method should scale, the experiments do not demonstrate that it can handle more realistic scenarios, such as scaling beyond MNIST-level images, to 3D or real images, or higher-dimensional control tasks. Evaluating the method in this more complex scenario is important, because unexpected issues can come up when trying to scale. If scaling-up is straight-forward, then running this experiment (and including it in the paper) should be straight-forward.\n\nIn summary, here are the pros and cons of this paper:\nCons\n- The approach does not necessarily share information across tasks for better learning, and requires learning a different policy for each task\n- Only one experimental set-up that evaluates learned policy with multi-task state representation\n- No experiments on more realistic scenarios, such 3D environments or high-dimensional control problems\nPros: \n- This approach enables using the same network for multiple tasks, which is often not true for transfer and multi-task learning approaches\n- Novel way to learn a single policy for multiple tasks, including a task coherence prior which ensures that the task classification is meaningful\n- Experimentally validated on two toy tasks. One task shows improvement over baseline approaches\n\nThus, my rating would be higher if the paper included an evaluation of the control policy for navigation and included another more challenging and compelling scenario.\n\n\nLastly, here are some minor comments/questions on how I think the paper could be improved, but are not as important as the above:\n\nApproach:\nCould this approach be combined with other state representation learning approaches? e.g. approaches that use an autoencoder.\n\nExperiments:\nOne additional useful comparison would be to evaluate performance in the single-task setting (e.g. only controlling the red car), as an upper bound on how well the policy should be able to perform. Does the learned multi-task policy reach the same level of performance? This upper bound will be tighter than the \u201cknown car position\u201d baseline (which is also useful in its own right).\n\nDoes the \u201cobservations\u201d baseline eventually reach the performance of the LRP approach? It would be useful to know if this approach simply speeds up learning (significantly) or if it enables better performance.\n\nIf there are aliasing issues with the images, why not just use higher resolution images?"
  },
  {
    "people": [
      "Wen",
      "Wen",
      "Tsung-Hsien",
      "Milica Gasic",
      "Nikola Mrksic",
      "Lina M. Rojas-Barahona",
      "Pei-Hao Su",
      "Stefan Ultes",
      "David Vandyke",
      "Steve Young"
    ],
    "review": "This paper extends neural conversational models into the batch reinforcement learning setting. The idea is that you can collect human scoring data for some responses from a dialogue model, however such scores are expensive. Thus, it is natural to use off-policy learning \u2013 training a base policy on unsupervised data, deploying that policy to collect human scores, and then learning off-line from those scores.\n\nWhile the overall contribution is modest (extending off-policy actor-critic to the application of dialogue generation), the approach is well-motivated, and the paper is written clearly and is easy to understand. \n\nMy main concern is that the primary dataset used (restaurant recommendations) is very small (6000 conversations). In fact, it is several orders of magnitude smaller than other datasets used in the literature (e.g. Twitter, the Ubuntu Dialogue Corpus) for dialogue generation. It is a bit surprising to me that RNN chatbots (with no additional structure) are able to generate reasonable utterances on such a small dataset. Wen et al. (2016) are able to do this on a similarly small restaurant dataset, but this is mostly because they map directly from dialogue states to surface form, rather than some embedding representation of the context. Thus, it remains to be seen if the approaches in this paper also result in improvements when much more unsupervised data is available.\n\nReferences:\n\nWen, Tsung-Hsien, Milica Gasic, Nikola Mrksic, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David Vandyke, and Steve Young. \"A Network-based End-to-End Trainable Task-oriented Dialogue System.\" arXiv preprint arXiv:1604.04562 (2016).\n"
  },
  {
    "people": [
      "Wen",
      "Wen",
      "Tsung-Hsien",
      "Milica Gasic",
      "Nikola Mrksic",
      "Lina M. Rojas-Barahona",
      "Pei-Hao Su",
      "Stefan Ultes",
      "David Vandyke",
      "Steve Young"
    ],
    "review": "This paper extends neural conversational models into the batch reinforcement learning setting. The idea is that you can collect human scoring data for some responses from a dialogue model, however such scores are expensive. Thus, it is natural to use off-policy learning \u2013 training a base policy on unsupervised data, deploying that policy to collect human scores, and then learning off-line from those scores.\n\nWhile the overall contribution is modest (extending off-policy actor-critic to the application of dialogue generation), the approach is well-motivated, and the paper is written clearly and is easy to understand. \n\nMy main concern is that the primary dataset used (restaurant recommendations) is very small (6000 conversations). In fact, it is several orders of magnitude smaller than other datasets used in the literature (e.g. Twitter, the Ubuntu Dialogue Corpus) for dialogue generation. It is a bit surprising to me that RNN chatbots (with no additional structure) are able to generate reasonable utterances on such a small dataset. Wen et al. (2016) are able to do this on a similarly small restaurant dataset, but this is mostly because they map directly from dialogue states to surface form, rather than some embedding representation of the context. Thus, it remains to be seen if the approaches in this paper also result in improvements when much more unsupervised data is available.\n\nReferences:\n\nWen, Tsung-Hsien, Milica Gasic, Nikola Mrksic, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David Vandyke, and Steve Young. \"A Network-based End-to-End Trainable Task-oriented Dialogue System.\" arXiv preprint arXiv:1604.04562 (2016).\n"
  },
  {
    "people": [
      "Kyuyeon Hwang",
      "Wonyong Sung",
      "Jonghong Kim",
      "Kyuyeon Hwang",
      "Wonyong Sung"
    ],
    "review": "I suggest to refer the following two papers.\n\n- Kyuyeon Hwang and Wonyong Sung. \"Fixed-point feedforward deep neural network design using weights +1, 0, and \u22121.\" 2014 IEEE Workshop on Signal Processing Systems (SiPS). IEEE, 2014.\n\n- Jonghong Kim, Kyuyeon Hwang, and Wonyong Sung. \"X1000 real-time phoneme recognition VLSI using feed-forward deep neural networks.\" 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014.\n\nThe retrain-based neural network quantization algorithm was first published in these two papers.\n\nThanks."
  },
  {
    "people": [
      "Kyuyeon Hwang",
      "Wonyong Sung",
      "Jonghong Kim",
      "Kyuyeon Hwang",
      "Wonyong Sung"
    ],
    "review": "I suggest to refer the following two papers.\n\n- Kyuyeon Hwang and Wonyong Sung. \"Fixed-point feedforward deep neural network design using weights +1, 0, and \u22121.\" 2014 IEEE Workshop on Signal Processing Systems (SiPS). IEEE, 2014.\n\n- Jonghong Kim, Kyuyeon Hwang, and Wonyong Sung. \"X1000 real-time phoneme recognition VLSI using feed-forward deep neural networks.\" 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014.\n\nThe retrain-based neural network quantization algorithm was first published in these two papers.\n\nThanks."
  },
  {
    "people": [
      "Sutton",
      "Barto"
    ],
    "review": "This paper investigates the use of eligibility traces with recurrent DQN agents. As in other recent work on deep RL, the forward view of Sutton and Barto is used to make eligibility traces practical to use with neural networks. Experiments on the Atari games Pong and Tennis show that traces work better than standard Q-learning.\n\nThe paper is well written and the use of traces in deep RL is indeed underexplored, but the experiments in the paper are too limited and do not answer the most interesting questions.\n\nAs pointed out in the questions, n-step returns have been shown to work better than 1-step returns both in the classical RL literature and more recently with deep networks. [1] shows that using n-step returns in the forward view with neural networks leads to big improvements on both Atari and TORCS. Their n-step Q-learning method also combines returns of different length in expectation, while traces do this explicitly. This paper does not compare traces with n-step returns and simply shows that traces used in the forward view help on two Atari games. This is not a very significant result. It would be much more interesting to see whether traces improve on what is already known to work well with neural networks.\n\nThe other claimed contribution of the paper is showing the strong effect of optimization. As with traces, I find it hard to make any conclusions from experiments on two games with fixed hyperparameter settings. This has already been demonstrated with much more thorough experiments in other papers. One could argue that these experiments show that importance of hyperparameter values and not of the optimization algorithm itself. Without tuning the optimization hyperparameters it's hard to claim anything about the relative merits of the methods.\n\n[1] \"Asynchronous Methods for Deep Reinforcement Learning\", ICML 2016."
  },
  {
    "people": [
      "Sutton",
      "Barto"
    ],
    "review": "This paper investigates the use of eligibility traces with recurrent DQN agents. As in other recent work on deep RL, the forward view of Sutton and Barto is used to make eligibility traces practical to use with neural networks. Experiments on the Atari games Pong and Tennis show that traces work better than standard Q-learning.\n\nThe paper is well written and the use of traces in deep RL is indeed underexplored, but the experiments in the paper are too limited and do not answer the most interesting questions.\n\nAs pointed out in the questions, n-step returns have been shown to work better than 1-step returns both in the classical RL literature and more recently with deep networks. [1] shows that using n-step returns in the forward view with neural networks leads to big improvements on both Atari and TORCS. Their n-step Q-learning method also combines returns of different length in expectation, while traces do this explicitly. This paper does not compare traces with n-step returns and simply shows that traces used in the forward view help on two Atari games. This is not a very significant result. It would be much more interesting to see whether traces improve on what is already known to work well with neural networks.\n\nThe other claimed contribution of the paper is showing the strong effect of optimization. As with traces, I find it hard to make any conclusions from experiments on two games with fixed hyperparameter settings. This has already been demonstrated with much more thorough experiments in other papers. One could argue that these experiments show that importance of hyperparameter values and not of the optimization algorithm itself. Without tuning the optimization hyperparameters it's hard to claim anything about the relative merits of the methods.\n\n[1] \"Asynchronous Methods for Deep Reinforcement Learning\", ICML 2016."
  },
  {
    "people": [
      "Sutton",
      "Barto"
    ],
    "review": "This paper investigates the use of eligibility traces with recurrent DQN agents. As in other recent work on deep RL, the forward view of Sutton and Barto is used to make eligibility traces practical to use with neural networks. Experiments on the Atari games Pong and Tennis show that traces work better than standard Q-learning.\n\nThe paper is well written and the use of traces in deep RL is indeed underexplored, but the experiments in the paper are too limited and do not answer the most interesting questions.\n\nAs pointed out in the questions, n-step returns have been shown to work better than 1-step returns both in the classical RL literature and more recently with deep networks. [1] shows that using n-step returns in the forward view with neural networks leads to big improvements on both Atari and TORCS. Their n-step Q-learning method also combines returns of different length in expectation, while traces do this explicitly. This paper does not compare traces with n-step returns and simply shows that traces used in the forward view help on two Atari games. This is not a very significant result. It would be much more interesting to see whether traces improve on what is already known to work well with neural networks.\n\nThe other claimed contribution of the paper is showing the strong effect of optimization. As with traces, I find it hard to make any conclusions from experiments on two games with fixed hyperparameter settings. This has already been demonstrated with much more thorough experiments in other papers. One could argue that these experiments show that importance of hyperparameter values and not of the optimization algorithm itself. Without tuning the optimization hyperparameters it's hard to claim anything about the relative merits of the methods.\n\n[1] \"Asynchronous Methods for Deep Reinforcement Learning\", ICML 2016."
  },
  {
    "people": [
      "Sutton",
      "Barto"
    ],
    "review": "This paper investigates the use of eligibility traces with recurrent DQN agents. As in other recent work on deep RL, the forward view of Sutton and Barto is used to make eligibility traces practical to use with neural networks. Experiments on the Atari games Pong and Tennis show that traces work better than standard Q-learning.\n\nThe paper is well written and the use of traces in deep RL is indeed underexplored, but the experiments in the paper are too limited and do not answer the most interesting questions.\n\nAs pointed out in the questions, n-step returns have been shown to work better than 1-step returns both in the classical RL literature and more recently with deep networks. [1] shows that using n-step returns in the forward view with neural networks leads to big improvements on both Atari and TORCS. Their n-step Q-learning method also combines returns of different length in expectation, while traces do this explicitly. This paper does not compare traces with n-step returns and simply shows that traces used in the forward view help on two Atari games. This is not a very significant result. It would be much more interesting to see whether traces improve on what is already known to work well with neural networks.\n\nThe other claimed contribution of the paper is showing the strong effect of optimization. As with traces, I find it hard to make any conclusions from experiments on two games with fixed hyperparameter settings. This has already been demonstrated with much more thorough experiments in other papers. One could argue that these experiments show that importance of hyperparameter values and not of the optimization algorithm itself. Without tuning the optimization hyperparameters it's hard to claim anything about the relative merits of the methods.\n\n[1] \"Asynchronous Methods for Deep Reinforcement Learning\", ICML 2016."
  },
  {
    "people": [
      "Tamara Berg\u2019s"
    ],
    "review": "The paper presents a large-scale visual search system for finding product images given a fashion item. The exploration is interesting and the paper does a nice job of discussing the challenges of operating in this domain. The proposed approach addresses several of the challenges. \n\nHowever, there are several concerns.\n\n1) The main concern is that there are no comparisons or even mentions of the work done by Tamara Berg\u2019s group on fashion recognition and fashion attributes, e.g., \n-  \u201cAutomatic Attribute Discovery and Characterization from Noisy Web Data\u201d ECCV 2010 \n- \u201cWhere to Buy It: Matching Street Clothing Photos in Online Shops\u201d ICCV 2015,\n- \u201cRetrieving Similar Styles to Parse Clothing, TPAMI 2014,\netc\nIt is difficult to show the contribution and novelty of this work without discussing and comparing with this extensive prior art.\n\n2) There are not enough details about the attribute dataset and the collection process. What is the source of the images? Are these clean product images or real-world images? How is the annotation done? What instructions are the annotators given? What annotations are being collected? I understand data statistics for example may be proprietary, but these kinds of qualitative details are important to understand the contributions of the paper. How can others compare to this work?\n\n3) There are some missing baselines. How do the results in Table 2 compare to simpler methods, e.g., the BM or CM methods described in the text?\n\nWhile the paper presents an interesting exploration, all these concerns would need to be addressed before the paper can be ready for publication."
  },
  {
    "people": [
      "Tamara Berg\u2019s"
    ],
    "review": "The paper presents a large-scale visual search system for finding product images given a fashion item. The exploration is interesting and the paper does a nice job of discussing the challenges of operating in this domain. The proposed approach addresses several of the challenges. \n\nHowever, there are several concerns.\n\n1) The main concern is that there are no comparisons or even mentions of the work done by Tamara Berg\u2019s group on fashion recognition and fashion attributes, e.g., \n-  \u201cAutomatic Attribute Discovery and Characterization from Noisy Web Data\u201d ECCV 2010 \n- \u201cWhere to Buy It: Matching Street Clothing Photos in Online Shops\u201d ICCV 2015,\n- \u201cRetrieving Similar Styles to Parse Clothing, TPAMI 2014,\netc\nIt is difficult to show the contribution and novelty of this work without discussing and comparing with this extensive prior art.\n\n2) There are not enough details about the attribute dataset and the collection process. What is the source of the images? Are these clean product images or real-world images? How is the annotation done? What instructions are the annotators given? What annotations are being collected? I understand data statistics for example may be proprietary, but these kinds of qualitative details are important to understand the contributions of the paper. How can others compare to this work?\n\n3) There are some missing baselines. How do the results in Table 2 compare to simpler methods, e.g., the BM or CM methods described in the text?\n\nWhile the paper presents an interesting exploration, all these concerns would need to be addressed before the paper can be ready for publication."
  },
  {
    "people": [
      "Gatys"
    ],
    "review": "The framework of Gatys et al. demonstrated that correlation statistics (empirical Gram matrices) of deep feature responses provide an excellent characterisation of visual textures. This paper investigates in detail which kind of deep or shallow networks may work well in this framework. One of the main findings is that that very shallow nets, consisting of a single filter bank with random weights, work surprisingly well, and for simple and regular textures may produce results which are visually superior to complex data-adapted filters such as the ones in networks like VGG-19. More broadly, the paper contains an interesting and informative discussion on the strength and limitations on such methods for texture synthesis.\n\nFigure 4 shows that the optimisation of images with respect to shallow filter banks may result in texture images that have a lower VGG-19 loss than optimising the VGG-19 objective directly. This is imputed to the difficulty of optimising the highly non-linear VGG-19 cost function, which is a reasonable explanation. In the new supplementary material, the authors show that better optimisation results can be obtained by initialising the VGG-19-based optimisation with the shallow network optimisation results, which is a useful complement to the original experiments.\n\nThe main limitation of the paper is that it does not systematically compare different methods against a quantifiable objective. It is trivial to define image statistics that would allow to simply generate an exact copy of any reference texture, hence with very good visual quality. Such trivial statistics would also be very shallow. The aim is instead to capture a texture distribution, and measuring how well a method meets this challenge remains an open problem. Hence, while the empirical results seem to confirm the intuition that simple statistics are good enough for texture synthesis both in terms of quality and diversity (when compared to more complex statistics), it is difficult to conclusively confirm that this is the case.\n\nThe authors indicate that diversity could be measured in terms of entropy. This is reasonable, but, as they acknowledge in their answers to questions, difficult to do in practice. Furthermore, this would still not account for the other aspect of the problem, namely visual quality. They also suggest to perform a psychophysical assessment, which may be the only practical way of addressing this problem, but deem that to be material for future work.\n\nOverall, since evaluation of image generation is such an hard problem, I think the paper still has sufficient strengths to warrant publication in ICLR. Still, some form of psychophysical assessment would be useful to confirm the intuitions that, at present, can only be obtained by inspecting the figure sin the paper and in the supplementary material."
  },
  {
    "people": [
      "Gatys"
    ],
    "review": "The framework of Gatys et al. demonstrated that correlation statistics (empirical Gram matrices) of deep feature responses provide an excellent characterisation of visual textures. This paper investigates in detail which kind of deep or shallow networks may work well in this framework. One of the main findings is that that very shallow nets, consisting of a single filter bank with random weights, work surprisingly well, and for simple and regular textures may produce results which are visually superior to complex data-adapted filters such as the ones in networks like VGG-19. More broadly, the paper contains an interesting and informative discussion on the strength and limitations on such methods for texture synthesis.\n\nFigure 4 shows that the optimisation of images with respect to shallow filter banks may result in texture images that have a lower VGG-19 loss than optimising the VGG-19 objective directly. This is imputed to the difficulty of optimising the highly non-linear VGG-19 cost function, which is a reasonable explanation. In the new supplementary material, the authors show that better optimisation results can be obtained by initialising the VGG-19-based optimisation with the shallow network optimisation results, which is a useful complement to the original experiments.\n\nThe main limitation of the paper is that it does not systematically compare different methods against a quantifiable objective. It is trivial to define image statistics that would allow to simply generate an exact copy of any reference texture, hence with very good visual quality. Such trivial statistics would also be very shallow. The aim is instead to capture a texture distribution, and measuring how well a method meets this challenge remains an open problem. Hence, while the empirical results seem to confirm the intuition that simple statistics are good enough for texture synthesis both in terms of quality and diversity (when compared to more complex statistics), it is difficult to conclusively confirm that this is the case.\n\nThe authors indicate that diversity could be measured in terms of entropy. This is reasonable, but, as they acknowledge in their answers to questions, difficult to do in practice. Furthermore, this would still not account for the other aspect of the problem, namely visual quality. They also suggest to perform a psychophysical assessment, which may be the only practical way of addressing this problem, but deem that to be material for future work.\n\nOverall, since evaluation of image generation is such an hard problem, I think the paper still has sufficient strengths to warrant publication in ICLR. Still, some form of psychophysical assessment would be useful to confirm the intuitions that, at present, can only be obtained by inspecting the figure sin the paper and in the supplementary material.\n\n\n\n"
  },
  {
    "people": [
      "Gatys"
    ],
    "review": "The framework of Gatys et al. demonstrated that correlation statistics (empirical Gram matrices) of deep feature responses provide an excellent characterisation of visual textures. This paper investigates in detail which kind of deep or shallow networks may work well in this framework. One of the main findings is that that very shallow nets, consisting of a single filter bank with random weights, work surprisingly well, and for simple and regular textures may produce results which are visually superior to complex data-adapted filters such as the ones in networks like VGG-19. More broadly, the paper contains an interesting and informative discussion on the strength and limitations on such methods for texture synthesis.\n\nFigure 4 shows that the optimisation of images with respect to shallow filter banks may result in texture images that have a lower VGG-19 loss than optimising the VGG-19 objective directly. This is imputed to the difficulty of optimising the highly non-linear VGG-19 cost function, which is a reasonable explanation. In the new supplementary material, the authors show that better optimisation results can be obtained by initialising the VGG-19-based optimisation with the shallow network optimisation results, which is a useful complement to the original experiments.\n\nThe main limitation of the paper is that it does not systematically compare different methods against a quantifiable objective. It is trivial to define image statistics that would allow to simply generate an exact copy of any reference texture, hence with very good visual quality. Such trivial statistics would also be very shallow. The aim is instead to capture a texture distribution, and measuring how well a method meets this challenge remains an open problem. Hence, while the empirical results seem to confirm the intuition that simple statistics are good enough for texture synthesis both in terms of quality and diversity (when compared to more complex statistics), it is difficult to conclusively confirm that this is the case.\n\nThe authors indicate that diversity could be measured in terms of entropy. This is reasonable, but, as they acknowledge in their answers to questions, difficult to do in practice. Furthermore, this would still not account for the other aspect of the problem, namely visual quality. They also suggest to perform a psychophysical assessment, which may be the only practical way of addressing this problem, but deem that to be material for future work.\n\nOverall, since evaluation of image generation is such an hard problem, I think the paper still has sufficient strengths to warrant publication in ICLR. Still, some form of psychophysical assessment would be useful to confirm the intuitions that, at present, can only be obtained by inspecting the figure sin the paper and in the supplementary material."
  },
  {
    "people": [
      "Gatys"
    ],
    "review": "The framework of Gatys et al. demonstrated that correlation statistics (empirical Gram matrices) of deep feature responses provide an excellent characterisation of visual textures. This paper investigates in detail which kind of deep or shallow networks may work well in this framework. One of the main findings is that that very shallow nets, consisting of a single filter bank with random weights, work surprisingly well, and for simple and regular textures may produce results which are visually superior to complex data-adapted filters such as the ones in networks like VGG-19. More broadly, the paper contains an interesting and informative discussion on the strength and limitations on such methods for texture synthesis.\n\nFigure 4 shows that the optimisation of images with respect to shallow filter banks may result in texture images that have a lower VGG-19 loss than optimising the VGG-19 objective directly. This is imputed to the difficulty of optimising the highly non-linear VGG-19 cost function, which is a reasonable explanation. In the new supplementary material, the authors show that better optimisation results can be obtained by initialising the VGG-19-based optimisation with the shallow network optimisation results, which is a useful complement to the original experiments.\n\nThe main limitation of the paper is that it does not systematically compare different methods against a quantifiable objective. It is trivial to define image statistics that would allow to simply generate an exact copy of any reference texture, hence with very good visual quality. Such trivial statistics would also be very shallow. The aim is instead to capture a texture distribution, and measuring how well a method meets this challenge remains an open problem. Hence, while the empirical results seem to confirm the intuition that simple statistics are good enough for texture synthesis both in terms of quality and diversity (when compared to more complex statistics), it is difficult to conclusively confirm that this is the case.\n\nThe authors indicate that diversity could be measured in terms of entropy. This is reasonable, but, as they acknowledge in their answers to questions, difficult to do in practice. Furthermore, this would still not account for the other aspect of the problem, namely visual quality. They also suggest to perform a psychophysical assessment, which may be the only practical way of addressing this problem, but deem that to be material for future work.\n\nOverall, since evaluation of image generation is such an hard problem, I think the paper still has sufficient strengths to warrant publication in ICLR. Still, some form of psychophysical assessment would be useful to confirm the intuitions that, at present, can only be obtained by inspecting the figure sin the paper and in the supplementary material.\n\n\n\n"
  },
  {
    "people": [
      "Pathak",
      "Radford",
      "Pathak",
      "Doersch",
      "Noroozi&Favaro",
      "Richard Zhang",
      "Pathak"
    ],
    "review": "After rebuttal:\n\nThanks for reporting the AlexNet results. The fact that they are not great is not so bad by itself, and as the authors mention, it would be interesting to understand why this happens. But the fact that these results  were not in the paper (and in fact still are not there) is disturbing. Moreover, some claims in the paper look wrong in the light of these results, for example:\n- \"This suggests that our gains stem from the CC-GAN method rather than the use of a better architecture.\"\n- \"Since discrimination of real/fake in-paintings is more closely related to the target task of object classification than extracting a feature representation suitable for in-filling, it is not surprising that we are able to exceed the performance of Pathak et al. (2016) on PASCAL classification.\"\n\nThese statements, and possibly other parts of the paper, have to be updated. I think the paper cannot be published in its current form. Perhaps after a revision.\n\n--------\nInitial review:\n\nThe paper demonstrates an application of generative adversarial networks (GAN) to unsupervised feature learning. The authors show that the representation learned by the discriminator of a conditional GAN trained for image inpainting performs well on image classification. As a side-effect, fairly convincing inpaintings are produced.\n\nThe proposed method combines two existing ideas: using the discriminator of a GAN as a feature learner [Radford et al. 2015] and performing unsupervised feature learning with image inpainting [Pathak et al. 2016]. Therefore conceptual novelty of the paper is limited. On the plus side, the authors implement their idea well and demonstrate state-of-the-art results on STL-10 and good results on Pascal VOC (although Pascal experiments are incomplete, see below). Overall, I am in the borderline mode, and I will gladly raise the score if the authors address my concerns regarding the experiments.\n\n1) Experimental evaluation on Pascal VOC is not quite satisfactory. Comparison with prior work is unfair because the network architecture used by the authors (VGG) is different from the architecture used by all existing methods (AlexNet). It is great that the authors do not try to hide this fact in the paper, but I do not understand why the authors are not willing to simply run their method with AlexNet architecture, although two commenters asked them to do so. Such an experiment would strongly support authors\u2019 claims. Current reasoning that \u201cwe thought it reasonable to use more current models while making the difference clear\u201d is not convincing. It is great that better architectures lead to better results, but it is also very important to properly compare to prior work. On a related topic, Doersch et al. also tried using VGG architecture, would it be possible to compare to that? Yet another question: why are you not comparing to [Noroozi&Favaro, ECCV 2016] ? I would also like the authors to address the comment by Richard Zhang.\n\n2) Qualitative inpainting results are incomplete: comparison with previous methods (for instance, [Pathak et al 2016]) is missing, and it is impossible to compare different versions of the proposed method because different images are used for different variants. I realize there may be too little space in the main paper to show all the results, but many more results should be shown in the supplementary material. Quantitative results are missing. Currently the inpainting results are just interesting pictures to look at, but they do not add as much to the paper as they could."
  },
  {
    "people": [
      "Pathak",
      "Radford",
      "Pathak",
      "Doersch",
      "Richard Zhang",
      "Pathak"
    ],
    "review": "\nAfter rebuttal:\n\nThanks for reporting the AlexNet results. The fact that they are not great is not so bad by itself, and as the authors mention, it would be interesting to understand why this happens. But the fact that these results  were not in the paper (and in fact still are not there) is disturbing. Moreover, some claims in the paper look wrong in the light of these results, for example:\n- \"This suggests that our gains stem from the CC-GAN method rather than the use of a better architecture.\"\n- \"Since discrimination of real/fake in-paintings is more closely related to the target task of object classification than extracting a feature representation suitable for in-filling, it is not surprising that we are able to exceed the performance of Pathak et al. (2016) on PASCAL classification.\"\n\nThese statements, and possibly other parts of the paper, have to be updated. I think the paper cannot be published in its current form. Perhaps after a revision.\n\n--------\nInitial review:\n\nThe paper demonstrates an application of generative adversarial networks (GAN) to unsupervised feature learning. The authors show that the representation learned by the discriminator of a conditional GAN trained for image inpainting performs well on image classification. As a side-effect, fairly convincing inpaintings are produced.\n\nThe proposed method combines two existing ideas: using the discriminator of a GAN as a feature learner [Radford et al. 2015] and performing unsupervised feature learning with image inpainting [Pathak et al. 2016]. Therefore conceptual novelty of the paper is limited. On the plus side, the authors implement their idea well and demonstrate state-of-the-art results on STL-10 and good results on Pascal VOC (although Pascal experiments are incomplete, see below). Overall, I am in the borderline mode, and I will gladly raise the score if the authors address my concerns regarding the experiments.\n\n1) Experimental evaluation on Pascal VOC is not quite satisfactory. Comparison with prior work is unfair because the network architecture used by the authors (VGG) is different from the architecture used by all existing methods (AlexNet). It is great that the authors do not try to hide this fact in the paper, but I do not understand why the authors are not willing to simply run their method with AlexNet architecture, although two commenters asked them to do so. Such an experiment would strongly support authors\u2019 claims. Current reasoning that \u201cwe thought it reasonable to use more current models while making the difference clear\u201d is not convincing. It is great that better architectures lead to better results, but it is also very important to properly compare to prior work. On a related topic, Doersch et al. also tried using VGG architecture, would it be possible to compare to that? Yet another question: why are you not comparing to [Noroozi&Favaro, ECCV 2016] ? I would also like the authors to address the comment by Richard Zhang.\n\n2) Qualitative inpainting results are incomplete: comparison with previous methods (for instance, [Pathak et al 2016]) is missing, and it is impossible to compare different versions of the proposed method because different images are used for different variants. I realize there may be too little space in the main paper to show all the results, but many more results should be shown in the supplementary material. Quantitative results are missing. Currently the inpainting results are just interesting pictures to look at, but they do not add as much to the paper as they could."
  },
  {
    "people": [
      "Pathak",
      "Radford",
      "Pathak",
      "Doersch",
      "Noroozi&Favaro",
      "Richard Zhang",
      "Pathak"
    ],
    "review": "After rebuttal:\n\nThanks for reporting the AlexNet results. The fact that they are not great is not so bad by itself, and as the authors mention, it would be interesting to understand why this happens. But the fact that these results  were not in the paper (and in fact still are not there) is disturbing. Moreover, some claims in the paper look wrong in the light of these results, for example:\n- \"This suggests that our gains stem from the CC-GAN method rather than the use of a better architecture.\"\n- \"Since discrimination of real/fake in-paintings is more closely related to the target task of object classification than extracting a feature representation suitable for in-filling, it is not surprising that we are able to exceed the performance of Pathak et al. (2016) on PASCAL classification.\"\n\nThese statements, and possibly other parts of the paper, have to be updated. I think the paper cannot be published in its current form. Perhaps after a revision.\n\n--------\nInitial review:\n\nThe paper demonstrates an application of generative adversarial networks (GAN) to unsupervised feature learning. The authors show that the representation learned by the discriminator of a conditional GAN trained for image inpainting performs well on image classification. As a side-effect, fairly convincing inpaintings are produced.\n\nThe proposed method combines two existing ideas: using the discriminator of a GAN as a feature learner [Radford et al. 2015] and performing unsupervised feature learning with image inpainting [Pathak et al. 2016]. Therefore conceptual novelty of the paper is limited. On the plus side, the authors implement their idea well and demonstrate state-of-the-art results on STL-10 and good results on Pascal VOC (although Pascal experiments are incomplete, see below). Overall, I am in the borderline mode, and I will gladly raise the score if the authors address my concerns regarding the experiments.\n\n1) Experimental evaluation on Pascal VOC is not quite satisfactory. Comparison with prior work is unfair because the network architecture used by the authors (VGG) is different from the architecture used by all existing methods (AlexNet). It is great that the authors do not try to hide this fact in the paper, but I do not understand why the authors are not willing to simply run their method with AlexNet architecture, although two commenters asked them to do so. Such an experiment would strongly support authors\u2019 claims. Current reasoning that \u201cwe thought it reasonable to use more current models while making the difference clear\u201d is not convincing. It is great that better architectures lead to better results, but it is also very important to properly compare to prior work. On a related topic, Doersch et al. also tried using VGG architecture, would it be possible to compare to that? Yet another question: why are you not comparing to [Noroozi&Favaro, ECCV 2016] ? I would also like the authors to address the comment by Richard Zhang.\n\n2) Qualitative inpainting results are incomplete: comparison with previous methods (for instance, [Pathak et al 2016]) is missing, and it is impossible to compare different versions of the proposed method because different images are used for different variants. I realize there may be too little space in the main paper to show all the results, but many more results should be shown in the supplementary material. Quantitative results are missing. Currently the inpainting results are just interesting pictures to look at, but they do not add as much to the paper as they could."
  },
  {
    "people": [
      "Pathak",
      "Radford",
      "Pathak",
      "Doersch",
      "Richard Zhang",
      "Pathak"
    ],
    "review": "\nAfter rebuttal:\n\nThanks for reporting the AlexNet results. The fact that they are not great is not so bad by itself, and as the authors mention, it would be interesting to understand why this happens. But the fact that these results  were not in the paper (and in fact still are not there) is disturbing. Moreover, some claims in the paper look wrong in the light of these results, for example:\n- \"This suggests that our gains stem from the CC-GAN method rather than the use of a better architecture.\"\n- \"Since discrimination of real/fake in-paintings is more closely related to the target task of object classification than extracting a feature representation suitable for in-filling, it is not surprising that we are able to exceed the performance of Pathak et al. (2016) on PASCAL classification.\"\n\nThese statements, and possibly other parts of the paper, have to be updated. I think the paper cannot be published in its current form. Perhaps after a revision.\n\n--------\nInitial review:\n\nThe paper demonstrates an application of generative adversarial networks (GAN) to unsupervised feature learning. The authors show that the representation learned by the discriminator of a conditional GAN trained for image inpainting performs well on image classification. As a side-effect, fairly convincing inpaintings are produced.\n\nThe proposed method combines two existing ideas: using the discriminator of a GAN as a feature learner [Radford et al. 2015] and performing unsupervised feature learning with image inpainting [Pathak et al. 2016]. Therefore conceptual novelty of the paper is limited. On the plus side, the authors implement their idea well and demonstrate state-of-the-art results on STL-10 and good results on Pascal VOC (although Pascal experiments are incomplete, see below). Overall, I am in the borderline mode, and I will gladly raise the score if the authors address my concerns regarding the experiments.\n\n1) Experimental evaluation on Pascal VOC is not quite satisfactory. Comparison with prior work is unfair because the network architecture used by the authors (VGG) is different from the architecture used by all existing methods (AlexNet). It is great that the authors do not try to hide this fact in the paper, but I do not understand why the authors are not willing to simply run their method with AlexNet architecture, although two commenters asked them to do so. Such an experiment would strongly support authors\u2019 claims. Current reasoning that \u201cwe thought it reasonable to use more current models while making the difference clear\u201d is not convincing. It is great that better architectures lead to better results, but it is also very important to properly compare to prior work. On a related topic, Doersch et al. also tried using VGG architecture, would it be possible to compare to that? Yet another question: why are you not comparing to [Noroozi&Favaro, ECCV 2016] ? I would also like the authors to address the comment by Richard Zhang.\n\n2) Qualitative inpainting results are incomplete: comparison with previous methods (for instance, [Pathak et al 2016]) is missing, and it is impossible to compare different versions of the proposed method because different images are used for different variants. I realize there may be too little space in the main paper to show all the results, but many more results should be shown in the supplementary material. Quantitative results are missing. Currently the inpainting results are just interesting pictures to look at, but they do not add as much to the paper as they could."
  },
  {
    "people": [
      "Richard"
    ],
    "review": "Dear reviewers, \n\nI added another dataset to the draft (appendix): SemEval 2014 Task 1 semantic relatedness SICK dataset, as several of the recent works the reviewers pointed out have been reported on this dataset. Despite its simplicity, Doc2VecC significantly out-performs the winning solutions of the competition and several baseline methods, noticeably the dependency-tree RNNs introduced in [1], which relies on additional dependency parsers to compose sentence vectors from word embeddings. The performance of Doc2VecC is comparable (slightly worse) than the LSTM based methods or skip-thought vectors on this dataset. On the other hand, it is much faster to train and test. As reported in the original paper, training of the skip-thought vector models on the book corpus dataset takes around 2 weeks on GPU. In contrast, it takes less than 2 hours to learn the embeddings for Doc2VecC on a desktop with Intel i7 2.2Ghz cpu.\n\nI also provided more insight on why Skip-thought vectors did not perform well on the movie review dataset in Section 4.2 (Accuracy). \n\nI would greatly appreciate it if you could take another look at revisions and provide me with some feedbacks. \n\n[1] Socher, Richard, et al. \"Grounded compositional semantics for finding and describing images with sentences.\" Transactions of the Association for Computational Linguistics 2 (2014): 207-218."
  },
  {
    "people": [
      "Lebret",
      "Collobert"
    ],
    "review": "This paper proposes learning document embeddings as a sum of the constituent word embeddings, which are jointly learned and randomly dropped out ('corrupted') during training. While none of the pieces of this model are particularly novel, the result is an efficient learning algorithm for document representation with good empirical performance.\n\nJoint training of word and document embeddings is not a new idea, nor is the idea of enforcing the document to be represented by the sum of its word embeddings (see, e.g. '\u201cThe Sum of Its Parts\u201d: Joint Learning of Word and Phrase Representations with Autoencoders' by Lebret and Collobert). Furthermore, the corruption mechanism is nothing other than traditional dropout on the input layer. Coupled with the word2vec-style loss and training methods, this paper offers little on the novelty front.\n\nOn the other hand, it is very efficient at generation time, requiring only an average of the word embeddings rather than a complicated inference step as in Doc2Vec. Moreover, by construction, the embedding captures salient global information about the document -- it captures specifically that information that aids in local-context prediction. For such a simple model, the performance on sentiment analysis and document classification is quite encouraging.\n\nOverall, despite the lack of novelty, the simplicity, efficiency, and performance of this model make it worthy of wider readership and study, and I recommend acceptance."
  },
  {
    "people": [
      "Richard"
    ],
    "review": "Dear reviewers, \n\nI added another dataset to the draft (appendix): SemEval 2014 Task 1 semantic relatedness SICK dataset, as several of the recent works the reviewers pointed out have been reported on this dataset. Despite its simplicity, Doc2VecC significantly out-performs the winning solutions of the competition and several baseline methods, noticeably the dependency-tree RNNs introduced in [1], which relies on additional dependency parsers to compose sentence vectors from word embeddings. The performance of Doc2VecC is comparable (slightly worse) than the LSTM based methods or skip-thought vectors on this dataset. On the other hand, it is much faster to train and test. As reported in the original paper, training of the skip-thought vector models on the book corpus dataset takes around 2 weeks on GPU. In contrast, it takes less than 2 hours to learn the embeddings for Doc2VecC on a desktop with Intel i7 2.2Ghz cpu.\n\nI also provided more insight on why Skip-thought vectors did not perform well on the movie review dataset in Section 4.2 (Accuracy). \n\nI would greatly appreciate it if you could take another look at revisions and provide me with some feedbacks. \n\n[1] Socher, Richard, et al. \"Grounded compositional semantics for finding and describing images with sentences.\" Transactions of the Association for Computational Linguistics 2 (2014): 207-218."
  },
  {
    "people": [
      "Lebret",
      "Collobert"
    ],
    "review": "This paper proposes learning document embeddings as a sum of the constituent word embeddings, which are jointly learned and randomly dropped out ('corrupted') during training. While none of the pieces of this model are particularly novel, the result is an efficient learning algorithm for document representation with good empirical performance.\n\nJoint training of word and document embeddings is not a new idea, nor is the idea of enforcing the document to be represented by the sum of its word embeddings (see, e.g. '\u201cThe Sum of Its Parts\u201d: Joint Learning of Word and Phrase Representations with Autoencoders' by Lebret and Collobert). Furthermore, the corruption mechanism is nothing other than traditional dropout on the input layer. Coupled with the word2vec-style loss and training methods, this paper offers little on the novelty front.\n\nOn the other hand, it is very efficient at generation time, requiring only an average of the word embeddings rather than a complicated inference step as in Doc2Vec. Moreover, by construction, the embedding captures salient global information about the document -- it captures specifically that information that aids in local-context prediction. For such a simple model, the performance on sentiment analysis and document classification is quite encouraging.\n\nOverall, despite the lack of novelty, the simplicity, efficiency, and performance of this model make it worthy of wider readership and study, and I recommend acceptance."
  },
  {
    "people": [
      "Melamud"
    ],
    "review": "This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity.\n\nAlthough the paper presents this notion as new, basically every pre-trained embedding (be it auto-encoders or word2vec) has been doing the same: representing items in a low-dimensional space that inherently encodes their similarities. Even when looking at the specific case of word/context embeddings, the method is not novel either: this method is almost identical to one of the similarity functions presented in \"A Simple Word Embedding Model for Lexical Substitution\" (Melamud et al., 2015). The novelty claim must be more accurate and position itself with respect to existing work.\n\nIn addition, I think the evaluation could be done better. There are plenty of benchmarks for word embeddings in context, for example: \n*"
  },
  {
    "people": [
      "Melamud"
    ],
    "review": "This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity.\n\nAlthough the paper presents this notion as new, basically every pre-trained embedding (be it auto-encoders or word2vec) has been doing the same: representing items in a low-dimensional space that inherently encodes their similarities. Even when looking at the specific case of word/context embeddings, the method is not novel either: this method is almost identical to one of the similarity functions presented in \"A Simple Word Embedding Model for Lexical Substitution\" (Melamud et al., 2015). The novelty claim must be more accurate and position itself with respect to existing work.\n\nIn addition, I think the evaluation could be done better. There are plenty of benchmarks for word embeddings in context, for example: \n* "
  },
  {
    "people": [
      "Neelakantan"
    ],
    "review": "This paper introduces a similarity encoder based on a standard feed-forward neural network with the aim of generating similarity-preserving embeddings. The approach is utilized to generate a simple extension of the CBOW word2vec model that transforms the learned embeddings by their average context vectors. Experiments are performed on an analogy task and named entity recognition.\n\nWhile this paper offers some reasonable intuitive arguments for why a feed-forward neural network can generate good similarity-preserving embeddings, the architecture and approach is far from novel. As far as I can tell, the model is nothing more than the most vanilla neural network trained with SGD on similarity signals.\n\nSlightly more original is the idea to use context embeddings to augment the expressive capacity of learned word representations. Of course, using explicit contextual information is not a new idea, especially for tasks like word sense disambiguation (see, e.g., 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space' by Neelakantan et al, which should also be cited), but the specific method used here is original, as far as I know.\n\nThe evaluation of the method is far from convincing. The corpora used to train the embeddings are far smaller than would ever be used in practice for unsupervised or semi-supervised embedding learning. The performance on the analogy task says little about the benefit of this method for larger corpora, and, as the authors mentioned in the comments, they expect \"the gain will be less significant, as the global context statistics brought in by the ConEc can also be picked up by word2vec with more training.\"\n\nThe argument can be made (and the authors do claim) that extrinsic evaluations are more important for real-world applications, so it is good to see experiments on NER. However, again the embeddings were trained on a very small corpus and I am not convinced that the observed benefit will persist when trained on larger corpora.\n\nOverall, I believe this paper offers little novelty and weak experimental evidence supporting its claims. I cannot recommend it for acceptance."
  },
  {
    "people": [
      "Melamud"
    ],
    "review": "This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity.\n\nAlthough the paper presents this notion as new, basically every pre-trained embedding (be it auto-encoders or word2vec) has been doing the same: representing items in a low-dimensional space that inherently encodes their similarities. Even when looking at the specific case of word/context embeddings, the method is not novel either: this method is almost identical to one of the similarity functions presented in \"A Simple Word Embedding Model for Lexical Substitution\" (Melamud et al., 2015). The novelty claim must be more accurate and position itself with respect to existing work.\n\nIn addition, I think the evaluation could be done better. There are plenty of benchmarks for word embeddings in context, for example: \n*"
  },
  {
    "people": [
      "Melamud"
    ],
    "review": "This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity.\n\nAlthough the paper presents this notion as new, basically every pre-trained embedding (be it auto-encoders or word2vec) has been doing the same: representing items in a low-dimensional space that inherently encodes their similarities. Even when looking at the specific case of word/context embeddings, the method is not novel either: this method is almost identical to one of the similarity functions presented in \"A Simple Word Embedding Model for Lexical Substitution\" (Melamud et al., 2015). The novelty claim must be more accurate and position itself with respect to existing work.\n\nIn addition, I think the evaluation could be done better. There are plenty of benchmarks for word embeddings in context, for example: \n* "
  },
  {
    "people": [
      "Neelakantan"
    ],
    "review": "This paper introduces a similarity encoder based on a standard feed-forward neural network with the aim of generating similarity-preserving embeddings. The approach is utilized to generate a simple extension of the CBOW word2vec model that transforms the learned embeddings by their average context vectors. Experiments are performed on an analogy task and named entity recognition.\n\nWhile this paper offers some reasonable intuitive arguments for why a feed-forward neural network can generate good similarity-preserving embeddings, the architecture and approach is far from novel. As far as I can tell, the model is nothing more than the most vanilla neural network trained with SGD on similarity signals.\n\nSlightly more original is the idea to use context embeddings to augment the expressive capacity of learned word representations. Of course, using explicit contextual information is not a new idea, especially for tasks like word sense disambiguation (see, e.g., 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space' by Neelakantan et al, which should also be cited), but the specific method used here is original, as far as I know.\n\nThe evaluation of the method is far from convincing. The corpora used to train the embeddings are far smaller than would ever be used in practice for unsupervised or semi-supervised embedding learning. The performance on the analogy task says little about the benefit of this method for larger corpora, and, as the authors mentioned in the comments, they expect \"the gain will be less significant, as the global context statistics brought in by the ConEc can also be picked up by word2vec with more training.\"\n\nThe argument can be made (and the authors do claim) that extrinsic evaluations are more important for real-world applications, so it is good to see experiments on NER. However, again the embeddings were trained on a very small corpus and I am not convinced that the observed benefit will persist when trained on larger corpora.\n\nOverall, I believe this paper offers little novelty and weak experimental evidence supporting its claims. I cannot recommend it for acceptance."
  },
  {
    "people": [
      "Li"
    ],
    "review": "This paper proposes the graph convolutional networks, motivated from approximating graph convolutions.  In one propagation step, what the model does can be simplified as, first linearly transform the node representations for each node, and then multiply the transformed node representations with the normalized affinity matrix (with self-connections added), and then pass through nonlinearity.\n\nThis model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines, outperforming them by a significant margin.  The evaluation of propagation model is also interesting, where different variants of the model and design decisions are evaluated and compared.\n\nIt is surprising that such a simple model works so much better than all the baselines.  Considering that the model used is just a two-layer model in most experiments, this is really surprising as a two-layer model is very local, and the output of a node can only be affected by nodes in a 2-hop neighborhood, and no longer range interactions can play any roles in this.  Since computation is quite efficient (sec. 6.3), I wonder if adding more layers helped anything or not.\n\nEven though motivated from graph convolutions, when simplified as the paper suggests, the operations the model does are quite simple.  Compared to Duvenaud et al. 2015 and Li et al. 2016, the proposed method is simpler and does almost strictly less things.  So how would the proposed GCN compare against these methods?\n\nOverall I think this model is simple, but the connection to graph convolutions is interesting, and the experiment results are quite good.  There are a few questions that still remain, but I feel this paper can be accepted."
  },
  {
    "people": [
      "Defferrard",
      "Qing Lu",
      "Lise Getoor",
      "Gideon S Mann",
      "Andrew McCallum",
      "David Jensen",
      "Jennifer Neville",
      "Brian Gallagher",
      "Joseph J Pfeiffer III",
      "Jennifer Neville",
      "Paul N Bennett",
      "Stephane Peters",
      "Ludovic Denoyer",
      "Patrick Gallinari"
    ],
    "review": "The paper introduces a method for semi-supervised learning in graphs that exploits the spectral structure of the graph in a convolutional NN implementation. The proposed algorithm has a limited complexity and it is shown to scale well on a large dataset. The comparison with baselines on different datasets show a clear jump of performance with the proposed method.\n\nThe paper is technically fine and clear, the algorithm seems to scale well, and the results on the different datasets compare very favorably with the different baselines. The algorithm is simple and training seems easy. Concerning the originality, the proposed algorithm is a simple adaptation of graph convolutional networks (ref Defferrard 2016 in the paper) to a semi-supervised transductive setting. This is clearly mentioned in the paper, but the authors could better highlight the differences and novelty wrt this reference paper. Also, there is no comparison with the family of iterative classifiers, which usually compare favorably, both in performance and training time, with regularization based approaches, although they are mostly used in inductive settings. Below are some references for this family of methods.\n\nThe authors mention that more complex filters could be learned by stacking layers but they limit their architecture to one hidden layer. They should comment on the interest of using more layers for graph classification.\n\n\nSome references on iterative classification Qing Lu and Lise Getoor. 2003. Link-based classification. In ICML, Vol. 3. 496\u2013503.\n\nGideon S Mann and Andrew McCallum. 2010. Generalized expectation criteria for semi-supervised learning with weakly labeled data. The Journal of Machine Learning Research 11 (2010), 955\u2013984.\nDavid Jensen, Jennifer Neville, and Brian Gallagher. 2004. Why collective inference improves relational classification. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 593\u2013598.\nJoseph J Pfeiffer III, Jennifer Neville, and Paul N Bennett. 2015. Overcoming Relational Learning Biases\nto Accurately Predict Preferences in Large Scale Networks. In Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 853\u2013\n863.\nStephane Peters, Ludovic Denoyer, and Patrick Gallinari. 2010. Iterative annotation of multi-relational social networks. In Advances in Social Networks Analysis and Mining (ASONAM), 2010 International Conference on. IEEE, 96\u2013103.\n"
  },
  {
    "people": [
      "Li"
    ],
    "review": "This paper proposes the graph convolutional networks, motivated from approximating graph convolutions.  In one propagation step, what the model does can be simplified as, first linearly transform the node representations for each node, and then multiply the transformed node representations with the normalized affinity matrix (with self-connections added), and then pass through nonlinearity.\n\nThis model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines, outperforming them by a significant margin.  The evaluation of propagation model is also interesting, where different variants of the model and design decisions are evaluated and compared.\n\nIt is surprising that such a simple model works so much better than all the baselines.  Considering that the model used is just a two-layer model in most experiments, this is really surprising as a two-layer model is very local, and the output of a node can only be affected by nodes in a 2-hop neighborhood, and no longer range interactions can play any roles in this.  Since computation is quite efficient (sec. 6.3), I wonder if adding more layers helped anything or not.\n\nEven though motivated from graph convolutions, when simplified as the paper suggests, the operations the model does are quite simple.  Compared to Duvenaud et al. 2015 and Li et al. 2016, the proposed method is simpler and does almost strictly less things.  So how would the proposed GCN compare against these methods?\n\nOverall I think this model is simple, but the connection to graph convolutions is interesting, and the experiment results are quite good.  There are a few questions that still remain, but I feel this paper can be accepted."
  },
  {
    "people": [
      "Li"
    ],
    "review": "This paper proposes the graph convolutional networks, motivated from approximating graph convolutions.  In one propagation step, what the model does can be simplified as, first linearly transform the node representations for each node, and then multiply the transformed node representations with the normalized affinity matrix (with self-connections added), and then pass through nonlinearity.\n\nThis model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines, outperforming them by a significant margin.  The evaluation of propagation model is also interesting, where different variants of the model and design decisions are evaluated and compared.\n\nIt is surprising that such a simple model works so much better than all the baselines.  Considering that the model used is just a two-layer model in most experiments, this is really surprising as a two-layer model is very local, and the output of a node can only be affected by nodes in a 2-hop neighborhood, and no longer range interactions can play any roles in this.  Since computation is quite efficient (sec. 6.3), I wonder if adding more layers helped anything or not.\n\nEven though motivated from graph convolutions, when simplified as the paper suggests, the operations the model does are quite simple.  Compared to Duvenaud et al. 2015 and Li et al. 2016, the proposed method is simpler and does almost strictly less things.  So how would the proposed GCN compare against these methods?\n\nOverall I think this model is simple, but the connection to graph convolutions is interesting, and the experiment results are quite good.  There are a few questions that still remain, but I feel this paper can be accepted."
  },
  {
    "people": [
      "Defferrard",
      "Qing Lu",
      "Lise Getoor",
      "Gideon S Mann",
      "Andrew McCallum",
      "David Jensen",
      "Jennifer Neville",
      "Brian Gallagher",
      "Joseph J Pfeiffer III",
      "Jennifer Neville",
      "Paul N Bennett",
      "Stephane Peters",
      "Ludovic Denoyer",
      "Patrick Gallinari"
    ],
    "review": "The paper introduces a method for semi-supervised learning in graphs that exploits the spectral structure of the graph in a convolutional NN implementation. The proposed algorithm has a limited complexity and it is shown to scale well on a large dataset. The comparison with baselines on different datasets show a clear jump of performance with the proposed method.\n\nThe paper is technically fine and clear, the algorithm seems to scale well, and the results on the different datasets compare very favorably with the different baselines. The algorithm is simple and training seems easy. Concerning the originality, the proposed algorithm is a simple adaptation of graph convolutional networks (ref Defferrard 2016 in the paper) to a semi-supervised transductive setting. This is clearly mentioned in the paper, but the authors could better highlight the differences and novelty wrt this reference paper. Also, there is no comparison with the family of iterative classifiers, which usually compare favorably, both in performance and training time, with regularization based approaches, although they are mostly used in inductive settings. Below are some references for this family of methods.\n\nThe authors mention that more complex filters could be learned by stacking layers but they limit their architecture to one hidden layer. They should comment on the interest of using more layers for graph classification.\n\n\nSome references on iterative classification Qing Lu and Lise Getoor. 2003. Link-based classification. In ICML, Vol. 3. 496\u2013503.\n\nGideon S Mann and Andrew McCallum. 2010. Generalized expectation criteria for semi-supervised learning with weakly labeled data. The Journal of Machine Learning Research 11 (2010), 955\u2013984.\nDavid Jensen, Jennifer Neville, and Brian Gallagher. 2004. Why collective inference improves relational classification. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 593\u2013598.\nJoseph J Pfeiffer III, Jennifer Neville, and Paul N Bennett. 2015. Overcoming Relational Learning Biases\nto Accurately Predict Preferences in Large Scale Networks. In Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 853\u2013\n863.\nStephane Peters, Ludovic Denoyer, and Patrick Gallinari. 2010. Iterative annotation of multi-relational social networks. In Advances in Social Networks Analysis and Mining (ASONAM), 2010 International Conference on. IEEE, 96\u2013103.\n"
  },
  {
    "people": [
      "Li"
    ],
    "review": "This paper proposes the graph convolutional networks, motivated from approximating graph convolutions.  In one propagation step, what the model does can be simplified as, first linearly transform the node representations for each node, and then multiply the transformed node representations with the normalized affinity matrix (with self-connections added), and then pass through nonlinearity.\n\nThis model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines, outperforming them by a significant margin.  The evaluation of propagation model is also interesting, where different variants of the model and design decisions are evaluated and compared.\n\nIt is surprising that such a simple model works so much better than all the baselines.  Considering that the model used is just a two-layer model in most experiments, this is really surprising as a two-layer model is very local, and the output of a node can only be affected by nodes in a 2-hop neighborhood, and no longer range interactions can play any roles in this.  Since computation is quite efficient (sec. 6.3), I wonder if adding more layers helped anything or not.\n\nEven though motivated from graph convolutions, when simplified as the paper suggests, the operations the model does are quite simple.  Compared to Duvenaud et al. 2015 and Li et al. 2016, the proposed method is simpler and does almost strictly less things.  So how would the proposed GCN compare against these methods?\n\nOverall I think this model is simple, but the connection to graph convolutions is interesting, and the experiment results are quite good.  There are a few questions that still remain, but I feel this paper can be accepted."
  },
  {
    "people": [
      "B. Amos",
      "J. Kolter"
    ],
    "review": "A layer wise optimization for CNNs with ReLU activations and max-pooling is proposed and shown to correspond to a series of latent structured SVM problems. Using CCCP style optimization a monotonic decrease of the overall objective function can be guaranteed.\n\nSummary:\n\u2014\u2014\u2014\nI think the discussed insights are very interesting but not presented convincingly. Firstly, claims are emphasized which are often violated in practice (e.g., no convergence guarantees due to mini-batches), statements could be validated more convincingly (e.g., is monotone convergence a curse or a blessing), the experimental evaluation should be extended. In summary, I think the paper requires some more attention to form a compelling story.\n\nQuality: I think some of the techniques could be described more carefully to better convey the intuition. At times apples are compared to oranges, e.g., back propagation is contrasted with CCCP.\nClarity: Some of the derivations and intuitions could be explained in more detail.\nOriginality: The suggested idea is reasonable albeit heuristics are required.\nSignificance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge at this point in time.\n\nDetails:\n\u2014\u2014\u2014\u2014\n1. I think the provided guarantees for the optimization procedure are certainly convenient theoretically but their practical relevance still needs to be demonstrated more convincingly, e.g., mini-batch optimization alleviates any form of monotonic decrease. Hence the emphasize in the paper is somewhat misguided according to my opinion and given he current experimental evaluation.\n\n2. In spirit similar is work by B. Amos and J. Kolter, Input-Convex Deep Networks ("
  },
  {
    "people": [
      "B. Amos",
      "J. Kolter"
    ],
    "review": "A layer wise optimization for CNNs with ReLU activations and max-pooling is proposed and shown to correspond to a series of latent structured SVM problems. Using CCCP style optimization a monotonic decrease of the overall objective function can be guaranteed.\n\nSummary:\n\u2014\u2014\u2014\nI think the discussed insights are very interesting but not presented convincingly. Firstly, claims are emphasized which are often violated in practice (e.g., no convergence guarantees due to mini-batches), statements could be validated more convincingly (e.g., is monotone convergence a curse or a blessing), the experimental evaluation should be extended. In summary, I think the paper requires some more attention to form a compelling story.\n\nQuality: I think some of the techniques could be described more carefully to better convey the intuition. At times apples are compared to oranges, e.g., back propagation is contrasted with CCCP.\nClarity: Some of the derivations and intuitions could be explained in more detail.\nOriginality: The suggested idea is reasonable albeit heuristics are required.\nSignificance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge at this point in time.\n\nDetails:\n\u2014\u2014\u2014\u2014\n1. I think the provided guarantees for the optimization procedure are certainly convenient theoretically but their practical relevance still needs to be demonstrated more convincingly, e.g., mini-batch optimization alleviates any form of monotonic decrease. Hence the emphasize in the paper is somewhat misguided according to my opinion and given he current experimental evaluation.\n\n2. In spirit similar is work by B. Amos and J. Kolter, Input-Convex Deep Networks ("
  },
  {
    "people": [
      "B. Amos",
      "J. Kolter"
    ],
    "review": "A layer wise optimization for CNNs with ReLU activations and max-pooling is proposed and shown to correspond to a series of latent structured SVM problems. Using CCCP style optimization a monotonic decrease of the overall objective function can be guaranteed.\n\nSummary:\n\u2014\u2014\u2014\nI think the discussed insights are very interesting but not presented convincingly. Firstly, claims are emphasized which are often violated in practice (e.g., no convergence guarantees due to mini-batches), statements could be validated more convincingly (e.g., is monotone convergence a curse or a blessing), the experimental evaluation should be extended. In summary, I think the paper requires some more attention to form a compelling story.\n\nQuality: I think some of the techniques could be described more carefully to better convey the intuition. At times apples are compared to oranges, e.g., back propagation is contrasted with CCCP.\nClarity: Some of the derivations and intuitions could be explained in more detail.\nOriginality: The suggested idea is reasonable albeit heuristics are required.\nSignificance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge at this point in time.\n\nDetails:\n\u2014\u2014\u2014\u2014\n1. I think the provided guarantees for the optimization procedure are certainly convenient theoretically but their practical relevance still needs to be demonstrated more convincingly, e.g., mini-batch optimization alleviates any form of monotonic decrease. Hence the emphasize in the paper is somewhat misguided according to my opinion and given he current experimental evaluation.\n\n2. In spirit similar is work by B. Amos and J. Kolter, Input-Convex Deep Networks ("
  },
  {
    "people": [
      "B. Amos",
      "J. Kolter"
    ],
    "review": "A layer wise optimization for CNNs with ReLU activations and max-pooling is proposed and shown to correspond to a series of latent structured SVM problems. Using CCCP style optimization a monotonic decrease of the overall objective function can be guaranteed.\n\nSummary:\n\u2014\u2014\u2014\nI think the discussed insights are very interesting but not presented convincingly. Firstly, claims are emphasized which are often violated in practice (e.g., no convergence guarantees due to mini-batches), statements could be validated more convincingly (e.g., is monotone convergence a curse or a blessing), the experimental evaluation should be extended. In summary, I think the paper requires some more attention to form a compelling story.\n\nQuality: I think some of the techniques could be described more carefully to better convey the intuition. At times apples are compared to oranges, e.g., back propagation is contrasted with CCCP.\nClarity: Some of the derivations and intuitions could be explained in more detail.\nOriginality: The suggested idea is reasonable albeit heuristics are required.\nSignificance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge at this point in time.\n\nDetails:\n\u2014\u2014\u2014\u2014\n1. I think the provided guarantees for the optimization procedure are certainly convenient theoretically but their practical relevance still needs to be demonstrated more convincingly, e.g., mini-batch optimization alleviates any form of monotonic decrease. Hence the emphasize in the paper is somewhat misguided according to my opinion and given he current experimental evaluation.\n\n2. In spirit similar is work by B. Amos and J. Kolter, Input-Convex Deep Networks ("
  },
  {
    "people": [
      "Du"
    ],
    "review": "This paper introduces a new reinforcement learning environment called \u00ab The Retro Learning Environment\u201d, that interfaces with the open-source LibRetro API to offer access to various emulators and associated games (i.e. similar to the Atari 2600 Arcade Learning Environment, but more generic). The first supported platform is the SNES, with 5 games (more consoles and games may be added later). Authors argue that SNES games pose more challenges than Atari\u2019s (due to more complex graphics, AI and game mechanics). Several DQN variants are evaluated in experiments, and it is also proposed to compare learning algorihms by letting them compete against each other in multiplayer games.\n\nI like the idea of going toward more complex games than those found on Atari 2600, and having an environment where new consoles and games can easily be added sounds promising. With OpenAI Universe and DeepMind Lab that just came out, though, I am not sure we really need another one right now. Especially since using ROMs of emulated games we do not own is technically illegal: it looks like this did not cause too much trouble for Atari but it might start raising eyebrows if the community moves to more advanced and recent games, especially some Nintendo still makes money from.\n\nBesides the introduction of the environment, it is good to have DQN benchmarks on five games, but this does not add a lot of value. The authors also mention as contribution \"A new benchmarking technique, allowing algorithms to compete against each other, rather than playing against the in-game AI\", but this seems a bit exaggerated to me: the idea of pitting AIs against each other has been at the core of many AI competitions for decades, so it is hardly something new. The finding that reinforcement learning algorithms tend to specialize to their opponent is also not particular surprising.\n\nOverall I believe this is an ok paper but I do not feel it brings enough to the table for a major conference. This does not mean, however, that this new environment won't find a spot in the (now somewhat crowded) space of game-playing frameworks.\n\nOther small comments:\n- There are lots of typos (way too many to mention them all)\n- It is said that Infinite Mario \"still serves as a benchmark platform\", however as far as I know it had to be shutdown due to Nintendo not being too happy about it\n- \"RLE requires an emulator and a computer version of the console game (ROM file) upon initialization rather than a ROM file only. The emulators are provided with RLE\" => how is that different from ALE that requires the emulator Stella which is also provided with ALE?\n- Why is there no DQN / DDDQN result on Super Mario?\n- It is not clear if Figure 2 displays the F-Zero results using reward shaping or not\n- The Du et al reference seems incomplete"
  },
  {
    "people": [
      "Du"
    ],
    "review": "This paper introduces a new reinforcement learning environment called \u00ab The Retro Learning Environment\u201d, that interfaces with the open-source LibRetro API to offer access to various emulators and associated games (i.e. similar to the Atari 2600 Arcade Learning Environment, but more generic). The first supported platform is the SNES, with 5 games (more consoles and games may be added later). Authors argue that SNES games pose more challenges than Atari\u2019s (due to more complex graphics, AI and game mechanics). Several DQN variants are evaluated in experiments, and it is also proposed to compare learning algorihms by letting them compete against each other in multiplayer games.\n\nI like the idea of going toward more complex games than those found on Atari 2600, and having an environment where new consoles and games can easily be added sounds promising. With OpenAI Universe and DeepMind Lab that just came out, though, I am not sure we really need another one right now. Especially since using ROMs of emulated games we do not own is technically illegal: it looks like this did not cause too much trouble for Atari but it might start raising eyebrows if the community moves to more advanced and recent games, especially some Nintendo still makes money from.\n\nBesides the introduction of the environment, it is good to have DQN benchmarks on five games, but this does not add a lot of value. The authors also mention as contribution \"A new benchmarking technique, allowing algorithms to compete against each other, rather than playing against the in-game AI\", but this seems a bit exaggerated to me: the idea of pitting AIs against each other has been at the core of many AI competitions for decades, so it is hardly something new. The finding that reinforcement learning algorithms tend to specialize to their opponent is also not particular surprising.\n\nOverall I believe this is an ok paper but I do not feel it brings enough to the table for a major conference. This does not mean, however, that this new environment won't find a spot in the (now somewhat crowded) space of game-playing frameworks.\n\nOther small comments:\n- There are lots of typos (way too many to mention them all)\n- It is said that Infinite Mario \"still serves as a benchmark platform\", however as far as I know it had to be shutdown due to Nintendo not being too happy about it\n- \"RLE requires an emulator and a computer version of the console game (ROM file) upon initialization rather than a ROM file only. The emulators are provided with RLE\" => how is that different from ALE that requires the emulator Stella which is also provided with ALE?\n- Why is there no DQN / DDDQN result on Super Mario?\n- It is not clear if Figure 2 displays the F-Zero results using reward shaping or not\n- The Du et al reference seems incomplete"
  },
  {
    "people": [
      "Mozer",
      "Mozer",
      "Smolensky",
      "Taylor"
    ],
    "review": "The paper introduces a new pruning method for neural networks based on the second-order Taylor expansion and compares the results against a first-order method and brute-force pruning. It performs experiments of the three methods on several toy examples - including a two-layer network on MNIST - and shows that the second-order method behaves much worse then the brute-force baseline. In addition, from the success of the brute-force pruning the authors conclude that the hypothesis of Mozer et al - that neurons either contribute to performance or cancel out the effect of other neurons - is probably correct.\n\nThe authors put in considerable effort to explain all details of the paper clearly and at length, so the content of the paper is accessible even to people novel to pruning methods. Additionally, the authors have very carefully answered all questions that were coming up through the pre-review and have been very responsive.\n\nMy major criticism is that the paper lacks focus, does not have a concrete conclusion and does not explain what it adds to the literature. To make this apparent, I here summarise each paragraph of the conclusion section:\n\nParagraph 1: We do not benchmark / Pruning methods do not fare well against brute-force baseline / Some evidence for hypothesis of Mozer & Smolensky, but further investigation needed\n\nParagraph 2: Introduced 2nd order Taylor method / Does not fare well against baseline\n\nParagraph 3: Re-training may help but is not fair\n\nParagraph 4: Brute-force can prune 40-70% in shallow networks\n\nParagraph 5: Brute-force less effective in deep networks\n\nParagraph 6: Not all neurons contribute equally to performance of network\n\nThe title of the paper and answers of the authors to the pre-review questions seemed to strongly suggest that the paper is not about the new second-order method, is not about benchmarking pruning algorithms but is instead about the learnt representations. But only two or three sentences in the conclusion, and no sentence in the part on results in the abstract, even refers to neural representations. In an answer to the pre-review questions the authors stated:\n\n> Furthermore, we do not have to accept the conclusion that re-training is a necessary part of pruning because a brute force search reveals that neurons can in fact be \n> pruned from trained networks in a piecemeal fashion with no retraining and minimal adverse effect on the overall performance of the network. This would be \n> impossible if neurons did not belong to the distinct classes we describe.\"\n\nBut this can already be concluded from the 2nd order method, which has a similar characteristic and is based on other 2nd order methods (not shown here). What is the motivation to introduce a new 2nd order method here?\n\nIn addition, some other minor conclusions about representations - in particular the cancellation effect - might be based on side-effects of the greedy serial pruning method. Optimally, one would need to consider all the different ways of pruning (which, of course, scales exponentially with the number of neurons and is computationally infeasible). Notably, the authors do consider this limitation in the context of conventional pruning methods in the conclusions: \"Third, we assumed that pruning could be done in a serial fashion [...]. We found that all of these assumptions are deeply flawed in the sense that the true relevance of a neuron can only be partially approximated [...] at certain stages of the pruning process\". But the brute-force pruning process is also serial - why is that not a problem?\n\nAll in all it is unclear to me what the paper adds: there are little conclusions regarding the learnt representations nor is there sufficient benchmarking against state-of-the-art pruning methods. I would suggest to focus the paper in the following way: first, use a state-of-the-art pruning method from the literature (that works without re-training) or do not use any other pruning methods besides brute-force (depending on whether you want to compare pruning methods against brute-force, or want to learn something about the learnt representations). In this way you need to write little about this second-order tuning methods, and readers are not so easily confused about the purpose of this paper (plus it will be considerably shorter!). Then concentrate on 2-layer MNIST and a deeper CIFAR10 network. Further focus the paper by adding an itemised list of the exact contributions that you make, and streamline the paper accordingly. These measures could strongly boost the impact of your work but will require a major revision.\n\nPS: I think the confusion starts with the following sentence in the abstract: \"In this work we set out to test several long-held hypothesis about neural network learning representations and numerical approaches to pruning.\" Both aspects are pretty orthogonal, but are completely mixed up in the paper."
  },
  {
    "people": [
      "Wen",
      "Wei",
      "Lebedev",
      "Vadim",
      "Victor Lempitsky",
      "Alvarez",
      "Jose M.",
      "Mathieu Salzmann"
    ],
    "review": "1) Wen, Wei, et al. \"Learning structured sparsity in deep neural networks.\" Advances in Neural Information Processing Systems. 2016.\n2) Lebedev, Vadim, and Victor Lempitsky. \"Fast convnets using group-wise brain damage.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.\n3) Alvarez, Jose M., and Mathieu Salzmann. \"Learning the Number of Neurons in Deep Networks.\" Advances in Neural Information Processing Systems. 2016."
  },
  {
    "people": [
      "Mozer",
      "Mozer",
      "Smolensky",
      "Taylor"
    ],
    "review": "The paper introduces a new pruning method for neural networks based on the second-order Taylor expansion and compares the results against a first-order method and brute-force pruning. It performs experiments of the three methods on several toy examples - including a two-layer network on MNIST - and shows that the second-order method behaves much worse then the brute-force baseline. In addition, from the success of the brute-force pruning the authors conclude that the hypothesis of Mozer et al - that neurons either contribute to performance or cancel out the effect of other neurons - is probably correct.\n\nThe authors put in considerable effort to explain all details of the paper clearly and at length, so the content of the paper is accessible even to people novel to pruning methods. Additionally, the authors have very carefully answered all questions that were coming up through the pre-review and have been very responsive.\n\nMy major criticism is that the paper lacks focus, does not have a concrete conclusion and does not explain what it adds to the literature. To make this apparent, I here summarise each paragraph of the conclusion section:\n\nParagraph 1: We do not benchmark / Pruning methods do not fare well against brute-force baseline / Some evidence for hypothesis of Mozer & Smolensky, but further investigation needed\n\nParagraph 2: Introduced 2nd order Taylor method / Does not fare well against baseline\n\nParagraph 3: Re-training may help but is not fair\n\nParagraph 4: Brute-force can prune 40-70% in shallow networks\n\nParagraph 5: Brute-force less effective in deep networks\n\nParagraph 6: Not all neurons contribute equally to performance of network\n\nThe title of the paper and answers of the authors to the pre-review questions seemed to strongly suggest that the paper is not about the new second-order method, is not about benchmarking pruning algorithms but is instead about the learnt representations. But only two or three sentences in the conclusion, and no sentence in the part on results in the abstract, even refers to neural representations. In an answer to the pre-review questions the authors stated:\n\n> Furthermore, we do not have to accept the conclusion that re-training is a necessary part of pruning because a brute force search reveals that neurons can in fact be \n> pruned from trained networks in a piecemeal fashion with no retraining and minimal adverse effect on the overall performance of the network. This would be \n> impossible if neurons did not belong to the distinct classes we describe.\"\n\nBut this can already be concluded from the 2nd order method, which has a similar characteristic and is based on other 2nd order methods (not shown here). What is the motivation to introduce a new 2nd order method here?\n\nIn addition, some other minor conclusions about representations - in particular the cancellation effect - might be based on side-effects of the greedy serial pruning method. Optimally, one would need to consider all the different ways of pruning (which, of course, scales exponentially with the number of neurons and is computationally infeasible). Notably, the authors do consider this limitation in the context of conventional pruning methods in the conclusions: \"Third, we assumed that pruning could be done in a serial fashion [...]. We found that all of these assumptions are deeply flawed in the sense that the true relevance of a neuron can only be partially approximated [...] at certain stages of the pruning process\". But the brute-force pruning process is also serial - why is that not a problem?\n\nAll in all it is unclear to me what the paper adds: there are little conclusions regarding the learnt representations nor is there sufficient benchmarking against state-of-the-art pruning methods. I would suggest to focus the paper in the following way: first, use a state-of-the-art pruning method from the literature (that works without re-training) or do not use any other pruning methods besides brute-force (depending on whether you want to compare pruning methods against brute-force, or want to learn something about the learnt representations). In this way you need to write little about this second-order tuning methods, and readers are not so easily confused about the purpose of this paper (plus it will be considerably shorter!). Then concentrate on 2-layer MNIST and a deeper CIFAR10 network. Further focus the paper by adding an itemised list of the exact contributions that you make, and streamline the paper accordingly. These measures could strongly boost the impact of your work but will require a major revision.\n\nPS: I think the confusion starts with the following sentence in the abstract: \"In this work we set out to test several long-held hypothesis about neural network learning representations and numerical approaches to pruning.\" Both aspects are pretty orthogonal, but are completely mixed up in the paper."
  },
  {
    "people": [
      "Mozer",
      "LeCun",
      "Hassibi",
      "Stork",
      "Taylor",
      "Taylor",
      "Taylor",
      "Taylor"
    ],
    "review": "Here are answers to some common questions the authors have been asked about the current work in the past by readers of the manuscript. We hope these will help clarify any other questions our reviewers/readers might have.\n\nQ: Why doesn't the paper present numerical comparision to state-of-the-art/recent pruning techniques?\n\nA: Under certain motivational assumptions, it is understandable to demand benchmarking comparisons against state-of-the-art methods, but this may be missing the fundamental purpose of the present research. Our investigation is intended less to propose a competing alternative to existing pruning techniques and more to shed light on the limitations of generally accepted approaches to pruning and the degree to which increased numbers of parameters affect learning representations in neural networks. The paper does talk about most, if not all popoular pruning techniques out there. In fact, we examined the literature for numerical methods to approximate the importance of network elements, and the widely-cited 1st & 2nd order techniques proposed by Mozer, LeCun, Hassibi, Stork, et al. provided our initial inspiration. This is the jumping off point for our research in terms of key insights.\n\nQ: The idea of using Taylor series approximations seems interesting but not really effective.\n\nA: It is not effective when used as a pruning technique but it is VERY effective to test out the effectiveness of existing pruning techniques, which is what we do here. We have mentioned it multiple times in the paper that the motivation behind this work is NOT to propose a new pruning technique that will outperform all other techniques out there but to tap into learning representations to see how effective our established techniques are when seen from the perspective of representations. The Taylor series approximations play an important role here. A lot of pruning techniques out there use 2nd Order error gradients and assume that using them is the most effective way to prune networks. We have conclusively proved using the Taylor series that this is very much not the case. Our results with the brute-force method show us that there is a much larger extent to which networks can be pruned. This makes for a great starting-off point for future research to find methods that can produce similar results.\n\nQ: Why did you decide in favor of sigmoid activation functions instead of something more recent and more popular like ReLUs? \n\nA: As mentioned above, the main contribution of this work is to demonstrate the feasibility of pruning entire neurons from trained networks, and offer novel insight on learning representations. We use Taylor methods to approximate the results achieved by the brute-force method but this is not an ideal solution to the problem, as we discuss. The 2nd order approximation technique will not work for ReLU networks because ReLUs do not have a 2nd derivative, unless we use the soft-plus function as a continuous approximation. Furthermore, due to the fact that we are approximating the error surface of a network element with respect to the output using a parabola, if there is no useful parabola to approximate this relationship, then the method breaks down. The derivatives of the activation function are simply parameters of the Taylor series. It doesn\u2019t cease to be a parabolic approximation or become more effective if we use a different doubly-differentiable activation function. \n\nQ: Why carry out your experiments on the MNIST dataset and not go for a larger and more practical image dataset?\n\nA: All experiments were necessarily carried out on optimally trained networks (not counting Section 4.5, which specifically examines non-optimally trained networks), so there is no way to improve them. We derived the algorithm assuming the well-studied sigmoid activation function. Furthermore, the MNIST dataset is a de-facto standard for demonstrating the potential of new techniques. A different dataset, task, activation function, or network architecture will not change the trends we see in the results but could make the results less interpretable. \n\nQ: The best setting is Iterative Re-ranking with Brute Force removal which is too expensive.\n\nA: The brute-force method is highly parallelizable, so time complexity is not necessarily a deal-breaker. Our focus is the proof of concept, and we intend to investigate potential speedups in future work. Also, since pruning is anyways a single step carried out after the training process is over (which usually takes orders of magnitude more time), this is potentially acceptable. \n\n\n"
  },
  {
    "people": [
      "Mozer",
      "Mozer",
      "Smolensky",
      "Taylor"
    ],
    "review": "The paper introduces a new pruning method for neural networks based on the second-order Taylor expansion and compares the results against a first-order method and brute-force pruning. It performs experiments of the three methods on several toy examples - including a two-layer network on MNIST - and shows that the second-order method behaves much worse then the brute-force baseline. In addition, from the success of the brute-force pruning the authors conclude that the hypothesis of Mozer et al - that neurons either contribute to performance or cancel out the effect of other neurons - is probably correct.\n\nThe authors put in considerable effort to explain all details of the paper clearly and at length, so the content of the paper is accessible even to people novel to pruning methods. Additionally, the authors have very carefully answered all questions that were coming up through the pre-review and have been very responsive.\n\nMy major criticism is that the paper lacks focus, does not have a concrete conclusion and does not explain what it adds to the literature. To make this apparent, I here summarise each paragraph of the conclusion section:\n\nParagraph 1: We do not benchmark / Pruning methods do not fare well against brute-force baseline / Some evidence for hypothesis of Mozer & Smolensky, but further investigation needed\n\nParagraph 2: Introduced 2nd order Taylor method / Does not fare well against baseline\n\nParagraph 3: Re-training may help but is not fair\n\nParagraph 4: Brute-force can prune 40-70% in shallow networks\n\nParagraph 5: Brute-force less effective in deep networks\n\nParagraph 6: Not all neurons contribute equally to performance of network\n\nThe title of the paper and answers of the authors to the pre-review questions seemed to strongly suggest that the paper is not about the new second-order method, is not about benchmarking pruning algorithms but is instead about the learnt representations. But only two or three sentences in the conclusion, and no sentence in the part on results in the abstract, even refers to neural representations. In an answer to the pre-review questions the authors stated:\n\n> Furthermore, we do not have to accept the conclusion that re-training is a necessary part of pruning because a brute force search reveals that neurons can in fact be \n> pruned from trained networks in a piecemeal fashion with no retraining and minimal adverse effect on the overall performance of the network. This would be \n> impossible if neurons did not belong to the distinct classes we describe.\"\n\nBut this can already be concluded from the 2nd order method, which has a similar characteristic and is based on other 2nd order methods (not shown here). What is the motivation to introduce a new 2nd order method here?\n\nIn addition, some other minor conclusions about representations - in particular the cancellation effect - might be based on side-effects of the greedy serial pruning method. Optimally, one would need to consider all the different ways of pruning (which, of course, scales exponentially with the number of neurons and is computationally infeasible). Notably, the authors do consider this limitation in the context of conventional pruning methods in the conclusions: \"Third, we assumed that pruning could be done in a serial fashion [...]. We found that all of these assumptions are deeply flawed in the sense that the true relevance of a neuron can only be partially approximated [...] at certain stages of the pruning process\". But the brute-force pruning process is also serial - why is that not a problem?\n\nAll in all it is unclear to me what the paper adds: there are little conclusions regarding the learnt representations nor is there sufficient benchmarking against state-of-the-art pruning methods. I would suggest to focus the paper in the following way: first, use a state-of-the-art pruning method from the literature (that works without re-training) or do not use any other pruning methods besides brute-force (depending on whether you want to compare pruning methods against brute-force, or want to learn something about the learnt representations). In this way you need to write little about this second-order tuning methods, and readers are not so easily confused about the purpose of this paper (plus it will be considerably shorter!). Then concentrate on 2-layer MNIST and a deeper CIFAR10 network. Further focus the paper by adding an itemised list of the exact contributions that you make, and streamline the paper accordingly. These measures could strongly boost the impact of your work but will require a major revision.\n\nPS: I think the confusion starts with the following sentence in the abstract: \"In this work we set out to test several long-held hypothesis about neural network learning representations and numerical approaches to pruning.\" Both aspects are pretty orthogonal, but are completely mixed up in the paper."
  },
  {
    "people": [
      "Wen",
      "Wei",
      "Lebedev",
      "Vadim",
      "Victor Lempitsky",
      "Alvarez",
      "Jose M.",
      "Mathieu Salzmann"
    ],
    "review": "1) Wen, Wei, et al. \"Learning structured sparsity in deep neural networks.\" Advances in Neural Information Processing Systems. 2016.\n2) Lebedev, Vadim, and Victor Lempitsky. \"Fast convnets using group-wise brain damage.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.\n3) Alvarez, Jose M., and Mathieu Salzmann. \"Learning the Number of Neurons in Deep Networks.\" Advances in Neural Information Processing Systems. 2016."
  },
  {
    "people": [
      "Mozer",
      "Mozer",
      "Smolensky",
      "Taylor"
    ],
    "review": "The paper introduces a new pruning method for neural networks based on the second-order Taylor expansion and compares the results against a first-order method and brute-force pruning. It performs experiments of the three methods on several toy examples - including a two-layer network on MNIST - and shows that the second-order method behaves much worse then the brute-force baseline. In addition, from the success of the brute-force pruning the authors conclude that the hypothesis of Mozer et al - that neurons either contribute to performance or cancel out the effect of other neurons - is probably correct.\n\nThe authors put in considerable effort to explain all details of the paper clearly and at length, so the content of the paper is accessible even to people novel to pruning methods. Additionally, the authors have very carefully answered all questions that were coming up through the pre-review and have been very responsive.\n\nMy major criticism is that the paper lacks focus, does not have a concrete conclusion and does not explain what it adds to the literature. To make this apparent, I here summarise each paragraph of the conclusion section:\n\nParagraph 1: We do not benchmark / Pruning methods do not fare well against brute-force baseline / Some evidence for hypothesis of Mozer & Smolensky, but further investigation needed\n\nParagraph 2: Introduced 2nd order Taylor method / Does not fare well against baseline\n\nParagraph 3: Re-training may help but is not fair\n\nParagraph 4: Brute-force can prune 40-70% in shallow networks\n\nParagraph 5: Brute-force less effective in deep networks\n\nParagraph 6: Not all neurons contribute equally to performance of network\n\nThe title of the paper and answers of the authors to the pre-review questions seemed to strongly suggest that the paper is not about the new second-order method, is not about benchmarking pruning algorithms but is instead about the learnt representations. But only two or three sentences in the conclusion, and no sentence in the part on results in the abstract, even refers to neural representations. In an answer to the pre-review questions the authors stated:\n\n> Furthermore, we do not have to accept the conclusion that re-training is a necessary part of pruning because a brute force search reveals that neurons can in fact be \n> pruned from trained networks in a piecemeal fashion with no retraining and minimal adverse effect on the overall performance of the network. This would be \n> impossible if neurons did not belong to the distinct classes we describe.\"\n\nBut this can already be concluded from the 2nd order method, which has a similar characteristic and is based on other 2nd order methods (not shown here). What is the motivation to introduce a new 2nd order method here?\n\nIn addition, some other minor conclusions about representations - in particular the cancellation effect - might be based on side-effects of the greedy serial pruning method. Optimally, one would need to consider all the different ways of pruning (which, of course, scales exponentially with the number of neurons and is computationally infeasible). Notably, the authors do consider this limitation in the context of conventional pruning methods in the conclusions: \"Third, we assumed that pruning could be done in a serial fashion [...]. We found that all of these assumptions are deeply flawed in the sense that the true relevance of a neuron can only be partially approximated [...] at certain stages of the pruning process\". But the brute-force pruning process is also serial - why is that not a problem?\n\nAll in all it is unclear to me what the paper adds: there are little conclusions regarding the learnt representations nor is there sufficient benchmarking against state-of-the-art pruning methods. I would suggest to focus the paper in the following way: first, use a state-of-the-art pruning method from the literature (that works without re-training) or do not use any other pruning methods besides brute-force (depending on whether you want to compare pruning methods against brute-force, or want to learn something about the learnt representations). In this way you need to write little about this second-order tuning methods, and readers are not so easily confused about the purpose of this paper (plus it will be considerably shorter!). Then concentrate on 2-layer MNIST and a deeper CIFAR10 network. Further focus the paper by adding an itemised list of the exact contributions that you make, and streamline the paper accordingly. These measures could strongly boost the impact of your work but will require a major revision.\n\nPS: I think the confusion starts with the following sentence in the abstract: \"In this work we set out to test several long-held hypothesis about neural network learning representations and numerical approaches to pruning.\" Both aspects are pretty orthogonal, but are completely mixed up in the paper."
  },
  {
    "people": [
      "Mozer",
      "LeCun",
      "Hassibi",
      "Stork",
      "Taylor",
      "Taylor",
      "Taylor",
      "Taylor"
    ],
    "review": "Here are answers to some common questions the authors have been asked about the current work in the past by readers of the manuscript. We hope these will help clarify any other questions our reviewers/readers might have.\n\nQ: Why doesn't the paper present numerical comparision to state-of-the-art/recent pruning techniques?\n\nA: Under certain motivational assumptions, it is understandable to demand benchmarking comparisons against state-of-the-art methods, but this may be missing the fundamental purpose of the present research. Our investigation is intended less to propose a competing alternative to existing pruning techniques and more to shed light on the limitations of generally accepted approaches to pruning and the degree to which increased numbers of parameters affect learning representations in neural networks. The paper does talk about most, if not all popoular pruning techniques out there. In fact, we examined the literature for numerical methods to approximate the importance of network elements, and the widely-cited 1st & 2nd order techniques proposed by Mozer, LeCun, Hassibi, Stork, et al. provided our initial inspiration. This is the jumping off point for our research in terms of key insights.\n\nQ: The idea of using Taylor series approximations seems interesting but not really effective.\n\nA: It is not effective when used as a pruning technique but it is VERY effective to test out the effectiveness of existing pruning techniques, which is what we do here. We have mentioned it multiple times in the paper that the motivation behind this work is NOT to propose a new pruning technique that will outperform all other techniques out there but to tap into learning representations to see how effective our established techniques are when seen from the perspective of representations. The Taylor series approximations play an important role here. A lot of pruning techniques out there use 2nd Order error gradients and assume that using them is the most effective way to prune networks. We have conclusively proved using the Taylor series that this is very much not the case. Our results with the brute-force method show us that there is a much larger extent to which networks can be pruned. This makes for a great starting-off point for future research to find methods that can produce similar results.\n\nQ: Why did you decide in favor of sigmoid activation functions instead of something more recent and more popular like ReLUs? \n\nA: As mentioned above, the main contribution of this work is to demonstrate the feasibility of pruning entire neurons from trained networks, and offer novel insight on learning representations. We use Taylor methods to approximate the results achieved by the brute-force method but this is not an ideal solution to the problem, as we discuss. The 2nd order approximation technique will not work for ReLU networks because ReLUs do not have a 2nd derivative, unless we use the soft-plus function as a continuous approximation. Furthermore, due to the fact that we are approximating the error surface of a network element with respect to the output using a parabola, if there is no useful parabola to approximate this relationship, then the method breaks down. The derivatives of the activation function are simply parameters of the Taylor series. It doesn\u2019t cease to be a parabolic approximation or become more effective if we use a different doubly-differentiable activation function. \n\nQ: Why carry out your experiments on the MNIST dataset and not go for a larger and more practical image dataset?\n\nA: All experiments were necessarily carried out on optimally trained networks (not counting Section 4.5, which specifically examines non-optimally trained networks), so there is no way to improve them. We derived the algorithm assuming the well-studied sigmoid activation function. Furthermore, the MNIST dataset is a de-facto standard for demonstrating the potential of new techniques. A different dataset, task, activation function, or network architecture will not change the trends we see in the results but could make the results less interpretable. \n\nQ: The best setting is Iterative Re-ranking with Brute Force removal which is too expensive.\n\nA: The brute-force method is highly parallelizable, so time complexity is not necessarily a deal-breaker. Our focus is the proof of concept, and we intend to investigate potential speedups in future work. Also, since pruning is anyways a single step carried out after the training process is over (which usually takes orders of magnitude more time), this is potentially acceptable. \n\n\n"
  },
  {
    "people": [
      "Goodfellow",
      "Goodfellow"
    ],
    "review": "The paper is dedicated to better understanding the optimization landscape in deep learning, in particular when explored with different optimization algorithms, and thus it also characterizes the behavior of these algorithms. It heavily re-uses the approach of Goodfellow et al. (2015). I find it hard to understand the contributions of the paper, for example: is it surprising that different algorithms reach different solutions when starting from the same initialization? It would be useful if the authors build such basic intuition in the paper. I also did not receive a clear answer to the question I posed to reviewers regarding clarifying how does the findings of the paper can contribute to future works on optimization in deep learning. And this is what I find fundamentally missing. So for example, there are probably plenty of ways to modify approach of Goodfellow et al. (2015), and similar works, and come up with interesting visualization methods for deep learning - but the question is: how is this helpful in terms of designing better algorithms, gaining more intuition how the optimization surface looks like in general, etc.? This is an interesting paper, though I am fairly confident it is a better fit for the journal than this conference. \n\nIt would be interesting and instructive, even for sanity check, to plot the eigenspectra of the solutions recovered by the algorithms to see the order of critical points recovered."
  },
  {
    "people": [
      "Goodfellow",
      "Goodfellow"
    ],
    "review": "The paper is dedicated to better understanding the optimization landscape in deep learning, in particular when explored with different optimization algorithms, and thus it also characterizes the behavior of these algorithms. It heavily re-uses the approach of Goodfellow et al. (2015). I find it hard to understand the contributions of the paper, for example: is it surprising that different algorithms reach different solutions when starting from the same initialization? It would be useful if the authors build such basic intuition in the paper. I also did not receive a clear answer to the question I posed to reviewers regarding clarifying how does the findings of the paper can contribute to future works on optimization in deep learning. And this is what I find fundamentally missing. So for example, there are probably plenty of ways to modify approach of Goodfellow et al. (2015), and similar works, and come up with interesting visualization methods for deep learning - but the question is: how is this helpful in terms of designing better algorithms, gaining more intuition how the optimization surface looks like in general, etc.? This is an interesting paper, though I am fairly confident it is a better fit for the journal than this conference. \n\nIt would be interesting and instructive, even for sanity check, to plot the eigenspectra of the solutions recovered by the algorithms to see the order of critical points recovered."
  },
  {
    "people": [
      "Leordeanu",
      "Revaud",
      "Sevilla-Lara",
      "Mathieu",
      "Brox"
    ],
    "review": "This paper describes an approach to predict (unseen) future frames of a video given a set of known past frames. The approach is based on a CNN that, in contrast to most related papers, work in the space of affine transformations (instead of pixels or flow). Said another way, the network takes as input a set of affine transforms that describe the motion of patches in the past frames, and likewise, outputs a set of affine transforms that predict future patch motion.\n\nTo that aim, the authors make a few simplifying hypotheses, namely, that a sequence of frames can be modeled accurately enough in their patch-affine framework. This is not unreasonable. A lot of papers in the optical flow community are based on similar hypotheses, i.e. model the flow as a smoothly varying affine field (for instance see \"Locally affine sparse-to-dense matching for motion and occlusion estimation\" by Leordeanu et al., \"EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow\" by Revaud et al., \"Optical Flow With Semantic Segmentation and Localized Layers\" by Sevilla-Lara et al.). These methods are state of the art, which gives a hint about the validity of this kind of approach. \nIn addition, it also seems very reasonable to reformulate the prediction task as predicting motion rather than predicting raw pixels. Indeed, the (patch-affine) motion space is considerably smaller than the image space, making the problem much more tractable and amenable to high-resolution videos.\n\nWhile I agree with the authors on these points, I also find that the paper suffer from important flaws. Specifically:\n\n  - the choice of not comparing with previous approaches in term of pixel prediction error seems very \"convenient\", to say the least. While it is clear that the evaluation metric is imperfect, it is not a reason to completely dismiss all quantitative comparisons with previous work. The frames output by the network on, e.g. the moving digits datasets (Figure 4), looks ok and can definitely be compared with other papers. Yet, the authors chose not to, which is suspicious.\n  \n  - The newly proposed metric poses several problems. First, action classification is evaluated with C3D, which is not a state-of-the-art approach at all for this task. Second, this metric actually *does not* evaluate what the network is claimed to do, that is, next frame prediction. Instead, it evaluates if another network, which was never trained to distinguish between real or synthetic frames by the way, can accurately classify an action from the predicted frames. I find that this proxy metric is only weakly related to what is supposed to be measured. In adition, it does not really make sense to train a network for something else that the final task it is evaluated for.\n  \n  - how is the affine motion of patches estimated? It is only explained that the problem is solved globally (not treating each patch independently) in a pretty vague manner. Estimating the motion of all patches is akin to solving the optical flow, which is still an active subject of research. Therefore, an important flaw of the paper lies in the potentially erroneous etimation of the motion input to the network. In the videos made available, it is clear that the motion is wrongly estimated sometimes. Since the entire approach depends on this input, I find it important to discuss this aspect. How do motion estimation failures impact the network? Also, the patch-affine hypothesis does not hold when patches are large enough that they cover several objects with contradictory motion. Which appears to be the case on UCF101 videos.\n  \n  - Even ignoring the weird proxy-evaluation part, the network is still not trained end-to-end. That is, the network is trained to minimize the difference between (noisy) ground-truth and output affine transforms, instead of minimizing a loss in the actual output space (frame pixels) for which an (exact) ground-truth is available. It is true that the MSE loss on raw pixels leads to blurry results, but other types of losses do exist, for instance the gradient loss introduced by Mathieu et al. was shown to solve this issue. As noted by the authors themselves, minimizing a loss in the transformation space, where affine parameters are harder to intepret, introduces unexpected artifacts. The motion is often largely underestimated, as is obvious in Figure 5 where it is hard to tell the difference between the input and output frames. \n  \n  - The proposed approach is not sufficiently compared to previous work. In particular, the approach is closely related to \"SPATIO-TEMPORAL VIDEO AUTOENCODER WITH DIFFERENTIABLE MEMORY\" of Taraucean et al, ICLR'15. This paper also output prediction in the motion space. Experimental results should compare against it.\n\n  - The comparison with optical flow is unfair. First, the approach of Brox et al. is more than 10 years old. Second, it is not really fair to assume a constant flow for all frames. At least some basic extrapolation could be done to take into account the flow of all pairs of input frames and not just the last one. Overall, the approach is not compared to very challenging baselines.\n\n  - I disagree with the answer that the authors gave to a reviewer's question. Denote ground-truth frames as {X_0, X_1 ...} and predicted frames as {Y_1, Y_2, ...}. When asked if the videos at"
  },
  {
    "people": [
      "Mathieu"
    ],
    "review": "Paper Summary\nThis paper makes two contributions -\n(1) A model for next step prediction, where the inputs and outputs are in the\nspace of affine transforms between adjacent frames.\n(2) An evaluation method in which the quality of the generated data is assessed\nby measuring the reduction in performance of another model (such as a\nclassifier) when tested on the generated data.\n\nThe authors show that according to this metric, the proposed model works better\nthan other baseline models (including the recent work of Mathieu et al. which\nuses adversarial training).\n\nStrengths\n- This paper attempts to solve a major problem in unsupervised learning\n  with videos, which is evaluating them.\n- The results show that using MSE in transform space does prevent the blurring\n  problem to a large extent (which is one of the main aims of this paper).\n- The results show that the generated data reduces the performance of the C3D\n  model on UCF-101 to a much less extent than other baselines.\n- The paper validates the assumption that videos can be approximated to quite a\n  few time steps by a sequence of affine transforms starting from an initial\nframe.\n\nWeaknesses\n- The proposed metric makes sense only if we truly just care about the performance\n  of a particular classifier on a given task. This significantly narrows the\nscope of applicability of this metric because arguably, one the important\nreasons for doing unsupervised learning is to come up a representation that is\nwidely applicable across a variety of tasks. The proposed metric would not help\nevaluate generative models designed to achieve this objective.\n\n- It is possible that one of the generative models being compared will interact\n  with the idiosyncrasies of the chosen classifier in unintended ways.\nTherefore, it would be hard to draw strong conclusions about the relative\nmerits of generative models from the results of such experiments. One way to\nameliorate this would be to use several different classifiers (C3D,\ndual-stream network, other state-of-the-art methods) and show that the ranking\nof different generative models is consistent across the choice of classifier.\nAdding such experiments would help increase certainty in the conclusions drawn\nin this paper.\n\n- Using only 4 or 8 input frames sampled at 25fps seems like very little context\n  if we really expect the model to extrapolate the kind of motion seen in\nUCF-101. The idea of working in the space of affine transforms would be much\nmore appealing if the model can be shown to really generated non-trivial motion\npatterns. Currently, the motion patterns seem to be almost linear\nextrapolations.\n\n- The model that predicts motion does not have access to content at all. It only\n  gets access to previous motion. It seems that this might be a disadvantage\nbecause the motion predictor cannot use any cues like object boundaries, or\ndecide what to do when two motion fields collide (it is probably easier to argue\nabout occlusions in content space).\n\nQuality/Clarity\nThe paper is clearly written and easy to follow. The assumptions are clearly\nspecified and validated. Experimental details seem adequate.\n\nOriginality\nThe idea of generating videos by predicting motion has been used previously.\nSeveral recent papers also use this idea. However the exact implementation in\nthis paper is new. The proposed evaluation protocol is novel.\n\nSignificance\nThe proposed evaluation method is an interesting alternative, especially if it\nis extended to include multiple classifiers representative of different\nstate-of-the-art approaches. Given how hard it is to evaluate generative models\nof videos, this paper could help start an effort to standardize on a benchmark\nset.\n\nMinor comments and suggestions\n\n(1) In the caption for Table 1: ``Each column shows the accuracy on the test set\nwhen taking a different number of input frames as input\" - ``input\" here refers\nto the input to the classifier (Output of the next step prediction model). However\nin the next sentence ``Our approach maps 16 \\times 16 patches into 8 \\times 8\nwith stride 4, and it takes 4 frames at the input\" - here ``input\" refers to\nthe input to the next step prediction model. It might be a good idea to rephrase\nthese sentences to make the distinction clear.\n\n(2) In order to better understand the space of affine transform\nparameters, it might help to include a histogram of these parameters in the\npaper. This can help us see at a glance, what is the typical range of these\n6 parameters, should we expect a lot of outliers, etc.\n\n(3) In order to compare transforms A and B, instead of ||A - B||^2, one\ncould consider A^{-1}B being close to identity as the metric. Did the authors\ntry this ?\n\n(4) \"The performance of the classifier on ground truth data is an upper bound on\nthe performance of any generative model.\" This is not *strictly* true. It is\npossible (though highly unlikely) that a generative model might make the data\nlook cleaner, sharper, or highlight some aspect of it which could improve the\nperformance of the classifier (even compared to ground truth). This is\nespecially true if the the generative model had access to the classifier, it\ncould then see what makes the classifier fire and highlight those discriminative\nfeatures in the generated output.\n\nOverall\nThis paper proposes future prediction in affine transform space. This does\nreduce blurriness and makes the videos look relatively realistic (at least to the\nC3D classifier). However, the paper can be improved by showing that the model can\npredict more non-trivial motion flows and the experiments can be strengthened by\nadding more classifiers besides than C3D."
  },
  {
    "people": [
      "Leordeanu",
      "Revaud",
      "Sevilla-Lara",
      "Mathieu",
      "Brox"
    ],
    "review": "This paper describes an approach to predict (unseen) future frames of a video given a set of known past frames. The approach is based on a CNN that, in contrast to most related papers, work in the space of affine transformations (instead of pixels or flow). Said another way, the network takes as input a set of affine transforms that describe the motion of patches in the past frames, and likewise, outputs a set of affine transforms that predict future patch motion.\n\nTo that aim, the authors make a few simplifying hypotheses, namely, that a sequence of frames can be modeled accurately enough in their patch-affine framework. This is not unreasonable. A lot of papers in the optical flow community are based on similar hypotheses, i.e. model the flow as a smoothly varying affine field (for instance see \"Locally affine sparse-to-dense matching for motion and occlusion estimation\" by Leordeanu et al., \"EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow\" by Revaud et al., \"Optical Flow With Semantic Segmentation and Localized Layers\" by Sevilla-Lara et al.). These methods are state of the art, which gives a hint about the validity of this kind of approach. \nIn addition, it also seems very reasonable to reformulate the prediction task as predicting motion rather than predicting raw pixels. Indeed, the (patch-affine) motion space is considerably smaller than the image space, making the problem much more tractable and amenable to high-resolution videos.\n\nWhile I agree with the authors on these points, I also find that the paper suffer from important flaws. Specifically:\n\n  - the choice of not comparing with previous approaches in term of pixel prediction error seems very \"convenient\", to say the least. While it is clear that the evaluation metric is imperfect, it is not a reason to completely dismiss all quantitative comparisons with previous work. The frames output by the network on, e.g. the moving digits datasets (Figure 4), looks ok and can definitely be compared with other papers. Yet, the authors chose not to, which is suspicious.\n  \n  - The newly proposed metric poses several problems. First, action classification is evaluated with C3D, which is not a state-of-the-art approach at all for this task. Second, this metric actually *does not* evaluate what the network is claimed to do, that is, next frame prediction. Instead, it evaluates if another network, which was never trained to distinguish between real or synthetic frames by the way, can accurately classify an action from the predicted frames. I find that this proxy metric is only weakly related to what is supposed to be measured. In adition, it does not really make sense to train a network for something else that the final task it is evaluated for.\n  \n  - how is the affine motion of patches estimated? It is only explained that the problem is solved globally (not treating each patch independently) in a pretty vague manner. Estimating the motion of all patches is akin to solving the optical flow, which is still an active subject of research. Therefore, an important flaw of the paper lies in the potentially erroneous etimation of the motion input to the network. In the videos made available, it is clear that the motion is wrongly estimated sometimes. Since the entire approach depends on this input, I find it important to discuss this aspect. How do motion estimation failures impact the network? Also, the patch-affine hypothesis does not hold when patches are large enough that they cover several objects with contradictory motion. Which appears to be the case on UCF101 videos.\n  \n  - Even ignoring the weird proxy-evaluation part, the network is still not trained end-to-end. That is, the network is trained to minimize the difference between (noisy) ground-truth and output affine transforms, instead of minimizing a loss in the actual output space (frame pixels) for which an (exact) ground-truth is available. It is true that the MSE loss on raw pixels leads to blurry results, but other types of losses do exist, for instance the gradient loss introduced by Mathieu et al. was shown to solve this issue. As noted by the authors themselves, minimizing a loss in the transformation space, where affine parameters are harder to intepret, introduces unexpected artifacts. The motion is often largely underestimated, as is obvious in Figure 5 where it is hard to tell the difference between the input and output frames. \n  \n  - The proposed approach is not sufficiently compared to previous work. In particular, the approach is closely related to \"SPATIO-TEMPORAL VIDEO AUTOENCODER WITH DIFFERENTIABLE MEMORY\" of Taraucean et al, ICLR'15. This paper also output prediction in the motion space. Experimental results should compare against it.\n\n  - The comparison with optical flow is unfair. First, the approach of Brox et al. is more than 10 years old. Second, it is not really fair to assume a constant flow for all frames. At least some basic extrapolation could be done to take into account the flow of all pairs of input frames and not just the last one. Overall, the approach is not compared to very challenging baselines.\n\n  - I disagree with the answer that the authors gave to a reviewer's question. Denote ground-truth frames as {X_0, X_1 ...} and predicted frames as {Y_1, Y_2, ...}. When asked if the videos at "
  },
  {
    "people": [
      "Leordeanu",
      "Revaud",
      "Sevilla-Lara",
      "Mathieu",
      "Brox"
    ],
    "review": "This paper describes an approach to predict (unseen) future frames of a video given a set of known past frames. The approach is based on a CNN that, in contrast to most related papers, work in the space of affine transformations (instead of pixels or flow). Said another way, the network takes as input a set of affine transforms that describe the motion of patches in the past frames, and likewise, outputs a set of affine transforms that predict future patch motion.\n\nTo that aim, the authors make a few simplifying hypotheses, namely, that a sequence of frames can be modeled accurately enough in their patch-affine framework. This is not unreasonable. A lot of papers in the optical flow community are based on similar hypotheses, i.e. model the flow as a smoothly varying affine field (for instance see \"Locally affine sparse-to-dense matching for motion and occlusion estimation\" by Leordeanu et al., \"EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow\" by Revaud et al., \"Optical Flow With Semantic Segmentation and Localized Layers\" by Sevilla-Lara et al.). These methods are state of the art, which gives a hint about the validity of this kind of approach. \nIn addition, it also seems very reasonable to reformulate the prediction task as predicting motion rather than predicting raw pixels. Indeed, the (patch-affine) motion space is considerably smaller than the image space, making the problem much more tractable and amenable to high-resolution videos.\n\nWhile I agree with the authors on these points, I also find that the paper suffer from important flaws. Specifically:\n\n  - the choice of not comparing with previous approaches in term of pixel prediction error seems very \"convenient\", to say the least. While it is clear that the evaluation metric is imperfect, it is not a reason to completely dismiss all quantitative comparisons with previous work. The frames output by the network on, e.g. the moving digits datasets (Figure 4), looks ok and can definitely be compared with other papers. Yet, the authors chose not to, which is suspicious.\n  \n  - The newly proposed metric poses several problems. First, action classification is evaluated with C3D, which is not a state-of-the-art approach at all for this task. Second, this metric actually *does not* evaluate what the network is claimed to do, that is, next frame prediction. Instead, it evaluates if another network, which was never trained to distinguish between real or synthetic frames by the way, can accurately classify an action from the predicted frames. I find that this proxy metric is only weakly related to what is supposed to be measured. In adition, it does not really make sense to train a network for something else that the final task it is evaluated for.\n  \n  - how is the affine motion of patches estimated? It is only explained that the problem is solved globally (not treating each patch independently) in a pretty vague manner. Estimating the motion of all patches is akin to solving the optical flow, which is still an active subject of research. Therefore, an important flaw of the paper lies in the potentially erroneous etimation of the motion input to the network. In the videos made available, it is clear that the motion is wrongly estimated sometimes. Since the entire approach depends on this input, I find it important to discuss this aspect. How do motion estimation failures impact the network? Also, the patch-affine hypothesis does not hold when patches are large enough that they cover several objects with contradictory motion. Which appears to be the case on UCF101 videos.\n  \n  - Even ignoring the weird proxy-evaluation part, the network is still not trained end-to-end. That is, the network is trained to minimize the difference between (noisy) ground-truth and output affine transforms, instead of minimizing a loss in the actual output space (frame pixels) for which an (exact) ground-truth is available. It is true that the MSE loss on raw pixels leads to blurry results, but other types of losses do exist, for instance the gradient loss introduced by Mathieu et al. was shown to solve this issue. As noted by the authors themselves, minimizing a loss in the transformation space, where affine parameters are harder to intepret, introduces unexpected artifacts. The motion is often largely underestimated, as is obvious in Figure 5 where it is hard to tell the difference between the input and output frames. \n  \n  - The proposed approach is not sufficiently compared to previous work. In particular, the approach is closely related to \"SPATIO-TEMPORAL VIDEO AUTOENCODER WITH DIFFERENTIABLE MEMORY\" of Taraucean et al, ICLR'15. This paper also output prediction in the motion space. Experimental results should compare against it.\n\n  - The comparison with optical flow is unfair. First, the approach of Brox et al. is more than 10 years old. Second, it is not really fair to assume a constant flow for all frames. At least some basic extrapolation could be done to take into account the flow of all pairs of input frames and not just the last one. Overall, the approach is not compared to very challenging baselines.\n\n  - I disagree with the answer that the authors gave to a reviewer's question. Denote ground-truth frames as {X_0, X_1 ...} and predicted frames as {Y_1, Y_2, ...}. When asked if the videos at"
  },
  {
    "people": [
      "Mathieu"
    ],
    "review": "Paper Summary\nThis paper makes two contributions -\n(1) A model for next step prediction, where the inputs and outputs are in the\nspace of affine transforms between adjacent frames.\n(2) An evaluation method in which the quality of the generated data is assessed\nby measuring the reduction in performance of another model (such as a\nclassifier) when tested on the generated data.\n\nThe authors show that according to this metric, the proposed model works better\nthan other baseline models (including the recent work of Mathieu et al. which\nuses adversarial training).\n\nStrengths\n- This paper attempts to solve a major problem in unsupervised learning\n  with videos, which is evaluating them.\n- The results show that using MSE in transform space does prevent the blurring\n  problem to a large extent (which is one of the main aims of this paper).\n- The results show that the generated data reduces the performance of the C3D\n  model on UCF-101 to a much less extent than other baselines.\n- The paper validates the assumption that videos can be approximated to quite a\n  few time steps by a sequence of affine transforms starting from an initial\nframe.\n\nWeaknesses\n- The proposed metric makes sense only if we truly just care about the performance\n  of a particular classifier on a given task. This significantly narrows the\nscope of applicability of this metric because arguably, one the important\nreasons for doing unsupervised learning is to come up a representation that is\nwidely applicable across a variety of tasks. The proposed metric would not help\nevaluate generative models designed to achieve this objective.\n\n- It is possible that one of the generative models being compared will interact\n  with the idiosyncrasies of the chosen classifier in unintended ways.\nTherefore, it would be hard to draw strong conclusions about the relative\nmerits of generative models from the results of such experiments. One way to\nameliorate this would be to use several different classifiers (C3D,\ndual-stream network, other state-of-the-art methods) and show that the ranking\nof different generative models is consistent across the choice of classifier.\nAdding such experiments would help increase certainty in the conclusions drawn\nin this paper.\n\n- Using only 4 or 8 input frames sampled at 25fps seems like very little context\n  if we really expect the model to extrapolate the kind of motion seen in\nUCF-101. The idea of working in the space of affine transforms would be much\nmore appealing if the model can be shown to really generated non-trivial motion\npatterns. Currently, the motion patterns seem to be almost linear\nextrapolations.\n\n- The model that predicts motion does not have access to content at all. It only\n  gets access to previous motion. It seems that this might be a disadvantage\nbecause the motion predictor cannot use any cues like object boundaries, or\ndecide what to do when two motion fields collide (it is probably easier to argue\nabout occlusions in content space).\n\nQuality/Clarity\nThe paper is clearly written and easy to follow. The assumptions are clearly\nspecified and validated. Experimental details seem adequate.\n\nOriginality\nThe idea of generating videos by predicting motion has been used previously.\nSeveral recent papers also use this idea. However the exact implementation in\nthis paper is new. The proposed evaluation protocol is novel.\n\nSignificance\nThe proposed evaluation method is an interesting alternative, especially if it\nis extended to include multiple classifiers representative of different\nstate-of-the-art approaches. Given how hard it is to evaluate generative models\nof videos, this paper could help start an effort to standardize on a benchmark\nset.\n\nMinor comments and suggestions\n\n(1) In the caption for Table 1: ``Each column shows the accuracy on the test set\nwhen taking a different number of input frames as input\" - ``input\" here refers\nto the input to the classifier (Output of the next step prediction model). However\nin the next sentence ``Our approach maps 16 \\times 16 patches into 8 \\times 8\nwith stride 4, and it takes 4 frames at the input\" - here ``input\" refers to\nthe input to the next step prediction model. It might be a good idea to rephrase\nthese sentences to make the distinction clear.\n\n(2) In order to better understand the space of affine transform\nparameters, it might help to include a histogram of these parameters in the\npaper. This can help us see at a glance, what is the typical range of these\n6 parameters, should we expect a lot of outliers, etc.\n\n(3) In order to compare transforms A and B, instead of ||A - B||^2, one\ncould consider A^{-1}B being close to identity as the metric. Did the authors\ntry this ?\n\n(4) \"The performance of the classifier on ground truth data is an upper bound on\nthe performance of any generative model.\" This is not *strictly* true. It is\npossible (though highly unlikely) that a generative model might make the data\nlook cleaner, sharper, or highlight some aspect of it which could improve the\nperformance of the classifier (even compared to ground truth). This is\nespecially true if the the generative model had access to the classifier, it\ncould then see what makes the classifier fire and highlight those discriminative\nfeatures in the generated output.\n\nOverall\nThis paper proposes future prediction in affine transform space. This does\nreduce blurriness and makes the videos look relatively realistic (at least to the\nC3D classifier). However, the paper can be improved by showing that the model can\npredict more non-trivial motion flows and the experiments can be strengthened by\nadding more classifiers besides than C3D."
  },
  {
    "people": [
      "Leordeanu",
      "Revaud",
      "Sevilla-Lara",
      "Mathieu",
      "Brox"
    ],
    "review": "This paper describes an approach to predict (unseen) future frames of a video given a set of known past frames. The approach is based on a CNN that, in contrast to most related papers, work in the space of affine transformations (instead of pixels or flow). Said another way, the network takes as input a set of affine transforms that describe the motion of patches in the past frames, and likewise, outputs a set of affine transforms that predict future patch motion.\n\nTo that aim, the authors make a few simplifying hypotheses, namely, that a sequence of frames can be modeled accurately enough in their patch-affine framework. This is not unreasonable. A lot of papers in the optical flow community are based on similar hypotheses, i.e. model the flow as a smoothly varying affine field (for instance see \"Locally affine sparse-to-dense matching for motion and occlusion estimation\" by Leordeanu et al., \"EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow\" by Revaud et al., \"Optical Flow With Semantic Segmentation and Localized Layers\" by Sevilla-Lara et al.). These methods are state of the art, which gives a hint about the validity of this kind of approach. \nIn addition, it also seems very reasonable to reformulate the prediction task as predicting motion rather than predicting raw pixels. Indeed, the (patch-affine) motion space is considerably smaller than the image space, making the problem much more tractable and amenable to high-resolution videos.\n\nWhile I agree with the authors on these points, I also find that the paper suffer from important flaws. Specifically:\n\n  - the choice of not comparing with previous approaches in term of pixel prediction error seems very \"convenient\", to say the least. While it is clear that the evaluation metric is imperfect, it is not a reason to completely dismiss all quantitative comparisons with previous work. The frames output by the network on, e.g. the moving digits datasets (Figure 4), looks ok and can definitely be compared with other papers. Yet, the authors chose not to, which is suspicious.\n  \n  - The newly proposed metric poses several problems. First, action classification is evaluated with C3D, which is not a state-of-the-art approach at all for this task. Second, this metric actually *does not* evaluate what the network is claimed to do, that is, next frame prediction. Instead, it evaluates if another network, which was never trained to distinguish between real or synthetic frames by the way, can accurately classify an action from the predicted frames. I find that this proxy metric is only weakly related to what is supposed to be measured. In adition, it does not really make sense to train a network for something else that the final task it is evaluated for.\n  \n  - how is the affine motion of patches estimated? It is only explained that the problem is solved globally (not treating each patch independently) in a pretty vague manner. Estimating the motion of all patches is akin to solving the optical flow, which is still an active subject of research. Therefore, an important flaw of the paper lies in the potentially erroneous etimation of the motion input to the network. In the videos made available, it is clear that the motion is wrongly estimated sometimes. Since the entire approach depends on this input, I find it important to discuss this aspect. How do motion estimation failures impact the network? Also, the patch-affine hypothesis does not hold when patches are large enough that they cover several objects with contradictory motion. Which appears to be the case on UCF101 videos.\n  \n  - Even ignoring the weird proxy-evaluation part, the network is still not trained end-to-end. That is, the network is trained to minimize the difference between (noisy) ground-truth and output affine transforms, instead of minimizing a loss in the actual output space (frame pixels) for which an (exact) ground-truth is available. It is true that the MSE loss on raw pixels leads to blurry results, but other types of losses do exist, for instance the gradient loss introduced by Mathieu et al. was shown to solve this issue. As noted by the authors themselves, minimizing a loss in the transformation space, where affine parameters are harder to intepret, introduces unexpected artifacts. The motion is often largely underestimated, as is obvious in Figure 5 where it is hard to tell the difference between the input and output frames. \n  \n  - The proposed approach is not sufficiently compared to previous work. In particular, the approach is closely related to \"SPATIO-TEMPORAL VIDEO AUTOENCODER WITH DIFFERENTIABLE MEMORY\" of Taraucean et al, ICLR'15. This paper also output prediction in the motion space. Experimental results should compare against it.\n\n  - The comparison with optical flow is unfair. First, the approach of Brox et al. is more than 10 years old. Second, it is not really fair to assume a constant flow for all frames. At least some basic extrapolation could be done to take into account the flow of all pairs of input frames and not just the last one. Overall, the approach is not compared to very challenging baselines.\n\n  - I disagree with the answer that the authors gave to a reviewer's question. Denote ground-truth frames as {X_0, X_1 ...} and predicted frames as {Y_1, Y_2, ...}. When asked if the videos at "
  },
  {
    "people": [
      "Guu"
    ],
    "review": "The contribution of this paper can be summarized as:\n\n1, A TransGaussian model (in a similar idea of TransE) which models the subject / object embeddings in a parameterization of Gaussian distribution.  The model can be naturally adapted to path queries like the formulation of (Guu et al, 2015).\n2. Along with the entity / relation representations trained by TransGaussian, an LSTM + attention model is built on natural language questions, aiming at learning a distribution (not normalized though) over relations for question answering.\n3. Experiments on a generated WorldCup2014 dataset, focusing on path queries and conjunctive queries.\n\nOverall, I think the Gaussian parameterization exhibits some nice properties, and could be suitable to KB completion and question answering. However, some details and the main experimental results are not convincing enough to me.  The paper writing also needs to be improved. More comments below:\n\n[Major comments]\n\n- My main concern is that that evaluation results are NOT strong. Either knowledge base completion or KB-based question answering, there are many existing and competitive benchmarks (e.g., FB15k / WebQuestions). Experimenting with such a tiny WordCup2014 dataset is not convincing.  Moreover, the questions are just generated by a few templates, which is far from NL questions. I am not even not sure why we need to apply an LSTM in such scenario. The paper would be much stronger if you can demonstrate its effectiveness on the above benchmarks. \n\n- Conjunctive queries:  the current model assumes that all the detected entities in the question could be aligned to one or more relations and we can take conjunctions in the end. This assumption might be not always correct, so it is more necessary to justify this on real QA datasets.\n\n- The model is named as  \u201cGaussian attention\u201d and I kind of think it is not very closely related to well-known attention mechanism, but more related to KB embedding literature.\n\n[Minor comments]\n- I find Figure 2 a bit confusing. The first row of orange blocks denote KB relations, and the second row of those denote every single word of the NL question. Maybe make it clearer?\n\n- Besides \u201centity recognition\u201d, usually we still need an \u201centity linker\u201d component which links the text mention to the KB entity. \n"
  },
  {
    "people": [
      "Guu"
    ],
    "review": "The contribution of this paper can be summarized as:\n\n1, A TransGaussian model (in a similar idea of TransE) which models the subject / object embeddings in a parameterization of Gaussian distribution.  The model can be naturally adapted to path queries like the formulation of (Guu et al, 2015).\n2. Along with the entity / relation representations trained by TransGaussian, an LSTM + attention model is built on natural language questions, aiming at learning a distribution (not normalized though) over relations for question answering.\n3. Experiments on a generated WorldCup2014 dataset, focusing on path queries and conjunctive queries.\n\nOverall, I think the Gaussian parameterization exhibits some nice properties, and could be suitable to KB completion and question answering. However, some details and the main experimental results are not convincing enough to me.  The paper writing also needs to be improved. More comments below:\n\n[Major comments]\n\n- My main concern is that that evaluation results are NOT strong. Either knowledge base completion or KB-based question answering, there are many existing and competitive benchmarks (e.g., FB15k / WebQuestions). Experimenting with such a tiny WordCup2014 dataset is not convincing.  Moreover, the questions are just generated by a few templates, which is far from NL questions. I am not even not sure why we need to apply an LSTM in such scenario. The paper would be much stronger if you can demonstrate its effectiveness on the above benchmarks. \n\n- Conjunctive queries:  the current model assumes that all the detected entities in the question could be aligned to one or more relations and we can take conjunctions in the end. This assumption might be not always correct, so it is more necessary to justify this on real QA datasets.\n\n- The model is named as  \u201cGaussian attention\u201d and I kind of think it is not very closely related to well-known attention mechanism, but more related to KB embedding literature.\n\n[Minor comments]\n- I find Figure 2 a bit confusing. The first row of orange blocks denote KB relations, and the second row of those denote every single word of the NL question. Maybe make it clearer?\n\n- Besides \u201centity recognition\u201d, usually we still need an \u201centity linker\u201d component which links the text mention to the KB entity. \n"
  },
  {
    "people": [
      "Levy"
    ],
    "review": "This paper presents a principled optimization method for SGNS (word2vec).\n\nWhile the proposed method is elegant from a theoretical perspective, I am not sure what the tangible benefits of this approach are. For example, does using Riemannian optimization allow the model to converge faster than the alternatives? The evaluation doesn't show a dramatic advantage to RO-SGNS; the 1% difference on the word similarity benchmarks is within the range of hyperparameter effects (see \"Improving Distributional Similarity with Lessons Learned from Word Embeddings\", (Levy et al., 2015)). The theoretical connection to Riemannian optimization is nice though, and it might be useful for understanding related methods in the future."
  },
  {
    "people": [
      "Vandereycken",
      "Sepulchre"
    ],
    "review": "The paper considers Grassmannian SGD to optimize the skip gram negative sampling (SGNS) objective for learning better word embeddings. It is not clear why the proposed optimization approach has any advantage over the existing vanilla SGD-based approach - neither approach comes with theoretical guarantees - the empirical comparisons show marginal improvements. Furthermore, the key idea here - that of projector splitting algorithm - has been applied on numerous occasions to machine learning problems - see references by Vandereycken on matrix completion and by Sepulchre on matrix factorization. \n\nThe computational cost of the two approaches is not carefully discussed. For instance, how expensive is the SVD in (7)? One can always perform an efficient low-rank update to the SVD - therefore, a rank one update requires O(nd) operations. What is the computational cost of each iteration of the proposed approach? \n\n"
  },
  {
    "people": [
      "Levy"
    ],
    "review": "This paper presents a principled optimization method for SGNS (word2vec).\n\nWhile the proposed method is elegant from a theoretical perspective, I am not sure what the tangible benefits of this approach are. For example, does using Riemannian optimization allow the model to converge faster than the alternatives? The evaluation doesn't show a dramatic advantage to RO-SGNS; the 1% difference on the word similarity benchmarks is within the range of hyperparameter effects (see \"Improving Distributional Similarity with Lessons Learned from Word Embeddings\", (Levy et al., 2015)). The theoretical connection to Riemannian optimization is nice though, and it might be useful for understanding related methods in the future.\n"
  },
  {
    "people": [
      "Levy"
    ],
    "review": "This paper presents a principled optimization method for SGNS (word2vec).\n\nWhile the proposed method is elegant from a theoretical perspective, I am not sure what the tangible benefits of this approach are. For example, does using Riemannian optimization allow the model to converge faster than the alternatives? The evaluation doesn't show a dramatic advantage to RO-SGNS; the 1% difference on the word similarity benchmarks is within the range of hyperparameter effects (see \"Improving Distributional Similarity with Lessons Learned from Word Embeddings\", (Levy et al., 2015)). The theoretical connection to Riemannian optimization is nice though, and it might be useful for understanding related methods in the future."
  },
  {
    "people": [
      "Vandereycken",
      "Sepulchre"
    ],
    "review": "The paper considers Grassmannian SGD to optimize the skip gram negative sampling (SGNS) objective for learning better word embeddings. It is not clear why the proposed optimization approach has any advantage over the existing vanilla SGD-based approach - neither approach comes with theoretical guarantees - the empirical comparisons show marginal improvements. Furthermore, the key idea here - that of projector splitting algorithm - has been applied on numerous occasions to machine learning problems - see references by Vandereycken on matrix completion and by Sepulchre on matrix factorization. \n\nThe computational cost of the two approaches is not carefully discussed. For instance, how expensive is the SVD in (7)? One can always perform an efficient low-rank update to the SVD - therefore, a rank one update requires O(nd) operations. What is the computational cost of each iteration of the proposed approach? \n\n"
  },
  {
    "people": [
      "Levy"
    ],
    "review": "This paper presents a principled optimization method for SGNS (word2vec).\n\nWhile the proposed method is elegant from a theoretical perspective, I am not sure what the tangible benefits of this approach are. For example, does using Riemannian optimization allow the model to converge faster than the alternatives? The evaluation doesn't show a dramatic advantage to RO-SGNS; the 1% difference on the word similarity benchmarks is within the range of hyperparameter effects (see \"Improving Distributional Similarity with Lessons Learned from Word Embeddings\", (Levy et al., 2015)). The theoretical connection to Riemannian optimization is nice though, and it might be useful for understanding related methods in the future.\n"
  },
  {
    "people": [
      "Bertschinger"
    ],
    "review": "I think it would be useful to discuss the concept of *edge* of chaos here (see e.g. Bertschinger, Nachschlager - Real-Time Computation at the Edge of Chaos in Recurrent Neural Networks), i.e. the hypothesis that RNNs are optimal (in some sense) at the boundary between chaotic and deterministic regimes. Specifically, it would be nice to see if your network gets closer to this edge during training (I think it will).\n\nIt wasn't clear to me if you studied the chaoticity in the case *with* input... the \"epsilon-activation\" thing seems very nonstandard. Why didn't you just compute the mean Lyapunov exponent? You can do that with or without input. I think you might find that the RNN with input will approach the edge of chaos during training (Lyapunov exp gets closer to zero, probably starting from negative values in your case).\n\nThe LSTM phase space diagram in Fig. 2 looks pretty bad... I think that particular unit is not behaving well at all. What you should get in properly trained models is something like in Fig. 1 (a), but more noisy because there's effects from the input.\n\nAnyway, overall a very interesting paper! I'm glad to see RNNs studied from a chaotic dynamics perspective."
  },
  {
    "people": [
      "Bertschinger"
    ],
    "review": "I think it would be useful to discuss the concept of *edge* of chaos here (see e.g. Bertschinger, Nachschlager - Real-Time Computation at the Edge of Chaos in Recurrent Neural Networks), i.e. the hypothesis that RNNs are optimal (in some sense) at the boundary between chaotic and deterministic regimes. Specifically, it would be nice to see if your network gets closer to this edge during training (I think it will).\n\nIt wasn't clear to me if you studied the chaoticity in the case *with* input... the \"epsilon-activation\" thing seems very nonstandard. Why didn't you just compute the mean Lyapunov exponent? You can do that with or without input. I think you might find that the RNN with input will approach the edge of chaos during training (Lyapunov exp gets closer to zero, probably starting from negative values in your case).\n\nThe LSTM phase space diagram in Fig. 2 looks pretty bad... I think that particular unit is not behaving well at all. What you should get in properly trained models is something like in Fig. 1 (a), but more noisy because there's effects from the input.\n\nAnyway, overall a very interesting paper! I'm glad to see RNNs studied from a chaotic dynamics perspective."
  },
  {
    "people": [
      "Daniel",
      "Andrychowicz",
      "Daniel"
    ],
    "review": "The paper proposes using an actor-critic RL algorithm for training learning rate controllers for supervised learning. The proposed method outperforms standard optimizers like SGD, ADAM and RMSprop in experiments conducted on MNIST and CIFAR 10.\n\nI have two main concerns. One is the lack of comparisons to similar recently proposed methods - \"Learning Step Size Controllers for Robust Neural Network Training\" by Daniel et al. and \"Learning to learn by gradient descent by gradient descent\" by Andrychowicz et al. The work of Daniel et al. is quite similar because it also proposes using a policy search RL method (REPS) and it is not clear what the downsides of their approach are. Their work does use more prior knowledge as the authors stated, but why is this a bad thing?\n\nMy second concern is with the experiments. Some of the numbers reported for the other methods are surprisingly low. For example, why is RMSprop so bad in Table 2 and Table 3? These results suggest that the methods are not being tuned properly, which reinforces the need for comparisons on standard architectures with previously reported results. For example, if the baselines used a better architecture like a ResNet or, for simplicty, Network in Network from this list:"
  },
  {
    "people": [
      "Daniel",
      "Andrychowicz",
      "Daniel"
    ],
    "review": "The paper proposes using an actor-critic RL algorithm for training learning rate controllers for supervised learning. The proposed method outperforms standard optimizers like SGD, ADAM and RMSprop in experiments conducted on MNIST and CIFAR 10.\n\nI have two main concerns. One is the lack of comparisons to similar recently proposed methods - \"Learning Step Size Controllers for Robust Neural Network Training\" by Daniel et al. and \"Learning to learn by gradient descent by gradient descent\" by Andrychowicz et al. The work of Daniel et al. is quite similar because it also proposes using a policy search RL method (REPS) and it is not clear what the downsides of their approach are. Their work does use more prior knowledge as the authors stated, but why is this a bad thing?\n\nMy second concern is with the experiments. Some of the numbers reported for the other methods are surprisingly low. For example, why is RMSprop so bad in Table 2 and Table 3? These results suggest that the methods are not being tuned properly, which reinforces the need for comparisons on standard architectures with previously reported results. For example, if the baselines used a better architecture like a ResNet or, for simplicty, Network in Network from this list:\n"
  },
  {
    "people": [
      "Daniel",
      "Andrychowicz",
      "Daniel"
    ],
    "review": "The paper proposes using an actor-critic RL algorithm for training learning rate controllers for supervised learning. The proposed method outperforms standard optimizers like SGD, ADAM and RMSprop in experiments conducted on MNIST and CIFAR 10.\n\nI have two main concerns. One is the lack of comparisons to similar recently proposed methods - \"Learning Step Size Controllers for Robust Neural Network Training\" by Daniel et al. and \"Learning to learn by gradient descent by gradient descent\" by Andrychowicz et al. The work of Daniel et al. is quite similar because it also proposes using a policy search RL method (REPS) and it is not clear what the downsides of their approach are. Their work does use more prior knowledge as the authors stated, but why is this a bad thing?\n\nMy second concern is with the experiments. Some of the numbers reported for the other methods are surprisingly low. For example, why is RMSprop so bad in Table 2 and Table 3? These results suggest that the methods are not being tuned properly, which reinforces the need for comparisons on standard architectures with previously reported results. For example, if the baselines used a better architecture like a ResNet or, for simplicty, Network in Network from this list:"
  },
  {
    "people": [
      "Daniel",
      "Andrychowicz",
      "Daniel"
    ],
    "review": "The paper proposes using an actor-critic RL algorithm for training learning rate controllers for supervised learning. The proposed method outperforms standard optimizers like SGD, ADAM and RMSprop in experiments conducted on MNIST and CIFAR 10.\n\nI have two main concerns. One is the lack of comparisons to similar recently proposed methods - \"Learning Step Size Controllers for Robust Neural Network Training\" by Daniel et al. and \"Learning to learn by gradient descent by gradient descent\" by Andrychowicz et al. The work of Daniel et al. is quite similar because it also proposes using a policy search RL method (REPS) and it is not clear what the downsides of their approach are. Their work does use more prior knowledge as the authors stated, but why is this a bad thing?\n\nMy second concern is with the experiments. Some of the numbers reported for the other methods are surprisingly low. For example, why is RMSprop so bad in Table 2 and Table 3? These results suggest that the methods are not being tuned properly, which reinforces the need for comparisons on standard architectures with previously reported results. For example, if the baselines used a better architecture like a ResNet or, for simplicty, Network in Network from this list:\n"
  },
  {
    "people": [
      "Graves",
      "Graves"
    ],
    "review": "Quality:\nThe paper initiates a framework to incorporate active learning into the deep learning framework, mainly addressing challenges such as scalability that accompanies the training of a deep neural network. \nHowever, I think the paper is not well polished; there are quite a lot of grammatical and typing errors.\n\nClarity:\nThe paper needs major improvements in terms of clarity. The motivations in the introduction, i.e., why it is difficult to do active learning in deep architectures, could be better explained, and tied to the explanation in Section 3 of the paper. For example, the authors motivated the need of (mini)batch label queries, but never mention it again in Section 3, when they describe their main methodology. \nThe related work section, although appearing systematic and thorough, is a little detached from the main body of the paper (related work section should not be a survey of the literature, but help readers locate your work in the relevant literature, and highlight the pros and cons. In this perspective, maybe the authors could shorten some explanations over the related work that are not directly related, while spending more time on discussing/comparing with works that are most related to your current work, e.g., that of Graves '11.\n\nOriginality & Significance:\nThe authors proposed an active learning training framework. The idea is to treat the network parameter optimization problem as a Bayesian inference problem (which is proposed previously by Graves) and formulate the active learning problem as that of sampling the most informative data, where the informativeness is defined by the variational free energy, which depends on the Fisher information. To reconcile the computational burden of computing the inverse of Fisher Information matrix, the authors proposed techniques to approximate it (which seems to be novel)\n\nI think that this paper initiates an interesting direction: one that adapts deep learning to label-expensive problems, via active learning. But the paper needs to be improved in terms of presentation.\n\n"
  },
  {
    "people": [
      "Graves",
      "Graves"
    ],
    "review": "Quality:\nThe paper initiates a framework to incorporate active learning into the deep learning framework, mainly addressing challenges such as scalability that accompanies the training of a deep neural network. \nHowever, I think the paper is not well polished; there are quite a lot of grammatical and typing errors.\n\nClarity:\nThe paper needs major improvements in terms of clarity. The motivations in the introduction, i.e., why it is difficult to do active learning in deep architectures, could be better explained, and tied to the explanation in Section 3 of the paper. For example, the authors motivated the need of (mini)batch label queries, but never mention it again in Section 3, when they describe their main methodology. \nThe related work section, although appearing systematic and thorough, is a little detached from the main body of the paper (related work section should not be a survey of the literature, but help readers locate your work in the relevant literature, and highlight the pros and cons. In this perspective, maybe the authors could shorten some explanations over the related work that are not directly related, while spending more time on discussing/comparing with works that are most related to your current work, e.g., that of Graves '11.\n\nOriginality & Significance:\nThe authors proposed an active learning training framework. The idea is to treat the network parameter optimization problem as a Bayesian inference problem (which is proposed previously by Graves) and formulate the active learning problem as that of sampling the most informative data, where the informativeness is defined by the variational free energy, which depends on the Fisher information. To reconcile the computational burden of computing the inverse of Fisher Information matrix, the authors proposed techniques to approximate it (which seems to be novel)\n\nI think that this paper initiates an interesting direction: one that adapts deep learning to label-expensive problems, via active learning. But the paper needs to be improved in terms of presentation.\n\n"
  },
  {
    "people": [
      "YOu",
      "Goyal",
      "Raghav",
      "Dymetman",
      "Marc",
      "Gaussier",
      "Eric",
      "LIG"
    ],
    "review": "The paper presents two approaches for generating English poetry. The first\napproach combine a neural phonetic encoder predicting the next phoneme with a\nphonetic-orthographic HMM decoder computing the most likely word corresponding\nto a sequence of phonemes. The second approach combines a character language\nmodel with a weigthed FST to impose rythm constraints on the output of the\nlanguage model. For the second approach, the authors also present a heuristic\napproach which permit constraining the generated poem according to theme (e.g;,\nlove) or poetic devices (e.g., alliteration). The generated poems are evaluated\nboth instrinsically by comparing the rythm of the generated lines with a gold\nstandard and extrinsically by asking 70 human evaluators to (i) determine\nwhether the poem was written by a human or a machine and (ii) rate poems wrt to\nreadability, form and evocation.  The results indicate that the second model\nperforms best and that human evaluators find it difficult to distinguish\nbetween human written and machine generated poems.\n\nThis is an interesting, clearly written article with novel ideas (two different\nmodels for poetry generation, one based on a phonetic language model the other\non a character LM) and convincing results.\n\n For the evaluation, more precision about the evaluators and the protocol would\nbe good. Did all evaluators evaluate all poems and if not how many judgments\nwere collected for each poem for each task ? You mention 9 non English native\nspeakers. Poems are notoriously hard to read. How fluent were these ? \n\nIn the second model (character based), perhaps I missed it, but do you have a\nmechanism to avoid generating non words ? If not, how frequent are non words in\nthe generated poems ?\n\nIn the first model, why use an HMM to transliterate from phonetic to an\norhographic representation rather than a CRF? \n\nSince overall, you rule out the first model as a good generic model for\ngenerating poetry, it might have been more interesting to spend less space on\nthat model and more on the evaluation of the second model. In particular, I\nwould have been interested in a more detailed discussion of the impact of the\nheuristic you use to constrain theme or poetic devices. How do these impact\nevaluation results ? Could they be combined to jointly constrain theme and\npoetic devices ? \n\nThe combination of a neural mode with a WFST is reminiscent of the following\npaper which combine character based neural model to generate from dialog acts\nwith an WFST to avoid generating non words. YOu should relate your work to\ntheirs and cite them. \n\nNatural Language Generation through Character-Based RNNs with Finite-State\nPrior Knowledge\nGoyal, Raghav and Dymetman, Marc and Gaussier, Eric and LIG, Uni\nCOLING 2016"
  },
  {
    "people": [
      "Misztal",
      "Gabriele Barbieri",
      "Fran\u00e7ois Pachet",
      "Pierre Roy",
      "Mirko Degli Esposti",
      "Markov",
      "Luc De\nRaedt",
      "Christian Bessiere",
      "Didier Dubois",
      "Patrick Doherty",
      "Paolo Frasconi",
      "Stephen McGregor",
      "Matthew Purver",
      "Geraint Wiggins",
      "McGregor",
      "M. Ghazvininejad",
      "X. Shi",
      "Y. Choi",
      "K.\nKnight",
      "Manurung",
      "Barbieri",
      "Ghazvininejad"
    ],
    "review": "The paper describes two methodologies for the automatic generation of rhythmic\npoetry. Both rely on neural networks, but the second one allows for better\ncontrol of form.\n\n- Strengths:\n\nGood procedure for generating rhythmic poetry.\n\nProposals for adding control of theme and poetic devices (alliteration,\nconsonance, asonance).\n\nStrong results in evaluation of rhythm.\n\n- Weaknesses:\n\nPoor coverage of existing literature on poetry generation.\n\nNo comparison with existing approaches to poetry generation.\n\nNo evaluation of results on theme and poetic devices.\n\n- General Discussion:\n\nThe introduction describes the problem of poetry generation as divided into two\nsubtasks: the problem of content (the poem's semantics) and the problem of form\n(the \n\naesthetic rules the poem follows). The solutions proposed in the paper address\nboth of these subtasks in a limited fashion. They rely on neural networks\ntrained over corpora \n\nof poetry (represented at the phonetic or character level, depending on the\nsolution) to encode the linguistic continuity of the outputs. This does indeed\nensure that the \n\noutputs resemble meaningful text. To say that this is equivalent to having\nfound a way of providing the poem with appropriate semantics would be an\noverstatement. The \n\nproblem of form can be said to be addressed for the case of rhythm, and partial\nsolutions are proposed for some poetic devices. Aspects of form concerned with\nstructure at a \n\nlarger scale (stanzas and rhyme schemes) remain beyond the proposed solutions.\nNevertheless, the paper constitutes a valuable effort in the advancement of\npoetry generation.\n\nThe review of related work provided in section 2 is very poor. It does not even\ncover the set of previous efforts that the authors themselves consider worth\nmentioning in their paper (the work of Manurung et al 2000 and Misztal and\nIndurkhya 2014 is cited later in the paper - page 4 - but it is not placed in\nsection 2 with respect to the other authors mentioned there).\n\nA related research effort of particular relevance that the authors should\nconsider is:\n\n- Gabriele Barbieri, Fran\u00e7ois Pachet, Pierre Roy, and Mirko Degli Esposti.\n2012. Markov constraints for generating lyrics with style. In Proceedings of\nthe 20th European Conference on Artificial Intelligence (ECAI'12), Luc De\nRaedt, Christian Bessiere, Didier Dubois, Patrick Doherty, and Paolo Frasconi\n(Eds.). IOS Press, Amsterdam, The Netherlands, The Netherlands, 115-120. DOI:\nhttps://doi.org/10.3233/978-1-61499-098-7-115\n\nThis work addresses very similar problems to those discussed in the present\npaper (n-gram based generation and the problem of driving generation process\nwith additional constraints). The authors should include a review of this work\nand discuss the similarities and differences with their own.\n\nAnother research effort that is related to what the authors are attempting (and\nhas bearing on their evaluation process) is:\n\n- Stephen McGregor, Matthew Purver and Geraint Wiggins, Process Based\nEvaluation of Computer Generated Poetry,  in: Proceedings of the INLG 2016\nWorkshop on Computational Creativity and Natural Language Generation, pages\n51\u201360,Edinburgh, September 2016.c2016 Association for Computational\nLinguistics\n\nThis work is also similar to the current effort in that it models language\ninitially at a phonological level, but considers a word n-gram level\nsuperimposed on that, and also features a layer representint sentiment. Some of\nthe considerations McGregor et al make on evaluation of computer generated\npoetry are also relevant for the extrinsic evaluation described in the present\npaper.\n\nAnother work that I believe should be considered is:\n\n- \"Generating Topical Poetry\" (M. Ghazvininejad, X. Shi, Y. Choi, and K.\nKnight), Proc. EMNLP, 2016.\n\nThis work generates iambic pentameter by combining finite-state machinery with\ndeep learning. It would be interesting to see how the proposal in the current\npaper constrasts with this particular approach.\n\nAlthough less relevant to the present paper, the authors should consider\nextending their classification of poetry generation systems (they mention\nrule-based expert systems and statistical approaches) to include evolutionary\nsolutions. They already mention in their paper the work of Manurung, which is\nevolutionary in nature, operating over TAG grammars.\n\nIn any case, the paper as it stands holds little to no effort of comparison to\nprior approaches to poetry generation. The authors should make an effort to\ncontextualise their work with respect to previous efforts, specially in the\ncase were similar problems are being addressed (Barbieri et al, 2012) or\nsimilar methods are being applied (Ghazvininejad,  et al, 2016)."
  },
  {
    "people": [
      "Covington",
      "Covington",
      "Covington"
    ],
    "review": "- Strengths:\n\nThe paper makes several novel contributions to (transition-based) dependency\nparsing by extending the notion of non-monotonic transition systems and dynamic\noracles to unrestricted non-projective dependency parsing. The theoretical and\nalgorithmic analysis is clear and insightful, and the paper is admirably clear.\n\n- Weaknesses:\n\nGiven that the main motivation for using Covington's algorithm is to be able to\nrecover non-projective arcs, an empirical error analysis focusing on\nnon-projective structures would have further strengthened the paper. And even\nthough the main contributions of the paper are on the theoretical side, it\nwould have been relevant to include a comparison to the state of the art on the\nCoNLL data sets and not only to the monotonic baseline version of the same\nparser.\n\n- General Discussion:\n\nThe paper extends the transition-based formulation of Covington's dependency\nparsing algorithm (for unrestricted non-projective structures) by allowing\nnon-monotonicity in the sense that later transitions can change structure built\nby earlier transitions. In addition, it shows how approximate dynamic oracles\ncan be formulated for the new system. Finally, it shows experimentally that the\noracles provide a tight approximation and that the non-monotonic system leads\nto improved parsing accuracy over its monotonic counterpart for the majority of\nthe languages included in the study.\n\nThe theoretical contributions are in my view significant enough to merit\npublication, but I also think the paper could be strengthened on the empirical\nside. In particular, it would be relevant to investigate, in an error analysis,\nwhether the non-monotonic system improves accuracy specifically on\nnon-projective structures. Such an analysis can be motivated on two grounds:\n(i) the ability to recover non-projective structures is the main motivation for\nusing Covington's algorithm in the first place; (ii) non-projective structures\noften involved long-distance dependencies that are hard to predict for a greedy\ntransition-based parser, so it is plausible that the new system would improve\nthe situation. \n\nAnother point worth discussion is how the empirical results relate to the state\nof the art in light of recent improvements thanks to word embeddings and neural\nnetwork techniques. For example, the non-monotonicity is claimed to mitigate\nthe error propagation typical of classical greedy transition-based parsers. But\nanother way of mitigating this problem is to use recurrent neural networks as\npreprocessors to the parser in order to capture more of the global sentence\ncontext in word representations. Are these two techniques competing or\ncomplementary? A full investigation of these issues is clearly outside the\nscope of the paper, but some discussion would be highly relevant.\n\nSpecific questions:\n\nWhy were only 9 out of the 13 data sets from the CoNLL-X shared task used? I am\nsure there is a legitimate reason and stating it explicitly may prevent readers\nfrom becoming suspicious. \n\nDo you have any hypothesis about why accuracy decreases for Basque with the\nnon-monotonic system? Similar (but weaker) trends can be seen also for Turkish,\nCatalan, Hungarian and (perhaps) German.\n\nHow do your results compare to the state of the art on these data sets? This is\nrelevant for contextualising your results and allowing readers to estimate the\nsignificance of your improvements.\n\nAuthor response:\n\nI am satisfied with the author's response and see no reason to change my\nprevious review."
  },
  {
    "people": [
      "Goldberg"
    ],
    "review": "This paper introduces new configurations and training objectives for neural\nsequence models in a multi-task setting. As the authors describe well, the\nmulti-task setting is important because some tasks have shared information\nand in some scenarios learning many tasks can improve overall performance.\n\nThe methods section is relatively clear and logical, and I like where it ended\nup, though it could be slightly better organized. The organization that I\nrealized after reading is that there are two problems: 1) shared features end\nup in the private feature space, and 2) private features end up in the \nshared space. There is one novel method for each problem. That organization up\nfront would make the methods more cohesive. In any case, they introduce one \nmethod that keeps task-specific features out of shared representation\n(adversarial\nloss) and another to keep shared features out of task-specific representations\n(orthogonality constraints). My only point of confusion is the adversarial\nsystem.\nAfter LSTM output there is another layer, D(s^k_T, \\theta_D), relying on\nparameters\nU and b. This output is considered a probability distribution which is compared\nagainst the actual. This means it is possible it will just learn U and b that\neffectively mask task-specific information from  the LSTM outputs, and doesn't \nseem like it can guarantee task-specific information is removed.\n\nBefore I read the evaluation section I wrote down what I hoped the experiments\nwould look like and it did most of it. This is an interesting idea and there\nare \na lot more experiments one can imagine but I think here they have the basics\nto show the validity of their methods. It would be helpful to have best known\nresults on these tasks.\n\nMy primary concern with this paper is the lack of deeper motivation for the \napproach. I think it is easy to understand that in a totally shared model\nthere will be problems due to conflicts in feature space. The extension to \npartially shared features seems like a reaction to that issue -- one would \nexpect that the useful shared information is in the shared latent space and \neach task-specific space would learn features for that space. Maybe this works\nand maybe it doesn't, but the logic is clear to me. In contrast, the authors\nseem to start from the assumption that this \"shared-private\" model has this\nissue. I expected the argument flow to be 1) Fully-shared obviously has this\nproblem; 2) shared-private seems to address this; 3) in practice shared-private\ndoes not fully address this issue for reasons a,b,c.; 4) we introduce a method\nthat more effectively constrains the spaces.\nTable 4 helped me to partially understand what's going wrong with\nshared-private\nand what your methods do; some terms are _usually_ one connotation\nor another, and that general trend can probably get them into the shared\nfeature\nspace. This simple explanation, an example, and a more logical argument flow\nwould help the introduction and make this a really nice reading paper.\n\nFinally, I think this research ties into some other uncited MTL work [1],\nwhich does deep hierarchical MTL - supervised POS tagging at a lower level,\nchunking\nat the next level up, ccg tagging higher, etc. They then discuss at the end\nsome of the qualities that make MTL possible and conclude that MTL only works\n\"when tasks are sufficiently similar.\" The ASP-MTL paper made me think of this\nprevious work because potentially this model could learn what sufficiently\nsimilar is -- i.e., if two tasks are not sufficiently similar the shared model\nwould learn nothing and it would fall back to learning two independent systems,\nas compared to a shared-private model baseline that might overfit and perform\npoorly.\n\n[1]\n@inproceedings{sogaard2016deep,\n  title={Deep multi-task learning with low level tasks supervised at lower\nlayers},\n  author={S{\\o}gaard, Anders and Goldberg, Yoav},\n  booktitle={Proceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics},\n  volume={2},\n  pages={231--235},\n  year={2016},\n  organization={Association for Computational Linguistics}\n}"
  },
  {
    "people": [
      "Chinchor"
    ],
    "review": "# Paper summary\n\nThis paper presents a method for learning well-partitioned shared and\ntask-specific feature spaces for LSTM text classifiers. Multiclass adversarial\ntraining encourages shared space representations from which a discriminative\nclassifier cannot identify the task source (and are thus generic). The models\nevaluates are a fully-shared, shared-private and adversarial shared-private --\nthe lattermost ASP model is one of the main contributions. They also use\northogonality constraints to help reward shared and private spaces that are\ndistinct. The ASP model has lower error rate than single-task and other\nmulti-task neural models. They also experiment with a task-level cross\nvalidation to explore whether the shared representation can transfer across\ntasks, and it seems to favourably. Finally, there is some analysis of shared\nlayer activations suggesting that the ASP model is not being misled by strong\nweights learned on a specific (inappropriate) task.\n\n# Review summary\n\nGood ideas, well expressed and tested. Some minor comments.\n\n# Strengths\n\n* This is a nice set of ideas working well together. I particularly like the\nfocus on explicitly trying to create useful shared representations. These have\nbeen quite successful in the CV community, but it appears that one needs to\nwork quite hard to create them for NLP.\n* Sections 2, 3 and 4 are very clearly expressed.\n* The task-level cross-validation in Section 5.5 is a good way to evaluate the\ntransfer.\n* There is an implementation and data.\n\n# Weaknesses\n\n* There are a few minor typographic and phrasing errors. Individually, these\nare fine, but there are enough of them to warrant fixing:\n** l:84 the \u201cinfantile cart\u201d is slightly odd -- was this a real example\nfrom the data?\n** l:233 \u201care different in\u201d -> \u201cdiffer in\u201d\n** l:341 \u201cworking adversarially towards\u201d -> \u201cworking against\u201d or\n\u201ccompeting with\u201d?\n** l:434 \u201ctwo matrics\u201d -> \u201ctwo matrices\u201d\n** l:445 \u201care hyperparameter\u201d -> \u201care hyperparameters\u201d\n** Section 6 has a number of number agreement errors\n(l:745/746/765/766/767/770/784) and should be closely re-edited.\n** The shading on the final row of Tables 2 and 3 prints strangely\u2026\n* There is mention of unlabelled data in Table 1 and semi-supervised learning\nin Section 4.2, but I didn\u2019t see any results on these experiments. Were they\nomitted, or have I misunderstood?\n* The error rate differences are promising in Tables 2 and 3, but statistical\nsignificance testing would help make them really convincing. Especially between\nSP-MLT and ASP-MTL results to highlight the benefit of adversarial training. It\nshould be pretty straightforward to adapt the non-parametric approximate\nrandomisation test (see\nhttp://www.lr.pi.titech.ac.jp/~takamura/pubs/randtest.pdf for promising notes a\nreference to the Chinchor paper) to produce these.\n* The colours are inconsistent in the caption of Figure 5 (b). In 5 (a), blue\nis used for \u201cOurs\u201d, but this seems to have swapped for 5 (b). This is worth\nchecking, or I may have misunderstood the caption.\n\n# General Discussion\n\n* I wonder if there\u2019s some connection with regularisation here, as the effect\nof the adversarial training with orthogonal training is to help limit the\nshared feature space. It might be worth drawing that connection to other\nregularisation literature."
  },
  {
    "people": [
      "Schlangen",
      "Lazaridou",
      "Schlangen",
      "Abhijeet  Gupta",
      "Gemma  Boleda",
      "Marco  Baroni",
      "Sebastian  Pado",
      "Aurelie Herbelot",
      "Eva Maria Vecchi",
      "Roy",
      "Frome",
      "Norouzi"
    ],
    "review": "COMMENTS AFTER AUTHOR RESPONSE:\n\nThanks for your response, particularly for the clarification wrt the\nhypothesis. I agree with the comment wrt cross-modal mapping. What I don't\nshare is the kind of equation \"visual = referential\" that you seem to assume. A\nreferent can be visually presented, but visual information can be usefully\nadded to a word's representation in aggregate form to encode perceptual aspects\nof the words' meaning, the same way that it is done for textual information;\nfor instance, the fact that bananas are yellow\nwill not frequently be mentioned in text, and adding visual information\nextracted from images will account for this aspect of the semantic\nrepresentation of the word. This is kind of technical and specific to how we\nbuild distributional models, but it's also relevant if you think of human\ncognition (probably our representation for \"banana\" has some aggregate\ninformation about all the bananas we've seen --and touched, tasted, etc.). \nIt would be useful if you could discuss this issue explicitly, differentiating\nbetween multi-modal distributional semantics in general and the use of\ncross-modal mapping in particular.\n\nAlso, wrt the \"all models perform similarly\" comment: I really\nurge you, if the paper is accepted, to state it in this form, even if it\ndoesn't completely align with your hypotheses/goals (you have enough results\nthat do). It is a better description of the results, and more useful for the\ncommunity, than clinging to the\nn-th digit difference (and this is to a large extent independent of whether the\ndifference\nis actually statistical significant or not: If one bridge has 49% chances of\ncollapsing and another one 50%, the difference may be statistically\nsignificant, but that doesn't really make the first bridge a better bridge to\nwalk on).\n\nBtw, small quibble, could you find a kind of more compact and to the point\ntitle? (More geared towards either generally what you explore or to what you\nfind?)\n\n----------\n\nThe paper tackles an extremely interesting issue, that the authors label\n\"referential word meaning\", namely, the connection between a word's meaning and\nthe referents (objects in the external world) it is applied to. If I understood\nit correctly, they argue that\nthis is different from a typical word meaning representation as obtained e.g.\nwith distributional\nmethods, because one thing is the abstract \"lexical meaning\" of a word and the\nother which label is appropriate for a given referent with specific properties\n(in a specific context, although context is something they explicitly leave\naside in this paper). This hypothesis has been previously explored in work by\nSchlangen and colleagues (cited in the paper). The paper explores referential\nword meaning empirically on a specific version of the task of Referential\nExpression Generation (REG), namely, generating the appropriate noun for a\ngiven visually represented object.\n\n- Strengths:\n\n1) The problem they tackle I find extremely interesting; as they argue, REG is\na problem that had previously been addressed mainly using symbolic methods,\nthat did not easily allow for an exploration of how speakers choose the names\nof the objects. The scope of the research goes beyond REG as such, as it\naddresses the link between semantic representations and reference more broadly.\n\n2) I also like how they use current techniques and datasets (cross-modal\nmapping and word classifiers, the ReferIt dataset containing large amounts of\nimages with human-generated referring expressions) to address the problem at\nhand. \n\n3) There are a substantial number of experiments as well as analysis into the\nresults. \n\n- Weaknesses:\n\n1) The main weakness for me is the statement of the specific hypothesis, within\nthe general research line, that the paper is probing: I found it very\nconfusing.  As a result, it is also hard to make sense of the kind of feedback\nthat the results give to the initial hypothesis, especially because there are a\nlot of them and they don't all point in the same direction.\n\nThe paper says:\n\n\"This paper pursues the hypothesis that an accurate\nmodel of referential word meaning does not\nneed to fully integrate visual and lexical knowledge\n(e.g. as expressed in a distributional vector\nspace), but at the same time, has to go beyond\ntreating words as independent labels.\"\n\nThe first part of the hypothesis I don't understand: What is it to fully\nintegrate (or not to fully integrate) visual and lexical knowledge? Is the goal\nsimply to show that using generic distributional representation yields worse\nresults than using specific, word-adapted classifiers trained on the dataset?\nIf so, then the authors should explicitly discuss the bounds of what they are\nshowing: Specifically, word classifiers must be trained on the dataset itself\nand only word classifiers with a sufficient amount of items in the dataset can\nbe obtained, whereas word vectors are available for many other words and are\nobtained from an independent source (even if the cross-modal mapping itself is\ntrained on the dataset); moreover, they use the simplest Ridge Regression,\ninstead of the best method from Lazaridou et al. 2014, so any conclusion as to\nwhich method is better should be taken with a grain of salt. However, I'm\nhoping that the research goal is both more constructive and broader. Please\nclarify. \n\n2) The paper uses three previously developed methods on a previously available\ndataset. The problem itself has been defined before (in Schlangen et al.). In\nthis sense, the originality of the paper is not high. \n\n3) As the paper itself also points out, the authors select a very limited\nsubset of the ReferIt dataset, with quite a small vocabulary (159 words). I'm\nnot even sure why they limited it this way (see detailed comments below).\n\n4) Some aspects could have been clearer (see detailed comments).\n\n5) The paper contains many empirical results and analyses, and it makes a\nconcerted effort to put them together; but I still found it difficult to get\nthe whole picture: What is it exactly that the experiments in the paper tell us\nabout the underlying research question in general, and the specific hypothesis\ntested in particular? How do the different pieces of the puzzle that they\npresent fit together?\n\n- General Discussion: [Added after author response]\n\nDespite the weaknesses, I find the topic of the paper very relevant and also\nnovel enough, with an interesting use of current techniques to address an \"old\"\nproblem, REG and reference more generally, in a way that allows aspects to be\nexplored that have not received enough attention. The experiments and analyses\nare a substantial contribution, even though, as mentioned above, I'd like the\npaper to present a more coherent overall picture of how the many experiments\nand analyses fit together and address the question pursued.\n\n- Detailed comments:\n\nSection 2 is missing the following work in computational semantic approaches to\nreference:\n\nAbhijeet  Gupta,  Gemma  Boleda,  Marco  Baroni,  and Sebastian  Pado. 2015.  \nDistributional                                            vectors  encode \nreferential        \n\nattributes.\nProceedings of\nEMNLP,\n12-21\n\nAurelie Herbelot and Eva Maria Vecchi.                                           \n2015. \nBuilding\na\nshared\nworld:\nmapping\ndistributional to model-theoretic semantic spaces. Proceedings of EMNLP,\n22\u201332.\n\n142 how does Roy's work go beyond early REG work?\n\n155 focusses links\n\n184 flat \"hit @k metric\": \"flat\"?\n\nSection 3: please put the numbers related to the dataset in a table, specifying\nthe image regions, number of REs, overall number of words, and number of object\nnames in the original ReferIt dataset and in the version you use. By the way,\nwill you release your data? I put a \"3\" for data because in the reviewing form\nyou marked \"Yes\" for data, but I can't find the information in the paper.\n\n229 \"cannot be considered to be names\" ==> \"image object names\"\n\n230 what is \"the semantically annotated portion\" of ReferIt?\n\n247 why don't you just keep \"girl\" in this example, and more generally the head\nnouns of non-relational REs? More generally, could you motivate your choices a\nbit more so we understand why you ended up with such a restricted subset of\nReferIt?\n\n258 which 7 features? (list) How did you extract them?\n\n383 \"suggest that lexical or at least distributional knowledge is detrimental\nwhen learning what a word refers to in the world\": How does this follow from\nthe results of Frome et al. 2013 and Norouzi et al. 2013? Why should\ncross-modal projection give better results? It's a very different type of\ntask/setup than object labeling.\n\n394-395 these numbers belong in the data section\n\nTable 1: Are the differences between the methods statistically significant?\nThey are really numerically so small that any other conclusion to \"the methods\nperform similarly\" seems unwarranted to me. Especially the \"This suggests...\"\npart (407). \n\nTable 1: Also, the sim-wap method has the highest accuracy for hit @5 (almost\nidentical to wac); this is counter-intuitive given the @1 and @2 results. Any\nidea of what's going on?\n\nSection 5.2: Why did you define your ensemble classifier by hand instead of\nlearning it? Also, your method amounts to majority voting, right? \n\nTable 2: the order of the models is not the same as in the other tables + text.\n\nTable 3: you report cosine distances but discuss the results in terms of\nsimilarity. It would be clearer (and more in accordance with standard practice\nin CL imo) if you reported cosine similarities.\n\nTable 3: you don't comment on the results reported in the right columns. I\nfound it very curious that the gold-top k data similarities are higher for\ntransfer+sim-wap, whereas the results on the task are the same. I think that\nyou could squeeze more information wrt the phenomenon and the models out of\nthese results.\n\n496 format of \"wac\"\n\nSection 6 I like the idea of the task a lot, but I was very confused as to how\nyou did and why: I don't understand lines 550-553. What is the task exactly? An\nexample would help. \n\n558 \"Testsets\"\n\n574ff Why not mix in the train set examples with hypernyms and non-hypernyms?\n\n697 \"more even\": more wrt what?\n\n774ff \"Previous cross-modal mapping models ... force...\": I don't understand\nthis claim.\n\n792 \"larger test sets\": I think that you could even exploit ReferIt more (using\nmore of its data) before moving on to other datasets."
  },
  {
    "people": [
      "Lazaridou"
    ],
    "review": "AFTER AUTHOR RESPONSE\n\nI accept the response about emphasizing novelty of the task and comparison with\nprevious work. Also increase ratings for the dataset and software that are\npromised to become public before the article publishing.\n\n======================\n\nGENERAL \nThe paper presents an interesting empirical comparison of 3 referring\nexpression generation models. The main novelty lies in the comparison of a yet\nunpublished model called SIM-WAP (in press by Anonymous). The model is\ndescribed in SECTION 4.3 but it is not clear whether it is extended or modified\nanyhow in the current paper.  \n\nThe novelty of the paper may be considered as the comparison of the unpublished\nSIM-WAP model to existing 2 models. This complicates evaluation of the novelty\nbecause similar experiments were already performed for the other two models and\nit is unclear why this comparison was not performed in the paper where SIM-WAP\nmodel was presented. A significant novelty might be the combined model yet this\nis not stated clearly and the combination is not described with enough details.\n\nThe contribution of the paper may be considered the following: the side-by-side\ncomparison of the 3 methods for REG; analysis of zero-shot experiment results\nwhich mostly confirms similar observations in previous works; analysis of the\ncomplementarity of the combined model.                     \n\nWEAKNESSES\nUnclear novelty and significance of contributions. The work seems like an\nexperimental extension of the cited Anonymous paper where the main method was\nintroduced.    \n\nAnother weakness is the limited size of the vocabulary in the zero-shot\nexperiments that seem to be the most contributive part. \n\nAdditionally, the authors never presented significance scores for their\naccuracy results. This would have solidified the empirical contribution of the\nwork which its main value.   \n\nMy general feeling is that the paper is more appropriate for a conference on\nempirical methods such as EMNLP. \n\nLastly, I have not found any link to any usable software. Existing datasets\nhave been used for the work.  \n\nObservations by Sections: \n\nABSTRACT\n\"We compare three recent models\" -- Further in the abstract you write that you\nalso experiment with the combination of approaches. In Section 2 you write that\n\"we present a model that exploits distributional knowledge for learning\nreferential word meaning as well, but explore and compare different ways of\ncombining visual and lexical aspects of referential word meaning\" which\neventually might be a better summarization of the novelty introduced in the\npaper and give more credit to the value of your work. \n\nMy suggestion is to re-write the abstract (and eventually even some sections in\nthe paper) focusing on the novel model and results and not just stating that\nyou compare models of others.                  \n\nINTRODUCTION \n\"Determining such a name is is\" - typo \n\"concerning e.g.\" -> \"concerning, e.g.,\" \n\"having disjunct extensions.\" - specify or exemplify, please \n\"building in Figure 1\" -> \"building in Figure 1 (c)\"\n\nSECTION 4\n\"Following e.g. Lazaridou et al. (2014),\" - \"e.g.\" should be omitted  \n\nSECTION 4.2\n\"associate the top n words with their corresponding distributional vector\" -\nWhat are the values of N that you used? If there were any experiments for\nfinding the optimal values, please, describe because this is original work. The\nuse top N = K is not obvious and not obvious why it should be optimal (how\nabout finding similar vectors to each 5 in top 20?)    \n\nSECTION 4.3 \n\"we annotate its training instances with a fine-grained similarity signal\naccording to their object names.\" - please, exemplify. \n\nLANGUAGE   \nQuite a few typos in the draft. Generally, language should be cleaned up (\"as\nwell such as\"). \nAlso, I believe the use of American English spelling standard is preferable\n(e.g., \"summarise\" -> \"summarize\"). Please, double check with your conference\ntrack chairs."
  },
  {
    "people": [
      "Liu"
    ],
    "review": "- Strengths:\n     - The related work is quite thorough and the comparison with the approach\npresented in this paper makes the hypothesis of the paper stronger. The\nevaluation section is also extensive and thus, the experiments are convincing.\n\n- Weaknesses:\n     - In Section 3 it is not clear what is exactly the dataset that you used\nfor training the SVM and your own model. Furthermore, you only give the\nstarting date for collecting the testing data, but there is no other\ninformation related to the size of the dataset or the time frame when the data\nwas collected. This might also give some insight for the results and statistics\ngiven in Section 3.2.\n     - In Table 3 we can see that the number of reviewers is only slightly\nlower than the number of reviews posted (at least for hotels), which means that\nonly a few reviewers posted more than one review, in the labeled dataset. How\ndoes this compare with the full dataset in Table 2? What is the exact number of\nreviewers in Table 2 (to know what is the percentage of labeled reviewers)? It\nis also interesting to know how many reviews are made by one person on average.\nIf there are only a few reviewers that post more than one review (i.e., not\nthat much info to learn from), the results would benefit from a thorough\ndiscussion. \n\n- General Discussion:\n     This paper focuses on identifying spam reviews under the assumption that\nwe deal with a cold-start problem, i.e., we do not have enough information to\ndraw a conclusion. The paper proposes a neural network model that learns how to\nrepresent new reviews by jointly using embedded textual information and\nbehaviour information. Overall, the paper is very well written and the results\nare compelling.\n\n- Typos and/or grammar:                                 \n     - The new reviewer only provide us                                        \n\n     - Jindal and Liu (2008) make the first step -> the work is quite old, you\ncould use past tense to refer to it\n     - Usage of short form \u201ccan\u2019t\u201d, \u201ccouldn\u2019t\u201d, \u201cwhat\u2019s\u201d\ninstead of the prefered long form\n     - The following sentence is not clear and should be rephrased: \u201cThe new\nreviewer just posted one review and we have to filter it out immediately, there\nis not any historical reviews provided to us.\u201c"
  },
  {
    "people": [
      "Mikolov",
      "Hamilton"
    ],
    "review": "- Strengths: A nice, solid piece of work that builds on previous studies in a\nproductive way. Well-written and clear. \n\n- Weaknesses:\n\n Very few--possibly avoid some relatively \"empty\" statements:\n\n191 : For example, if our task is to identify words used similarly across\ncontexts, our scoring function can be specified to give high scores to terms\nwhose usage is similar across the contexts.\n\n537 : It is educational to study how annotations drawn from the same data are\nsimilar or different.\n\n- General Discussion:\nIn the first sections I was not sure that much was being done that was new or\ninteresting, as the methods seemed very reminiscent of previous methods used\nover the past 25 years to measure similarity, albeit with a few new statistical\ntwists, but conceptually in the same vein. Section 5, however, describes an\ninteresting and valuable piece of work that will be useful for future studies\non the topic. In retrospect, the background provided in sections 2-4 is useful,\nif not necessary, to support the experiments in section 5. \n\nIn short, the work and results described will be useful to others working in\nthis area, and the paper is worthy of presentation at ACL.\n\nMinor comments:\n\nWord, punctuation missing?\n264 : For word annotations, we used PPMI, SVD, and SGNS (skipgram with negative\nsampling from Mikolov et al. (2013b)) word vectors released by Hamilton et al.\n(2016).\n\nUnclear what \"multiple methods\" refers to :\n278 : some words were detected by multiple methods with CCLA"
  },
  {
    "people": [
      "Hamilton"
    ],
    "review": "This paper propose a general framework for analyzing similarities and\ndifferences in term meaning and representation in different contexts.\n\n- Strengths:\n* The framework proposed in this paper is generalizable and can be applied to\ndifferent applications, and accommodate difference notation of context,\ndifferent similarity functions, different type of word annotations. \n* The paper is well written. Very easy to follow.\n\n- Weaknesses:\n* I have concerns in terms of experiment evaluation. The paper uses qualitative\nevaluation metrics, which makes it harder to evaluate the effectiveness, or\neven the validity of proposed method. For example, table 1 compares the result\nwith Hamilton et, al using different embedding vector by listing top 10 words\nthat changed from 1900 to 1990. It's hard to tell, quantitatively, the\nperformances of CCLA. The same issue also applies to experiment 2 (comparative\nlexical analysis over context). The top 10 words may be meaningful, but what\nabout top 20, 100? what about the words that practitioner actually cares?\nWithout addressing the evaluation issue, I find it difficult to claim that CCLA\nwill benefit downstream applications."
  },
  {
    "people": [
      "Firth",
      "Arora",
      "Paperno",
      "Baroni",
      "M. Baroni"
    ],
    "review": "This paper delves into the mathematical properties of the skip-gram model,\nexplaining the reason for its success on the analogy task and for the general\nsuperiority of additive composition models. It also establishes a link between\nskip-gram and Sufficient Dimensionality Reduction.\n\nI liked the focus of this paper on explaining the properties of skip-gram, and\ngenerally found it inspiring to read. I very much appreciate the effort to\nunderstand the assumptions of the model, and the way it affects (or is affected\nby) the composition operations that it is used to perform. In that respect, I\nthink it is a very worthwhile read for the community.\n\nMy main criticism is however that the paper is linguistically rather naive. The\nauthors' use of 'compositionality' (as an operation that takes a set of words\nand returns another with the same meaning) is extremely strange. Two words can\nof course be composed and produce a vector that is a) far away from both; b)\ndoes not correspond to any other concept in the space; c) still has meaning\n(productivity wouldn't exist otherwise!) Compositionality in linguistic terms\nsimply refers to the process of combining linguistic constituents to produce\nhigher-level constructs. It does not assume any further constraint, apart from\nsome vague (and debatable) notion of semantic transparency. The paper's\nimplication (l254) that composition takes place over sets is also wrong:\nordering matters hugely (e.g. 'sugar cane' is not 'cane sugar'). This is a\nwell-known shortcoming of additive composition. \n\nAnother important aspect is that there are pragmatic factors that make humans\nprefer certain phrases to single words in particular contexts (and the\nopposite), naturally changing the underlying distribution of words in a large\ncorpus. For instance, talking of a 'male royalty' rather than a 'king' or\n'prince' usually has implications with regard to the intent of the speaker\n(here, perhaps highlighting a gender difference). This means that the equation\nin l258 (or for that matter the KL-divergence modification) does not hold, not\nbecause of noise in the data, but because of fundamental linguistic processes.\nThis point may be addressed by the section on SDR, but I am not completely sure\n(see my comments below).\n\nIn a nutshell, I think the way that the authors present composition is flawed,\nbut the paper convinces me that this is indeed what happens in skip-gram, and I\nthink this is an interesting contribution. \n\nThe part about Sufficient Dimensionality Reduction seems a little disconnected\nfrom the previous argument as it stands. I'm afraid I wasn't able to fully\nfollow the argument, and I would be grateful for some clarification in the\nauthors' response. If I understand it well, the argument is that skip-gram\nproduces a model where a word's neighbours follow some exponential\nparametrisation of a categorical distribution, but it is unclear whether this\nactually reflects the distribution of the corpus (as opposed to what happens\nin, say, a pure count-based model). The fact that skip-gram performs well\ndespite not reflecting the data is that it implements some form of SDR, which\ndoes not need to make any assumption about the underlying form of the data. But\nthen, is it fair to say that the resulting representations are optimised for\ntasks where geometrical regularities are important, regardless of the actual\npattern of the data? I.e. there some kind of denoising going on?\n\nMinor comments:\n\n- The abstract is unusually long and could, I think, be shortened.\n\n- para starting l71: I think it would be misconstrued to see circularity here.\nFirth observed that co-occurrence effects were correlated with similarity\njudgements, but those judgements are the very cognitive processes that we are\ntrying to model with statistical methods. Co-occurrence effects and vector\nspace word representations are in some sense 'the same thing', modelling an\nunderlying linguistic process we do not have direct observations for. So\npair-wise similarity is not there to break any circularity, it is there because\nit better models the kind of judgements humans known to make.\n\n- l296: I think 'paraphrase' would be a better word than 'synonym' here, given\nthat we are comparing a set of words with a unique lexical item.\n\n- para starting l322: this is interesting, and actually, a lot of the zipfian\ndistribution (the long tail) is fairly uniform.\n\n- l336: it is probably worth pointing out that the analogy relation does not\nhold so well in practice and requires to 'ignore' the first returned neighbour\nof the analogy computation (which is usually one of the observed terms).\n\n- para starting l343: I don't find it so intuitive to say that 'man' would be a\nsynonym/paraphrase of anything involving 'woman'. The subtraction involved in\nthe analogy computation is precisely not a straightforward composition\noperation, as it involves an implicit negation. \n\n- A last, tiny general comment. It is usual to write p(w|c) to mean the\nprobability of a word given a context, but in the paper 'w' is actually the\ncontext and 'c' the target word. It makes reading a little bit harder...\nPerhaps change the notation?\n\nLiterature:\n\nThe claim that Arora (2016) is the only work to try and understand vector\ncomposition is a bit strong. For instance, see the work by Paperno & Baroni on\nexplaining the success of addition as a composition method over PMI-weighted\nvectors:\n\nD. Paperno and M. Baroni. 2016. When the whole is less than the sum of its\nparts: How composition affects PMI values in distributional semantic vectors.\nComputational Linguistics 42(2): 345-350.\n\n***\nI thank the authors for their response and hope to see this paper accepted."
  },
  {
    "people": [
      "Choi",
      "Chiu"
    ],
    "review": "Summary: This paper presents a model for embedding words, phrases and concepts\ninto vector spaces. To do so, it uses an ontology of concepts, each of which is\nmapped to phrases. These phrases are found in text corpora and treated as\natomic symbols. Using this, the paper uses what is essentially the skip-gram\nmethod to train embeddings for words, the now atomic phrases and also the\nconcepts associated with them. The proposed work is evaluated on the task of\nconcept similarity and relatedness using UMLS and Yago to act as the backing\nontologies.\n\nStrengths:\n\nThe key question addressed by the paper is that phrases that are not lexically\nsimilar can be semantically close and, furthermore, not all phrases are\ncompositional in nature. To this end, the paper proposes a plausible model to\ntrain phrase embeddings. The trained embeddings are shown to be competitive or\nbetter at identifying similarity between concepts.\n\nThe software released with the paper could be useful for biomedical NLP\nresearchers.\n\n- Weaknesses:\n\nThe primary weakness of the paper is that the model is not too novel. It is\nessentially a tweak to skip-gram. \n\nFurthermore, the full model presented by the paper doesn't seem to be the best\none in the results (in Table 4). On the two Mayo datasets, the Choi baseline is\nsubstantially better. A similar trend seems to dominate Table 6 too. On the\nlarger UMNSRS data, the proposed model is at best competitive with previous\nsimpler models (Chiu).\n\n- General Discussion:\n\nThe paper says that it is uses known phrases as distant supervision to train\nembeddings. However, it is not clear what the \"supervision\" here is. If I\nunderstand the paper correctly, every occurrence of a phrase associated with a\nconcept provides the context to train word embeddings. But this is not\nsupervision in the traditional sense (say for identifying the concept in the\ntext or other such predictive tasks). So the terminology is a bit confusing.\n\n The notation introduced in Section 3.2 (E_W, etc) is never used in the rest of\nthe paper.\n\nThe use of \\beta to control for compositionality of phrases by words is quite\nsurprising. Essentially, this is equivalent to saying that there is a single\nglobal constant that decides \"how compositional\" any phrase should be. The\nsurprising part here is that the actual values of \\beta chosen by cross\nvalidation from Table 3 are odd. For PM+CL and WikiNYT, it is zero, which\nbasically argues against compositionality. \n\nThe experimental setup for table 4 needs some explanation. The paper says that\nthe data labels similarity/relatedness of concepts (or entities). However, if\nthe concepts-phrases mapping is really many-to-many, then how are the\nphrase/word vectors used to compute the similarities? It seems that we can only\nuse the concept vectors.\n\nIn table 5, the approximate phr method (which approximate concepts with the\naverage of the phrases in them) is best performing. So it is not clear why we\nneed the concept ontology. Instead, we could have just started with a seed set\nof phrases to get the same results."
  },
  {
    "people": [
      "Pedersen"
    ],
    "review": "The authors presents a method to jointly embed words, phrases and concepts,\nbased on plain text corpora and a manually-constructed ontology, in which\nconcepts are represented by one or more phrases. They apply their method in the\nmedical domain using the UMLS ontology, and in the general domain using the\nYAGO ontology. To evaluate their approach, the authors compare it to simpler\nbaselines and prior work, mostly on intrinsic similarity and relatedness\nbenchmarks. They use existing benchmarks in the medical domain, and use\nmechanical turkers to generate a new general-domain concept similarity and\nrelatedness dataset, which they also intend to release. They report results\nthat are comparable to prior work.\n\nStrengths:\n\n- The proposed joint embedding model is straightforward and makes reasonable\nsense to me. Its main value in my mind is in reaching a (configurable) middle\nground between treating phrases as atomic units on one hand to considering\ntheir\ncompositionallity on the other. The same approach is applied to concepts being\n\u2018composed\u2019 of several representative phrases.\n\n-  The paper describes a decent volume of work, including model development,\nan additional contribution in the form of a new evaluation dataset, and several\nevaluations and analyses performed.\n\nWeaknesses:\n\n- The evaluation reported in this paper includes only intrinsic tasks, mainly\non similarity/relatedness datasets. As the authors note, such evaluations are\nknown to have very limited power in predicting the utility of embeddings in\nextrinsic tasks. Accordingly, it has become recently much more common to\ninclude at least one or two extrinsic tasks as part of the evaluation of\nembedding models.\n\n- The similarity/relatedness evaluation datasets used in the paper are\npresented as datasets recording human judgements of similarity between\nconcepts. However, if I understand correctly, the actual judgements were made\nbased on presenting phrases to the human annotators, and therefore they should\nbe considered as phrase similarity datasets, and analyzed as such.\n\n- The medical concept evaluation dataset, \u2018mini MayoSRS\u2019 is extremely small\n(29 pairs), and its larger superset \u2018MayoSRS\u2019 is only a little larger (101\npairs) and was reported to have a relatively low human annotator agreement. The\nother medical concept evaluation dataset, \u2018UMNSRS\u2019, is more reasonable in\nsize, but is based only on concepts that can be represented as single words,\nand were represented as such to the human annotators. This should be mentioned\nin the paper and makes the relevance of this dataset questionable with respect\nto representations of phrases and general concepts. \n\n- As the authors themselves note, they (quite extensively) fine tune their\nhyperparameters on the very same datasets for which they report their results\nand compare them with prior work. This makes all the reported results and\nanalyses questionable.\n\n- The authors suggest that their method is superb to prior work, as it achieved\ncomparable results while prior work required much more manual annotation. I\ndon't think this argument is very strong because the authors also use large\nmanually-constructed ontologies, and also because the manually annotated\ndataset used in prior work comes from existing clinical records that did not\nrequire dedicated annotations.\n\n- In general, I was missing more useful insights into what is going on behind\nthe reported numbers. The authors try to treat the relation between a phrase\nand its component words on one hand, and a concept and its alternative phrases\non the other, as similar types of a compositional relation. However, they\nare different in nature and in my mind each deserves a dedicated analysis. For\nexample, around line 588, I would expect an NLP analysis specific to the\nrelation between phrases and their component words. Perhaps the reason for the\nreported behavior is dominant phrase headwords, etc. Another aspect that was\nabsent but could strengthen the work, is an investigation of the effect of the\nhyperparameters that control the tradeoff between the atomic and compositional\nviews of phrases and concepts.\n\nGeneral Discussion:\n\nDue to the above mentioned weaknesses, I recommend to reject this submission. I\nencourage the authors to consider improving their evaluation datasets and\nmethodology before re-submitting this paper.\n\nMinor comments:\n\n- Line 069: contexts -> concepts\n\n- Line 202: how are phrase overlaps handled?\n\n- Line 220: I believe the dimensions should be |W| x d. Also, the terminology\n\u2018negative sampling matrix\u2019 is confusing as the model uses these embeddings\nto represent contexts in positive instances as well.\n\n- Line 250: regarding \u2018the observed phrase just completed\u2019, it not clear to\nme how words are trained in the joint model. The text may imply that only the\nlast words of a phrase are considered as target words, but that doesn\u2019t make\nsense. \n\n- Notation in Equation 1 is confusing (using c instead of o)\n\n- Line 361: Pedersen et al 2007 is missing in the reference section.\n\n- Line 388: I find it odd to use such a fine-grained similarity scale (1-100) \nfor human annotations.\n\n- Line 430: The newly introduced term \u2018strings\u2019 here is confusing. I\nsuggest to keep using \u2018phrases\u2019 instead.\n\n- Line 496: Which task exactly was used for the hyper-parameter tuning?\nThat\u2019s important. I couldn\u2019t find that even in the appendix.\n\n- Table 3: It\u2019s hard to see trends here, for instance PM+CL behaves rather\ndifferently than either PM or CL alone. It would be interesting to see\ndevelopment set trends with respect to these hyper-parameters.\n\n- Line 535: missing reference to Table 5."
  },
  {
    "people": [
      "Pust",
      "Pourdamghani",
      "Xue",
      "Xue",
      "Zhou",
      "Puzikov",
      "Palmer",
      "Palmer"
    ],
    "review": "- Strengths:\n\nThe paper demonstrates that seq2seq models can be comparatively effectively\napplied to the tasks of AMR parsing and AMR realization by linearization of an\nengineered pre-processed version of the AMR graph and associated sentence,\ncombined with 'Paired Training' (iterative back-translation of monolingual data\ncombined with fine-tuning). While parsing performance is worse than other\nreported papers (e.g., Pust et al., 2015), those papers used additional\nsemantic information. \n\nOn the task of AMR realization, the paper demonstrates that utilizing\nadditional monolingual data (via back-translation) is effective relative to a\nseq2seq model that does not use such information. (See note below about\ncomparing realization results to previous non-seq2seq work for the realization\ntask.)\n\n- Weaknesses:\n\n At a high-level, the main weakness is that the paper aims for empirical\ncomparisons, but in comparing to other work, multiple aspects/dimensions are\nchanging at the same time (in some cases, not comparable due to access to\ndifferent information), complicating comparisons. \n\nFor example, with the realization results (Table 2), PBMT (Pourdamghani et al.,\n2016) is apparently trained on LDC2014T12, which consists of 13,051 sentences,\ncompared to the model of the paper, which is trained on LDC2015E86, which\nconsists of 19,572 sentences, according to http://amr.isi.edu/download.html.\nThis is used in making the claim of over 5 points improvement over the\nstate-of-the-art (PBMT) in line 28/29, 120/121, and line 595, and is only\nqualified in the caption of Table 2. To make a valid comparison, the approach\nof the paper or PBMT needs to be re-evaluated after using the same training\ndata.\n\n- General Discussion:\n\nIs there any overlap between the sentences in your Gigaword sample and the test\nsentences of LDC2015E86? Apparently LDC2015E86 contains data from the ''proxy\nreport data in LDC's DEFT Narrative Text Source Data R1 corpus (LDC2013E19)''\n(Accessible with LDC account: https://catalog.ldc.upenn.edu/LDC2015E86). It\nseems LDC2013E19 contains data from Gigaword\n(https://catalog.ldc.upenn.edu/LDC2013E19). Apparently AMR corpus LDC2014T12\nalso contained ''data from newswire articles selected from the English Gigaword\nCorpus, Fifth Edition'' (publicly accessible link:\nhttps://catalog.ldc.upenn.edu/docs/LDC2014T12/README.txt). Please check that\nthere is no test set contamination.\n\nLine 244-249: Did these two modifications to the encoder make a significant\ndifference in effectiveness? What was the motivation behind these changes?\n\nPlease make it clear (in an appendix is fine) for replication purposes whether\nthe implementation is based on an existing seq2seq framework.\n\nLine 321: What was the final sequence length used? (Consider adding such\ndetails in an appendix.)\n\nPlease label the columns of Table 1 (presumably dev and test). Also, there is a\nmismatch between Table 1 and the text: ''Table 1 summarizes our development\nresults for different rounds of self-training.'' It appears that only the\nresults of the second round of self-training are shown.\n\nAgain, the columns for Table 1 are not labeled, but should the results for\ncolumn 1 for CAMR instead be 71.2, 63.9, 67.3--the last line of Table 2 in\nhttp://www.aclweb.org/anthology/S16-1181 which is the configuration for\n+VERB+RNE+SRL+WIKI? It looks like the second from last row of Table 2 in CAMR\n(Wang et al., 2016) is currently being used. On this note, how does your\napproach handle the wikification information introduced in LDC2015E86? \n\n7.1.Stochastic is missing a reference to the example.\n\nLine 713-715: This seems like a hypothesis to be tested empirically rather than\na forgone conclusion, as implied here.\n\nGiven an extra page, please add a concluding section.\n\nHow are you performing decoding? Are you using beam search?\n\nAs a follow-up to line 161-163, it doesn't appear that the actual vocabulary\nsize used in the experiments is mentioned. After preprocessing, are there any\nremaining unseen tokens in dev/test? In other words, is the unknown word\nreplacement mechanism (using the attention weights), as described in Section\n3.2, ever used? \n\nFor the realization case study, it would be of interest to see performance on\nphenomena that are known limitations of AMR, such as quantification and tense\n(https://github.com/amrisi/amr-guidelines/blob/master/amr.md).\n\nThe paper would benefit from a brief discussion (perhaps a couple sentences)\nmotivating the use of AMR as opposed to other semantic formalisms, as well as\nwhy the human-annotated AMR information/signal might be useful as opposed to\nlearning a model (e.g., seq2seq itself) directly for a task (e.g., machine\ntranslation).\n\nFor future work (not taken directly into account in the scores given here for\nthe review, since the applicable paper is not yet formally published in the\nEACL proceedings): For parsing, what accounts for the difference from previous\nseq2seq approaches? Namely, between Peng and Xue, 2017 and AMR-only (as in\nTable 1) is the difference in effectiveness being driven by the architecture,\nthe preprocessing, linearization, data, or some combination thereof? Consider\nisolating this difference. (Incidentally, the citation for Peng and Xue, 2017\n[''Addressing the Data Sparsity Issue in Neural AMR Parsing''] should\napparently be Peng et al. 2017\n(http://eacl2017.org/index.php/program/accepted-papers;\nhttps://arxiv.org/pdf/1702.05053.pdf). The authors are flipped in the\nReferences section.\n\nProofreading (not necessarily in the order of occurrence; note that these are\nprovided for reference and did not influence my scoring of the paper):\n\noutperform state of the art->outperform the state of the art\n\nZhou et al. (2016), extend->Zhou et al. (2016) extend\n\n(2016),Puzikov et al.->(2016), Puzikov et al.\n\nPOS-based features, that->POS-based features that\n\nlanguage pairs, by creating->language pairs by creating\n\nusing a back-translation MT system and mix it with the human\ntranslations.->using a back-translation MT system, and mix it with the human\ntranslations.\n\nProbBank-style (Palmer et al., 2005)->PropBank-style (Palmer et al., 2005)\n\nindependent parameters ,->independent parameters,\n\nfor the 9.6% of tokens->for 9.6% of tokens\n\nmaintaining same embedding sizes->maintaining the same embedding sizes\n\nTable 4.Similar->Table 4. Similar\n\nrealizer.The->realizer. The\n\nNotation: Line 215, 216: The sets C and W are defined, but never subsequently\nreferenced. (However, W could/should be used in place of ''NL'' in line 346 if\nthey are referring to the same vocabulary.)"
  },
  {
    "people": [
      "Pourdamghani",
      "Pourdamghani",
      "Luong",
      "Xue"
    ],
    "review": "The authors use self-training to train a seq2seq-based AMR parser using a small\nannotated corpus and large amounts of unlabeled data. They then train a\nsimilar,\nseq2seq-based AMR-to-text generator using the annotated corpus and automatic\nAMRs produced by their parser from the unlabeled data. They use careful\ndelexicalization for named entities in both tasks to avoid data sparsity. This\nis the first sucessful application of seq2seq models to AMR parsing and\ngeneration, and for generation, it most probably improves upon state-of-the\nart.\n\nIn general, I really liked the approach as well as the experiments and the\nfinal performance analysis.\nThe methods used are not revolutionary, but they are cleverly combined to\nachieve practial results.\nThe description of the approach is quite detailed, and I believe that it is\npossible to reproduce the experiments without significant problems.\nThe approach still requires some handcrafting, but I believe that this can be\novercome in the future and that the authors are taking a good direction.\n\n(RESOLVED BY AUTHORS' RESPONSE) However, I have been made aware by another\nreviewer of a data overlap in the\nGigaword and the Semeval 2016 dataset. This is potentially a very serious\nproblem -- if there is a significant overlap in the test set, this would\ninvalidate the results for generation (which are the main achievemnt of the\npaper). Unless the authors made sure that no test set sentences made their way\nto training through Gigaword, I cannot accept their results.\n\n(RESOLVED BY AUTHORS' RESPONSE)  Another question raised by another reviewer,\nwhich I fully agree with, is the \n5.4 point claim when comparing to a system tested on an earlier version of the\nAMR dataset. The paper could probably still claim improvement over state-of-the\nart, but I am not sure I can accept the 5.4 points claim in a direct comparison\nto Pourdamghani et al. -- why haven't the authors also tested their system on\nthe older dataset version (or obtained Pourdamghani et al.'s scores for the\nnewer version)?\n\nOtherwise I just have two minor comments to experiments: \n\n- Statistical significance tests would be advisable (even if the performance\ndifference is very big for generation).\n\n- The linearization order experiment should be repeated with several times with\ndifferent random seeds to overcome the bias of the particular random order\nchosen.\n\nThe form of the paper definitely could be improved.\nThe paper is very dense at some points and proofreading by an independent\nperson (preferably an English native speaker) would be advisable. \nThe model (especially the improvements over Luong et al., 2015) could be\nexplained in more detail; consider adding a figure. The experiment description\nis missing the vocabulary size used.\nMost importantly, I missed a formal conclusion very much -- the paper ends\nabruptly after qualitative results are described, and it doesn't give a final\noverview of the work or future work notes.\n\nMinor factual notes:\n\n- Make it clear that you use the JAMR aligner, not the whole parser (at\n361-364). Also, do you not use the recorded mappings also when testing the\nparser (366-367)?\n\n- Your non-Gigaword model only improves on other seq2seq models by 3.5 F1\npoints, not 5.4 (at 578).\n\n- \"voters\" in Figure 1 should be \"person :ARG0-of vote-01\" in AMR.\n\nMinor writing notes:\n\n- Try rewording and simplifying text near 131-133, 188-190, 280-289, 382-385,\n650-659, 683, 694-695.\n\n- Inter-sentitial punctuation is sometimes confusing and does not correspond to\nmy experience with English syntax. There are lots of excessive as well as\nmissing commas.\n\n- There are a few typos (e.g., 375, 615), some footnotes are missing full\nstops.\n\n- The linearization description is redundant at 429-433 and could just refer to\nSect. 3.3.\n\n- When refering to the algorithm or figures (e.g., near 529, 538, 621-623),\nenclose the references in brackets rather than commas.\n\n- I think it would be nice to provide a reference for AMR itself and for the\nmulti-BLEU script.\n\n- Also mention that you remove AMR variables in Footnote 3.\n\n- Consider renaming Sect. 7 to \"Linearization Evaluation\".\n\n- The order in Tables 1 and 2 seems a bit confusing to me, especially when your\nsystems are not explicitly marked (I would expect your systems at the bottom).\nAlso, Table 1 apparently lists development set scores even though its\ndescription says otherwise.\n\n- The labels in Table 3 are a bit confusing (when you read the table before\nreading the text).\n\n- In Figure 2, it's not entirely visible that you distinguish month names from\nmonth numbers, as you state at 376.\n\n- Bibliography lacks proper capitalization in paper titles, abbreviations and\nproper names should be capitalized (use curly braces to prevent BibTeX from\nlowercasing everything).\n\n- The \"Peng and Xue, 2017\" citation is listed improperly, there are actually\nfour authors.\n\n***\nSummary:\n\nThe paper presents first competitive results for neural AMR parsing and\nprobably new state-of-the-art for AMR generation, using seq2seq models with\nclever\npreprocessing and exploiting large a unlabelled corpus. Even though revisions\nto the text are advisable, I liked the paper and would like to see it at the\nconference. \n\n(RESOLVED BY AUTHORS' RESPONSE) However, I am not sure if the comparison with\nprevious\nstate-of-the-art on generation is entirely sound, and most importantly, whether\nthe good results are not actually caused by data overlap of Gigaword\n(additional training set) with the test set.\n\n***\nComments after the authors' response:\n\nI thank the authors for addressing both of the major problems I had with the\npaper. I am happy with their explanation, and I raised my scores assuming that\nthe authors will reflect our discussion in the final paper."
  },
  {
    "people": [
      "Faruqui",
      "Soricut",
      "Och",
      "Wieting"
    ],
    "review": "The authors propose \u2018morph-fitting\u2019, a method that retrofits any given set\nof trained word embeddings based on a morphologically-driven objective that (1)\npulls inflectional forms of the same word together (as in \u2018slow\u2019 and\n\u2018slowing\u2019) and (2) pushes derivational antonyms apart (as in\n\u2018expensive\u2019 and \u2018inexpensive\u2019). With this, the authors aim to improve\nthe representation of low-frequency inflections of words as well as mitigate\nthe tendency of corpus-based word embeddings to assign similar representations\nto antonyms. The method is based on relatively simple manually-constructed\nmorphological rules and is demonstrated on both English, German, Italian and\nRussian. The experiments include intrinsic word similarity benchmarks, showing\nnotable performance improvements achieved by applying morph-fitting to several\ndifferent corpus-based embeddings. Performance improvement yielding new\nstate-of-the-art results is also demonstrated for German and Italian on an\nextrinsic task - dialog state tracking. \n\nStrengths:\n\n- The proposed method is simple and shows nice performance improvements across\na number of evaluations and in several languages. Compared to previous\nknowledge-based retrofitting approaches (Faruqui et al., 2015), it relies on a\nfew manually-constructed rules, instead of a large-scale knowledge base, such\nas an ontology.\n\n- Like previous retrofitting approaches, this method is easy to apply to\nexisting sets of embeddings and therefore it seems like the software that the\nauthors intend to release could be useful to the community.\n\n- The method and experiments are clearly described.\u2028\n\nWeaknesses:\n\n- I was hoping to see some analysis of why the morph-fitted embeddings worked\nbetter in the evaluation, and how well that corresponds with the intuitive\nmotivation of the authors. \n\n- The authors introduce a synthetic word similarity evaluation dataset,\nMorph-SimLex. They create it by applying their presumably\nsemantic-meaning-preserving morphological rules to SimLex999 to generate many\nmore pairs with morphological variability. They do not manually annotate these\nnew pairs, but rather use the original similarity judgements from SimLex999.\nThe obvious caveat with this dataset is that the similarity scores are presumed\nand therefore less reliable. Furthermore, the fact that this dataset was\ngenerated by the very same rules that are used in this work to morph-fit word\nembeddings, means that the results reported on this dataset in this work should\nbe taken with a grain of salt. The authors should clearly state this in their\npaper.\n\n- (Soricut and Och, 2015) is mentioned as a future source for morphological\nknowledge, but in fact it is also an alternative approach to the one proposed\nin this paper for generating morphologically-aware word representations. The\nauthors should present it as such and differentiate their work.\n\n- The evaluation does not include strong morphologically-informed embedding\nbaselines. \n\nGeneral Discussion:\n\nWith the few exceptions noted, I like this work and I think it represents a\nnice contribution to the community. The authors presented a simple approach and\nshowed that it can yield nice improvements using various common embeddings on\nseveral evaluations and four different languages. I\u2019d be happy to see it in\nthe conference.\n\nMinor comments:\n\n- Line 200: I found this phrasing unclear: \u201cWe then query \u2026 of linguistic\nconstraints\u201d.\n\n- Section 2.1: I suggest to elaborate a little more on what the delta is\nbetween the model used in this paper and the one it is based on in Wieting\n2015. It seemed to me that this was mostly the addition of the REPEL part.\n\n- Line 217: \u201cThe method\u2019s cost function consists of three terms\u201d - I\nsuggest to spell this out in an equation.\n\n- Line 223:  x and t in this equation (and following ones) are the vector\nrepresentations of the words. I suggest to denote that somehow. Also, are the\nvectors L2-normalized before this process? Also, when computing \u2018nearest\nneighbor\u2019 examples do you use cosine or dot-product? Please share these\ndetails.\n\n- Line 297-299: I suggest to move this text to Section 3, and make the note\nthat you did not fine-tune the params in the main text and not in a footnote.\n\n- Line 327: (create, creates) seems like a wrong example for that rule.\u2028\n\n* I have read the author response"
  },
  {
    "people": [
      "Peter Hoff et al",
      "P.D. Hoff",
      "A.E. Raftery",
      "M.S. Handcock",
      "Airoldi",
      "Blei",
      "Fienberg",
      "Xing"
    ],
    "review": "This paper addresses the network embedding problem by introducing a neural\nnetwork model which uses both the network structure and associated text on the\nnodes, with an attention model to vary the textual representation based on the\ntext of the neighboring nodes.\n\n- Strengths:\n\nThe model leverages both the network and the text to construct the latent\nrepresentations, and the mutual attention approach seems sensible.\n\nA relatively thorough evaluation is provided, with multiple datasets,\nbaselines, and evaluation tasks.\n\n- Weaknesses:\n\nLike many other papers in the \"network embedding\" literature, which use neural\nnetwork techniques inspired by word embeddings to construct latent\nrepresentations of nodes in a network, the previous line of work on\nstatistical/probabilistic modeling of networks is ignored.  In particular, all\n\"network embedding\" papers need to start citing, and comparing to, the work on\nthe latent space model of Peter Hoff et al., and subsequent papers in both\nstatistical and probabilistic machine learning publication venues:\n\nP.D. Hoff, A.E. Raftery, and M.S. Handcock. Latent space approaches to social\nnetwork analysis. J. Amer. Statist. Assoc., 97(460):1090\u20131098, 2002.\n\nThis latent space network model, which embeds each node into a low-dimensional\nlatent space, was written as far back as 2002, and so it far pre-dates neural\nnetwork-based network embeddings.\n\nGiven that the aim of this paper is to model differing representations of\nsocial network actors' different roles, it should really cite and compare to\nthe mixed membership stochastic blockmodel (MMSB):\n\nAiroldi, E. M., Blei, D. M., Fienberg, S. E., & Xing, E. P. (2008). Mixed\nmembership stochastic blockmodels. Journal of Machine Learning Research.\n\nThe MMSB allows each node to randomly select a different \"role\" when deciding\nwhether to form each edge.\n\n- General Discussion:\n\nThe aforementioned statistical models do not leverage text, and they do not use\nscalable neural network implementations based on negative sampling, but they\nare based on well-principled generative models instead of heuristic neural\nnetwork objective functions and algorithms.  There are more recent extensions\nof these models and inference algorithms which are more scalable, and which do\nleverage text.\n\nIs the difference in performance between CENE and CANE in Figure 3\nstatistically insignificant? (A related question: were the experiments repeated\nmore than once with random train/test splits?)\n\nWere the grid searches for hyperparameter values, mentioned in Section 5.3,\nperformed with evaluation on the test set (which would be problematic), or on a\nvalidation set, or on the training set?"
  },
  {
    "people": [
      "Mostafazadeh",
      "Mostafazadeh",
      "Turker",
      "Kathy",
      "Kathy",
      "Turker"
    ],
    "review": "The paper analyzes the story endings (last sentence of a 5-sentence story) in\nthe corpus built for the story cloze task (Mostafazadeh et al. 2016), and\nproposes a model based on character and word n-grams to classify story endings.\nThe paper also shows better performance on the story cloze task proper\n(distinguishing between \"right\" and \"wrong\" endings) than prior work.\n\nWhereas style analysis is an interesting area and you show better results than\nprior work on the story cloze task, there are several issues with the paper.\nFirst, how do you define \"style\"? Also, the paper needs to be restructured (for\ninstance, your section\n\"Results\" actually mixes some results and new experiments) and clarified (see\nbelow for questions/comments): right now, it is quite difficult for the reader\nto follow what data is used for the different experiments, and what data the\ndiscussion refers to.\n\n(1) More details about the data used is necessary in order to assess the claim\nthat \"subtle writing task [...] imposes different styles on the author\" (lines\n729-732). How many stories are you looking at, written by how many different\npersons? And how many stories are there per person? From your description of\nthe post-analysis of coherence, only pairs of stories written by the same\nperson in which one was judged as \"coherent\" and the other one as \"neutral\" are\nchosen. Can you confirm that this is the case? So perhaps your claim is\njustified for your \"Experiment 1\". However my understanding is that in\nexperiment 2 where you compare \"original\" vs. \"right\" or \"original\" vs.\n\"wrong\", we do not have the same writers. So I am not convinced lines 370-373\nare correct.\n\n(2) A lot in the paper is simply stated without any justifications. For\ninstance how are the \"five frequent\" POS and words chosen? Are they the most\nfrequent words/POS? (Also theses tables are puzzling: why two bars in the\nlegend for each category?). Why character *4*-grams? Did you tune that on the\ndevelopment set? If these were not the most frequent features, but some that\nyou chose among frequent POS and words, you need to justify this choice and\nespecially link the choice to \"style\". How are these features reflecting\n\"style\"?\n\n(3) I don't understand how the section \"Design of NLP tasks\" connects to the\nrest of the paper, and to your results. But perhaps this is because I am lost\nin what \"training\" and \"test\" sets refer to here.\n\n(4) It is difficult to understand how your model differs from previous work.\nHow do we reconcile lines 217-219 (\"These results suggest that real\nunderstanding of text is required in order to solve the task\") with your\napproach?\n\n(5) The terminology of \"right\" and \"wrong\" endings is coming from Mostafazadeh\net al., but this is a very bad choice of terms. What exactly does a \"right\" or\n\"wrong\" ending mean (\"right\" as in \"coherent\" or \"right\" as in \"morally good\")?\nI took a quick look, but couldn't find the exact prompts given to the Turkers.\nI think this needs to be clarified: as it is, the first paragraph of your\nsection \"Story cloze task\" (lines 159-177) is not understandable.\n\nOther questions/comments:\n\nTable 1. Why does the \"original\" story differ from the coherent and incoherent\none? From your description of the corpus, it seems that one Turker saw the\nfirst 4 sentences of the original story and was then ask to write one sentence\nending the story in a \"right\" way (or did they ask to provide a \"coherent\"\nending?) and one sentence ending the story in a \"wrong\" way (or did they ask to\nprovide an \"incoherent\" ending)? I don't find the last sentence of the\n\"incoherent\" story that incoherent... If the only shoes that Kathy finds great\nare $300, I can see how Kathy doesn't like buying shoes ;-) This led me to\nwonder how many Turkers judged the coherence of the story/ending and how\nvariable the judgements were. What criterion was used to judge a story coherent\nor incoherent? Also does one Turker judge the coherence of both the \"right\" and\n\"wrong\" endings, making it a relative judgement? Or was this an absolute\njudgement? This would have huge implications on the ratings.\n\nLines 380-383: What does \"We randomly sample 5 original sets\" mean?\n\nLine 398: \"Virtually all sentences\"? Can you quantify this?\n\nTable 5: Could we see the weights of the features? \n\nLine 614: \"compared to ending an existing task\": the Turkers are not ending a\n\"task\"\n\nLine 684-686: \"made sure each pair of endings was written by the same author\"\n-> this is true for the \"right\"/\"wrong\" pairs, but not for the \"original\"-\"new\"\npairs, according to your description.\n\nLine 694: \"shorter text spans\": text about what? This is unclear.\n\nLines 873-875: where is this published?"
  },
  {
    "people": [
      "Liu",
      "Zhiyuan",
      "Yabin Zheng",
      "Maosong Sun",
      "Zhang",
      "Wang",
      "Gong",
      "Huang"
    ],
    "review": "This paper divides the keyphrases into two types: (1) Absent key phrases (such\nphrases do not match any contiguous subsequences of the source document) and\n(2) Present key phrases (such key phrases fully match a part of the text). The\nauthors used RNN based generative models (discussed as RNN and Copy RNN) for\nkeyphrase prediction and copy mechanism in RNN to predict the already occurred\nphrases. \n\nStrengths:\n\n1. The formation and extraction of key phrases, which are absent in the current\ndocument is an interesting idea of significant research interests. \n\n2. The paper is easily understandable.\n\n3. The use of RNN and Copy RNN in the current context is a new idea. As, deep\nrecurrent neural networks are already used in keyphrase extraction (shows very\ngood performance also), so, it will be interesting to have a proper motivation\nto justify the use of  RNN and Copy RNN over deep recurrent neural networks. \n\nWeaknesses:\n\n1. Some discussions are required on the convergence of the proposed joint\nlearning process (for RNN and CopyRNN), so that readers can understand, how the\nstable points in probabilistic metric space are obtained? Otherwise, it may be\ntough to repeat the results.\n\n2. The evaluation process shows that the current system (which extracts 1.\nPresent and 2. Absent both kinds of keyphrases) is evaluated against baselines\n(which contains only \"present\" type of keyphrases). Here there is no direct\ncomparison of the performance of the current system w.r.t. other\nstate-of-the-arts/benchmark systems on only \"present\" type of key phrases. It\nis important to note that local phrases (keyphrases) are also important for the\ndocument. The experiment does not discuss it explicitly. It will be interesting\nto see the impact of the RNN and Copy RNN based model on automatic extraction\nof local or \"present\" type of key phrases.\n\n3. The impact of document size in keyphrase extraction is also an important\npoint. It is found that the published results of [1], (see reference below)\nperforms better than (with a sufficiently high difference) the current system\non Inspec (Hulth, 2003) abstracts dataset. \n\n4. It is reported that current system uses 527,830 documents for training,\nwhile 40,000 publications are held out for training baselines. Why are all\npublications not used in training the baselines? Additionally,        The topical\ndetails of the dataset (527,830 scientific documents) used in training RNN and\nCopy RNN are also missing. This may affect the chances of repeating results.\n\n5. As the current system captures the semantics through RNN based models. So,\nit would be better to compare this system, which also captures semantics. Even,\nRef-[2] can be a strong baseline to compare the performance of the current\nsystem.\n\nSuggestions to improve:\n\n1. As, per the example, given in the Figure-1, it seems that all the \"absent\"\ntype of key phrases are actually \"Topical phrases\". For example: \"video\nsearch\", \"video retrieval\", \"video indexing\" and \"relevance ranking\", etc.\nThese all define the domain/sub-domain/topics of the document. So, In this\ncase, it will be interesting to see the results (or will be helpful in\nevaluating \"absent type\" keyphrases): if we identify all the topical phrases of\nthe entire corpus by using tf-idf and relate the document to the high-ranked\nextracted topical phrases (by using Normalized Google Distance, PMI, etc.). As\nsimilar efforts are already applied in several query expansion techniques (with\nthe aim to relate the document with the query, if matching terms are absent in\ndocument).\n\nReference:\n1. Liu, Zhiyuan, Peng Li, Yabin Zheng, and Maosong Sun. 2009b. Clustering to\nfind exemplar terms for keyphrase extraction. In Proceedings of the 2009\nConference on Empirical Methods in Natural Language Processing, pages\n257\u2013266.\n\n2. Zhang, Q., Wang, Y., Gong, Y., & Huang, X. (2016). Keyphrase extraction\nusing deep recurrent neural networks on Twitter. In Proceedings of the 2016\nConference on Empirical Methods in Natural Language Processing (pp. 836-845)."
  },
  {
    "people": [
      "Chen",
      "Schnabel"
    ],
    "review": "This work showed that word representation learning can benefit from sememes\nwhen used in an appropriate attention scheme. Authors hypothesized that sememes\ncan act as an essential regularizer for WRL and WSI tasks and proposed SE-WL\nmodel which detects word senses and learn representations simultaneously.\nThough experimental results indicate that WRL benefits, exact gains for WSI are\nunclear since a qualitative case study of a couple of examples has only been\ndone. Overall, paper is well-written and well-structured.\n\nIn the last paragraph of introduction section, authors tried to tell three\ncontributions of this work. (1) and (2) are more of novelties of the work\nrather than contributions. I see the main contribution of the work to be the\nresults which show that we can learn better word representations (unsure about\nWSI) by modeling sememe information than other competitive baselines. (3) is\nneither a contribution nor a novelty.\n\nThe three strategies tried for SE-WRL modeling makes sense and can be\nintuitively ranked in terms of how well they will work. Authors did a good job\nexplaining that and experimental results supported the intuition but the\nreviewer also sees MST as a fourth strategy rather than a baseline inspired by\nChen et al. 2014 (many WSI systems assume one sense per word given a context).\nMST many times performed better than SSA and SAC. Unless authors missed to\nclarify otherwise, MST seems to be exactly like SAT with a difference that\ntarget word is represented by the most probable sense rather than taking an\nattention weighted average over all its senses. MST is still an attention based\nscheme where sense with maximum attention weight is chosen though it has not\nbeen clearly mentioned if target word is represented by chosen sense embedding\nor some function of it.\n\nAuthors did not explain the selection of datasets for training and evaluation\ntasks. Reference page to Sogou-T text corpus did not help as reviewer does not\nknow Chinese language. It was unclear which exact dataset was used as there are\nseveral datasets mentioned on that page. Why two word similarity datasets were\nused and how they are different  (like does one has more rare words than\nanother) since different models performed differently on these datasets. The\nchoice of these datasets did not allow evaluating against results of other\nworks which makes the reviewer wonder about next question.\n\nAre proposed SAT model results state of the art for Chinese word similarity? \nE.g. Schnabel et al. (2015) report a score of 0.640 on WordSim-353 data by\nusing CBOW word embeddings.\n\nReviewer needs clarification on some model parameters like vocabulary sizes for\nwords (Does Sogou-T contains 2.7 billion unique words) and word senses (how\nmany word types from HowNet). Because of the notation used it is not clear if\nembeddings for senses and sememes for different words were shared. Reviewer\nhopes that is the case but then why 200 dimensional embeddings were used for\nonly 1889 sememes. It would be better if complexity of model parameters can\nalso be discussed.\n\nMay be due to lack of space but experiment results discussion lack insight into\nobservations other than SAT performing the best. Also, authors claimed that\nwords with lower frequency were learned better with sememes without evaluating\non a rare words dataset.\n\nI have read author's response."
  },
  {
    "people": [
      "Wang"
    ],
    "review": "This paper presents the gated self-matching network for reading comprehension\nstyle question answering. There are three key components in the solution: \n\n(a) The paper introduces the gated attention-based recurrent network to obtain\nthe question-aware representation for the passage. Here, the paper adds an\nadditional gate to attention-based recurrent networks to determine the\nimportance of passage parts and attend to the ones relevant to the question.\nHere they use word as well as character embeddings to handle OOV words.\nOverall, this component is inspired from Wang and Jiang 2016.\n\n(b) Then the paper proposes a self-matching attention mechanism to improve the\nrepresentation for the question and passage by looking at wider passage context\nnecessary to infer the answer. This component is completely novel in the paper.\n\n(c) At the output layer, the paper uses pointer networks to locate answer\nboundaries. This is also inspired from Wang and Jiang 2016\n\nOverall, I like the paper and think that it makes a nice contribution.\n\n- Strengths:\n\nThe paper clearly breaks the network into three component for descriptive\npurposes, relates each of them to prior work and mentions its novelties with\nrespect to them. It does a sound empirical analysis by describing the impact of\neach component by doing an ablation study. This is appreciated.\n\nThe results are impressive!\n\n- Weaknesses:\n\nThe paper describes the results on a single model and an ensemble model. I\ncould not find any details of the ensemble and how was it created. I believe it\nmight be the ensemble of the character based and word based model. Can the\nauthors please describe this in the rebuttal and the paper.\n\n- General Discussion:\n\nAlong with the ablation study, it would be nice if we can have a\nqualitative analysis describing some example cases where the components of\ngating, character embedding, self embedding, etc. become crucial ... where a\nsimple model doesn't get the question right but adding one or more of these\ncomponents helps. This can go in some form of appendix or supplementary."
  },
  {
    "people": [
      "Barker",
      "Jacobson"
    ],
    "review": "This paper presents a purpose-built neural network architecture for textual\nentailment/NLI based on a three step process of encoding, attention-based\nmatching, and aggregation. The model has two variants, one based on TreeRNNs\nand the other based on sequential BiLSTMs. The sequential model outperforms all\npublished results, and an ensemble with the tree model does better still.\n\nThe paper is clear, the model is well motivated, and the results are\nimpressive. Everything in the paper is solidly incremental, but I nonetheless\nrecommend acceptance. \n\nMajor issues that I'd like discussed in the response:\n\u2013 You suggest several times that your system can serve as a new baseline for\nfuture work on NLI. This isn't an especially helpful or meaningful claim\u2014it\ncould be said of just about any model for any task. You could argue that your\nmodel is unusually simple or elegant, but I don't think that's really a major\nselling point of the model.\n\u2013 Your model architecture is symmetric in some ways that seem like\noverkill\u2014you compute attention across sentences in both directions, and run a\nseparate inference composition (aggregation) network for each direction. This\npresumably nearly doubles the run time of your model. Is this really necessary\nfor the very asymmetric task of NLI? Have you done ablation studies on this?**\n\u2013 You present results for the full sequential model (ESIM) and the ensemble\nof that model and the tree-based model (HIM). Why don't you present results for\nthe tree-based model on its own?**\n\nMinor issues:\n\u2013 I don't think the Barker and Jacobson quote means quite what you want it to\nmean. In context, it's making a specific and not-settled point about *direct*\ncompositionality in formal grammar. You'd probably be better off with a more\ngeneral claim about the widely accepted principle of compositionality.\n\u2013 The vector difference feature that you use (which has also appeared in\nprior work) is a bit odd, since it gives the model redundant parameters. Any\nmodel that takes vectors a, b, and (a - b) as input to some matrix\nmultiplication is exactly equivalent to some other model that takes in just a\nand b and has a different matrix parameter. There may be learning-related\nreasons why using this feature still makes sense, but it's worth commenting on.\n\u2013 How do you implement the tree-structured components of your model? Are\nthere major issues with speed or scalability there?\n\u001f\u2013 Typo: (Klein and D. Manning, 2003) \n\u2013 Figure 3: Standard tree-drawing packages like (tikz-)qtree produce much\nmore readable parse trees without crossing lines. I'd suggest using them.\n\n---\n\nThanks for the response! I still solidly support publication. This work is not\ngroundbreaking, but it's novel in places, and the results are surprising enough\nto bring some value to the conference."
  },
  {
    "people": [
      "Parikh",
      "Mou"
    ],
    "review": "The paper proposes a model for the Stanford Natural Language Inference (SNLI)\ndataset, that builds on top of sentence encoding models and the decomposable\nword level alignment model by Parikh et al. (2016). The proposed improvements\ninclude performing decomposable attention on the output of a BiLSTM and feeding\nthe attention output to another BiLSTM, and augmenting this network with a\nparallel tree variant.\n\n- Strengths:\n\nThis approach outperforms several strong models previously proposed for the\ntask. The authors have tried a large number of experiments, and clearly report\nthe ones that did not work, and the hyperparameter settings of the ones that\ndid. This paper serves as a useful empirical study for a popular problem.\n\n- Weaknesses:\n\nUnfortunately, there are not many new ideas in this work that seem useful\nbeyond the scope the particular dataset used. While the authors claim that the\nproposed network architecture is simpler than many previous models, it is worth\nnoting that the model complexity (in terms of the number of parameters) is\nfairly high. Due to this reason, it would help to see if the empirical gains\nextend to other datasets as well. In terms of ablation studies, it would help\nto see 1) how well the tree-variant of the model does on its own and 2) the\neffect of removing inference composition from the model.\n\nOther minor issues:\n1) The method used to enhance local inference (equations 14 and 15) seem very\nsimilar to the heuristic matching function used by Mou et al., 2015 (Natural\nLanguage Inference by Tree-Based Convolution and Heuristic Matching). You may\nwant to cite them.\n\n2) The first sentence in section 3.2 is an unsupported claim. This either needs\na citation, or needs to be stated as a hypothesis.\n\nWhile the work is not very novel, the the empirical study is rigorous for the\nmost part, and could be useful for researchers working on similar problems.\nGiven these strengths, I am changing my recommendation score to 3. I have read\nthe authors' responses."
  },
  {
    "people": [
      "REBUTTAL"
    ],
    "review": "This work proposes to apply dilated convolutions for sequence tagging\n(specifically, named entity recognition). It also introduces some novel ideas\n(sharing the dilated convolution block, predicting the tags at each convolution\nlevel), which I think will prove useful to the community. The paper performs\nextensive ablation experiments to show the effectiveness of their approach.\nI found the writing to be very clear, and the experiments were exceptionally\nthorough.\n\nStrengths:  \n- Extensive experiments against various architectures (LSTM, LSTM + CRF)       \n- Novel architectural/training ideas (sharing blocks)  \n\nWeaknesses:  \n- Only applied to English NER--this is a big concern since the title of the\npaper seems to reference sequence-tagging directly.  \n- Section 4.1 could be clearer. For example, I presume there is padding to make\nsure the output resolution after each block is the same as the input\nresolution.  Might be good to mention this.  \n- I think an ablation study of number of layers vs perf might be interesting.\n\nRESPONSE TO AUTHOR REBUTTAL:\n\nThank you very much for a thoughtful response. Given that the authors have\nagreed to make the content be more specific to NER as opposed to\nsequence-tagging, I have revised my score upward."
  },
  {
    "people": [
      "Joshi"
    ],
    "review": "This paper compares different ways of inducing embeddings for the task of\npolarity classification. The authors focus on different types of corpora and\nfind that not necessarily the largest corpus provides the most appropriate\nembeddings for their particular task but it is more effective to consider a\ncorpus (or subcorpus) in which a higher concentration of subjective content can\nbe found. The latter type of data are also referred to as \"task-specific data\".\nMoreover, the authors compare different embeddings that combine information\nfrom \"task-specific\" corpora and generic corpora. A combination outperforms\nembeddings just drawn from a single corpus. This combination is not only\nevaluated on English but also on a less resourced language (i.e. Catalan).\n\n- Strengths:\nThe paper addresses an important aspect of sentiment analysis, namely how to\nappropriately induce embeddings for training supervised classifers for polarity\nclassification. The paper is well-structured and well-written. The major claims\nmade by the authors are sufficiently supported by their experiments.\n\n- Weaknesses:\nThe outcome of the experiments is very predictable. The methods that are\nemployed are very simple and ad-hoc. I found hardly any new idea in\nthat paper. Neither are there any significant lessons that the reader learns\nabout embeddings or sentiment analysis. The main idea (i.e. focusing on more\ntask-specific data for training more accurate embeddings) was already published\nin the context of named-entity recognition by Joshi et al. (2015). The\nadditions made in this paper are very incremental in nature.\n\nI find some of the experiments inconclusive as (apparently) no statistical\nsignficance testing between different classifiers has been carried out. In\nTables\n2, 3 and 6, various classifier configurations produce very similar scores. In\nsuch cases, only statistical signficance testing can really give a proper\nindication whether these difference are meaningful. For instance, in Table 3 on\nthe left half reporting results on RT, one may wonder whether there is a\nsignificant difference between \"Wikipedia Baseline\" and any of the\ncombinations. Furthermore, one doubts whether there is any signficant\ndifference between the different combinations (i.e. either using \"subj-Wiki\",\n\"subj-Multiun\" or \"subj-Europarl\") in that table.\nThe improvement by focusing on subjective subsets is plausible in general.\nHowever, I wonder whether in real life, in particular, a situation in which\nresources are sparse this is very helpful. Doing a pre-selection with\nOpinionFinder is some pre-processing step which will not be possible in most\nlanguages other than English. There are no equivalent tools or fine-grained\ndatasets on which such functionality could be learnt. The fact that in the\nexperiments\nfor Catalan, this information is not considered proves that. \n\nMinor details:\n\n- lines 329-334: The discussion of this dataset is confusing. I thought the\ntask is plain polarity classification but the authors here also refer to\n\"opinion holder\" and \"opinion targets\". If these information are not relevant\nto the experiments carried out in this paper, then they should not be mentioned\nhere.\n\n- lines 431-437: The variation of \"splicing\" that the authors explain is not\nvery well motivated. First, why do we need this? In how far should this be more\neffective than simple \"appending\"?\n\n- lines 521-522: How is the subjective information isolated for these\nconfigurations? I assume the authors here again employ OpinionFinder? However,\nthere is no explicit mention of this here.\n\n- lines 580-588: The definitions of variables do not properly match the\nformula (i.e. Equation 3). I do not find n_k in Equation 3.\n\n- lines 689-695: Similar to lines 329-334 it is unclear what precise task is\ncarried out. Do the authors take opinion holders and targets in consideration?\n\n***AFTER AUTHORS' RESPONSE***\nThank you very much for these clarifying remarks.\nI do not follow your explanations regarding the incorporation of opinion\nholders and targets, though.\n\nOverall, I will not change my scores since I think that this work lacks\nsufficient novelty (the things the authors raised in their response are just\ninsufficient to me). This submission is too incremental in nature."
  },
  {
    "people": [
      "Sutskever",
      "Ruty",
      "Laha",
      "Anirban",
      "Vikas Raykar"
    ],
    "review": "The paper presents an application of Pointer Networks, a recurrent neural\nnetwork model original used for solving algorithmic tasks, to two subtasks of\nArgumentation Mining: determining the types of Argument Components, and finding\nthe links between them. The model achieves state-of-the-art results.\n\nStrengths:\n\n- Thorough review of prior art in the specific formulation of argument mining\nhandled in this paper.\n- Simple and effective modification of an existing model to make it suitable\nfor\nthe task. The model is mostly explained clearly.\n- Strong results as compared to prior art in this task.\n\nWeaknesses:\n\n- 071: This formulation of argumentation mining is just one of several proposed\nsubtask divisions, and this should be mentioned. For example, in [1], claims\nare detected and classified before any supporting evidence is detected.\nFurthermore, [2] applied neural networks to this task, so it is inaccurate to\nsay (as is claimed in the abstract of this paper) that this work is the first\nNN-based approach to argumentation mining.\n- Two things must be improved in the presentation of the model: (1) What is the\npooling method used for embedding features (line 397)? and (2) Equation (7) in\nline 472 is not clear enough: is E_i the random variable representing the\n*type* of AC i, or its *identity*? Both are supposedly modeled (the latter by\nfeature representation), and need to be defined. Furthermore, it seems like the\nLHS of equation (7) should be a conditional probability.\n- There are several unclear things about Table 2: first, why are the three\nfirst\nbaselines evaluated only by macro f1 and the individual f1 scores are missing?\nThis is not explained in the text. Second, why is only the \"PN\" model\npresented? Is this the same PN as in Table 1, or actually the Joint Model? What\nabout the other three?\n- It is not mentioned which dataset the experiment described in Table 4 was\nperformed on.\n\nGeneral Discussion:\n\n- 132: There has to be a lengthier introduction to pointer networks, mentioning\nrecurrent neural networks in general, for the benefit of readers unfamiliar\nwith \"sequence-to-sequence models\". Also, the citation of Sutskever et al.\n(2014) in line 145 should be at the first mention of the term, and the\ndifference with respect to recursive neural networks should be explained before\nthe paragraph starting in line 233 (tree structure etc.).\n- 348: The elu activation requires an explanation and citation (still not\nenough\nwell-known).\n- 501: \"MC\", \"Cl\" and \"Pr\" should be explained in the label.\n- 577: A sentence about how these hyperparameters were obtained would be\nappropriate.\n- 590: The decision to do early stopping only by link prediction accuracy\nshould\nbe explained (i.e. why not average with type accuracy, for example?).\n- 594: Inference at test time is briefly explained, but would benefit from more\ndetails.\n- 617: Specify what the length of an AC is measured in (words?).\n- 644: The referent of \"these\" in \"Neither of these\" is unclear.\n- 684: \"Minimum\" should be \"Maximum\".\n- 694: The performance w.r.t. the amount of training data is indeed surprising,\nbut other models have also achieved almost the same results - this is\nespecially surprising because NNs usually need more data. It would be good to\nsay this.\n- 745: This could alternatively show that structural cues are less important\nfor\nthis task.\n- Some minor typos should be corrected (e.g. \"which is show\", line 161).\n\n[1] Rinott, Ruty, et al. \"Show Me Your Evidence-an Automatic Method for Context\nDependent Evidence Detection.\" EMNLP. 2015.\n\n[2] Laha, Anirban, and Vikas Raykar. \"An Empirical Evaluation of various Deep\nLearning Architectures for Bi-Sequence Classification Tasks.\" COLING. 2016."
  },
  {
    "people": [
      "Lee",
      "Lee",
      "Lewis",
      "He",
      "Zettlemoyer"
    ],
    "review": "This paper describes a state-of-the-art CCG parsing model that decomposes into\ntagging and dependency scores, and has an efficient A* decoding algorithm.\nInterestingly, the paper slightly outperforms Lee et al. (2016)'s more\nexpressive global parsing model, presumably because this factorization makes\nlearning easier. It's great that they also report results on another language,\nshowing large improvements over existing work on Japanese CCG parsing. One\nsurprising original result is that modeling the first word of a constituent as\nthe head substantially outperforms linguistically motivated head rules. \n\nOverall this is a good paper that makes a nice contribution. I only have a few\nsuggestions:\n- I liked the way that the dependency and supertagging models interact, but it\nwould be good to include baseline results for simpler variations (e.g. not\nconditioning the tag on the head dependency).\n- The paper achieves new state-of-the-art results on Japanese by a large\nmargin. However, there has been a lot less work on this data - would it also be\npossible to train the Lee et al. parser on this data for comparison?\n- Lewis, He and Zettlemoyer (2015) explore combined dependency and supertagging\nmodels for CCG and SRL, and may be worth citing."
  },
  {
    "people": [
      "Clark",
      "Wilkes-Gibbs",
      "Walker"
    ],
    "review": "This paper proposes a method for building dialogue agents involved in a\nsymmetric collaborative task, in which the agents need to strategically\ncommunicate to achieve a common goal.  \n\nI do like this paper.  I am very interested in how much data-driven techniques\ncan be used for dialogue management.  However, I am concerned that the approach\nthat this paper proposes, is actually not specific to symmetric collaborative\ntasks, but to tasks that can be represented as graph operations, such as\nfinding an intersection between objects that the two people know about.\n\nIn Section 2.1, the authors introduce symmetric collaborative dialogue setting.\n However, such dialogs have been studied before, such as Clark and Wilkes-Gibbs\nexplored (Cognition '86), and Walker's furniture layout task (Journal of\nArtificial Research '00).\n\nOn line 229, the authors say that this domain is too rich for slot-value\nsemantics.  However, their domain is based on attribute value pairs, so their\ndomain could use a semantics represenation based on attribute value-pairs, such\nas first order logic.\n\nSection 3.2 is hard to follow.        The authors often refer to Figure 2, but I\ndidn't find this example that helpful.        For example, for section 3.1, at what\npoint of the dialogue does this represent?  Is this the same after `anyone went\nto columbia?'"
  },
  {
    "people": [
      "Kris Cao",
      "Marek Rei",
      "Piotr Bojanowski",
      "Edouard Grave",
      "Armand Joulin",
      "Tomas Mikolov"
    ],
    "review": "This is a nice paper on morphological segmentation utilizing word \nembeddings. The paper presents a system which uses word embeddings to \nboth measure local semantic similarity of word pairs with a potential \nmorphological relation, and global information about the semantic validity\nof potential morphological segment types. The paper is well written and \nrepresents a nice extension to earlier approaches on semantically driven \nmorphological segmentation.\n\nThe authors present experiments on Morpho Challenge data for three \nlanguages: English, Turkish and Finnish. These languages exhibit varying \ndegrees of morphological complexity. All systems are trained on Wikipedia \ntext. \n\nThe authors show that the proposed MORSE system delivers clear \nimprovements w.r.t. F1-score for English and Turkish compared to the well \nknown Morfessor system which was used as baseline. The system fails to \nreach the performance of Morfessor for Finnish. As the authors note, this \nis probably a result of the richness of Finnish morphology which leads to \ndata sparsity and, therefore, reduced quality of word embeddings. To \nimprove the performance for Finnish and other languages with a similar \ndegree of morphological complexity, the authors could consider word \nembeddings which take into account sub-word information. For example,\n\n@article{DBLP:journals/corr/CaoR16,\n  author    = {Kris Cao and\n               Marek Rei},\n  title     = {A Joint Model for Word Embedding and Word Morphology},\n  journal   = {CoRR},\n  volume    = {abs/1606.02601},\n  year                  = {2016},\n  url                 = {http://arxiv.org/abs/1606.02601},\n  timestamp = {Fri, 01 Jul 2016 17:39:49 +0200},\n  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/CaoR16},\n  bibsource = {dblp computer science bibliography, http://dblp.org}\n}\n\n@article{DBLP:journals/corr/BojanowskiGJM16,\n  author    = {Piotr Bojanowski and\n               Edouard Grave and\n               Armand Joulin and\n               Tomas Mikolov},\n  title     = {Enriching Word Vectors with Subword Information},\n  journal   = {CoRR},\n  volume    = {abs/1607.04606},\n  year                  = {2016},\n  url                 = {http://arxiv.org/abs/1607.04606},\n  timestamp = {Tue, 02 Aug 2016 12:59:27 +0200},\n  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/BojanowskiGJM16},\n  bibsource = {dblp computer science bibliography, http://dblp.org}\n}\n\nThe authors critique the existing Morpho Challenge data sets. \nFor example, there are many instances of incorrectly segmented words in \nthe material. Moreover, the authors note that, while some segmentations \nin the the data set may be historically valid (for example the \nsegmentation of business into busi-ness), these segmentations are no \nlonger semantically motivated. The authors provide a new data set \nconsisting of 2000 semantically motivated segmentation of English word \nforms from the English Wikipedia. They show that MORSE deliver highly \nsubstantial improvements compared to Morfessor on this data set.\n\nIn conclusion, I think this is a well written paper which presents \ncompetitive results on the interesting task of semantically driven \nmorphological segmentation. The authors accompany the submission with \ncode and a new data set which definitely add to the value of the \nsubmission."
  },
  {
    "people": [
      "Soricut",
      "Och",
      "Ruokolainen",
      "Kohonen",
      "Gr\u00f6nroos",
      "Narasimhan",
      "Creutz",
      "Lagus",
      "Kohonen",
      "Gr\u00f6nroos",
      "Mathias Creutz",
      "Krista Lagus",
      "Narasimhan"
    ],
    "review": "This paper continues the line of work for applying word embeddings for the\nproblem of unsupervised morphological segmentation (e.g. Soricut & Och, 2015;\n\u00dcst\u00fcn & Can, 2016). The proposed method, MORSE, applies a local optimization\nfor segmentation of each word, based on a set of orthographic and semantic\nrules and a few heuristic threshold values associated with them.\n\n- Strengths:\n\nThe paper presents multiple ways to evaluate segmentation hypothesis on word\nembeddings, and these may be useful also in other type of methods. The results\non English and Turkish data sets are convincing.\n\nThe paper is clearly written and organized, and the biliography is extensive.\n\nThe submission includes software for testing the English MORSE model and three\nsmall data sets used in the expriments.\n\n- Weaknesses:\n\nThe ideas in the paper are quite incremental, based mostly on the work by\nSoricut & Och (2015). However, the main problems of the paper concern\nmeaningful comparison to prior work and analysis of the method's limitations.\n\nFirst, the proposed method does not provide any sensible way for segmenting\ncompounds. Based on Section 5.3, the method does segment some of the compounds,\nbut using the terminology of the method, it considers either of the\nconstituents as an affix. Unsuprisingly, the limitation shows up especially in\nthe results of a highly-compounding language, Finnish. While the limitation is\nindicated in the end of the discussion section, the introduction and\nexperiments seem to assume otherwise.\n\nIn particular, the limitation on modeling compounds makes the evaluation of\nSection 4.4/5.3 quite unfair: Morfessor is especially good at segmenting\ncompounds (Ruokolainen et al., 2014), while MORSE seems to segment them only\n\"by accident\". Thus it is no wonder that Morfessor segments much larger\nproportion of the semantically non-compositional compounds. A fair experiment\nwould include an equal number of compounds that _should_ be segmented to their\nconstituents.\n\nAnother problem in the evaluations (in 4.2 and 4.3) concerns hyperparameter\ntuning. The hyperparameters of MORSE are optimized on a tuning data, but\napparently the hyperparameters of Morfessor are not. The recent versions of\nMorfessor (Kohonen et al. 2010, Gr\u00f6nroos et al. 2014) have a single\nhyperparameter that can be used to balance precision and recall of the\nsegmentation. Given that the MORSE outperforms Morfessor both in precision and\nrecall in many cases, this does not affect the conclusions, but should at least\nbe mentioned.\n\nSome important details of the evaluations and results are missing: The\n\"morpheme-level evaluation\" method in 5.2 should be described or referred to.\nMoreover, Table 7 seems to compare results from different evaluation sets: the\nMorfessor and Base Inference methods seem to be from official Morpho Challenge\nevaluations, LLSM is from Narasimhan et al. (2015), who uses aggregated data\nfrom Morpho Challenges (probably including both development and training sets),\nand MORSE is evaluated Morpho Challenges 2010 development set. This might not\naffect the conclusions, as the differences in the scores are rather large, but\nit should definitely be mentioned.\n\nThe software package does not seem to support training, only testing an\nincluded model for English.\n\n- General Discussion:\n\nThe paper puts a quite lot of focus on the issue of segmenting semantically\nnon-compositional compounds. This is problematic in two ways: First, as\nmentioned above, the proposed method does not seem to provide sensible way of\nsegmenting _any_ compound. Second, finding the level of lexicalized base forms\n(e.g. freshman) and the morphemes as smallest meaning-bearing units (fresh,\nman) are two different tasks with different use cases (for example, the former\nwould be more sensible for phrase-based SMT and the latter for ASR). The\nunsupervised segmentation methods, such as Morfessor, typically target at the\nlatter, and critizing the method for a different goal is confusing.\n\nFinally, there is certainly a continuum on the (semantic) compositionality of\nthe compound, and the decision is always somewhat arbitrary. (Unfortunately\nmany gold standards, including the Morpho Challenge data sets, tend to be also\ninconsistent with their decisions.)\n\nSections 4.1 and 5.1 mention the computational efficiency and limitation to one\nmillion input word forms, but does not provide any details: What is the\nbottleneck here? Collecting the transformations, support sets, and clusters? Or\nthe actual optimization problem? What were the computation times and how do\nthese scale up?\n\nThe discussion mentions a few benefits of the MORSE approach: Adaptability as a\nstemmer, ability to control precision and recall, and need for only a small\nnumber of gold standard segmentations for tuning. As far as I can see, all or\nsome of these are true also for many of the Morfessor variants (Creutz and\nLagus, 2005; Kohonen et al., 2010; Gr\u00f6nroos et al., 2014), so this is a bit\nmisleading. It is true that Morfessor works usually fine as a completely\nunsupervised method, but the extensions provide at least as much flexibility as\nMORSE has.\n\n(Ref: Mathias Creutz and Krista Lagus. 2005. Inducing the Morphological Lexicon\nof a Natural Language from Unannotated Text. In Proceedings of the\nInternational and Interdisciplinary Conference on Adaptive Knowledge\nRepresentation and Reasoning (AKRR'05), Espoo, Finland, June 15-17.)\n\n- Miscellaneous:\n\nAbstract should maybe mention that this is a minimally supervised method\n(unsupervised to the typical extent, i.e. excluding hyperparameter tuning).\n\nIn section 3, it should be mentioned somewhere that phi is an empty string.\n\nIn section 5, it should be mentioned what specific variant (and implementation)\nof Morfessor is applied in the experiments.\n\nIn the end of section 5.2, I doubt that increasing the size of the input\nvocabulary would alone improve the performance of the method for Finnish. For a\nlanguage that is morphologically as complex, you never encounter even all the\npossible inflections of the word forms in the data, not to mention derivations\nand compounds.\n\nI would encourage improving the format of the data sets (e.g.  using something\nsimilar to the MC data sets): For example using \"aa\" as a separator for\nmultiple analyses is confusing and makes it impossible to use the format for\nother languages.\n\nIn the references, many proper nouns and abbreviations in titles are written in\nlowercase letters. Narasimhan et al. (2015) is missing all the publication\ndetails."
  },
  {
    "people": [
      "Gladkova",
      "Vylomova"
    ],
    "review": "- Strengths:\n I find the idea of using morphological compositionality to make decisions on\nsegmentation quite fruitful.\n\nMotivation is quite clear\n\nThe paper is well-structured\n\n- Weaknesses:\nSeveral points are still unclear: \n  -- how the cases of rule ambiguity are treated (see \"null->er\" examples in\ngeneral discussion)\n  -- inference stage seems to be suboptimal\n  -- the approach is limited to known words only\n\n- General Discussion:\nThe paper presents semantic-aware method for morphological segmentation. The\nmethod considers sets of simple morphological composition rules, mostly\nappearing as 'stem plus suffix or prefix'. The approach seems to be quite\nplausible and the motivation behind is clear and well-argumented.\n\nThe method utilizes the idea of vector difference to evaluate semantic\nconfidence score for a proposed transformational rule. It's been previously\nshown by various studies that morpho-syntactic relations are captured quite\nwell by doing word analogies/vector differences. But, on the other hand, it has\nalso been shown that in case of derivational morphology (which has much less\nregularity than inflectional) the performance substantially drops (see\nGladkova, 2016; Vylomova, 2016). \n\n The search space in the inference stage although being tractable, still seems\nto be far from optimized (to get a rule matching \"sky->skies\" the system first\nneeds to searhc though the whole R_add set and, probably, quite huge set of\nother possible substitutions) and limited to known words only (for which we can\nthere exist rules). \n\n It is not clear how the rules for the transformations which are\northographically the same, but semantically completely different are treated.\nFor instance, consider \"-er\" suffix. On one hand, if used with verbs, it\ntransforms them into agentive nouns, such as \"play->player\". On the other hand,\nit could also be used with adjectives for producing comparative form, for\ninstance, \"old->older\". Or consider \"big->bigger\" versus \"dig->digger\".\nMore over, as mentioned before, there is quite a lot of irregularity in\nderivational morphology. The same suffix might play various roles. For\ninstance, \"-er\" might also represent patiental meanings (like in \"looker\"). Are\nthey merged into a single rule/cluster? \n\n No exploration of how the similarity threshold and measure may affect the\nperformance is presented."
  },
  {
    "people": [
      "Allen Schmaltz",
      "Yoon Kim",
      "Alexander M. Rush",
      "Stuart Shieber",
      "Minh-Thang Luong",
      "Ilya Sutskever",
      "Quoc V. Le",
      "Oriol Vinyals",
      "Lukasz\nKaiser",
      "Daxiang",
      "Wu",
      "Hua",
      "He",
      "Wei",
      "Yu",
      "Dianhai",
      "Wang",
      "Haifeng"
    ],
    "review": "Summary:\n\nThe paper applies a sequence to sequence (seq2seq) approach for German\nhistorical text normalization, and showed that using a grapheme-to-phoneme\ngeneration as an auxiliary task in a multi-task learning (MTL) seq2seq\nframework improves performance. The authors argue that the MTL approach\nreplaces the need for an attention menchanism, showing experimentally that the\nattention mechanism harms the MTL performance. The authors also tried to show\nstatistical correlation between the weights of an MTL normalizer and an\nattention-based one.\n\nStrengths:\n\n1) Novel application of seq2seq to historical text correction, although it has\nbeen applied recently to sentence grammatical error identification [1]. \n\n2) Showed that using grapheme-to-phoneme as an auxiliary task in a MTL setting\nimproves text normalization accuracy.\n\nWeaknesses:\n\n1) Instead of arguing that the MTL approach replaces the attention mechanism, I\nthink the authors should investigate why attention did not work on MTL, and\nperhaps modify the attention mechanism so that it would not harm performance.\n\n2) I think the authors should reference past seq2seq MTL work, such as [2] and\n[3]. The MTL work in [2] also worked on non-attention seq2seq models.\n\n3) This paper only tested on one German historical text data set of 44\ndocuments. It would be interesting if the authors can evaluate the same\napproach in another language or data set.\n\nReferences:\n\n[1] Allen Schmaltz, Yoon Kim, Alexander M. Rush, and Stuart Shieber. 2016.\nSentence-level grammatical error identification as sequence-to-sequence\ncorrection. In Proceedings of the 11th Workshop on Innovative Use of NLP for\nBuilding Educational Applications.\n\n[2] Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Lukasz\nKaiser. Multi-task Sequence to Sequence Learning. ICLR\u201916. \n\n[3] Dong, Daxiang, Wu, Hua, He, Wei, Yu, Dianhai, and Wang, Haifeng. \nMulti-task learning for multiple language translation. ACL'15\n\n---------------------------\nHere is my reply to the authors' rebuttal:\n\nI am keeping my review score of 3, which means I do not object to accepting the\npaper. However, I am not raising my score for 2 reasons:\n\n* the authors did not respond to my questions about other papers on seq2seq\nMTL, which also avoided using attention mechanism. So in terms of novelty, the\nmain novelty lies in applying it to text normalization.\n\n* it is always easier to show something (i.e. attention in seq2seq MTL) is not\nworking, but the value would lie in finding out why it fails and changing the\nattention mechanism so that it works."
  },
  {
    "people": [
      "Azawi"
    ],
    "review": "- Strengths: well written, solid experimental setup and intriguing qualitative\nanalysis\n\n- Weaknesses: except for the qualitative analysis, the paper may belong better\nto the applications area, since the models are not particularly new but the\napplication itself is most of its novelty\n\n- General Discussion: This paper presents a \"sequence-to-sequence\" model with\nattention mechanisms and an auxiliary phonetic prediction task to tackle\nhistorical text normalization. None of the used models or techniques are new by\nthemselves, but they seem to have never been used in this problem before,\nshowing and improvement over the state-of-the-art. \n\nMost of the paper seem like a better fit for the applications track, except for\nthe final analysis where the authors link attention with multi-task learning,\nclaiming that the two produce similar effects. The hypothesis is intriguing,\nand it's supported with a wealth of evidence, at least for the presented task. \nI do have some questions on this analysis though:\n\n1) In Section 5.1, aren't you assuming that the hidden layer spaces of the two\nmodels are aligned? Is it safe to do so?\n\n2) Section 5.2, I don't get what you mean by the errors that each of the models\nresolve independently of each other. This is like symmetric-difference? That\nis, if we combine the two models these errors are not resolved anymore?\n\nOn a different vein, 3) Why is there no comparison with Azawi's model?\n\n========\n\nAfter reading the author's response.\n\nI'm feeling more concerned than I was before about your claims of alignment in\nthe hidden space of the two models. If accepted, I would strongly encourage the\nauthors to make clear\nin the paper the discussion you have shared with us for why you think that\nalignment holds in practice."
  },
  {
    "people": [
      "Miceli Barone",
      "Ellen Riloff's",
      "Artetxe",
      "Artetxe",
      "Duong",
      "Vulic",
      "Korhonen",
      "Smith",
      "Smith",
      "Vulic",
      "Korhonen",
      "Dinu",
      "Lazaridou",
      "Smith",
      "Smith"
    ],
    "review": "This work proposes a self-learning bootstrapping approach to learning bilingual\nword embeddings, which achieves competitive results in tasks of bilingual\nlexicon induction and cross-lingual word similarity although it requires a\nminimal amount of bilingual supervision: the method leads to competitive\nperformance even when the seed dictionary is extremely small (25 dictionary\nitems!) or is constructed without any language pair specific information (e.g.,\nrelying on numerals shared between languages). \n\nThe paper is very well-written, admirably even so. I find this work 'eclectic'\nin a sense that its original contribution is not a breakthrough finding (it is\nmore a 'short paper idea' in my opinion), but it connects the dots from prior\nwork drawing inspiration and modelling components from a variety of previous\npapers on the subject, including the pre-embedding work on\nself-learning/bootstrapping (which is not fully recognized in the current\nversion of the paper). I liked the paper in general, but there are few other\nresearch questions that could/should have been pursued in this work. These,\nalong with only a partial recognition of related work and a lack of comparisons\nwith several other relevant baselines, are my main concern regarding this\npaper, and they should be fixed in the updated version(s).\n\n*Self-learning/bootstrapping of bilingual vector spaces: While this work is one\nof the first to tackle this very limited setup for learning cross-lingual\nembeddings (although not the first one, see Miceli Barone and more works\nbelow), this is the first truly bootstrapping/self-learning approach to\nlearning cross-lingual embeddings. However, this idea of bootstrapping\nbilingual vector spaces is not new at all (it is just reapplied to learning\nembeddings), and there is a body of work which used exactly the same idea with\ntraditional 'count-based' bilingual vector spaces. I suggest the authors to\ncheck the work of Peirsman and Pado (NAACL 2010) or Vulic and Moens (EMNLP\n2013), and recognize the fact that their proposed bootstrapping approach is not\nso novel in this domain. There is also related work of Ellen Riloff's group on\nbootstrapping semantic lexicons in monolingual settings.\n\n*Relation to Artetxe et al.: I might be missing something here, but it seems\nthat the proposed bootstrapping algorithm is in fact only an iterative approach\nwhich repeatedly utilises the previously proposed model/formulation of Artetxe\net al. The only difference is the reparametrization (line 296-305). It is not\nclear to me whether the bootstrapping approach draws its performance from this\nreparametrization (and whether it would work with the previous\nparametrization), or the performance is a product of both the algorithm and\nthis new parametrization. Perhaps a more explicit statement in the text is\nneeded to fully understand what is going on here.\n\n*Comparison with prior work: Several very relevant papers have not been\nmentioned nor discussed in the current version of the paper. For instance, the\nrecent work of Duong et al. (EMNLP 2016) on 'learning crosslingual word\nembeddings without bilingual corpora' seems very related to this work (as the\nbasic word overlap between the two titles reveals!), and should be at least\ndiscussed if not compared to. Another work which also relies on mappings with\nseed lexicons and also partially analyzes the setting with only a few hundred\nseed lexicon pairs is the work of Vulic and Korhonen (ACL 2016) 'on the role of\nseed lexicons in learning bilingual word embeddings': these two papers might\nalso help the authors to provide more details for the future work section\n(e.g., the selection of reliable translation pairs might boost the performance\nfurther during the iterative process). Another very relevant work has appeared\nonly recently: Smith et al. (ICLR 2017) discuss 'offline bilingual word\nvectors, orthogonal transformations and the inverted softmax'. This paper also\ndiscusses learning bilingual embeddings in very limited settings (e.g., by\nrelying only on shared words and cognates between two languages in a pair). As\na side note, it would be interesting to report results obtained using only\nshared words between the languages (such words definitely exist for all three\nlanguage pairs used in the experiments). This would also enable a direct\ncomparison with the work of Smith et al. (ICLR 2017) which rely on this setup.\n\n*Seed dictionary size and bilingual lexicon induction: It seems that the\nproposed algorithm (as discussed in Section 5) is almost invariant to the\nstarting seed lexicon, yielding very similar final BLI scores regardless of the\nstarting point. While a very intriguing finding per se, this also seems to\nsuggest an utter limitation of the current 'offline' approaches: they seem to\nhave hit the ceiling with the setup discussed in the paper; Vulic and Korhonen\n(ACL 2016) showed that we cannot really improve the results by simply\ncollecting more seed lexicon pairs, and this work suggests that any number of\nstarting pairs (from 25 to 5k) is good enough to reach this near-optimal\nperformance, which is also very similar to the numbers reported by Dinu et al.\n(arXiv 2015) or Lazaridou et al. (ACL 2015). I would like to see more\ndiscussion on how to break this ceiling and further improve BLI results with\nsuch 'offline' methods. Smith et al. (ICLR 2017) seem to report higher numbers\non the same dataset, so again it would be very interesting to link this work to\nthe work of Smith et al.\n\nIn other words, the authors state that in future work they plan to fine-tune\nthe method so that it can learn without any bilingual evidence. This is an\nadmirable 'philosophically-driven' feat, but from a more pragmatic point of\nview, it seems more pragmatic to detect how we can go over the plateau/ceiling\nwhich seems to be hit with these linear mapping approaches regardless of the\nnumber of used seed lexicon pairs (Figure 2).\n\n*Convergence criterion/training efficiency: The convergence criterion, although\ncrucial for the entire algorithm, both in terms of efficiency and efficacy, is\nmentioned only as a side note, and it is not entirely clear how the whole\nprocedure terminates. I suspect that the authors use the vanishing variation in\ncrosslingual word similarity performance as the criterion to stop the\nprocedure, but that makes the method applicable only to languages which have a\ncross-lingual word similarity dataset. I might be missing here given the\ncurrent description in the paper, but I do not fully understand how the\nprocedure stops for Finnish, given that there is no crosslingual word\nsimilarity dataset for English-Finnish.\n\n*Minor:\n- There is a Finnish 'Web as a Corpus' (WaC) corpus (lines 414-416):\nhttps://www.clarin.si/repository/xmlui/handle/11356/1074\n- Since the authors claim that the method could work with a seed dictionary\ncontaining only shared numerals, it would be very interesting to include an\nadditional language pair which does not share the alphabet (e.g.,\nEnglish-Russian, English-Bulgarian or even something more distant such as\nArabic and/or Hindi).\n\n*After the response: I would like to thank the authors for investing their time\ninto their response which helped me clarify some doubts and points raised in my\ninitial review. I hope that they would indeed clarify these points in the final\nversion, if given the opportunity."
  },
  {
    "people": [
      "Mikolov"
    ],
    "review": "This paper investigates the application of distributional vectors of meaning in\ntasks that involve the identification of semantic relations, similar to the\nanalogical reasoning task of Mikolov et al. (2013): Given an expression of the\nform \u201cX is for France what London is for the UK\u201d, X can be approximated by\nthe simple vector arithmetic operation London-UK+France. The authors argue that\nthis simple method can only capture very specific forms of analogies, and they\npresent a measure that aims at identifying a wider range of relations in a more\neffective way.\n\nI admit I find the idea of a single vector space model being able to capture a\nnumber of semantic relationships and analogies rather radical and infeasible.\nAs the authors mention in the paper, a number of studies already suggest for\nthe opposite. The reason is quite simple: behind all these models lies (some\nform of) the distributional hypothesis (words in similar contexts have similar\nmeanings), and this poses certain limitations in their expressive abilities;\nfor example, words like \u201cbig\u201d and \u201csmall\u201d will always be considered as\nsemantically similar from a vector perspective (although they express opposite\nmeanings), since they occur in similar contexts. So I cannot see how the\nexample given in Figure 1 is relevant to the very nature of vector spaces (or\nto any other semantic model for that matter!): there is a certain analogy\nbetween \u201cman-king\u201d, and \u201cwoman-queen\u201d, but asking from a word space to\ncapture \u201chas-a\u201d relationships of the form \u201cowl-has-claws\u201d, hence\n\u201chospital-has-walls\u201d, doesn\u2019t make much sense to me.\n\nThe motivation behind the main proposal of the paper (a similarity measure that\ninvolves a form of cross-comparison between vectors of words and vectors\nrepresenting the contexts of the words) is not clearly explained. Further, the\nmeasure is tested on the relation categories of the SemEval 2010 task with\nrather unsatisfactory results; in almost all cases, a simple baseline that\ntakes into account only partial similarities between the tested word pairs\npresent very high performance, with a difference from the best-performing model\nwhich seems to me statistically insignificant. So from both a methodological\nand an experimental perspective, the paper has weaknesses, and in its current\nform seems to describe work in progress;  as such I am inclined against its\npresentation in ACL.\n\n(Formatting issue: The authors use the LaTeX styles for ACL 2016 \u2014 this\nshould be fixed in case the paper is accepted).\n\nAUTHORS RESPONSE\n================\nThank you for the clarifications. I am still not comfortable with the idea of a\nmetric or a vector space that tries to capture both semantic and relational\nsimilarity, and I think you don't present enough experimental evidence that\nyour method works. I have to agree with one of the other reviewers that a more\nappropriate format for this work would be a short paper."
  },
  {
    "people": [
      "Koehn",
      "Hoang",
      "Chahuneau",
      "Zhand",
      "Zhang",
      "Collobert"
    ],
    "review": "This paper details a method of achieving translation from morphologically\nimpoverished languages (e.g. Chinese) to morphologically rich ones (e.g.\nSpanish) in a two-step process. First, a system translates into a simplified\nversion of the target language. Second, a system chooses morphological features\nfor each generated target word, and inflects the words based on those features.\n\nWhile I wish the authors would apply the work to more than one language pair, I\nbelieve the issue addressed by this work is one of the most important and\nunder-addressed problems with current MT systems. The approach taken by the\nauthors is very different than many modern approaches based on BPE and\ncharacter-level models, and instead harkens back to approaches such as\n\"Factored Translation Models\" (Koehn and Hoang, 2007) and \"Translating into\nMorphologically Rich Languages with Synthetic Phrases\" (Chahuneau et a. 2013),\nboth of which are unfortunately uncited.\n\nI am also rather suspicious of the fact that the authors present only METEOR\nresults and no BLEU or qualitative improvements. If BLEU scores do not rise,\nperhaps the authors could argue why they believe their approach is still a net\nplus, and back the claim up with METEOR and example sentences.\n\nFurthermore, the authors repeatedly talk about gender and number as the two\nlinguistic features they seek to correctly handle, but seem to completely\noverlook person. Perhaps this is because first and second person pronouns and\nverbs rarely occur in news, but certainly this point at least merits brief\ndiscussion. I would also like to see some discussion of why rescoring hurts\nwith gender. If the accuracy is very good, shouldn the reranker learn to just\nkeep the 1-best?\n\nFinally, while the content of this paper is good overall, it has a huge amount\nof spelling, grammar, word choice, and style errors that render it unfit for\npublication in its current form. Below is dump of some errors that I found.\n\nOverall, I would like to this work in a future conference, hopefully with more\nthan one language pair, more evaluation metrics, and after further\nproofreading.\n\nGeneral error dump:\nLine 062: Zhand --> Zhang\nLine 122: CFR --> CRF\nWhole related work section: consistent use of \\cite when \\newcite is\nappropriate\nIt feels like there's a lot of filler: \"it is important to mention that\", \"it\nis worth mentioning that\", etc\nLine 182, 184: \"The popular phrase-based MT system\" = moses? or PBMT systems in\ngeneral?\nLine 191: \"a software\"\nLine 196: \"academic and commercial level\" -- this should definitely be\npluralized, but are these even levels?\nLine 210: \"a morphology-based simplified target\" makes it sound like this\nsimplified target uses morphology. Perhaps the authors mean \"a morphologically\nsimplified target\"?\nLine 217: \"decide on the morphological simplifications\"?\nTable 1: extra space in \"cuesti\u00f3n\" on the first line and \"titulado\" in the\nlast line.\nTable 1: Perhaps highlight differences between lines in this table somehow?\nHow is the simplification carried out? Is this simplifier hand written by the\nauthors, or does it use an existing tool?\nLine 290: i.e. --> e.g.\nLine 294: \"train on\" or \"train for\"\nLine 320: \"our architecture is inspired by\" or \"Collobert's proposal inspires\nour architecture\"\nLine 324: drop this comma\nLine 338: This equation makes it look like all words share the same word vector\nW\nLine 422: This could also be \"casas blancas\", right? How does the system choose\nbetween the sg. and pl. forms? Remind the reader of the source side\nconditioning here.\nLine 445: This graph is just a lattice, or perhaps more specifically a \"sausage\nlattice\"\nLine 499: Insert \"e.g.\" or similiar: (e.g. producirse)\nLine 500: misspelled \"syllable\"\nLine 500/503: I'd like some examples or further clarity on what palabras llanas\nand palabras estr\u00fajulas are and how you handle all three of these special\ncases.\nLine 570: \"and sentences longer than 50 words\"\nLine 571: \"by means of zh-seg\" (no determiner) or \"by means of the zh-seg tool\"\nLine 574: are you sure this is an \"and\" and not an \"or\"?\nLine 596: \"trained for\" instead of \"trained on\"\nLine 597: corpus --> copora\nLine 604: size is --> sizes are\nLine 613: would bigger embedding sizes help? 1h and 12h are hardly unreasonable\ntraining times.\nLine 615: \"seven and five being the best values\"\nLine 617: Why 70? Increased from what to 70?\nTable 3: These are hyperparameters and not just ordinary parameters of the\nmodel\nLine 650: \"coverage exceeds 99%\"?\nLine 653: \"descending\"\nLine 666: \"quadratic\"\nLine 668: space before \\cites\nLine 676: \"by far\" or \"by a large margin\" instead of \"by large\"\nLine 716: below\nLine 729: \"The standard phrase-based ...\"\nzh-seg citation lists the year as 2016, but the tool actually was released in\n2009"
  },
  {
    "people": [
      "markov",
      "Markov"
    ],
    "review": "The paper describes a method for improving two-step translation using deep\nlearning. Results are presented for Chinese->Spanish translation, but the\napproach seems to be largely language-independent.\n\nThe setting is fairly typical for two-step MT. The first step translates into a\nmorphologically underspecified version of the target language. The second step\nthen uses machine learning to fill in the missing morphological categories and\nproduces the final system output by inflecting the underspecified forms (using\na morphological generator). The main novelty of this work is the choice of deep\nNNs as classifiers in the second step. The authors also propose a rescoring\nstep which uses a LM to select the best variant.\n\nOverall, this is solid work with good empirical results: the classifier models\nreach a high accuracy (clearly outperforming baselines such as SVMs) and the\nimprovement is apparent even in the final translation quality.\n\nMy main problem with the paper is the lack of a comparison with some\nstraightforward deep-learning baselines. Specifically, you have a structured\nprediction problem and you address it with independent local decisions followed\nby a rescoring step. (Unless I misunderstood the approach.) But this is a\nsequence labeling task which RNNs are well suited for. How would e.g. a\nbidirectional LSTM network do when trained and used in the standard sequence\nlabeling setting? After reading the author response, I still think that\nbaselines (including the standard LSTM) are run in the same framework, i.e.\nindependently for each local label. If that's not the case, it should have been\nclarified better in the response. This is a problem because you're not using\nthe RNNs in the standard way and yet you don't justify why your way is better\nor compare the two approaches.\n\nThe final re-scoring step is not entirely clear to me. Do you rescore n-best\nsentences? What features do you use? Or are you searching a weighted graph for\nthe single optimal path? This needs to be explained more clearly in the paper.\n(My current impression is that you produce a graph, then look for K best paths\nin it, generate the inflected sentences from these K paths and *then* use a LM\n-- and nothing else -- to select the best variant. But I'm not sure from\nreading the paper.) This was not addressed in the response.\n\nYou report that larger word embeddings lead to a longer training time. Do they\nalso influence the final results?\n\nCan you attempt to explain why adding information from the source sentence\nhurts? This seems a bit counter-intuitive -- does e.g. the number information\nnot get entirely lost sometimes because of this? I would appreciate a more\nthorough discussion on this in the final version, perhaps with a couple of\nconvincing examples.\n\nThe paper contains a number of typos and the general level of English may not\nbe sufficient for presentation at ACL.\n\nMinor corrections:\n\ncontext of the application of MT -> context of application for MT\n\nIn this cases, MT is faced in two-steps -> In this case, MT is divided into two\nsteps\n\nmarkov -> Markov\n\nCFR -> CRF\n\ntask was based on a direct translation -> task was based on direct translation\n\ntask provided corpus -> task provided corpora\n\nthe phrase-based system has dramatically -> the phrase-based approach...\n\ninvestigated different set of features -> ...sets of features\n\nwords as source of information -> words as the source...\n\ncorrespondant -> corresponding\n\nClasses for gender classifier -> Classes for the...\n\nfor number classifier -> for the...\n\nThis layer's input consists in -> ...consists of\n\nto extract most relevant -> ...the most...\n\nSigmoid does not output results in [-1, 1] but rather (0, 1). A tanh layer\nwould produce (-1, 1).\n\ninformation of a word consists in itself -> ...of itself\n\nthis $A$ set -> the set $A$\n\nempty sentences and longer than 50 words -> empty sentences and sentences\nlonger than...\n\nclassifier is trained on -> classifier is trained in\n\naproximately -> approximately\n\ncoverage raises the 99% -> coverage exceeds 99% (unless I misunderstand)\n\nin descendant order -> in descending order\n\ncuadratic -> quadratic (in multiple places)\n\nbut best results -> but the best results\n\nRescoring step improves -> The rescoring step...\n\nare not be comparable -> are not comparable"
  },
  {
    "people": [
      "Junczys-Dowmunt",
      "Toutanova",
      "Toutanova"
    ],
    "review": "This paper presents a method for generating morphology, focusing on gender and\nnumber, using deep learning techniques. From a morphologically simplified\nSpanish text, the proposed approach uses a classifier to reassign the gender\nand number for each token, when necessary. The authors compared their approach\nwith other learning algorithms, and evaluated it in machine translation on the\nChinese-to-Spanish (Zh->Es) translation direction.\n\nRecently, the task of generating gender and number has been rarely tackled,\nmorphology generation methods usually target, and are evaluated on,\nmorphologically-rich languages like German or Finnish.\nHowever, calling the work presented in this paper \u201cmorphology\ngeneration\u201c\u00a0is a bit overselling as the proposed method clearly deals only\nwith\ngender and number. And given the fact that some rules are handcrafted for this\nspecific task, I do not think this method can be straightforwardly applied to\ndo more complex morphology generation for morphologically-rich languages.\n\nThis paper is relatively clear in the sections presenting the proposed method.\nA\nlot of work has been done to design the method and I think it can have some\ninteresting impact on various NLP tasks. However the evaluation part of\nthis work is barely understandable as many details of what is done, or why it\nis done, are missing. From this evaluation, we cannot know if the proposed\nmethod brings improvements over state-of-the-art methods while the experiments\ncannot be replicated. Furthermore, no analysis of the results obtained is\nprovided. Since half a page is still available, there was the possibility\nto provide more information to make more clear the evaluation. This work lacks\nof motivation. Why do you think deep learning can especially improve gender and\nnumber generation over state-of-the-art methods?\n\nIn your paper, the word \u201ccontribution\u201c should be used more wisely, as it is\nnow in the paper, it is not obvious what are the real contributions (more\ndetails below). \n\nabstract:\nwhat do you mean by unbalanced languages?\n\nsection 1:\nYou claim that your main contribution is the use of deep learning. Just the use\nof deep learning in some NLP task is not a contribution.\n\nsection 2:\nYou claim that neural machine translation (NMT), mentioned as \u201cneural\napproximations\u201c,  does not achieve state-of-the-art results for Zh->Es. I\nrecommend to remove this claim from the paper, or to discuss it more, since\nJunczys-Dowmunt et al. (2016), during the last IWSLT, presented some results\nfor Zh->Es with the UN corpus, showing that NMT outperforms SMT by around 10\nBLEU points.\n\nsection 5.1:\nYou wrote that using the Zh->Es language pair is one of your main\ncontributions. Just using a language pair is not a contribution. Nonetheless, I\nthink it is nice to see a paper on machine translation that does not focus of\nimproving machine translation for English.\nThe numbers provided in Table 2 were computed before or after preprocessing?\nWhy did you remove the sentences longer than 50 tokens?\nPrecise how did you obtain development and test sets, or provide them. Your\nexperiments are currently no replicable especially because of that.\n\nsection 5.2:\nYou wrote that you used Moses and its default parameters, but the default\nparameters of Moses are not the same depending on the version, so you should\nprovide the number of the version used.\n\nsection 5.3:\nWhat do you mean by \u201chardware cost\u201c?\nTable 3: more details should be provided regarding how did you obtain these\nvalues. You chose these values given the classifier accuracy, but how precisely\nand on what data did you train and test the classifiers? On the same data used\nin section 6?\nIf I understood the experiments properly, you used simplified Spanish. But I\ncannot find in the text how do you simplify Spanish. And how do you use it to\ntrain the classifier and the SMT system? \n\nsection 6:\nYour method is better than other classification\nalgorithms, but it says nothing about how it performs compared to the\nstate-of-the-art methods. You should at least precise why you chose these\nclassifications algorithms for comparison. Furthermore, how your rules impact\nthese results? And more generally, how do you explain such a high accuracy for\nyou method?\nDid you implement all these classification algorithms by yourselves? If not,\nyou must provide the URL or cite the framework you used.\nFor the SMT experiments, I guess you trained your phrase table on simplified\nSpanish. You must precise it.\nYou chose METEOR over other metrics like BLEU to evaluate your results. You\nmust provide some explanation for this choice. I particularly appreciate when I\nsee a MT paper that does not use BLEU for evaluation, but if you use METEOR,\nyou must mention which version you used. METEOR has largely changed since 2005.\nYou cited the paper of 2005, did you use the 2005 version? Or did you use the\nlast one with paraphrases? \nAre your METEOR scores statistically significant?\n\nsection 7:\nAs future work you mentioned \u201cfurther simplify morphology\u201c. In this paper,\nyou do not present any simplification of morphology, so I think that choosing\nthe word\n\u201cfurther\u201c is misleading.\n\nsome typos:\nfemenine\nensambling\ncuadratic\n\nstyle:\nplain text citations should be rewritten like this: \u201c(Toutanova et al, 2008)\nbuilt\u00a0\u201c should be \u201cToutanova et al. (2008) built \u201c\nplace the caption of your tables below the table and not above, and with more\nspace between the table and its caption.\nYou used the ACL 2016 template. You must use the new one prepared for ACL 2017.\nMore generally, I suggest that you read again the FAQ and the submission\ninstructions provided on the ACL 2017 website. It will greatly help you to\nimprove the paper. There are also important information regarding references:\nyou must provide DOI or URL of all ACL papers in your references.\n\n-----------------------\n\nAfter authors response:\n\nThank you for your response.\n\nYou wrote that rules are added just as post-processing, but does it mean that\nyou do not apply them to compute your classification results? Or if you do\napply them before computing these results, I'm still wondering about their\nimpact on these results.\n\nYou wrote that Spanish is simplified as shown in Table 1, but it does not\nanswer my question: how did you obtain these simplifications exactly? (rules?\nsoftware? etc.) The reader need to now that to reproduce your approach.\n\nThe classification algorithms presented in Table 5 are not state-of-the-art, or\nif they are you need to cite some paper. Furthermore, this table only tells\nthat deep learning gives the best results for classification, but it does not\ntell at all if your approach is better than state-of-the-art approach for\nmachine translation. You need to compare your approach with other\nstate-of-the-art morphology generation approaches (described in related work)\ndesigned for machine translation. If you do that your paper will be much more\nconvincing in my opinion."
  },
  {
    "people": [
      "Schnober",
      "Carsten",
      "Eger",
      "Steffen",
      "Do Dinh",
      "Gurevych",
      "Nicolai",
      "Cherry",
      "Kondrak",
      "Schnober",
      "Cohn",
      "Rastogi"
    ],
    "review": "- Strengths: A new encoder-decoder model is proposed that explicitly takes \ninto account monotonicity.\n\n- Weaknesses: Maybe the model is just an ordinary BiRNN with alignments\nde-coupled.\nOnly evaluated on morphology, no other monotone Seq2Seq tasks.\n\n- General Discussion:\n\nThe authors propose a novel encoder-decoder neural network architecture with\n\"hard monotonic attention\". They evaluate it on three morphology datasets.\n\nThis paper is a tough one. One the one hand it is well-written, mostly very\nclear and also presents a novel idea, namely including monotonicity in\nmorphology tasks. \n\nThe reason for including such monotonicity is pretty obvious: Unlike machine\ntranslation, many seq2seq tasks are monotone, and therefore general\nencoder-decoder models should not be used in the first place. That they still\nperform reasonably well should be considered a strong argument for neural\ntechniques, in general. The idea of this paper is now to explicity enforce a\nmonotonic output character generation. They do this by decoupling alignment and\ntransduction and first aligning input-output sequences monotonically and\nthen training to generate outputs in agreement with the monotone alignments.\nHowever, the authors are unclear on this point. I have a few questions:\n\n1) How do your alignments look like? On the one hand, the alignments seem to\nbe of the kind 1-to-many (as in the running example, Fig.1), that is, 1 input\ncharacter can be aligned with zero, 1, or several output characters. However,\nthis seems to contrast with the description given in lines 311-312 where the\nauthors speak of several input characters aligned to 1 output character. That\nis, do you use 1-to-many, many-to-1 or many-to-many alignments?\n\n2) Actually, there is a quite simple approach to monotone Seq2Seq. In a first\nstage, align input and output characters monotonically with a 1-to-many\nconstraint (one can use any monotone aligner, such as the toolkit of\nJiampojamarn and Kondrak). Then one trains a standard sequence tagger(!) to\npredict exactly these 1-to-many alignments. For example, flog->fliege (your\nexample on l.613): First align as in \"f-l-o-g / f-l-ie-ge\". Now use any tagger\n(could use an LSTM, if you like) to predict \"f-l-ie-ge\" (sequence of length 4)\nfrom \"f-l-o-g\" (sequence of length 4). Such an approach may have been suggested\nin multiple papers, one reference could be [*, Section 4.2] below. \nMy two questions here are: \n\n2a) How does your approach differ from this rather simple idea?\n\n2b) Why did you not include it as a baseline?\n\nFurther issues:\n\n3) It's really a pitty that you only tested on morphology, because there are\nmany other interesting monotonic seq2seq tasks, and you could have shown your\nsystem's superiority by evaluating on these, given that you explicitly model\nmonotonicity (cf. also [*]).\n\n4) You perform \"on par or better\" (l.791). There seems to be a general\ncognitive bias among NLP researchers to map instances where they perform worse\nto\n\"on par\" and all the rest to \"better\". I think this wording should be\ncorrected, but otherwise I'm fine with the experimental results.\n\n5) You say little about your linguistic features: From Fig. 1, I infer that\nthey include POS, etc. \n\n5a) Where did you take these features from?\n\n5b) Is it possible that these are responsible for your better performance in\nsome cases, rather than the monotonicity constraints?\n\nMinor points:\n\n6) Equation (3): please re-write $NN$ as $\\text{NN}$ or similar\n\n7) l.231 \"Where\" should be lower case\n\n8) l.237 and many more: $x_1\\ldots x_n$. As far as I know, the math community\nrecommends to write $x_1,\\ldots,x_n$ but $x_1\\cdots x_n$. That is, dots should\nbe on the same level as surrounding symbols.\n\n9) Figure 1: is it really necessary to use cyrillic font? I can't even address\nyour example here, because I don't have your fonts.\n\n10) l.437: should be \"these\"\n\n[*] \n\n@InProceedings{schnober-EtAl:2016:COLING, \n\n  author    = {Schnober, Carsten  and  Eger, Steffen  and  Do Dinh,\nErik-L\\^{a}n  and  Gurevych, Iryna},\n  title     = {Still not there? Comparing Traditional Sequence-to-Sequence\nModels to Encoder-Decoder Neural Networks on Monotone String Translation\nTasks},\n  booktitle = {Proceedings of COLING 2016, the 26th International Conference on\nComputational Linguistics: Technical Papers},\n  month     = {December},\n  year                                                      = {2016},\n  address   = {Osaka, Japan},\n  publisher = {The COLING 2016 Organizing Committee},\n  pages     = {1703--1714},\n  url                                               =\n{http://aclweb.org/anthology/C16-1160}\n\n}\n\nAFTER AUTHOR RESPONSE\n\nThanks for the clarifications. I think your alignments got mixed up in the\nresponse somehow (maybe a coding issue), but I think you're aligning 1-0, 0-1,\n1-1, and later make many-to-many alignments from these. \nI know that you compare to  Nicolai, Cherry and Kondrak (2015) but my question\nwould have rather been: why not use 1-x (x in 0,1,2) alignments as in  Schnober\net al. and then train a neural tagger on these (e.g. BiLSTM). I wonder how much\nyour results would have differed from such a rather simple baseline. (A tagger\nis a monotone model to start with and given the monotone alignments, everything\nstays monotone. In contrast, you start out with a more general model and then\nput hard monotonicity constraints on this ...)\n\nNOTES FROM AC\n\nAlso quite relevant is Cohn et al. (2016),\nhttp://www.aclweb.org/anthology/N16-1102 .\n\nIsn't your architecture also related to methods like the Stack LSTM, which\nsimilarly predicts a sequence of actions that modify or annotate an input?  \n\nDo you think you lose anything by using a greedy alignment, in contrast to\nRastogi et al. (2016), which also has hard monotonic attention but sums over\nall alignments?"
  },
  {
    "people": [
      "Kim",
      "Kim",
      "Kim",
      "Kim",
      "Kim",
      "Kim",
      "Kim",
      "Kim",
      "Kim",
      "Kim",
      "Kim",
      "S.",
      "Hori"
    ],
    "review": "This paper proposes joint CTC-attention end-to-end ASR, which utilizes both\nadvantages in training and decoding. \n\n- Strengths:\nIt provides a solid work of hybrid CTC-attention framework in training and\ndecoding, and the experimental results showed that the proposed method could\nprovide an improvement in Japanese CSJ and Mandarin Chinese telephone speech\nrecognition task. \n\n- Weaknesses:\nThe only problem is that the paper sounds too similar with Ref [Kim et al.,\n2016] which will be officially published in the coming IEEE International\nConference on Acoustics, Speech, and Signal Processing (ICASSP), March 2017.\nKim at al., 2016, proposes joint CTC-attention using MTL for English ASR task,\nand this paper proposes joint CTC-attention using MTL+joint decoding for\nJapanese and Chinese ASR tasks. I guess the difference is on joint decoding and\nthe application to Japanese/Chinese ASR tasks. However, the difference is not\nclearly explained by the authors. So it took sometimes to figure out the\noriginal contribution of this paper.\n\n(a) Title: \nThe title in Ref [Kim et al., 2016] is \u201cJoint CTC- Attention Based End-to-End\nSpeech Recognition Using Multi-task Learning\u201d, while the title of this paper\nis \u201cJoint CTC-attention End-to-end Speech Recognition\u201d. I think the title\nis too general. If this is the first paper about \"Joint CTC-attention\" than it\nis absolutely OK. Or if Ref [Kim et al., 2016] will remain only as\npre-published arXiv, then it might be still acceptable. But since [Kim et al.,\n2016] will officially publish in IEEE conference, much earlier than this paper,\nthen a more specified title that represents the main contribution of this paper\nin contrast with the existing publication would be necessary. \n\n(b) Introduction:\nThe author claims that \u201cWe propose to take advantage of the constrained CTC\nalignment in a hybrid CTC-attention based system. During training, we attach a\nCTC objective to an attention-based encoder network as a regularization, as\nproposed by [Kim at al., 2016].\u201c Taking advantage of the constrained CTC\nalignment in a hybrid CTC-attention is the original idea from [Kim at al.,\n2016]. So the whole argument about attention-based end-to-end ASR versus\nCTC-based ASR, and the necessary of CTC-attention combination is not novel.\nFurthermore, the statement \u201cwe propose \u2026 as proposed by [Kim et al,\n2016]\u201d is somewhat weird. We can build upon someone proposal with additional\nextensions, but not just re-propose other people's proposal. Therefore, what\nwould be important here is to state clearly the original contribution of this\npaper and the position of the proposed method with respect to existing\nliterature\n\n(c) Experimental Results:\nKim at al., 2016 applied the proposed method on English task, while this paper\napplied the proposed method on Japanese and Mandarin Chinese tasks. I think it\nwould be interesting if the paper could explain in more details about the\nspecific problems in Japanese and Mandarin Chinese tasks that may not appear in\nEnglish task. For example, how the system could address multiple possible\noutputs. i.e., Kanji, Hiragana, and Katakana given Japanese speech input\nwithout using any linguistic resources. This could be one of the important\ncontributions from this paper.\n\n- General Discussion:\nI think it would be better to cite Ref [Kim et al., 2016] from\nthe official IEEE ICASSP conference, rather than pre-published arXiv:\nKim, S., Hori, T., Watanabe, S., \"Joint CTC- Attention Based End-to-End Speech\nRecognition Using Multi-task Learning\", IEEE International Conference on\nAcoustics, Speech, and Signal Processing (ICASSP), March 2017, pp. to appear."
  },
  {
    "people": [
      "Kim",
      "Kim",
      "Kim"
    ],
    "review": "The paper considers a synergistic combination of two non-HMM based speech\nrecognition techniques: CTC and attention-based seq2seq networks. The\ncombination is two-fold:\n1. first, similarly to Kim et al. 2016 multitask learning is used to train a\nmodel with a joint CTC and seq2seq cost.\n2. second (novel contribution), the scores of the CTC model and seq2seq model\nare ensembled during decoding (results of beam search over the seq2seq model\nare rescored with the CTC model).\n\nThe main novelty of the paper is in using the CTC model not only as an\nauxiliary training objective (originally proposed by Kim et al. 2016), but also\nduring decoding.\n\n- Strengths:\nThe paper identifies several problems stemming from the flexibility offered by\nthe attention mechanism and shows that by combining the seq2seq network with\nCTC the problems are mitigated.\n\n- Weaknesses:\nThe paper is an incremental improvement over Kim et al. 2016 (since two models\nare trained, their outputs can just as well be ensembled). However, it is nice\nto see that such a simple change offers important performance improvements of\nASR systems.\n\n- General Discussion:\nA lot of the paper is spent on explaining the well-known, classical ASR\nsystems. A description of the core improvement of the paper (better decoding\nalgorithm) starts to appear only on p. 5. \n\nThe description of CTC is nonstandard and maybe should either be presented in a\nmore standard way, or the explanation should be expanded. Typically, the\nrelation p(C|Z) (eq. 5) is deterministic - there is one and only one character\nsequence that corresponds to the blank-expanded form Z. I am also unsure about\nthe last transformation of the eq. 5."
  },
  {
    "people": [
      "Faruqui",
      "Kumar",
      "Falke",
      "Nakashole",
      "Moro",
      "Delli Bovi",
      "Stanovsky",
      "Dagan",
      "Manaal Faruqui",
      "Shankar Kumar",
      "Tobias Falke",
      "Gabriel Stanovsky",
      "Iryna Gurevych",
      "Ido Dagan",
      "Ndapandula Nakashole",
      "Gerhard Weikum",
      "Fabian Suchanek",
      "Andrea Moro",
      "Roberto Navigli",
      "Andrea Moro",
      "Roberto Navigli",
      "Claudio Delli Bovi",
      "Luca Telesca",
      "Roberto Navigli",
      "Gabriel Stanovsky",
      "Ido Dagan"
    ],
    "review": "- Strengths:\n\n[+] Well motivated, tackles an interesting problem;\n\n[+] Clearly written and structured, accompanied by documented code and dataset;\n\n[+] Encouraging results.\n\n- Weaknesses:\n\n[-] Limited to completely deterministic, hand-engineered minimization rules;\n\n[-] Some relevant literature on OIE neglected;\n\n[-] Sound but not thorough experimental evaluation.\n\n- General Discussion:\n\nThis paper tackles a practical issue of most OIE systems, i.e. redundant,\nuninformative and inaccurate extractions. The proposed approach, dubbed MinOIE,\nis designed to actually \"minimize\" extractions by removing overly specific\nportions and turning them into structured annotations of various types\n(similarly to OLLIE). The authors put MinIE on top of a state-of-the-art OIE\nsystem (ClausIE) and test it on two publicly available datasets, showing that\nit effectively leads to more concise extractions compared to standard OIE\napproaches, while at the same time retaining accuracy.\n\nOverall, this work focuses on an interesting (and perhaps underinvestigated)\naspect of OIE in a sound and principled way. The paper is clearly written,\nsufficiently detailed, and accompanied by supplementary material and a neat\nJava implementation.\nMy main concern is, however, with the entirely static, deterministic and\nrule-based structure of MinIE. Even though I understand that a handful of\nmanually engineered rules is technically the best strategy when precision is\nkey, these approaches are typically very hard to scale, e.g. in terms of\nlanguages (a recent trend of OIE, see Faruqui and Kumar, 2015; Falke et al.,\n2016). In other words, I think that this contribution somehow falls short of\nnovelty and substance in proposing a pipeline of engineered rules that are\nmostly inspired by other OIE systems (such as ClausIE or ReVerb); for instance,\nI would have really appreciated an attempt to learn these minimization rules\ninstead of hard-coding them.\n\nFurthermore, the authors completely ignore a recent research thread on\n\u201csemantically-informed\u201d OIE (Nakashole et al., 2012; Moro and Navigli,\n2012; 2013; Delli Bovi et al., 2015) where traditional extractions are\naugmented with links to underlying knowledge bases and sense inventories\n(Wikipedia, Wikidata, Yago, BabelNet). These contributions are not only\nrelevant in terms of related literature: in fact, having text fragments (or\nconstituents) explicitly linked to a knowledge base would reduce the need for\nad-hoc minimization rules such as those in Sections 6.1 and 6.2. In the example\nwith \"Bill of Rights\" provided by the authors (line 554), an OIE pipeline with\na proper Entity Linking module would recognize automatically the phrase as\nmention of a registered entity, regardless of the shape of its subconstituents.\nAlso, an underlying sense inventory would seamlessly incorporate the external\ninformation about collocations and multi-word expressions used in Section 6.2:\nnot by chance, the authors rely on WordNet and Wiktionary to compile their\ndictionary of collocations.\n\nFinally, some remarks on the experimental evaluation:\n\n- Despite the claim of generality of MinIE, the authors choose to experiment\nonly with ClausIE as underlying OIE system (most likely the optimal match). It\nwould have been very interesting to see if the improvement brought by MinIE is\nconsistent also with other OIE systems, in order to actually assess its\nflexibility as a post-processing tool.\n\n- Among the test datasets used in Section 7, I would have included the recent\nOIE benchmark of Stanovsky and Dagan (2016), where results are reported also\nfor comparison systems not included in this paper (TextRunner, WOIE, KrakeN).\n\nReferences:\n\n- Manaal Faruqui and Shankar Kumar. Multilingual Open Relation Extraction using\nCross-lingual Projection. NAACL-HLT, 2015.\n\n- Tobias Falke, Gabriel Stanovsky, Iryna Gurevych and Ido Dagan. Porting an\nOpen Information Extraction System from English to German. EMNLP 2016.\n\n- Ndapandula Nakashole, Gerhard Weikum and Fabian Suchanek. PATTY: A Taxonomy\nof Relational Patterns with Semantic Types. EMNLP 2012.\n\n- Andrea Moro, Roberto Navigli. WiSeNet: Building a Wikipedia-based Semantic\nNetwork with Ontologized Relations. CIKM 2012.\n\n- Andrea Moro, Roberto Navigli. Integrating Syntactic and Semantic Analysis\ninto the Open Information Extraction Paradigm. IJCAI 2013.\n\n- Claudio Delli Bovi, Luca Telesca and Roberto Navigli. Large-Scale Information\nExtraction from Textual Definitions through Deep Syntactic and Semantic\nAnalysis. TACL vol. 3, 2015.\n\n- Gabriel Stanovsky and Ido Dagan. Creating a Large Benchmark for Open\nInformation Extraction. EMNLP 2016."
  },
  {
    "people": [
      "Lample"
    ],
    "review": "- Strengths:\n   - The paper states clearly the contributions from the beginning \n   - Authors provide system and dataset\n   - Figures help in illustrating the approach\n   - Detailed description of the approach\n   - The authors test their approach performance on other datasets and compare\nto other published work\n\n- Weaknesses:\n   -The explanation of methods in some paragraphs is too detailed and there is\nno mention of other work and it is repeated in the corresponding method\nsections, the authors committed to address this issue in the final version.\n   -README file for the dataset [Authors committed to add README file]\n\n- General Discussion:\n   - Section 2.2 mentions examples of DBpedia properties that were used as\nfeatures. Do the authors mean that all the properties have been used or there\nis a subset? If the latter please list them. In the authors' response, the\nauthors explain in more details this point and I strongly believe that it is\ncrucial to list all the features in details in the final version for clarity\nand replicability of the paper. \n   - In section 2.3 the authors use Lample et al. Bi-LSTM-CRF model, it might\nbe beneficial to add that the input is word embeddings (similarly to Lample et\nal.)\n   - Figure 3, KNs in source language or in English? (since the mentions have\nbeen translated to English). In the authors' response, the authors stated that\nthey will correct the figure.\n   - Based on section 2.4 it seems that topical relatedness implies that some\nfeatures are domain dependent. It would be helpful to see how much domain\ndependent features affect the performance. In the final version, the authors\nwill add the performance results for the above mentioned features, as mentioned\nin their response. \n   - In related work, the authors make a strong connection to Sil and Florian\nwork where they emphasize the supervised vs. unsupervised difference. The\nproposed approach is still supervised in the sense of training, however the\ngeneration of training data doesn\u2019t involve human interference"
  },
  {
    "people": [
      "Balikas COLING16's",
      "Balikas COLING16"
    ],
    "review": "- Strengths:\n1. The idea of assigning variable-length document segments with dependent\ntopics is novel. This prior knowledge is worth incorporated in the LDA-based\nframework.\n2. Whereas we do not have full knowledge on recent LDA literature, we find the\npart of related work quite convincing.\n3. The method proposed for segment sampling with O(M) complexity is impressive.\nIt is crucial for efficient computation. \n\n- Weaknesses:\n1. Compared to Balikas COLING16's work, the paper has a weaker visualization\n(Fig 5), which makes us doubt about the actual segmenting and assigning results\nof document. It could be more convincing to give a longer exemplar and make\ncolor assignment consistent with topics listed in Figure 4.\n2. Since the model is more flexible than that of Balikas COLING16, it may be\nunderfitting, could you please explain this more?\n\n- General Discussion:\nThe paper is well written and structured. The intuition introduced in the\nAbstract and again exemplified in the Introduction is quite convincing. The\nexperiments are of a full range, solid, and achieves better quantitative\nresults against previous works. If the visualization part is stronger, or\nexplained why less powerful visualization, it will be more confident. Another\nconcern is about computation efficiency, since the seminal LDA work proposed to\nuse Variational Inference which is faster during training compared to MCMC, we\nwish to see the author\u2019s future development."
  },
  {
    "people": [
      "Fu",
      "Fu",
      "Fu"
    ],
    "review": "- Strengths:\n\n  * Knowledge lean, language-independent approach\n\n- Weaknesses:\n\n  * Peculiar task/setting\n  * Marginal improvement over W_Emb (Fu et al, 2014)\n  * Waste of space\n  * Language not always that clear\n\n- General Discussion:\n\nIt seems to me that this paper is quite similar to (Fu et al, 2014) and only\nadds marginal improvements. It contains quite a lot of redundancy (e.g. related\nwork in  sec 1 and sec 2), uninformative figures (e.g. Figure 1 vs Figure 2),\nnot so useful descriptions of MLP and RNN, etc. A short paper might have been a\nbetter fit.\n\nThe task looks somewhat idiosyncratic to me. It is only useful if you already\nhave a method that gives you all and only the hypernyms of a given word. This\nseems to presuppose (Fu et al., 2013). \n\nFigure 4: why are the first two stars connected by conjunction and the last two\nstarts by disjunction?              Why is the output \"1\" (dark star) if the the\nthree\ninputs are \"0\" (white stars)?\n\nSec 4.2, lines 587-589 appears to suggest that thresholds were tuned on the\ntest data (?) \n\nW_Emb is poorly explained (lines 650-652).\n\nSome parts of the text are puzzling. I can't make sense of the section titled\n\"Combined with Manually-Built Hierarchies\". Same for sec 4.4. What do the red\nand dashed lines mean?"
  },
  {
    "people": [
      "Marcus",
      "Hunspell",
      "Nicholls"
    ],
    "review": "- Strengths: Useful application for teachers and learners; supports\nfine-grained comparison of GEC systems.\n\n- Weaknesses: Highly superficial description of the system; evaluation not\nsatisfying.\n\n- General Discussion:\n\nThe paper presents an approach of automatically enriching the output of GEC\nsystems with error types. This is a very useful application because both\nteachers and learners can benefit from this information (and many GEC systems\nonly output a corrected version, without making the type of error explicit). It\nalso allows for finer-grained comparison of GEC systems, in terms of precision\nin general, and error type-specific figures for recall and precision.\n\nUnfortunately, the description of the system remains highly superficial. The\ncore of the system consists of a set of (manually?) created rules but the paper\ndoes not provide any details about these rules. The authors should, e.g., show\nsome examples of such rules, specify the number of rules, tell us how complex\nthey are, how they are ordered (could some early rule block the application of\na later rule?), etc. -- Instead of presenting relevant details of the system,\nseveral pages of the paper are devoted to an evaluation of the systems that\nparticipated in CoNLL-2014. Table 6 (which takes one entire page) list results\nfor all systems, and the text repeats many facts and figures that can be read\noff the table. \n\nThe evaluation of the proposed system is not satisfying in several aspects. \nFirst, the annotators should have independently annotated a gold standard for\nthe 200 test sentences instead of simply rating the output of the system. Given\na fixed set of tags, it should be possible to produce a gold standard for the\nrather small set of test sentences. It is highly probable that the approach\ntaken in the paper yields considerably better ratings for the annotations than\ncomparison with a real gold standard (see, e.g., Marcus et al. (1993) for a\ncomparison of agreement when reviewing pre-annotated data vs. annotating from\nscratch). \nSecond, it is said that \"all 5 raters individually considered at least 95% of\nour rule-based error types to be either \u201cGood\u201d or \u201cAcceptable\u201d\".\nMultiple rates should not be considered individually and their ratings averaged\nthis way, this is not common practice. If each of the \"bad\" scores were\nassigned to different edits (we don't learn about their distribution from the\npaper), 18.5% of the edits were considered \"bad\" by some annotator -- this\nsounds much worse than the average 3.7%, as calculated in the paper.\nThird, no information about the test data is provided, e.g. how many error\ncategories they contain, or which error categories are covered (according to\nthe cateogories rated as \"good\" by the annotators).\nForth, what does it mean that \"edit boundaries might be unusual\"? A more\nprecise description plus examples are at need here. Could this be problematic\nfor the application of the system?\n\nThe authors state that their system is less domain dependent as compared to\nsystems that need training data. I'm not sure that this is true. E.g., I\nsuppose that Hunspell's vocabulary probably doesn't cover all domains in the\nsame detail, and manually-created rules can be domain-dependent as well -- and\nare completely language dependent, a clear drawback as compared to machine\nlearning approaches. Moreover, the test data used here (FCE-test, CoNLL-2014)\nare from one domain only: student essays.\n\nIt remains unclear why a new set of error categories was designed. One reason\nfor the tags is given: to be able to search easily for underspecified\ncategories (like \"NOUN\" in general). It seems to me that the tagset presented\nin Nicholls (2003) supports such searches as well. Or why not using the\nCoNLL-2014 tagset? Then the CoNLL gold standard could have been used for\nevaluation.\n\nTo sum up, the main motivation of the paper remains somewhat unclear. Is it\nabout a new system? But the most important details of it are left out. Is it\nabout a new set of error categories? But hardly any motivation or discussion of\nit is provided. Is it about evaluating the CoNLL-2014 systems? But the\npresentation of the results remains superficial.\n\nTypos:\n- l129 (and others): c.f. -> cf.\n- l366 (and others): M2 -> M^2 (= superscribed 2)\n- l319: 50-70 F1: what does this mean? 50-70%?\n\nCheck references for incorrect case\n- e.g. l908: esl -> ESL\n- e.g. l878/79: fleiss, kappa"
  },
  {
    "people": [
      "Ng",
      "Rozovskaya",
      "Roth",
      "Sakaguchi",
      "Sakaguchi",
      "Efron",
      "Tibshirani"
    ],
    "review": "The paper presents a novel approach for evaluating grammatical error\ncorrection (GEC) systems. This approach makes it possible to assess\nthe performance of GEC systems by error type not only in terms of\nrecall but also in terms of precision, which was previously not\npossible in general since system output is usually not annotated with\nerror categories.\n\nStrengths:\n\n - The proposed evaluation is an important stepping stone for\n   analyzing GEC system behavior.\n - The paper includes evaluation for a variety of systems.\n - The approach has several advantages over previous work:\n   - it computes precision by error type\n   - it is independent of manual error annotation\n   - it can assess the performance on multi token errors\n - The automatically selected error tags for pre-computed error spans\n   are mostly approved of by human experts\n\nWeaknesses:\n\n - A key part \u2013 the rules to derive error types \u2013 are not described.\n - The classifier evaluation lacks a thorough error analysis and based\n   upon that it lacks directions of future work on how to improve the\n   classifier.\n - The evaluation was only performed for English and it is unclear how\n   difficult it would be to use the approach on another language.\n\nClassifier and Classifier Evaluation\n====================================\n\nIt is unclear on what basis the error categories were devised. Are\nthey based on previous work?\n\nAlthough the approach in general is independent of the alignment\nalgorithm, the rules are probably not, but the authors don't provide\ndetails on that.  The error categories are a major part of the paper\nand the reader should at least get a glimpse of how a rule to assign\nan error type looks like.\n\nUnfortunately, the paper does not apply the proposed evaluation on\nlanguages other than English.  It also does not elaborate on what\nchanges would be necessary to run the classifier on other languages. I\nassume that the rules used for determining edit boundaries as well as\nfor determining the error tags depend on the language/the\npre-processing pipeline to a certain extent and therefore need to be\nadapted. Also, the error categories might need to be changed.  The\nauthors do not provide any detail on the rules for assigning error\ncategories (how many are there overall/per error type? how complex are\nthey?) to estimate the effort necessary to use the approach on another\nlanguage.\n\nThe error spans computed in the pre-processing step seem to be\ninherently continuous (which is also the case with the M2 scorer), which\nis problematic since there are errors which can only be tagged\naccurately when the error span is discontinuous. In German, for\nexample, verbs with separable prefixes are separated from each other\nin the main clause: [1st constituent] [verb] [other constituents]\n[verb prefix]. Would the classifier be able to tag discontinuous edit\nspans?\n\nThe authors write that all human judges rated at least 95\\% of the\nautomatically assigned error tags as appropriate \"despite the degree\nof noise introduced by automatic edit extraction\" (295). I would be\nmore cautious with this judgment since the raters might also have been\nmore forgiving when the boundaries were noisy. In addition, they were\nnot asked to select a tag without knowing the system output but could\nin case of noisy boundaries be more biased towards the system\noutput. Additionally, there was no rating option between \"Bad (Not\nAppropriate)\" and \"Appropriate\", which might also have led raters to\nselect \"Appropriate\" over \"Bad\". To make the evaluation more sound,\nthe authors should also evaluate how the human judges rate the\nclassifier output if the boundaries were manually created,\ni.e. without the noise introduced by faulty boundaries.\n\nThe classifier evaluation lacks a thorough error analysis. It is only\nmentioned that \"Bad\" is usually traced back to a wrong POS\ntag. Questions I'd like to see addressed: When did raters select\n\"Bad\", when \"Appropriate\"? Does the rating by experts point at\npossibilities to improve the classifier?\n\nGold Reference vs. Auto Reference\n=================================\n\nIt is unclear on what data the significance test was performed\nexactly. Did you test on the F0.5 scores? If so, I don't think this is\na good idea since it is a derived measure with weak discriminative\npower (the performance in terms of recall an precision can be totally\ndifferent but have the same F0.5 score). Also, at the beginning of\nSection 4.1 the authors refer to the mismatch between automatic and\nreference in terms of alignment and classification but as far as I can\ntell, the comparison between gold and reference is only in terms of\nboundaries and not in terms of classification.\n\nError Type Evaluation\n=====================\n\nI do not think it is surprising that 5 teams (~line 473) failed to correct\nany unnecessary token error. For at least two of the systems there is\na straightforward explanation why they cannot handle superfluous\nwords. The most obvious is UFC: Their rule-base approach works on POS\ntags (Ng et al., 2014) and it is just not possible to determine\nsuperfluous words based on POS alone. Rozovskaya & Roth (2016) provide\nan explanation why AMU performs poorly on superfluous words.\n\nThe authors do not analyze or comment the results in Table 6 with\nrespect to whether the systems were designed to handle the error\ntype. For some error types, there is a straight-forward mapping\nbetween error type in the gold standard and in the auto reference, for\nexample for word order error. It remains unclear whether the systems\nfailed completely on specific error types or were just not designed to\ncorrect them (CUUI for example is reported with precision+recall=0.0,\nalthough it does not target word order errors). In the CUUI case (and\nthere are probably similar cases), this also points at an error in the\nclassification which is neither analyzed nor discussed.\n\nPlease report also raw values for TP, FP, TN, FN in the appendix for\nTable 6. This makes it easier to compare the systems using other\nmeasures. Also, it seems that for some error types and systems the\nresults in Table 6 are based only on a few instances. This would also\nbe made clear when reporting the raw values.\n\nYour write \"All but 2 teams (IITB and IPN) achieved the best score in\nat least 1 category, which suggests that different approaches to GEC\ncomplement different error types.\" (606) It would be nice to mention\nhere that this is in line with previous research.\n\nMulti-token error analysis is helpful for future work but the result\nneeds more interpretation: Some systems are probably inherently unable\nto correct such errors but none of the systems were trained on a\nparallel corpus of learner data and fluent (in the sense of Sakaguchi\net al, 2016) corrections.\n\nOther\n=====\n\n- The authors should have mentioned that for some of the GEC\n  approaches, it was not impossible before to provide error\n  annotations, e.g. systems with submodules for one error type each.\n  Admittedly, the system would need to be adapted to include the\n  submodule responsible for a change in the system output. Still, the\n  proposed approach enables to compare GEC systems for which producing\n  an error tagged output is not straightforward to other systems in a\n  unified way.\n- References: Some titles lack capitalizations. URL for Sakaguchi et\n  al. (2016) needs to be wrapped. Page information is missing for\n  Efron and Tibshirani (1993).\n\nAuthor response\n===============\n\nI agree that your approach is not \"fatally flawed\" and I think this review\nactually points out quite some positive aspects. The approach is good, but the\npaper is not ready.\n\nThe basis for the paper are the rules for classifying errors and the lack of\ndescription is a major factor.        This is not just a matter about additional\nexamples. If the rules are not seen as a one-off implementation, they need to\nbe described to be replicable or to adapt them.\n\nGeneralization to other languages should not be an afterthought.  It would be\nserious limitation if the approach only worked on one language by design.  Even\nif you don't perform an adaption for other languages, your approach should be\ntransparent enough for others to estimate how much work such an adaptation\nwould be and how well it could reasonably work.  Just stating that most\nresearch is targeted at ESL only reinforces the problem.\n\nYou write that the error types certain systems tackle would be \"usually obvious\nfrom the tables\".  I don't think it is as simple as that -- see the CUUI\nexample mentioned above as well as the unnecessary token errors.  There are\nfive systems that don't correct them (Table 5) and it should therefore be\nobvious that they did not try to tackle them. However, in the paper you write\nthat \"There\nis also no obvious explanation as to why these teams had difficulty with this\nerror type\"."
  },
  {
    "people": [
      "Marneffe",
      "Roser Saur\u00ed's",
      "Saur\u00ed",
      "Pustejovsky",
      "Qian",
      "Prabhakaran",
      "Lee"
    ],
    "review": "Comments after author response\n\n- Thank you for clarifying that the unclear \"two-step framework\" reference was\nnot about the two facets. I still do not find this use of a pipeline to be a\nparticularly interesting contribution.\n- You state that \"5. de Marneffe (2012) used additional annotated features in\ntheir system. For fair comparison, we re-implement their system with annotated\ninformation in FactBank.\" But the de Marneffe et al. feature cited in the\npaper, \"Predicate Classes\" requires only a dependency parser and vocabulary\nlists from Roser Saur\u00ed's PhD thesis; \"general classes of event\" might be\nreferring to FactML event classes, and while I admit it is not particularly\nclear in their work, I am sure they could clarify.\n- I continue to find the use of \"combined properly\" to be obscure. I agree that\nusing LSTM and CNN where respectively appropriate is valuable, but you seem to\nimply that some prior work has been improper, and that it is their combination\nwhich must be proper.\n- Thank you for reporting on separate LSTMs for each of the paths. I am curious\nas to why this combination may less effective. In any case, experiments with\nthis kind of alternative structure deserve to be reported.\n\n---\n\nThis paper introduces deep neural net technologies to the task of factuality\nclassification as defined by FactBank, with performance exceeding alternative\nneural net models and baselines reimplemented from the literature.\n\n- Strengths:\n\nThis paper is very clear in its presentation of a sophisticated model for\nfactuality classification and of its evaluation.  It shows that the use of\nattentional features and BiLSTM clearly provide benefit over alternative\npooling strategies, and that the model also exceeds the performance of a more\ntraditional feature-based log-linear model.  Given the small amount of training\ndata in FactBank, this kind of highly-engineered model seems appropriate. It is\ninteresting to see that the BiLSTM/CNN model is able to provide benefit despite\nlittle training data.\n\n- Weaknesses:\n\nMy main concerns with this work regard its (a) apparent departure from the\nevaluation procedure in the prior literature; (b) failure to present prior work\nas a strong baseline; and (c) novelty.\n\nWhile I feel that the work is original in engineering deep neural nets for the\nfactuality classification task, and that such work is valuable, its approach is\nnot particularly novel, and \"the proposal of a two-step supervised framework\"\n(line 087) is not particularly interesting given that FactBank was always\ndescribed in terms of two facets (assuming I am correct to interpret \"two-step\"\nas referring to these facets, which I may not be).\n\nThe work cites Saur\u00ed and Pustejovsky (2012), but presents their much earlier\n(2008) and weaker system as a baseline; nor does it consider Qian et al.'s\n(IALP 2015) work which compares to the former.              Both these works are\ndeveloped\non the TimeBank portion of FactBank and evaluated on a held-out ACQUAINT\nTimeBank section, while the present work does not report results on a held-out\nset.\n\nde Marneffe et al.'s (2012) system is also chosen as a baseline, but not all\ntheir features are implemented, nor is the present system evaluated on their\nPragBank corpus (or other alternative representations of factuality proposed in\nPrabhakaran et al. (*SEM 2015) and Lee et al. (EMNLP 2015)).  The evaluation is\ntherefore somewhat lacking in comparability to prior work.\n\nThere were also important questions left unanswered in evaluation, such as the\neffect of using gold standard events or SIPs.\n\nGiven the famed success of BiLSTMs with little feature engineering, it is\nsomewhat disappointing that this work does not attempt to consider a more\nminimal system employing deep neural nets on this task with, for instance, only\nthe dependency path from a candidate event to its SIP plus a bag of modifiers\nto that path. The inclusion of heterogeneous information in one BiLSTM was an\ninteresting feature, which deserved more experimentation: what if the order of\ninputs were permuted? what if delimiters were used in concatenating the\ndependency paths in RS instead of the strange second \"nsubj\" in the RS chain of\nline 456? What if each of SIP_path, RS_path, Cue_path were input to a separate\nLSTM and combined? The attentional features were evaluated together for the CNN\nand BiLSTM components, but it might be worth reporting whether it was\nbeneficial for each of these components. Could you benefit from providing path\ninformation for the aux words? Could you benefit from character-level\nembeddings to account for morphology's impact on factuality via tense/aspect?\nProposed future work is lacking in specificity seeing as there are many\nquestions raised by this model and a number of related tasks to consider\napplying it to.\n\n- General Discussion:\n\n194: Into what classes are you classifying events?\n\n280: Please state which are parameters of the model.\n\n321: What do you mean by \"properly\"? You use the same term in 092 and it's not\nclear which work you consider improper nor why.\n\n353: Is \"the chain form\" defined anywhere? Citation? The repetition of nsubj in\nthe example of line 456 seems an unusual feature for the LSTM to learn.\n\n356: It may be worth footnoting here that each cue is classified separately.\n\n359: \"distance\" -> \"surface distance\"\n\n514: How many SIPs? Cues? Perhaps add to Table 3.\n\nTable 2. Would be good if augmented by the counts for embedded and author\nevents. Percentages can be removed if necessary.\n\n532: Why 5-fold? Given the small amount of training data, surely 10-fold would\nbe more useful and not substantially increase training costs.\n\n594: It's not clear that this benefit comes from PSen, nor that the increase is\nsignificant or substantial.  Does it affect overall results substantially?\n\n674: Is this significance across all metrics?\n\n683: Is the drop of F1 due to precision, recall or both?\n\n686: Not clear what this sentence is trying to say.\n\nTable 4: From the corpus sizes, it seems you should only report 2 significant\nfigures for most columns (except CT+, Uu and Micro-A).\n\n711: It seems unsurprising that RS_path is insufficient given that the task is\nwith respect to a SIP and other inputs do not encode that information. It would\nbe more interesting to see performance of SIP_path alone.\n\n761: This claim is not precise, to my understanding. de Marneffe et al (2012)\nevaluates on PragBank, not FactBank.\n\nMinor issues in English usage:\n\n112: \"non-application\" -> \"not applicable\"\n\n145: I think you mean \"relevant\" -> \"relative\"\n\n154: \"can be displayed by a simple source\" is unclear\n\n166: Not sure what you mean by \"basline\". Do you mean \"pipeline\"?"
  },
  {
    "people": [
      "Kenton Lee",
      "Yoav Artzi",
      "Yejin Choi",
      "Luke Zettlemoyer",
      "Sandeep Soni",
      "Tanushree Mitra",
      "Eric Gilbert",
      "Jacob Eisenstein"
    ],
    "review": "This paper proposes a supervised deep learning model for event factuality\nidentification.  The empirical results show that the model outperforms\nstate-of-the-art systems on the FactBank corpus, particularly in three classes\n(CT-, PR+ and PS+).  The main contribution of the paper is the proposal of an\nattention-based two-step deep neural model for event factuality identification\nusing bidirectional long short-term memory (BiLSTM) and convolutional neural\nnetwork (CNN).\n\n[Strengths:]\n\n- The structure of the paper is (not perfectly but) well organized.\n\n- The empirical results show convincing (statistically significant) performance\ngains of the proposed model over strong baseline.\n\n[Weaknesses:]\n\nSee below for details of the following weaknesses:\n\n- Novelties of the paper are relatively unclear.\n\n- No detailed error analysis is provided.\n\n- A feature comparison with prior work is shallow, missing two relevant papers.\n\n- The paper has several obscure descriptions, including typos.\n\n[General Discussion:]\n\nThe paper would be more impactful if it states novelties more explicitly.  Is\nthe paper presenting the first neural network based approach for event\nfactuality identification?  If this is the case, please state that.\n\nThe paper would crystallize remaining challenges in event factuality\nidentification and facilitate future research better if it provides detailed\nerror analysis regarding the results of Table 3 and 4.              What are dominant\nsources of errors made by the best system BiLSTM+CNN(Att)?  What impacts do\nerrors in basic factor extraction (Table 3) have on the overall performance of\nfactuality identification (Table 4)?  The analysis presented in Section 5.4 is\nmore like a feature ablation study to show how useful some additional features\nare.\n\nThe paper would be stronger if it compares with prior work in terms of\nfeatures.  Does the paper use any new features which have not been explored\nbefore?  In other words, it is unclear whether main advantages of the proposed\nsystem come purely from deep learning, or from a combination of neural networks\nand some new unexplored features.  As for feature comparison, the paper is\nmissing two relevant papers:\n\n- Kenton Lee, Yoav Artzi, Yejin Choi and Luke Zettlemoyer. 2015 Event Detection\nand Factuality Assessment with Non-Expert Supervision. In Proceedings of the\n2015 Conference on Empirical Methods in Natural Language Processing, pages\n1643-1648.\n\n- Sandeep Soni, Tanushree Mitra, Eric Gilbert and Jacob Eisenstein. 2014.\nModeling Factuality Judgments in Social Media Text. In Proceedings of the 52nd\nAnnual Meeting of the Association for Computational Linguistics, pages 415-420.\n\nThe paper would be more understandable if more examples are given to illustrate\nthe underspecified modality (U) and the underspecified polarity (u).  There are\ntwo reasons for that.  First, the definition of 'underspecified' is relatively\nunintuitive as compared to other classes such as 'probable' or 'positive'. \nSecond, the examples would be more helpful to understand the difficulties of Uu\ndetection reported in line 690-697.  Among the seven examples (S1-S7), only S7\ncorresponds to Uu, and its explanation is quite limited to illustrate the\ndifficulties.\n\nA minor comment is that the paper has several obscure descriptions, including\ntypos, as shown below:\n\n- The explanations for features in Section 3.2 are somewhat intertwined and\nthus confusing.  The section would be more coherently organized with more\nseparate paragraphs dedicated to each of lexical features and sentence-level\nfeatures, by:\n\n  - (1) stating that the SIP feature comprises two features (i.e.,\nlexical-level\nand sentence-level) and introduce their corresponding variables (l and c) *at\nthe beginning*;\n\n  - (2) moving the description of embeddings of the lexical feature in line\n280-283\nto the first paragraph; and\n\n  - (3) presenting the last paragraph about relevant source identification in a\nseparate subsection because it is not about SIP detection.\n\n- The title of Section 3 ('Baseline') is misleading.  A more understandable\ntitle would be 'Basic Factor Extraction' or 'Basic Feature Extraction', because\nthe section is about how to extract basic factors (features), not about a\nbaseline end-to-end system for event factuality identification.\n\n- The presented neural network architectures would be more convincing if it\ndescribes how beneficial the attention mechanism is to the task.\n\n- Table 2 seems to show factuality statistics only for all sources.  The table\nwould be more informative along with Table 4 if it also shows factuality\nstatistics for 'Author' and 'Embed'.\n\n- Table 4 would be more effective if the highest system performance with\nrespect to each combination of the source and the factuality value is shown in\nboldface.\n\n- Section 4.1 says, \"Aux_Words can describe the *syntactic* structures of\nsentences,\" whereas section 5.4 says, \"they (auxiliary words) can reflect the\n*pragmatic* structures of sentences.\"  These two claims do not consort with\neach other well, and neither of them seems adequate to summarize how useful the\ndependency relations 'aux' and 'mark' are for the task.\n\n- S7 seems to be another example to support the effectiveness of auxiliary\nwords, but the explanation for S7 is thin, as compared to the one for S6.  What\nis the auxiliary word for 'ensure' in S7?\n\n- Line 162: 'event go in S1' should be 'event go in S2'.\n\n- Line 315: 'in details' should be 'in detail'.\n\n- Line 719: 'in Section 4' should be 'in Section 4.1' to make it more specific.\n\n- Line 771: 'recent researches' should be 'recent research' or 'recent\nstudies'.  'Research' is an uncountable noun.\n\n- Line 903: 'Factbank' should be 'FactBank'."
  },
  {
    "people": [
      "Zhao",
      "Liu",
      "Eriguchi",
      "Ma"
    ],
    "review": "This paper proposes a neural network architecture that represent structural\nlinguistic knowledge in a memory network for sequence tagging tasks (in\nparticular, slot-filling of the natural language understanding unit in\nconversation systems). Substructures (e.g. a node in the parse tree) is encoded\nas a vector (a memory slot) and a weighted sum of the substructure embeddings\nare fed in a RNN at each time step as additional context for labeling.\n\n-----Strengths-----\n\nI think the main contribution of this paper is a simple way to \"flatten\"\nstructured information to an array of vectors (the memory), which is then\nconnected to the tagger as additional knowledge. The idea is similar to\nstructured / syntax-based attention (i.e. attention over nodes from treeLSTM);\nrelated work includes Zhao et al on textual entailment, Liu et al. on natural\nlanguage inference, and Eriguchi et al. for machine translation. The proposed\nsubstructure encoder is similar to DCNN (Ma et al.): each node is embedded from\na sequence of ancestor words. The architecture does not look entirely novel,\nbut I kind of like the simple and practical approach compared to prior work.\n\n-----Weaknesses-----\n\nI'm not very convinced by the empirical results, mostly due to the lack of\ndetails of the baselines. Comments below are ranked by decreasing importance.\n\n-  The proposed model has two main parts: sentence embedding and substructure\nembedding. In Table 1, the baseline models are TreeRNN and DCNN, they are\noriginally used for sentence embedding but one can easily take the\nnode/substructure embedding from them too. It's not clear how they are used to\ncompute the two parts.\n\n- The model uses two RNNs: a chain-based one and a knowledge guided one. The\nonly difference in the knowledge-guided RNN is the addition of a \"knowledge\"\nvector from the memory in the RNN input (Eqn 5 and 8). It seems completely\nunnecessary to me to have separate weights for the two RNNs. The only advantage\nof using two is an increase of model capacity, i.e. more parameters.\nFurthermore, what are the hyper-parameters / size of the baseline neural\nnetworks? They should have comparable numbers of parameters.\n\n- I also think it is reasonable to include a baseline that just input\nadditional knowledge as features to the RNN, e.g. the head of each word, NER\nresults etc.\n\n- Any comments / results on the model's sensitivity to parser errors?\n\nComments on the model:\n\n- After computing the substructure embeddings, it seems very natural to compute\nan attention over them at each word. Is there any reason to use a static\nattention for all words? I guess as it is, the \"knowledge\" is acting more like\na filter to mark important words. Then it is reasonable to include the baseline\nsuggest above, i.e. input additional features.\n\n- Since the weight on a word is computed by inner product of the sentence\nembedding and the substructure embedding, and the two embeddings are computed\nby the same RNN/CNN, doesn't it means nodes / phrases similar to the whole\nsentence gets higher weights, i.e. all leaf nodes?\n\n- The paper claims the model generalizes to different knowledge but I think the\nsubstructure has to be represented as a sequence of words, e.g. it doesn't seem\nstraightforward for me to use constituent parse as knowledge here.\n\nFinally, I'm hesitating to call it \"knowledge\". This is misleading as usually\nit is used to refer to world / external knowledge such as a knowledge base of\nentities, whereas here it is really just syntax, or arguably semantics if AMR\nparsing is used.\n\n-----General Discussion-----\n\nThis paper proposes a practical model which seems working well on one dataset,\nbut the main ideas are not very novel (see comments in Strengths). I think as\nan ACL paper there should be more takeaways. More importantly, the experiments\nare not convincing as it is presented now. Will need some clarification to\nbetter judge the results.\n\n-----Post-rebuttal-----\n\nThe authors did not address my main concern, which is whether the baselines\n(e.g. TreeRNN) are used to compute substructure embeddings independent of the\nsentence embedding and the joint tagger. Another major concern is the use of\ntwo separate RNNs which gives the proposed model more parameters than the\nbaselines. Therefore I'm not changing my scores."
  },
  {
    "people": [
      "Nayak",
      "Levy",
      "Lai",
      "Nayak",
      "Angeli",
      "Manning",
      "Nayak"
    ],
    "review": "- Strengths:\n\nThis paper presents a 2 x 2 x 3 x 10 array of accuracy results based on\nsystematically changing the parameters of embeddings models:\n\n(context type, position sensitive, embedding model, task), accuracy\n\n- context type \u2208 {Linear, Syntactic}\n- position sensitive \u2208 {True, False}\n- embedding model \u2208 {Skip Gram, BOW, GLOVE}\n- task \u2208 {Word Similarity, Analogies, POS, NER, Chunking, 5 text classific.\ntasks}\n\nThe aim of these experiments was to investigate the variation in\nperformance as these parameters are changed. The goal of the study itself\nis interesting for the ACL community and similar papers have appeared\nbefore as workshop papers and have been well cited, such as Nayak et al.'s\npaper mentioned below.\n\n- Weaknesses:\nSince this paper essentially presents the effect of systematically changing the\n\ncontext types and position sensitivity, I will focus on the execution of the\ninvestigation and the analysis of the results, which I am afraid is not \nsatisfactory.\n\nA) The lack of hyper-parameter tuning is worrisome. E.g.\n   - 395 Unless otherwise notes, the number of word embedding dimension is set\nto 500.\n   - 232 It still enlarges the context vocabulary about 5 times in practice.\n   - 385 Most hyper-parameters are the same as Levy et al' best configuration.\n\n  This is worrisome because lack of hyperparameter tuning makes it difficult to\nmake statements like method A is better than method B. E.g. bound methods may\nperform better with a lower dimensionality than unbound models, since their\neffective context vocabulary size is larger.\n\nB) The paper sometimes presents strange explanations for its results. E.g.\n   - 115 \"Experimental results suggest that although it's hard to find any \nuniversal insight, the characteristics of different contexts on different\nmodels are concluded according to specific tasks.\"\n\n   What does this sentence even mean? \n\n   - 580 Sequence labeling tasks tend to classify words with the same syntax \nto the same category. The ignorance of syntax for word embeddings which  are\nlearned by bound representation becomes beneficial. \n\n   These two sentences are contradictory, if a sequence labeling task\n   classified words with \"same syntax\" to same category then syntx becomes\n   a ver valuable feature. Bound representation's ignorance of syntax\n   should cause a drop in performance just like other tasks which does not\n   happen.\n\nC) It is not enough to merely mention Lai et. al. 2016 who have also done a\n   systematic study of the word embeddings, and similarly the paper \n   \"Evaluating Word Embeddings Using a Representative Suite of Practical\n   Tasks\", Nayak, Angeli, Manning. appeared at the repeval workshop at \n   ACL 2016. should have been cited. I understand that the focus of Nayak\n   et al's paper is not exactly the same as this paper, however they\n   provide recommendations about hyperparameter tuning and experiment\n   design and even provide a web interface for automatically running\n   tagging experiments using neural networks instead of the \"simple linear\n   classifiers\" used in the current paper.\n\nD) The paper uses a neural BOW words classifier for the text classification\ntasks\n   but a simple linear classifier for the sequence labeling tasks. What is\n   the justification for this choice of classifiers? Why not use a simple\n   neural classifier for the tagging tasks as well? I raise this point,\n   since the tagging task seems to be the only task where bound\n   representations are consistently beating the unbound representations,\n   which makes this task the odd one out. \n\n- General Discussion:\nFinally, I will make one speculative suggestion to the authors regarding\nthe analysis of the data. As I said earlier, this paper's main contribution is\nan\nanalysis of the following table.\n(context type, position sensitive, embedding model, task, accuracy)\nSo essentially there are 120 accuracy values that we want to explain in\nterms of the aspects of the model. It may be beneficial to perform\nfactor analysis or some other pattern mining technique on this 120 sample data."
  },
  {
    "people": [
      "Ling",
      "Levy"
    ],
    "review": "- Strengths: \nEvaluating bag of words and \"bound\" contexts from either dependencies or\nsentence ordering is important, and will be a useful reference to the\ncommunity. The experiments were relatively thorough (though some choices could\nuse further justification), and the authors used downstream tasks instead of\njust intrinsic evaluations.\n\n- Weaknesses: \nThe authors change the objective function of GBOW from p(c|\\sum w_i) to\np(w|\\sum c_i). This is somewhat justified as dependency-based context with a\nbound representation only has one word available for predicting the context,\nbut it's unclear exactly why that is the case and deserves more discussion.\nPresumably the\nnon-dependency context with a bound representation would also suffer from this\ndrawback? If so, how did Ling et al., 2015 do it? Unfortunately, the authors\ndon't compare any results against the original objective, which is a definite\nweakness. In addition, the authors change GSG to match GBOW, again without\ncomparing to the original objective. Adding results from word vectors trained\nusing the original GBOW and GSG objective functions would justify these changes\n(assuming the results don't show large changes).\nThe hyperparameter settings should be discussed further. This played a large\nrole in Levy et al. (2015), so you should consider trying different\nhyperparameter values. These depend pretty heavily on the task, so simply\ntaking good values from another task may not work well.\n\nIn addition, the authors are unclear on exactly what model is trained in\nsection 3.4. They say only that it is a \"simple linear classifier\". In section\n3.5, they use logistic regression with the average of the word vectors as\ninput, but call it  a Neural Bag-of-Words model. Technically previous work also\nused this name, but I find it misleading, since it's just logistic regression\n(and hence a linear model, which is not something I would call \"Neural\"). It is\nimportant to know if the model trained in section 3.4 is the same as the model\ntrained in 3.5, so we know if the different conclusions are the result of the\ntask or the model changing. \n\n- General Discussion: This paper evaluates context taken from dependency parses\nvs context taken from word position in a given sentence, and bag-of-words vs\ntokens with relative position indicators. This paper is useful to the\ncommunity, as they show when and where researchers should use word vectors\ntrained using these different decisions. \n\n- Emphasis to improve:\nThe main takeaway from this paper that future researchers will use is given at\nthe end of 3.4 and 3.5, but really should be summarized at the start of the\npaper. Specifically, the authors should put in the abstract that for POS,\nchunking, and NER, bound representations outperform bag-of-words\nrepresentations, and that dependency contexts work better than linear contexts\nin most cases. In addition, for a simple text classification model, bound\nrepresentations perform worse than bag-of-words representations, and there\nseemed to be no major difference between the different models or context types.\n\n- Small points of improvement: \nShould call \"unbounded\" context \"bag of words\". This may lead to some confusion\nas one of the techniques you use is Generalized Bag-Of-Words, but this can be\nclarified easily.\n043: it's the \"distributional hypothesis\", not the \"Distributed Hypothesis\". \n069: citations should have a comma instead of semicolon separating them.\n074: \"DEPS\" should be capitalized consistently throughout the paper (usually it\nappears as \"Deps\"). Also should be introduced as something like dependency\nparse tree context (Deps).\n085: typo: \"How different contexts affect model's performances...\" Should have\nthe word \"do\"."
  },
  {
    "people": [
      "Li",
      "Steve Young",
      "Milica Gasic",
      "Wen",
      "Kalatzis",
      "Li",
      "Li",
      "Li"
    ],
    "review": "Review, ACL 2017, paper 256:\n\nThis paper extends the line of work which models generation in dialogue as a\nsequence to sequence generation problem, where the past N-1 utterances (the\n\u2018dialogue context\u2019) are encoded into a context vector (plus potential\nother, hand-crafted features), which is then decoded into a response: the Nth\nturn in the dialogue. As it stands, such models tend to suffer from lack of\ndiversity, specificity and local coherence in the kinds of response they tend\nto produce when trained over large dialogue datasets containing many topics\n(e.g. Cornell, Opensubtitles, Ubuntu, etc.). Rather than attempting to produce\ndiverse responses using the decoder, e.g. through word-by-word beam search\n(which has been shown not to work very well, even lose crucial information\nabout grammar and valid sequences), or via a different objective function (such\nas in Li et. al.\u2019s work) the authors introduce a latent variable, z, over\nwhich a probability distribution is induced as part of the network. At\nprediction time, after encoding utterances 1 to k, a context z is sampled, and\nthe decoder is greedily used to generate a response from this. The evaluation\nshows small improvements in BLEU scores over a vanilla seq2seq model that does\nnot involve learning a probability distribution over contexts and sampling from\nthis.\n\nThe paper is certainly impressive from a technical point of view, i.e. in the\napplication of deep learning methods, specifically conditioned variational auto\nencoders, to the problem of response generation, and its attendant difficulties\nin training such models. Their use of Information-Retrieval techniques to get\nmore than one reference response is also interesting. \n\nI have some conceptual comments on the introduction and the motivations behind\nthe work, some on the model architecture, and the evaluation which I write\nbelow in turn:\n\nComments on the introduction and motivations\u2026. \n\nThe authors seem not fully aware of the long history of this field, and its\nvarious facets, whether from a theoretical perspective, or from an applied one.\n\n1. \u201c[the dialogue manager] typically takes a new utterance and the dialogue\ncontext as input, and generates discourse level decisions.\u201d \n\n        This is not accurate. Traditionally at least, the job of the dialogue\nmanager is to select actions (dialogue acts) in a particular dialogue context.\nThe                    action chosen is then passed to a separate generation\nmodule\nfor\nrealisation. Dialogue management is usually done in the context of task-based\nsystems which are goal driven. The dialogue manager is to choose actions which\nare optimal in some sense, e.g. reach a goal (e.g. book a restaurant) in as few\nsteps as possible. See publications from Lemon & Pietquin, 2012, Rieser, Keizer\nand colleagues, and various publications from Steve Young, Milica Gasic and\ncolleagues for an overview of the large literature on Reinforcement Learning\nand MDP models for task-based dialogue systems.\n\n2. The authors need to make a clear distinction between task-based,\ngoal-oriented dialogue, and chatbots/social bots, the latter being usually no\nmore than a language model, albeit a sophisticated one (though see Wen et. al.\n2016). What is required from these two types of system is usually distinct.\nWhereas the former is required to complete a task, the latter is, perhaps only\nrequired to keep the user engaged. Indeed the data-driven methods that have\nbeen used to build such systems are usually very different. \n3. The authors refer to \u2018open-domain\u2019 conversation. I would suggest that\nthere is no such thing as open-domain conversation - conversation is always in\nthe context of some activity and for doing/achieving something specific in the\nworld. And it is this overarching goal, the overarching activity, this\noverarching genre, which determines the outward shape of dialogues and\ndetermines what sorts of dialogue structure are coherent. Coherence itself is\nactivity/context-specific. Indeed a human is not capable of open-domain\ndialogue: if they are faced with a conversational topic or genre that they have\nnever participated in, they would embarrass themselves with utterances that\nwould look incoherent and out of place to others already familiar with it.\n(think of a random person on the street trying to follow the conversations at\nsome coffee break at ACL). This is the fundamental problem I see with systems\nthat attempt to use data from an EXTREMELY DIVERSE, open-ended set of\nconversational genres (e.g. movie subtitles) in order to train one model,\nmushing everything together so that what emerges at the other end is just very\ngood grammatical structure. Or very generic responses. \n\nComments on the model architecture:\n\nRather than generate from a single encoded context, the authors induce a\ndistribution over possible contexts, sample from this, and generate greedily\nwith the decoder. It seems to me that this general model is counter intuitive,\nand goes against evidence from the Linguistic/Psycholinguistic literature on\ndialogue: this literature shows that people tend to resolve potential problems\nin understanding and acceptance very locally - i.e. make sure they agree on\nwhat the context of the conversation is - and only then move on with the rest\nof the conversation, so that at any given point, there is little uncertainty\nabout the current context of the conversation. The massive diversity one sees\nresults from the diversity in what the conversation is actually trying to\nachieve (see above), diversity in topics and contexts etc, so that in a given,\nfixed context, there is a multitude of possible next actions, all coherent, but\nleading the conversation down a different path.\n\nIt therefore seems strange to me at least to shift the burden of explaining\ndiversity and coherence in follow-up actions to that of the\nlinguistic/verbal/surface contexts in which they are uttered, though of course,\nuncertainty here can also arise as a result of mismatches in vocabulary,\ngrammars, concepts, people\u2019s backgrounds etc. But this probably wouldn\u2019t\nexplain much of the variation in follow-up response. \n\nIn fact, at least as far as task-based Dialogue systems are concerned, the\nchallenge is to capture synonymy of contexts, i.e. dialogues that are distinct\non the surface, but lead to the same or similar context, either in virtue of\ninteractional and syntactic equivalence relations, or synonymy relations that\nmight hold in a particular domain between words or sequences of words (e.g.\n\u201cwhat is your destination?\u201d = \u201cwhere would you like to go?\u201d in a flight\nbooking domain). See e.g. Bordes & Weston, 2016; and Kalatzis, Eshghi & Lemon,\n2016 - the latter use a grammar to cluster semantically similar dialogues.\n\nComments on the evaluation:\n\nThe authors seek to show that their model can generate more coherent, and more\ndiverse responses. The evaluation method, though very interesting, seems to\naddress coherence but not diversity, despite what they say in section 5.2:\n\nThe precision and recall metrics measure distance between ground truth\nutterances and the ones the model generates, but not that between the generated\nutterances themselves (unless I\u2019m misunderstanding the evaluation method).\nSee e.g. Li et al. who measure diversity by counting the number distinct\nn-grams in the generated responses.\n\nFurthermore, I\u2019m not sure that the increase in BLEU scores are meaningful:\nthey are very small. In the qualitative assessment of the generated responses,\none certainly sees more diversity, and more contentful utterances in the\nexamples provided. But I can\u2019t see how frequent such cases in fact are.\n\nAlso, it would have made for a stronger, more meaningful paper if the authors\nhad compared their results with other work, (e.g. Li et. al) that use very\ndifferent methods to promote diversity (e.g. by using a different objective\nfunction). The authors in fact do not mention this, or characterise it\nproperly, despite actually referring to Li et. al. 2015."
  },
  {
    "people": [
      "Berant",
      "Berant",
      "Liang",
      "Ranzato",
      "Yin"
    ],
    "review": "This paper introduces Neural Symbolic Machines (NSMs) --- a deep neural model\nequipped with discrete memory to facilitate symbolic execution. An NSM includes\nthree components: (1) a manager that provides weak supervision for learning,\n(2) a differentiable programmer based on neural sequence to sequence model,\nwhich encodes input instructions and predicts simplified Lisp programs using\npartial execution results stored in external discrete memories. (3) a symbolic\ncomputer that executes programs and provide code assistance to the programmer\nto prune search space. The authors conduct experiments on a semantic parsing\ntask (WebQuestionsSP), and show that (1) NSM is able to model language\ncompositionality by saving and reusing intermediate execution results, (2)\nAugmented REINFORCE is superior than vanilla REINFROCE for sequence prediction\nproblems, and (3) NSM trained end-to-end with weak supervision is able to\noutperform existing sate-of-the-art method (STAGG).\n\n- Strengths\n\n* The idea of using discrete, symbolic memories for neural execution models is\nnovel.                    Although in implementation it may simply reduce to copying\npreviously\nexecuted variable tokens from an extra buffer, this approach is still\nimpressive since it works well for a large-scale semantic parsing task.\n\n* The proposed revised REINFORCE training schema using imperfect hypotheses\nderived from maximum likelihood training is interesting and effective, and\ncould inspire future exploration in mixing ML/RL training for neural\nsequence-to-sequence models.\n\n* The scale of experiments is larger than any previous works in modeling neural\nexecution and program induction. The results are impressive.\n\n* The paper is generally clear and well-written, although there are some points\nwhich might require further clarification (e.g., how do the keys ($v_i$'s in\nFig. 2) of variable tokens involved in computing action probabilities?\nConflicting notations: $v$ is used to refer to variables in Tab. 1 and memory\nkeys in Fig 1.).\n\nOverall, I like this paper and would like to see it in the conference.\n\n* Weaknesses\n\n* [Choice of Dataset] The authors use WebQuestionsSP as the testbed. Why not\nusing the most popular WebQuestions (Berant et al., 2013) benchmark set? Since\nNSM only requires weak supervision, using WebQuestions would be more intuitive\nand straightforward, plus it could facilitate direct comparison with\nmain-stream QA research.\n\n* [Analysis of Compositionality] One of the contribution of this work is the\nusage of symbolic intermediate execution results to facilitate modeling\nlanguage compositionality. One interesting question is how well questions with\nvarious compositional depth are handled. Simple one-hop questions are the\neasiest to solve, while complex multi-hop ones that require filtering and\nsuperlative operations (argmax/min) would be highly non-trivial. The authors\nshould present detailed analysis regarding the performance on question sets\nwith different compositional depth.\n\n* [Missing References] I find some relevant papers in this field missing. For\nexample, the authors should cite previous RL-based methods for knowledge-based\nsemantic parsing (e.g., Berant and Liang., 2015), the sequence level REINFORCE\ntraining method of (Ranzato et al., 2016) which is closely related to augmented\nREINFORCE, and the neural enquirer work (Yin et al., 2016) which uses\ncontinuous differentiable memories for modeling neural execution.\n\n* Misc.\n\n* Why is the REINFORCE algorithm randomly initialized (Algo. 1) instead of\nusing parameters pre-trained with iterative ML?\n\n* What is KG server in Figure 5?"
  },
  {
    "people": [
      "Gal",
      "Gal"
    ],
    "review": "- Strengths:\n\na) The paper presents a Bayesian learning approach for recurrent neural network\nlanguage model. The method outperforms standard SGD with dropout on three\ntasks. \nb) The idea of using Bayesian learning with RNNs appears to be novel. \nc) The computationally efficient Bayesian algorithm for RNN would be of\ninterest to the NLP community for various applications.\n\n- Weaknesses:\n\nPrimary concern is about evaluation:\n\nSec 5.1: The paper reports the performance of difference types of architectures\n(LSTM/GRU/vanilla RNN) on character LM task while comparing the learning\nalgorithms on the Penn Treebank task. Furthermore, RMSprop and pSGLD are\ncompared for the character LM while SGD +/- dropout is compared with SGLD +/-\ndropout on word language model task. This is inconsistent!  I would suggest\nreporting both these dimensions (i.e. architectures and the exact same learning\nalgorithms) on both character and word LM tasks. It would be useful to know if\nthe results from the proposed Bayesian learning approaches are portable across\nboth these tasks and data sets.\n\nL529: The paper states that 'the performance gain mainly comes from adding\ngradient noise and model averaging'. This statement is not justified\nempirically. To arrive at this conclusion, an A/B experiment with/without\nadding gradient noise and/or model averaging needs to be done. \n\nL724: Gal's dropout is run on the sentence classification task but not on\nlanguage model/captions task. Since Gal's dropout is not specific to sentence\nclassification,  I would suggest reporting the performance of this method on\nall three tasks. This would allow the readers to fully assess the utility of\nthe proposed algorithms relative to all existing dropout approaches.\n\nL544: Is there any sort order for the samples? (\\theta_1, ..., \\theta_K)? e.g.\nare samples with higher posterior probabilities likely to be at higher indices?\nWhy not report the result of randomly selecting K out of S samples, as an\nadditional alternative?\n\nRegular RNN LMs are known to be expensive to train and evaluate. It would be\nvery useful to compare the training/evaluation times for the proposed Bayesian\nlearning algorithms with SGD+ dropout. That would allow the readers to\ntrade-off improvements versus increase in training/run times.\n\nClarifications:\nL346: What does \\theta_s refer to? Is this a MAP estimate of parameters based\non only the sample s?\nL453-454: Clarify what \\theta means in the context of dropout/dropconnect. \n\nTypos:\nL211: output\nL738: RMSProp"
  },
  {
    "people": [
      "Kim",
      "Rush",
      "Kuncoro"
    ],
    "review": "- Strengths: This paper explores a relatively under-explored area of practical\napplication of ideas behind Bayesian neural nets in NLP tasks. With a Bayesian\ntreatment of the parameters of RNNs, it is possible to incorporate benefits of\nmodel averaging during inference. Further, their gradient\nbased sampling approximation to the posterior estimation leads to a procedure\nwhich is easy to implement and is potentially much cheaper than other\nwell-known techniques for model averaging like ensembling.  \nThe effectiveness of this approach is shown on three different tasks --\nlanguage modeling, image captioning and sentence classification; and\nperformance gains are observed over the baseline of single model optimization.\n\n- Weaknesses: Exact experimental setup is unclear. The supplementary material\ncontains important details about burn-in, number of epochs and samples\ncollected that should be in the main paper itself. Moreover, details on how the\ninference is performed would be helpful. Were the samples that were taken\nfollowing HMC for a certain number of epochs after burn in on the training data\nfixed for inference (for every \\tilda{Y} during test time, same samples were\nused according to eqn 5) ? Also, an explicit clarification regarding an\nindependence assumption that p(D|\\theta) = p(Y,X| \\theta) = p(Y| \\theta,X)p(X),\nwhich lets one use the conditional RNN model (if I understand correctly) for\nthe potential U(\\theta) would be nice for completeness.\n\nIn terms of comparison, this paper would also greatly benefit from a\ndiscussion/ experimental comparison with ensembling and distillation methods\n(\"Sequence level knowledge distillation\"; Kim and Rush, \"Distilling an Ensemble\nof Greedy Dependency Parsers into One MST Parser\"; Kuncoro et al.) which  are\nintimately related by a similar goal of incorporating effects of model\naveraging.\n\nFurther discussion related to preference of HMC related sampling\nmethods over other sampling methods or variational approximation would be\nhelpful.\n\nFinally, equation 8 hints at the potential equivalence between dropout and the\nproposed approach and the theoretical justification behind combining SGLD and\ndropout (by making the equivalence more concrete) would lead to a better\ninsight into the effectiveness of the proposed approach.  \n\n- General Discussion: Points addressed above."
  },
  {
    "people": [
      "Yamada"
    ],
    "review": "This paper addresses the problem of disambiguating/linking textual entity\nmentions into a given background knowledge base (in this case, English\nWikipedia).  (Its title and introduction are a little overblown/misleading,\nsince there is a lot more to bridging text and knowledge than the EDL task, but\nEDL is a core part of the overall task nonetheless.)  The method is to perform\nthis bridging via an intermediate layer of representation, namely mention\nsenses, thus following two steps: (1) mention to mention sense, and (2) mention\nsense to entity.  Various embedding representations are learned for the words,\nthe mention senses, and the entities, which are then jointly trained to\nmaximize a single overall objective function that maximizes all three types of\nembedding equally.  \n\nTechnically the approach is fairly clear and conforms to the current deep\nprocessing fashion and known best practices regarding embeddings; while one can\nsuggest all kinds of alternatives, it\u2019s not clear they would make a material\ndifference.  Rather, my comments focus on the basic approach.  It is not\nexplained, however, exactly why a two-step process, involving the mention\nsenses, is better than a simple direct one-step mapping from word mentions to\ntheir entities.  (This is the approach of Yamada et al., in what is called here\nthe ALIGN algorithm.)  Table 2 shows that the two-step MPME (and even its\nsimplification SPME) do better.  By why, exactly?  What is the exact\ndifference, and additional information, that the mention senses have compare4ed\nto the entities?  To understand, please check if the following is correct (and\nperhaps update the paper to make it exactly clear what is going on).  \n\nFor entities: their profiles consist of neighboring entities in a relatedness\ngraph.                    This graph is built (I assume) by looking at word-level\nrelatedness of\nthe entity definitions (pages in Wikipedia).  The profiles are (extended\nskip-gram-based) embeddings.  \n\nFor words: their profiles are the standard distributional semantics approach,\nwithout sense disambiguation.  \n\nFor mention senses: their profiles are the standard distributional semantics\napproach, but WITH sense disambiguation.  Sense disambiguation is performed\nusing a sense-based profile (\u2018language model\u2019) from local context words and\nneighboring mentions, as mentioned briefly just before Section 4, but without\ndetails.  This is a problem point in the approach.  How exactly are the senses\ncreated and differentiated?  Who defines how many senses a mention string can\nhave?  If this is done by looking at the knowledge base, then we get a\nbijective mapping between mention senses and entities -\u2013 that is, there is\nexactly one entity for each mention sense (even if there may be more entities).\n In that case, are the sense collection\u2019s definitional profiles built\nstarting with entity text as \u2018seed words\u2019?                    If so, what\ninformation\nis used\nat the mention sense level that is NOT used at the entity level?  Just and\nexactly the words in the texts that reliably associate with the mention sense,\nbut that do NOT occur in the equivalent entity webpage in Wikipedia?  How many\nsuch words are there, on average, for a mention sense?                    That is,\nhow\npowerful/necessary is it to keep this extra differentiation information in a\nseparate space (the mention sense space) as opposed to just loading these\nadditional words into the Entity space (by adding these words into the\nWikipedia entity pages)?  \n\nIf the above understanding is essentially correct, please update Section 5 of\nthe paper to say so, for (to me) it is the main new information in the paper.  \n\nIt is not true, as the paper says in Section 6, that \u201c\u2026this is the first\nwork to deal with mention ambiguity in the integration of text and knowledge\nrepresentations, so there is no exact baselines for comparison\u201d.  The TAC KBP\nevaluations for the past two years have hosted EDL tasks, involving eight or\nnine systems, all performing exactly this task, albeit against Freebase, which\nis considerably larger and more noisy than Wikipedia.  Please see\nhttp://nlp.cs.rpi.edu/kbp/2016/ .  \n\nOn a positive note: I really liked the idea of the smoothing parameter in\nSection 6.4.2.\n\nPost-response: I have read the authors' responses.  I am not really satisfied\nwith their reply about the KBP evaluation not being relevant, but that they are\ninterested in the goodness of the embeddings instead.  In fact, the only way to\nevaluate such 'goodness' is through an application.  No-one really cares how\nconceptually elegant an embedding is, the question is: does it perform better?"
  },
  {
    "people": [
      "Yamada",
      "Yamada"
    ],
    "review": "- Strengths:\nGood ideas, simple neural learning, interesting performance (altough not\nstriking) and finally large set of applications.\n\n- Weaknesses: amount of novel content. Clarity in some sections. \n\nThe paper presents a neural learning method for entity disambiguation and\nlinking. It introduces a good idea to integrate entity, mention and sense\nmodeling within the smame neural language modeling technique. The simple\ntraining procedure connected with the modeling allows to support a large set of\napplication.\n\nThe paper is clear formally, but the discussion is not always at the same level\nof the technical ideas.\n\nThe empirical evaluation is good although not striking improvements of the\nperformance are reported. Although it seems an extension of (Yamada et al.,\nCoNLL 2016), it adds novel ideas and it is of a releant interest.\n\nThe weaker points of the paper are:\n\n- The prose is not always clear. I found Section 3 not as clear. Some details\nof Figure 2 are not explained and the terminology is somehow redundant: for\nexample, why do you refer to the dictionary of mentions? or the dictionary of\nentity-mention pairs? are these different from text anchors and types for\nannotated text anchors?\n- Tha paper is quite close in nature to Yamada et al., 2016) and the authors\nshould at least outline the differences.\n\nOne general observation on the current version is:\nThe paper tests the Multiple Embedding model against entity\nlinking/disambiguation tasks. However, word embeddings are not only used to\nmodel such tasks, but also some processes not directly depending on entities of\nthe KB, e.g. parsing, coreference or semantic role labeling. \nThe authors should show that the word embeddings provided by the proposed MPME\nmethod are not weaker wrt to simpler wordspaces in such other semantic tasks,\ni.e. those involving directly entity mentions.\n\nI did read the author's response."
  },
  {
    "people": [
      "Earley",
      "Earley",
      "Kuhlmann",
      "Jonsson"
    ],
    "review": "The paper is concerned in finding such a family of graph languages that is\nclosed under intersection and can be made probabilistic.\n\n- Strengths:\n\nThe introduction shows relevance, the overall aim, high level context and is\nnice to read.\nThe motivation is clear and interesting.\n\nThe paper  is extremely clear but requires close reading and much formal\nbackground.\nIt nicely takes into account certain differences in terminology.\n\nIt was interesting to see how the hyper-edge grammars generalize familiar\ngrammars \nand Earley's algorithm.  For example, Predict applies to nonterminal edges, and\nScan applies to terminal edges.  \n\nIf the parsing vs. validation in NLP context is clarified, the paper is useful\nbecause it is formally correct, nice contribution, instructive and can give new\nideas to other researchers.  \n\nThe described algorithm can be used in semantic parsing to rerank hypergraphs\nthat are produced by another parser.   In this restricted way, the method can\nbe part of the machinery what we in NLP use in natural language parsing and\nthus relevant to the ACL.\n\n- Weaknesses:\n\nReranking use is not mentioned in the introduction.\n\nIt would be a great news in NLP context if an Earley parser would run in linear\ntime for NLP grammars (unlike special kinds of formal language grammars). \nUnfortunately, this result involves deep assumptions about the grammar and the\nkind of input. \n\nLinear complexity of parsing of an input graph seem right for a top-down\ndeterministic grammars but the paper does not recognise the fact that an input\nstring in NLP usually gives rise to an exponential number of graphs.  In other\nwords, the parsing complexity result must be interpreted in the context of\ngraph validation or where one wants to find out a derivation of the graph, for\nexample, for the purposes of graph transduction via synchronous derivations.\n\nTo me, the paper should be more clear in this as a random reader may miss the\ndifference between semantic parsing (from strings) and parsing of semantic\nparses \n(the current work).\n\nThere does not seem to be any control of the linear order of 0-arity edges.  It\nmight be useful to mention that if the parser is extended to string inputs with\nthe aim to find the (best?) hypergraph for a given external nodes, then the\nitem representations of the subgraphs must also keep track of the covered\n0-arity edges.                          This makes the string-parser variant\nexponential.  \n\n- Easily correctable typos or textual problems:\n\n1)  Lines 102-106 is misleading.   While intersection and probs are true, \"such\ndistribution\" cannot refer to the discussion in the above.\n\n2) line 173:  I think you should rather talk about validation or recognition\nalgorithms than parsing algorithms as \"parsing\" in NLP means usually completely\ndifferent thing that is much more challenging due to the lexical and structural\nambiguity.\n\n3) lines 195-196 are unclear:  what are the elements of att_G; in what sense\nthey are pairwise distinct.  Compare Example 1 where ext_G and att_G(e_1) are\nnot disjoint sets.\n\n4) l.206.  Move *rank* definition earlier and remove redundancy.\n\n5) l. 267:  rather \"immediately derives\", perhaps.\n\n6) 279: add \"be\"\n\n7) l. 352:  give an example of a nontrivial internal path.\n\n8) l. 472:   define a subgraph of a hypergraph\n\n9) l. 417, l.418:  since there are two propositions, you may want to tell how\nthey contribute to what is quoted.\n\n10) l. 458:  add \"for\"\n\nTable:                          Axiom:              this is only place where this is\nintroduced as an\naxiom.                    Link\nto the text that says it is a trigger.\n\n- General Discussion:\n\nIt might be useful to tell about MSOL graph languages and their yields, which\nare\ncontext-free string languages.                          \n\nWhat happens if the grammar is ambiguous and not top-down deterministic? \nWhat if there are exponential number of parses even for the input graph due to\nlexical ambiguity or some other reasons.  How would the parser behave then? \nWouldn't the given Earley recogniser actually be strictly polynomial to m or k\n?\n\nEven a synchronous derivation of semantic graphs can miss some linguistic\nphenomena where a semantic distinction is expressed by different linguistic\nmeans.                    E.g. one language may add an affix to a verb when another\nlanguage may\nexpress the same distinction by changing the object.  I am suggesting that\nalthough AMR increases language independence in parses it may have such\ncross-lingual\nchallenges.\n\nI did not fully understand the role of the marker in subgraphs.  It was elided\nlater\nand not really used.\n\nl. 509-510:                 I already started to miss the remark of lines 644-647\nat\nthis\npoint.\n\nIt seems that the normal order is not unique.  Can you confirm this?\n\nIt is nice that def 7, cond 1 introduces lexical anchors to predictions. \nCompare the anchors in lexicalized grammars.\n\nl. 760.  Are you sure that non-crossing links do not occur when parsing\nlinearized sentences to semantic graphs?\n\n- Significant questions to the Authors:\n\nLinear complexity of parsing of an input graph seem right for a top-down\ndeterministic grammars but the paper does not recognise the fact that an input\nstring in NLP usually gives rise to an exponential number of graphs.  In other\nwords, the parsing complexity result must be interpreted in the context of\ngraph validation or where one wants to find out a derivation of the graph, for\nexample, for the purposes of graph transduction via synchronous derivations.\n\nWhat would you say about parsing complexity in the case the RGG is a\nnon-deterministic, possibly ambiguous regular tree grammar, but one is\ninterested to use it to assign trees to frontier strings like a context-free\ngrammar?  Can one adapt the given Earley algorithm to this purpose (by guessing\ninternal nodes and their edges)?\nAlthough this question might seem like a confusion, it is relevant in the NLP\ncontext.\n\nWhat prevents the RGGs to generate hypergraphs whose 0-arity edges (~words) are\nthen linearised?   What principle determines how they are linearised?               \n  Is\nthe\nlinear order determined by the Earley paths (and normal order used in\nproductions) or can one consider an actual word order in strings of a natural\nlanguage? \n\nThere is no clear connection to (non)context-free string languages or sets of\n(non)projective dependency graphs used in semantic parsing.  What is written on\nlines 757-758 is just misleading:  Lines 757-758 mention that HRGs can be used\nto generate non-context-free languages.  Are these graph languages or string\nlanguages?    How an NLP expert should interpret the (implicit) fact that RGGs\ngenerate only context-free languages?  Does this mean that the graphs are\nnoncrossing graphs in the sense of Kuhlmann & Jonsson (2015)?"
  },
  {
    "people": [
      "Vilnis",
      "McCallum",
      "Tian",
      "Neelakantan",
      "J. Passos",
      "Li",
      "Jurafsky",
      "Liu Y.,",
      "Liu Z.",
      "Chua T.,Sun M."
    ],
    "review": "Review: Multimodal Word Distributions\n\n- Strengths:  Overall a very strong paper.\n\n- Weaknesses: The comparison against similar approaches could be extended.\n\n- General Discussion:\n\nThe main focus of this paper is the introduction of a new model for learning\nmultimodal word distributions formed from Gaussian mixtures for multiple word\nmeanings. i. e. representing a word by a set of many Gaussian distributions. \nThe approach, extend the model introduced by Vilnis and McCallum (2014) which\nrepresented word as unimodal Gaussian distribution. By using a multimodal, the\ncurrent approach attain the problem of polysemy.\n\nOverall, a very strong paper, well structured and clear. The experimentation is\ncorrect and the qualitative analysis made in table 1 shows results as expected\nfrom the approach.  There\u2019s not much that can be faulted and all my comments\nbelow are meant to help the paper gain additional clarity. \n\nSome comments: \n\n_ It may be interesting to include a brief explanation of the differences\nbetween the approach from Tian et al. 2014 and the current one. Both split\nsingle word representation into multiple prototypes by using a mixture model. \n\n_ There are some missing citations that could me mentioned in related work as :\n\nEfficient Non-parametric Estimation of Multiple Embeddings per Word in Vector\nSpace Neelakantan, A., Shankar. J. Passos, A., McCallum. EMNLP 2014\nDo Multi-Sense Embeddings Improve Natural Language Understanding? Li and\nJurafsky, EMNLP 2015\nTopical Word Embeddings. Liu Y., Liu Z., Chua T.,Sun M. AAAI 2015\n\n_ Also, the inclusion of the result from those approaches in tables 3 and 4\ncould be interesting. \n\n_ A question to the authors: What do you attribute the loss of performance of\nw2gm against w2g in the analysis of SWCS?\n\nI have read the response."
  },
  {
    "people": [
      "Luong"
    ],
    "review": "This work uses Gaussian mixtures to represent words and demonstrates its\npotential in capturing multiple word meanings for polysemy. The training\nprocess is done based on a max-margin objective. The expected likelihood kernel\nis used as the similarity between two words' distributions. Experiment results\non word similarity and entailment tasks show the effectiveness of the proposed\nwork.\n\n- Strengths:\n\nThe problem is clearly motivated and defined. Gaussian mixtures are much more\nexpressive than deterministic vector representations. It can potentially\ncapture different word meanings by its modes, along with probability mass and\nuncertainty around those modes. This work represents an important contribution\nto word embedding. \n\nThis work propose a max-margin learning objective with closed-form similarity\nmeasurement for efficient training.\n\nThis paper is mostly well written. \n\n- Weaknesses:\n\nSee below for some questions. \n\n- General Discussion:\n\nIn the Gaussian mixture models, the number of gaussian components (k) is\nusually an important parameter. In the experiments of this paper, k is set to\n2. What is your criteria to select k? Does the increase of k hurt the\nperformance of this model? What does the learned distribution look like for a\nword that only has one popular meaning?\n\nI notice that you use the spherical case in all the experiments (the covariance\nmatrix reduces to a single number). Is this purely for computation efficiency?\nI wonder what's the performance of using a general diagonal covariance matrix.\nSince in this more general case, the gaussian mixture defines different degrees\nof uncertainty along different directions in the semantic space, which seems\nmore interesting.\n\nMinor comments:\nTable 4 is not referred to in the text.\nIn reference, Luong et al. lacks the publication year.\n\nI have read the response."
  },
  {
    "people": [
      "Yoon\nKim"
    ],
    "review": "- Strengths:\nThis is  a well written paper.\nThe paper is very clear for the most part.\nThe experimental comparisons are very well done.\nThe experiments are well designed and executed.\nThe idea of using KD for zero-resource NMT is impressive.\n\n- Weaknesses:\nThere were many sentences in the abstract and in other places in the paper\nwhere the authors stuff too much information into a single sentence. This could\nbe avoided. One can always use an extra sentence to be more clear.\nThere could have been a section where the actual method used could be explained\nin a more detailed. This explanation is glossed over in the paper. It's\nnon-trivial to guess the idea from reading the sections alone.\nDuring test time, you need the source-pivot corpus as well. This is a major\ndisadvantage of this approach. This is played down - in fact it's not mentioned\nat all. I could strongly encourage the authors to mention this and comment on\nit. \n\n- General Discussion:\n\nThis paper uses knowledge distillation to improve zero-resource translation.\nThe techniques used in this paper are very similar to the one proposed in Yoon\nKim et. al. The innovative part is that they use it for doing zero-resource\ntranslation. They compare against other prominent works in the field. Their\napproach also eliminates the need to do double decoding.\n\nDetailed comments:\n- Line 21-27 - the authors could have avoided this complicated structure for\ntwo simple sentences.\nLine 41 - Johnson et. al has SOTA on English-French and German-English.\nLine 77-79 there is no evidence provided as to why combination of multiple\nlanguages increases complexity. Please retract this statement or provide more\nevidence. Evidence in literature seems to suggest the opposite.\n\nLine 416-420 - The two lines here are repeated again. They were first mentioned\nin the previous paragraph.\nLine 577 - Figure 2 not 3!"
  },
  {
    "people": [
      "Kadlec"
    ],
    "review": "This paper presents a gated attention mechanism for machine reading. \nA key idea is to extend Attention Sum Reader (Kadlec et al. 2016) to multi-hop\nreasoning by fine-grained gated filter. \nIt's interesting and intuitive for machine reading. \nI like the idea along with significant improvement on benchmark datasets, but\nalso have major concerns to get it published in ACL.\n\n- The proposed GA mechanism looks promising, but not enough to convince the\nimportance of this technique over other state-of-the-art systems, because\nengineering tricks presented 3.1.4 boost a lot on accuracy and are blended in\nthe result.\n\n- Incomplete bibliography: Nearly all published work in reference section\nrefers arxiv preprint version. \nThis makes me (and future readers) suspicious if this work thoroughly compares\nwith prior work. Please make them complete if the published version is\navailable. \n\n- Result from unpublished work (GA): GA baseline in table 1 and 3 is mentioned\nas previous work that is unpublished preprint. \nI don't think this is necessary at all. Alternately, I would like the author to\nreplace it with vanilla GA (or variant of the proposed model for baseline). \nIt doesn't make sense that result from the preprint which will end up being the\nsame as this ACL submission is presented in the same manuscript. \nFor fair blind-review, I didn't search on arvix archive though.\n\n- Conflict on table 1 and 2: GA-- (table 1) is the same as K=1(AS) in table 2,\nand GA (fix L(w)) is for K=3 in table 2. \nDoes this mean that GA-- is actually AS Reader? \nIt's not clear that GA-- is re-implementation of AS. \nI assumed K=1 (AS) in table 2 uses also GloVe initialization and\ntoken-attention, but it doesn't seem in GA--. \n\n- I wish the proposed method compared with prior work in related work section\n(i.e. what's differ from related work).\n\n- Fig 2 shows benefit of gated attention (which translates multi-hop\narchitecture), and it's very impressive. It would be great to see any\nqualitative example with comparison."
  },
  {
    "people": [
      "Caiming Xiong",
      "Shuohang Wang"
    ],
    "review": "This paper presents an interesting model for reading comprehension, by\ndepicting the multiplicative interactions between the query and local\ninformation around a word in a document, and the authors proposed a new\ngated-attention strategy to characterize the relationship. The work is quite\nsolid, with almost state of art result on the whole four cloze-style datasets\nachieved. Some of the further improvement can be helpful for the similar tasks.\n\n\nNevertheless, I have some concerns on the following aspect:\n\n1. The authors have referred many papers from arXiv, but I think some really\nrelated works are not included. Such as the works from Caiming Xiong, et al.\nhttps://openreview.net/pdf?id=rJeKjwvclx and the work form Shuohang Wang, et\nal. https://openreview.net/pdf?id=B1-q5Pqxl . Both of them concentrated on\nenhancing the attention operation to modeling the interaction between documents\nand queries. Although these works are not evaluated on the cloze-style corpus\nbut the SQuAD, an experimental or fundamental comparison may be necessary.\n\n2. There have been some studies that adopts attention mechanism or its variants\nspecially designed for the Reading Comprehension tasks, and the work actually\nshare the similar ideas with this paper. My suggestion is to conduct some\ncomparisons with such work to enhance the experiments of this paper."
  },
  {
    "people": [
      "Seo",
      "Neelakantan",
      "Verga",
      "Bordes",
      "Bordes"
    ],
    "review": "- Strengths:\nZero-shot relation extraction is an interesting problem. The authors have\ncreated a large dataset for relation extraction as question answering which\nwould likely be useful to the community.\n\n- Weaknesses:\nComparison and credit to existing work is severely lacking. Contributions of\nthe paper don't seen particularly novel.\n\n- General Discussion:\n\nThe authors perform relation extraction as reading comprehension. In order to\ntrain reading comprehension models to perform relation extraction, they create\na large dataset of 30m \u201cquerified\u201d (converted to natural language)\nrelations by asking mechanical turk annotators to write natural language\nqueries for relations from a schema. They use the reading comprehension model\nof Seo et al. 2016, adding the ability to return \u201cno relation,\u201d as the\noriginal model must always return an answer. The main motivation/result of the\npaper appears to be that the authors can perform zero-shot relation extraction,\nextracting relations only seen at test time.\n\nThis paper is well-written and the idea is interesting. However, there are\ninsufficient experiments and comparison to previous work to convince me that\nthe paper\u2019s contributions are novel and impactful.\n\nFirst, the authors are missing a great deal of related work: Neelakantan at al.\n2015 (https://arxiv.org/abs/1504.06662) perform zero-shot relation extraction\nusing RNNs over KB paths. Verga et al. 2017 (https://arxiv.org/abs/1606.05804)\nperform relation extraction on unseen entities. The authors cite Bordes et al.\n(https://arxiv.org/pdf/1506.02075.pdf), who collect a similar dataset and\nperform relation extraction using memory networks (which are commonly used for\nreading comprehension). However, they merely note that their data was annotated\nat the \u201crelation\u201d level rather than at the triple (relation, entity pair)\nlevel\u2026 but couldn\u2019t Bordes et al. have done the same in their annotation?\nIf there is some significant difference here, it is not made clear in the\npaper. There is also a NAACL 2016 paper\n(https://www.aclweb.org/anthology/N/N16/N16-2016.pdf) which performs relation\nextraction using a new model based on memory networks\u2026 and I\u2019m sure there\nare more. Your work is so similar to much of this work that you should really\ncite and establish novelty wrt at least some of them as early as the\nintroduction -- that's how early I was wondering how your work differed, and it\nwas not made clear.\n\nSecond, the authors neither 1) evaluate their model on another dataset or 2)\nevaluate any previously published models on their dataset. This makes their\nempirical results extremely weak. Given that there is a wealth of existing work\nthat performs the same task and the lack of novelty of this work, the authors\nneed to include experiments that demonstrate that their technique outperforms\nothers on this task, or otherwise show that their dataset is superior to others\n(e.g. since it is much larger than previous, does it allow for better\ngeneralization?)"
  },
  {
    "people": [
      "Wu",
      "Fei",
      "Daniel S. Weld"
    ],
    "review": "The paper models the relation extraction problem as reading comprehension and\nextends a previously proposed reading comprehension (RC) model to extract\nunseen relations. The approach has two main components:\n\n1. Queryfication: Converting a relation into natural question. Authors use\ncrowdsourcing for this part.\n\n2. Applying RC model on the generated questions and sentences to get the answer\nspans. Authors extend a previously proposed approach to accommodate situations\nwhere there is no correct answer in the sentence.\n\nMy comments:\n\n1. The paper reads very well and the approach is clearly explained.\n\n2. In my opinion, though the idea of using RC for relation extraction is\ninteresting and novel, the approach is not novel. A part of the approach is\ncrowdsourced and the other part is taken directly from a previous work, as I\nmention above.\n\n3. Relation extraction is a well studied problem and there are plenty of\nrecently published works on the problem. However, authors do not compare their\nmethods against any of the previous works. This raises suspicion on the\neffectiveness of the approach. As seen from Table 2, the performance numbers of\nthe proposed method on the core task are not very convincing. However, this\nmaybe because of the dataset used in the paper. Hence, a comparison with\nprevious methods would actually help assess how the current method stands with\nthe state-of-the-art.\n\n4. Slot-filling data preparation: You say \"we took the first sentence s in D to\ncontain both e and a\". How can you get the answer sentence for (all) the\nrelations of an entity from the first sentence of the entity's Wikipedia\narticle? Please clarify this. See the following paper. They have a set of rules\nto locate (answer) sentences corresponding to an entity property in its\nWikipedia page:\n\nWu, Fei, and Daniel S. Weld. \"Open information extraction using Wikipedia.\"\nProceedings of the 48th Annual Meeting of the Association for Computational\nLinguistics. Association for Computational Linguistics, 2010.\n\nOverall, I think the paper presents an interesting approach. However, unless\nthe effectiveness of the approach is demonstrated by comparing it against\nrecent works on relation extraction, the paper is not ready for publication."
  },
  {
    "people": [
      "Lu",
      "Lu",
      "Roth",
      "Lu",
      "Roth"
    ],
    "review": "The paper suggests an approach based on multigraphs (several edges may link two\nnodes) for detecting potentially overlapping entities.\n\nStrengths:\nThe problem itself could be rather interesting especially for crossing entities\nto decide which one might actually be mentioned in some text. The technique\nseems to work although the empirically results do not show some \"dramatic\"\neffect. I like that some words are spent on efficiency compared to a previous\nsystem. The paper in general is well-written but also needs some further\npolishing in some details (see minor remarks below).\n\nWeaknesses:\nThe problem itself is not really well motivated. Why is it important to detect\nChina as an entity within the entity Bank of China, to stay with the example in\nthe introduction? I do see a point for crossing entities but what is the use\ncase for nested entities? This could be much more motivated to make the reader\ninterested. As for the approach itself, some important details are missing in\nmy opinion: What is the decision criterion to include an edge or not? In lines\n229--233 several different options for the I^k_t nodes are mentioned but it is\nnever clarified which edges should be present!\n\nAs for the empirical evaluation, the achieved results are better than some\nprevious approaches but not really by a large margin. I would not really call\nthe slight improvements as \"outperformed\" as is done in the paper. What is the\neffect size? Does it really matter to some user that there is some improvement\nof two percentage points in F_1? What is the actual effect one can observe? How\nmany \"important\" entities are discovered, that have not been discovered by\nprevious methods? Furthermore, what performance would some simplistic\ndictionary-based method achieve that could also be used to find overlapping\nthings? And in a similar direction: what would some commercial system like\nGoogle's NLP cloud that should also be able to detect and link entities would\nhave achieved on the datasets. Just to put the results also into contrast of\nexisting \"commercial\" systems.\n\nAs for the result discussion, I would have liked to see some more emphasis on\nactual crossing entities. How is the performance there? This in my opinion is\nthe more interesting subset of overlapping entities than the nested ones. How\nmany more crossing entities are detected than were possible before? Which ones\nwere missed and maybe why? Is the performance improvement due to better nested\ndetection only or also detecting crossing entities? Some general error\ndiscussion comparing errors made by the suggested system and previous ones\nwould also strengthen that part.\n\nGeneral Discussion:\nI like the problems related to named entity recognition and see a point for\nrecognizing crossing entities. However, why is one interested in nested\nentities? The paper at hand does not really motivate the scenario and also\nsheds no light on that point in the evaluation. Discussing errors and maybe\nadvantages with some example cases and an emphasis on the results on crossing\nentities compared to other approaches would possibly have convinced me more.\nSo, I am only lukewarm about the paper with maybe a slight tendency to\nrejection. It just seems yet another try without really emphasizing the in my\nopinion important question of crossing entities.\n\nMinor remarks:\n\n- first mention of multigraph: some readers may benefit if the notion of a\nmultigraph would get a short description\n\n- previously noted by ... many previous: sounds a little odd\n\n- Solving this task: which one?\n\n- e.g.: why in italics?\n\n- time linear in n: when n is sentence length, does it really matter whether it\nis linear or cubic?\n\n- spurious structures: in the introduction it is not clear, what is meant\n\n- regarded as _a_ chunk\n\n- NP chunking: noun phrase chunking?\n\n- Since they set: who?\n\n- pervious -> previous\n\n- of Lu and Roth~(2015)\n\n- the following five types: in sentences with no large numbers, spell out the\nsmall ones, please\n\n- types of states: what is a state in a (hyper-)graph? later state seems to be\nused analogous to node?!\n\n- I would place commas after the enumeration items at the end of page 2 and a\nperiod after the last one\n\n- what are child nodes in a hypergraph?\n\n- in Figure 2 it was not obvious at first glance why this is a hypergraph.\ncolors are not visible in b/w printing. why are some nodes/edges in gray. it is\nalso not obvious how the highlighted edges were selected and why the others are\nin gray ...\n\n- why should both entities be detected in the example of Figure 2? what is the\ndifference to \"just\" knowing the long one?\n\n- denoting ...: sometimes in brackets, sometimes not ... why?\n\n- please place footnotes not directly in front of a punctuation mark but\nafterwards\n\n- footnote 2: due to the missing edge: how determined that this one should be\nmissing?\n\n- on whether the separator defines ...: how determined?\n\n- in _the_ mention hypergraph\n\n- last paragraph before 4.1: to represent the entity separator CS: how is the\nCS-edge chosen algorithmically here?\n\n- comma after Equation 1?\n\n- to find out: sounds a little odd here\n\n- we extract entities_._\\footnote\n\n- we make two: sounds odd; we conduct or something like that?\n\n- nested vs. crossing remark in footnote 3: why is this good? why not favor\ncrossing? examples to clarify?\n\n- the combination of states alone do_es_ not?\n\n- the simple first order assumption: that is what?\n\n- In _the_ previous section\n\n- we see that our model: demonstrated? have shown?\n\n- used in this experiments: these\n\n- each of these distinct interpretation_s_\n\n- published _on_ their website\n\n- The statistics of each dataset _are_ shown\n\n- allows us to use to make use: omit \"to use\"\n\n- tried to follow as close ... : tried to use the features suggested in\nprevious works as close as possible?\n\n- Following (Lu and Roth, 2015): please do not use references as nouns:\nFollowing Lu and Roth (2015)\n\n- using _the_ BILOU scheme\n\n- highlighted in bold: what about the effect size?\n\n- significantly better: in what sense? effect size?\n\n- In GENIA dataset: On the GENIA dataset\n\n- outperforms by about 0.4 point_s_: I would not call that \"outperform\"\n\n- that _the_ GENIA dataset\n\n- this low recall: which one?\n\n- due to _an_ insufficient\n\n- Table 5: all F_1 scores seems rather similar to me ... again, \"outperform\"\nseems a bit of a stretch here ...\n\n- is more confident: why does this increase recall?\n\n- converge _than_ the mention hypergraph\n\n- References: some paper titles are lowercased, others not, why?"
  },
  {
    "people": [
      "Rei",
      "Yannakoudakis"
    ],
    "review": "The paper proposes an approach to sequence labeling with multitask learning,\nwhere language modeling is uses as the auxiliary objective. Thus, a\nbidirectional neural network architecture learns to predict the output labels\nas well as to predict the previous or next word in the sentence. The joint\nobjectives lead to improvements over the baselines in grammatical error\ndetection, chunking, NER, and POS tagging.\n\n- Strengths:\n\nThe contribution is quite well-written and easy to follow for the most part.\nThe model is exposed in sufficient detail, and the experiments are thorough\nwithin the defined framework. The benefits of introducing an auxiliary\nobjective are nicely exposed.\n\n- Weaknesses:\n\nThe paper shows very limited awareness of the related work, which is extensive\nacross the tasks that the experiments highlight. Tables 1-3 only show the three\nsystems proposed by the contribution (Baseline, +dropout, and +LMcost), while\nsome very limited comparisons are sketched textually.\n\nA contribution claiming novelty and advancements over the previous state of the\nart should document these improvements properly: at least by reporting the\nrelevant scores together with the novel ones, and ideally through replication.\nThe datasets used in the experiments are all freely available, the previous\nresults well-documented, and the previous systems are for the most part\npublicly available.\n\nIn my view, for a long paper, it is a big flaw not to treat the previous work\nmore carefully.\n\nIn that sense, I find this sentence particularly troublesome: \"The baseline\nresults are comparable to the previous best results on each of these\nbenchmarks.\" The reader is here led to believe that the baseline system somehow\nsubsumes all the previous contributions, which is shady on first read, and\nfactually incorrect after a quick lookup in related work.\n\nThe paper states \"new state-of-the-art results for error detection on both FCE\nand CoNLL-14 datasets\". Looking into the CoNLL 2014 shared task report, it is\nnot straightforward to discern whether the\nlatter part of the claim does holds true, also as per Rei and Yannakoudakis'\n(2016) paper. The paper should support the claim by inclusion/replication of\nthe related work.\n\n- General Discussion:\n\nThe POS tagging is left as more of an afterthought. The comparison to Plank et\nal. (2016) is at least partly unfair as they test across multiple languages in\nthe Universal Dependencies realm, showing top-level performance across language\nfamilies, which I for one believe to be far more relevant than WSJ\nbenchmarking. How does the proposed system scale up/down to multiple languages,\nlow-resource languages with limited training data, etc.? The paper leaves a lot\nto ask for in that dimension to further substantiate its claims.\n\nI like the idea of including language modeling as an auxiliary task. I like the\narchitecture, and sections 1-4 in general. In my view, there is a big gap\nbetween those sections and the ones describing the experiments (5-8).\n\nI suggest that this nice idea should be further fleshed out before publication.\nThe rework should include at least a more fair treatment of related work, if\nnot replication, and at least a reflection on multilinguality. The data and the\nsystems are all there, as signs of the field's growing maturity. The paper\nshould in my view partake in reflecting this maturity, and not step away from\nit. In faith that these improvements can be implemented before the publication\ndeadline, I vote borderline."
  },
  {
    "people": [
      "Agres"
    ],
    "review": "This paper proposes an approach for classifying literal and metaphoric\nadjective-noun pairs. The authors create a word-context matrix for adjectives\nand nouns where each element of the matrix is the PMI score. They then use\ndifferent methods for selecting dimensions of this matrix to represent each\nnoun/adjective as a vector. The geometric properties of average, nouns, and\nadjective vectors and their normalized versions are used as features in\ntraining a regression model for classifying the pairs to literal or metaphor\nexpressions. Their approach performs similarly to previous work that learns a\nvector representation for each adjective.\n\nSupervision and zero-shot learning. The authors argue that their approach\nrequires less supervision (compared to previous work)  and can do zero-shot\nlearning. I don\u2019t think this is quite right and given that it seems to be one\nof the main points of the paper, I think it is worth clarifying. The approach\nproposed in the paper is a supervised classification task: The authors form\nvector representations from co-occurrence statistics, and then use the\nproperties of these representations and the gold-standard labels of each pair\nto train a classifier. The model (similarly to any other supervised classifier)\ncan be tested on words that did not occur in the training data; but, the model\ndoes not learn from such examples. Moreover, those words are not really\n\u201cunseen\u201d because the model needs to have a vector representation of those\nwords.\n\nInterpretation of the results. The authors provide a good overview of the\nprevious related work on metaphors. However, I am not sure what the intuition\nabout their approach is (that is, using the geometric properties such as vector\nlength in identifying metaphors). For example, why are the normalized vectors\nconsidered? It seems that they don\u2019t contribute to a better performance.\nMoreover, the most predictive feature is the noun vector; the authors explain\nthat this is a side effect of the data which is collected such that each\nadjective occurs in both metaphoric and literal expressions. (As a result, the\nadjective vector is less predictive.) It seems that the proposed approach might\nbe only suitable for the given data. This shortcoming is two-fold: (a) From the\ntheoretical perspective (and especially since the paper is submitted to the\ncognitive track), it is not clear what we learn about theories of metaphor\nprocessing. (b) From the NLP applications standpoint, I am not sure how\ngeneralizable this approach is compared to the compositional models.\n\nNovelty. The proposed approach for representing noun/adjective vectors is very\nsimilar to that of Agres et al. It seems that the main contribution of the\npaper is that they use the geometric properties to classify the vectors."
  },
  {
    "people": [
      "Dong",
      "Lapata",
      "Jia",
      "Liang",
      "Ling",
      "Chen Liang et al",
      "Lapata"
    ],
    "review": "Summary: The paper proposes a neural model for predicting Python syntax trees\nfrom text descriptions. Guided by the actual Python grammar, the model\ngenerates tree nodes sequentially in a depth-first fashion. Key ideas include\ninjecting the information from the parent node as part of the LSTM input, a\npointer network for copying the terminals, and unary closure which collapses\nchains of unary productions to reduce the tree size. The model is evaluated on\nthree datasets from different domains and outperforms almost all previous work.\n\nStrengths:\n\nThe paper is overall very well-written. The explanation of system is clear, and\nthe analysis is thorough.\n\nThe system itself is a natural extension of various ideas. The most similar\nwork include tree-based generation with parent feeding (Dong and Lapata, 2016)\nand various RNN-based semantic parsing with copy mechanism (Jia and\nLiang, 2016; Ling et al., 2016). [The guidance of parsing based on grammar is\nalso explored in Chen Liang et al., 2016 (https://arxiv.org/abs/1611.00020)\nwhere a code-assist system is used to ensure that the code\nis valid.] Nevertheless, the model is this paper stands out as it is able to\ngenerate much longer and more complex programs than most previous work\nmentioned. \n\nWeaknesses:\n\nThe evaluation is done on code accuracy (exact match) and BLEU score. These\nmetrics (especially BLEU) might not be the best metrics for evaluating the\ncorrectness of programs. For instance, the first example in Table 5 shows that\nwhile the first two lines in boxes A and B are different, they have the same\nsemantics. Another example is that variable names can be different. Evaluation\nbased on what the code does (e.g., using test cases or static code analysis)\nwould be more convincing.\n\nAnother point about evaluation: other systems (e.g., NMT baseline) may generate\ncode with syntactic error. Would it be possible to include the result on the\nhighest-scoring well-formed code (e.g., using beam search) that these baseline\nsystems generate? This would give a fairer comparison since these system can\nchoose to prune malformed code.\n\nGeneral Discussion:\n\n* Lines 120-121: some approaches that use domain-specific languages were also\nguided by a grammar. One example is Berant and Liang, 2014, which uses a pretty\nlimited grammar for logical forms (Table 1). In addition to comparing to that\nline of work, emphasizing that the grammar in this paper is much larger than\nmost previous work would make this work stronger.\n\n* Lines 389-397: For the parent feeding mechanism, is the child index being\nused? In other words, is p_t different when generating a first child versus a\nsecond child? In Seq2Tree (Dong and Lapata, 2016) the two non-terminals would\nhave different hidden states.\n\n* Line 373: Are the possible tokens embedded? Is it assumed that the set of\npossible tokens is known beforehand?\n\n* The examples in the appendix are nice.\n\n---\n\nI have read the author response."
  },
  {
    "people": [
      "Shen",
      "Shen"
    ],
    "review": "The paper proposes a recurrent neural architecture that can skip irrelevant\ninput units. This is achieved by specifying R (# of words to read at each\n\"skim\"), K (max jump size), and N (max # of jumps allowed). An LSTM processes R\nwords, predicts the jump size k in {0, 1...K} (0 signals stop), skips the next\nk-1 words and continues until either the number of jumps reaches N or the model\nreaches the last word. While the model is not differentiable, it can be trained\nby standard policy gradient. The work seems to have been heavily influenced by\nShen et al. (2016) who apply a similar reinforcement learning approach\n(including the same variance stabilization) to multi-pass machine reading. \n\n- Strengths:\n\nThe work simulates an intuitive \"skimming\" behavior of a reader, mirroring Shen\net al. who simulate (self-terminated) repeated reading. A major attribute of\nthis work is its simplicity. Despite the simplicity, the approach yields\nfavorable results. In particular, the authors show through a well-designed\nsynthetic experiment that the model is indeed able to learn to skip when given\noracle jump signals. In text classification using real-world datasets, the\nmodel is able to perform competitively with the non-skimming model while being\nclearly faster. \n\nThe proposed model can potentially have meaningful practical implications: for\ntasks in which skimming suffices (e.g., sentiment classification), it suggests\nthat we can obtain equivalent results without consuming all data in a\ncompletely automated fashion. To my knowledge this is a novel finding. \n\n- Weaknesses:\n\nIt's a bit mysterious on what basis the model determines its jumping behavior\nso effectively (other than the synthetic dataset). I'm thinking of a case where\nthe last part of the given sentence is a crucial evidence, for instance: \n\n\"The movie was so so and boring to the last minute but then its ending blew me\naway.\" \n\nIn this example, the model may decide to skip the rest of the sentence after\nreading \"so so and boring\". But by doing so it'll miss the turning point\n\"ending blew me away\" and mislabel the instance as negative. For such cases a\nsolution can be running the skimming model in both directions as the authors\nsuggest as future work. But in general the model may require more sophisticated\narchitecture for controlling skimming.\n\nIt seems one can achieve improved skimming by combining it with multi-pass\nreading (presumably in reverse directions). That's how humans read to\nunderstand text that can't be digested in one skim; indeed, that's how I read\nthis draft. \n\nOverall, the work raises an interesting problem and provides an effective but\nintuitive solution."
  },
  {
    "people": [
      "Goodfellow",
      "Goodfellow"
    ],
    "review": "- Strengths:\n\nThe authors use established neural network methods (adversarial networks --\nGoodfellow et al, NIPS-2014) to take advantage of 8 different Chinese work\nbreaking test sets, with 8 different notions of what counts as a word in\nChinese.\n\nThis paper could have implications for many NLP tasks where we have slightly\ndifferent notions of what counts as correct.  We have been thinking of that\nproblem in terms of adaptation, but it is possible that Goodfellow et al is a\nmore useful way of thinking about this problem.\n\n- Weaknesses:\n\nWe need a name for the problem mentioned above.  How about: the elusive gold\nstandard.  I prefer that term to multi-criteria.\n\nThe motivation seems to be unnecessarily narrow.  The elusive gold standard\ncomes up in all sorts of applications, not just Chinese Word Segmentation.\n\nThe motivation makes unnecessary assumptions about how much the reader knows\nabout Chinese.              When you don't know much about something, you think it is\neasier than it is.  Many non-Chinese readers (like this reviewer) think that\nChinese is simpler than it is.              It is easy to assume that Chinese Word\nSegmentation is about as easy as tokenizing English text into strings delimited\nby white space.  But my guess is that IAA (inter-annotator agreement) is pretty\nlow in Chinese.  The point you are trying to make in Table 1 is that there is\nconsiderable room for disagreement among native speakers of Chinese.\n\nI think it would help if you could point out that there are many NLP tasks\nwhere there is considerable room for disagreement.  Some tasks like machine\ntranslation, information retrieval and web search have so much room for\ndisagreement that the metrics for those tasks have been designed to allow for\nmultiple correct answers.  For other tasks, like part of speech tagging, we\ntend to sweep the elusive gold standard problem under a rug, and hope it will\njust go away.  But in fact, progress on tagging has stalled because we don't\nknow how to distinguish differences of opinions from errors.  When two\nannotators return two different answers, it is a difference of opinion.  But\nwhen a machine returns a different answer, the machine is almost always wrong.\n\nThis reader got stuck on the term: adversary.  I think the NIPS paper used that\nbecause it was modeling noise under \"murphy's law.\"  It is often wise to assume\nthe worst.\n\nBut I don't think it is helpful to think of differences of opinion as an\nadversarial game like chess.  In chess, it makes sense to think that your\nopponent is out to get you, but I'm not sure that's the most helpful way to\nthink about differences of opinion.\n\nI think it would clarify what you are doing to say that you are applying an\nestablished method from NIPS (that uses the term \"adversarial\") to deal with\nthe elusive gold standard problem.  And then point out that the elusive gold\nstandard problem is a very common problem.  You will study it in the context of\na particular problem in Chinese, but the problem is much more general than\nthat.\n\n- General Discussion:\n\nI found much of the paper unnecessarily hard going.  I'm not up on Chinese or\nthe latest in NIPS, which doesn't help.  But even so, there are some small\nissues with English, and some larger problems with exposition.\n\nConsider Table 4.  Line 525 makes an assertion about the first block and depth\nof networks.  Specifically, which lines in Table 4 support that assertion.\n\nI assume that P and R refer to precision and recall, but where is that\nexplained.  I assume that F is the standard F measure, and OOV is\nout-of-vocabulary, but again, I shouldn't have to assume such things.\n\nThere are many numbers in Table 4.  What counts as significance?  Which numbers\nare even comparable?  Can we compare numbers across cols?  Is performance on\none collection comparable to performance on another?  Line 560 suggests that\nthe adversarial method is not significant.  What should I take away from Table\n4?  Line 794 claims that you have a significant solution to what I call the\nelusive gold standard problem.              But which numbers in Table 4 justify that\nclaim?\n\nSmall quibbles about English:\n\nworks --> work (in many places).  Work is a  mass noun, not a count noun\n(unlike \"conclusion\").              One can say one conclusion, two conclusions, but\nmore/less/some work (not one work, two works).\n\nline 493: each dataset, not each datasets\n\nline 485: Three datasets use traditional Chinese (AS, CITY, CKIP) and the other\nfive use simplified Chinese.\n\nline 509: random --> randomize"
  },
  {
    "people": [
      "Wei Xu",
      "Kai Yu",
      "Eduard Hovy"
    ],
    "review": "The paper proposes a method to train models for Chinese word segmentation (CWS)\non datasets having multiple segmentation criteria.\n\n- Strengths:\n1. Multi-criteria learning is interesting and promising.\n2. The proposed model is also interesting and achieves a large improvement from\nbaselines.\n\n- Weaknesses:\n1. The proposed method is not compared with other CWS models. The baseline\nmodel (Bi-LSTM) is proposed in [1] and [2]. However, these model is proposed\nnot for CWS but for POS tagging and NE tagging. The description \"In this paper,\nwe employ the state-of-the-art architecture ...\" (in Section 2) is misleading.\n2. The purpose of experiments in Section 6.4 is unclear. In Sec. 6.4, the\npurpose is that investigating \"datasets in traditional Chinese and simplified\nChinese could help each other.\" However, in the experimental setting, the model\nis separately trained on simplified Chinese and traditional Chinese, and the\nshared parameters are fixed after training on simplified Chinese. What is\nexpected to fixed shared parameters?\n\n- General Discussion:\nThe paper should be more interesting if there are more detailed discussion\nabout the datasets that adversarial multi-criteria learning does not boost the\nperformance.\n\n[1] Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional lstm-crf models for\nsequence tagging. arXiv preprint arXiv:1508.01991.\n[2] Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via\nbi-directional lstm-cnns-crf. arXiv preprint arXiv:1603.01354 ."
  },
  {
    "people": [
      "Mary",
      "Manning"
    ],
    "review": "(the authors response answer most of the clarification questions of my review)\n\n=========================\n- Summary:\n=========================\n\nThe paper describes a transition-based system for UCCA graphs, featuring\nnon-terminal nodes,  reentrancy and discontinuities. The transition set is a\nmix of already proposed transitions\n(The key aspects are the swap transition to cope with discontinuities, and\ntransitions not popping the stack to allow multiple parents for a node.).\nThe best performance is obtained using as transition classifier a MLP with\nfeatures based on bidirectional LSTMs.\n\nThe authors compare the obtained performance with other state-of-the art\nparsers, using conversion schemes (to bilexical graphs, and to tree\napproximations): the parsers are trained on converted data, used to predict\ngraphs (or trees), and the predicted structures are converted ack to UCCA and\nconfronted with gold UCCA representations.\n\n=========================\n- Strengths:\n=========================\n\nThe paper presents quite solid work, with state-of-the art transition-based\ntechniques, and machine learning for parsing techniques.\n\nIt is very well written, formal and experimental aspects are described in a\nvery precise way, and the authors demonstrate a very good knowledge of the\nrelated work, both for parsing techniques and for shallow semantic\nrepresentations.\n\n=========================\n- Weaknesses:\n=========================\n\nMaybe the weakness of the paper is that the originality lies mainly in the\ntargeted representations (UCCA), not really in the proposed parser.\n\n=========================\n- More detailed comments and clarification questions:\n=========================\n\nIntroduction\n\nLines 46-49: Note that \"discontinuous nodes\" could be linked to\nnon-projectivity in the dependency framework. So maybe rather state that the\ndifference is with phrase-structure syntax not dependency syntax.\n\nSection 2:\n\nIn the UCCA scheme's description, the alternative \"a node (or unit) corresponds\nto a terminal or to several sub-units\" is not very clear. Do you mean something\nelse than a node is either a terminal or a non terminal? Can't a non terminal\nnode have one child only (and thus neither be a terminal nor have several\nsub-units) ?\n\nNote that \"movement, action or state\" is not totally appropriate, since there\nare processes which are neither movements nor actions (e.g. agentless\ntransformations).\n(the UCCA guidelines use these three terms, but later state the state/process\ndichotomy, with processes being an \"action, movement or some other relation\nthat evolves in time\").\n\nlines 178-181: Note that the contrast between \"John and Mary's trip\" and \"John\nand Mary's children\" is not very felicitous. The relational noun \"children\"\nevokes an underlying relation between two participants (the children and\nJohn+Mary), that has to be accounted for in UCCA too.\n\nSection 4:\n\nConcerning the conversion procedures:\n- While it is very complete to provide the precise description of the\nconversion procedure in the supplementary material, it would ease reading to\ndescribe it informally in the paper (as a variant of the\nconstituent-to-dependency conversion procedure \u00e0 la Manning, 95). Also, it\nwould be interesting to motivate the priority order used to define the head of\nan edge.\n\n- How l(u) is chosen in case of several children with same label should be made\nexplicit (leftmost ?).\n\n- In the converted graphs in figure 4, some edges seem inverted (e.g. the\ndirection between \"John\" and \"moved\" and between \"John\" and \"gave\" should be\nthe same).\n\n- Further, I am confused as to why the upper bound for remote edges in\nbilexical approximations is so low. The current description of the conversions\ndo not allow to get an quick idea of which kind of remote edges cannot be\nhandled.\n\nConcerning the comparison to other parsers:\nIt does not seem completely fair to tune the proposed parser, but to use\ndefault settings for the other parsers.\n\nSection 5\n\nLine 595: please better motivate the claim \"using better input encoding\"\n\nSection 6\n\nI am not convinced by the alledged superiority of representations with\nnon-terminal nodes. Although it can be considered more elegant not to choose a\nhead for some constructions, it can be noted that formally co-head labels can\nbe used in bilexical dependencies to recover the same information."
  },
  {
    "people": [
      "Kingma",
      "Li"
    ],
    "review": "- Strengths:\n\n - The paper is clearly written and well-structured. \n\n - The system newly applied several techniques including global optimization to\nend-to-end neural relation extraction, and the direct incorporation of the\nparser representation is interesting.\n\n - The proposed system has achieved the state-of-the-art performance on both\nACE05 and CONLL04 data sets.\n\n - The authors include several analyses.\n\n- Weaknesses:\n\n - The approach is incremental and seems like just a combination of existing\nmethods.  \n\n - The improvements on the performance (1.2 percent points on dev) are\nrelatively small, and no significance test results are provided.\n\n- General Discussion:\n\n- Major comments:\n\n - The model employed a recent parser and glove word embeddings. How did they\naffect the relation extraction performance?\n\n - In prediction, how did the authors deal with illegal predictions?\n\n- Minor comments:\n\n - Local optimization is not completely \"local\". It \"considers structural\ncorrespondences between incremental decisions,\" so this explanation in the\nintroduction is misleading.\n\n - Points in Figures 6 and 7 should be connected with straight lines, not\ncurves.\n\n - How are entities represented in \"-segment\"?\n\n - Some citations are incomplete. Kingma et al. (2014) is accepted to ICLR,\nand Li et al. (2014) misses pages."
  },
  {
    "people": [
      "Mikolov",
      "Mikolov",
      "Pilehvar",
      "Navigli"
    ],
    "review": "This paper describes a model for cross-lingual named entity recognition (NER).\nThe authors employ conditional random fields, maximum entropy Markov, and\nneural network-based NER methods. In addition, authors propose two methods to\ncombine the output of those methods (probability-based and ranking-based), and\na method to select the best training instances from cross-lingual comparable\ncorpora. The cross-lingual projection is done using a variant of Mikolov\u2019s\nproposal. In general, the paper is easy to follow, well-structured, and the\nEnglish quality is also correct. The results of the combined annotations are\ninteresting.\n\nDetailed comments:\n\nI was wondering which is the motivation behind proposing a Continuous\nBag-of-word (CBOW) model variation. You don\u2019t give much details about this\n(or the parameters employed). Was the original model (or the Continuous\nSkip-gram model) offering low results? I suggest to include also the results\nwith the CBOW model, so readers can analyse the improvements of your approach.\nSince you use a decay factor for the surrounding embeddings, I suggest to take\na look to the exponential decay used in [1].\n\nSimilarly to the previous comment, I would like to look at the differences\nbetween the original Mikolov\u2019s cross-lingual projections and your frequency\nweighted projections. These contributions are more valuable if readers can see\nthat your method is really superior.\n\n\u201cthe proposed data selection scheme is very effective in selecting\ngood-quality projection-labeled data and the improvement is significant\u201d \u2190\nHave you conducted a test of statistical significance? I would like to know if\nthe differences between result in this work are significant. \n\nI suggest to integrate the text of Section 4.4 at the beginning of Section 4.2.\nIt would look cleaner. I also recommend to move the evaluation of Table 2 to\nthe evaluation section.\n\nI miss a related work section. Your introduction includes part of that\ninformation. I suggest to divide the introduction in two sections.\n\nThe evaluation is quite short (1.5 pages with conclusion section there). You\nobtain state-of-the-art results, and I would appreciate more discussion and\nanalysis of the results.\n\nSuggested references:\n\n[1] Iacobacci, I., Pilehvar, M. T., & Navigli, R. (2016). Embeddings for word\nsense disambiguation: An evaluation study. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Linguistics (Vol. 1, pp. 897-907)."
  },
  {
    "people": [
      "Flati Tiziano",
      "Vannella Daniele",
      "Pasini Tommaso",
      "Navigli Roberto",
      "Soren Auer",
      "Christian Bizer",
      "Georgi Kobilarov",
      "Jens \u00a8\nLehmann",
      "Richard Cyganiak",
      "Zachary I",
      "ve",
      "Gerard de Melo",
      "Gerhard Weikum",
      "Zornitsa Kozareva",
      "Eduard H. Hovy",
      "Vivi Nastase",
      "Michael Strube",
      "Benjamin Boerschinger",
      "Caecilia Zirn",
      "Anas Elghafari",
      "Simone Paolo Ponzetto",
      "Michael Strube",
      "Simone Paolo Ponzetto",
      "Michael Strube",
      "Fabian M. Suchanek",
      "Gjergji Kasneci",
      "Paola Velardi",
      "Stefano Faralli",
      "Roberto Navigli"
    ],
    "review": "- Strengths:\n - the model if theoretically solid and motivated by formal semantics. \n\n- Weaknesses:\n\n - The paper is about is-a relation extraction but the majority of literature\nabout taxonomization is not referenced in the paper, inter alia:\n\nFlati Tiziano, Vannella Daniele, Pasini Tommaso, Navigli Roberto.\n2016. MultiWiBi: The multilingual Wikipedia bitaxonomy project.\n\nSoren Auer, Christian Bizer, Georgi Kobilarov, Jens \u00a8\nLehmann, Richard Cyganiak, and Zachary Ive.\n2007. DBpedia: A nucleus for a web of open data.\n\nGerard de Melo and Gerhard Weikum. 2010. MENTA:\nInducing Multilingual Taxonomies from Wikipedia.\n\nZornitsa Kozareva and Eduard H. Hovy. 2010. A\nSemi-Supervised Method to Learn and Construct\nTaxonomies Using the Web. \n\nVivi Nastase, Michael Strube, Benjamin Boerschinger,\nCaecilia Zirn, and Anas Elghafari. 2010. WikiNet:\nA Very Large Scale Multi-Lingual Concept Network.\n\nSimone Paolo Ponzetto and Michael Strube. 2007.\nDeriving a large scale taxonomy from Wikipedia.\n\nSimone Paolo Ponzetto and Michael Strube. 2011.\nTaxonomy induction based on a collaboratively built\nknowledge repository. \n\nFabian M. Suchanek, Gjergji Kasneci, and Gerhard\nWeikum. 2008. YAGO: A large ontology from\nWikipedia and WordNet. \n\nPaola Velardi, Stefano Faralli, and Roberto Navigli.\n2013. OntoLearn Reloaded: A graph-based algorithm\nfor taxonomy induction. \n\n - Experiments are poor, they only compare against \"Hearst patterns\" without\ntaking into account the works previously cited.\n\n- General Discussion:\n The paper is easy to follow and the supplementary material is also well\nwritten and useful, however the paper lack of references of is a relation\nextraction and taxonomization literature. The same apply for the experiments.\nIn fact no meaningful comparison is performed and the authors not even take\ninto account the existence of other systems (more recent than hearst patterns).\n\nI read authors answers but still i'm not convinced that they couldn't perform\nmore evaluations. I understand that they have a solid theoretical motivation\nbut still, i think that comparison are very important to asses if the\ntheoretical intuitions of the authors are confirmed also in practice. While\nit's true that all the works i suggested as comparison build taxonomies, is\nalso true that a comparison is possible considering the edges of a taxonomy.\n\nAnyway, considering the detailed author answer and the discussion with the\nother reviewer i can rise my score to 3 even if i still think that this paper\nis poor of experiments and does not frame correctly in the is-a relation\nextraction / taxonomy building literature."
  },
  {
    "people": [
      "Johansson",
      "Nieto Pi\u00f1a",
      "Arora"
    ],
    "review": "- Overview:\n\nThe paper proposes a new model for training sense embeddings grounded in a\nlexical-semantic resource (in this case WordNet). There is no direct evaluation\nthat the learned sense vectors are meaningful; instead, the sense vectors are\ncombined back into word embeddings, which are evaluated in a downstream task:\nPP attachment prediction.\n\n- Strengths:\n\nPP attachment results seem solid.\n\n- Weaknesses:\n\nWhether the sense embeddings are meaningful remains uninvestigated. \n\nThe probabilistic model has some details that are hard to understand. Are the\n\\lambda_w_i hyperparameters or trained? Where does \u201crank\u201d come from, is\nthis taken from the sense ranks in WordNet?\n\nRelated work: the idea of expressing embeddings of words as a convex\ncombination of sense embeddings has been proposed a number of times previously.\nFor instance, Johansson and Nieto Pi\u00f1a \u201cEmbedding a semantic network in a\nword space\u201d (NAACL, 2015) decomposed word embeddings into ontology-grounded\nsense embeddings based on this idea. Also in unsupervised sense vector training\nthis idea has been used, for instance by Arora et al \u201cLinear Algebraic\nStructure of Word Senses, with Applications to Polysemy\u201d.\n\nMinor comments:\n\nno need to define types and tokens, this is standard terminology\n\nwhy is the first \\lamba_w_i in equation 4 needed if the probability is\nunnormalized?\n\n- General Discussion:"
  },
  {
    "people": [
      "Chung",
      "Manning",
      "Luong"
    ],
    "review": "- Strengths: In general, the paper is well structured and clear. It is possible\nto follow most of the explanation, the ideas presented are original and the\nresults obtained are quite interesting.\n\n- Weaknesses: I have some doubts about the interpretation of the results. In\naddition, I think that some of the claims regarding the capability of the\nmethod proposed to learn morphology are not propperly backed by scientific\nevidence.\n\n- General Discussion:\n\nThis paper explores a complex architecture for character-level neural machine\ntranslation (NMT). The proposed architecture extends a classical\nencoder-decoder architecture by adding a new deep word-encoding layer capable\nof encoding the character-level input into sub-word representations of the\nsource-language sentence. In the same way, a deep word-decoding layer is added\nto the output to transform the target-language sub-word representations into a\ncharacter sequence as the final output of the NMT system. The objective of such\narchitecture is to take advantage of the benefits of character-level NMT\n(reduction of the size of the vocabulary and flexibility to deal with unseen\nwords) and, at the same time, improving the performance of the whole system by\nusing an intermediate representation of sub-words to reduce the size of the\ninput sequence of characters. In addition, the authors claim that their deep\nword-encoding model is able to learn morphology better than other\nstate-of-the-art approaches.\n\nI have some concerns regarding the evaluation. The authors compare their\napproach to other state-of-the-art systems taking into account two parameters:\ntraining time and BLEU score. However, I do not clearly see the advantage of\nthe model proposed (DCNMT) in front of other approaches such as bpe2char. The\ndifference between both approaches as regards BLEU score is very small (0.04 in\nCs-En and 0.1 in En-Cs) and it is hard to say if one of them is outperforming\nthe other one without statistical significance information: has statistical\nsignificance been evaluated? As regards the training time, it is worth\nmentioning that the bpe2char for Cs-En takes 8 days less than DCNMT. For En-Cs\ntraining time is not provided (why not?) and for En-Fr bpe2char is not\nevaluated. I think that a more complete comparison with this system should be\ncarried out to prove the advantages of the model proposed.\n\nMy second concern is on the 5.2 Section, where authors start claiming that they\ninvestigated about the ability of their system to learn morphology. However,\nthe section only contains a examples and some comments on them. Even though\nthese examples are very well chosen and explained in a very didactic way, it is\nworth noting that no experiments or formal evaluation seem to have been\ncarried out to support the claims of the authors. I would definitely encourage\nauthors to extend this very interesting part of the paper that could even\nbecome a different paper itself. On the other hand, this Section does not seem\nto be a critical point of the paper, so for the current work I may suggest just\nto move this section to an appendix and soften some of the claims done\nregarding the capabilities of the system to learn morphology.\n\nOther comments, doubts and suggestions:\n\n - There are many acronyms that are used but are not defined (such as LSTM,\nHGRU, CNN or PCA) or which are defined after starting to use them (such as RNN\nor BPE). Even though some of these acronyms are well known in the field of deep\nlearning, I would encourage the authors to defined them to improve clearness.\n\n - The concept of energy is mentioned for the first time in Section 3.1. Even\nthough the explanation provided is enough at that point, it would be nice to\nrefresh the idea of energy in Section 5.2 (where it is used several times) and\nproviding some hints about how to interpret it: a high energy on a character\nwould be indicating that the current morpheme should be split at that point? In\naddition, the concept of peak (in Figure 5) is not described.\n\n - When the acronym BPE is defined, capital letters are used, but then, for the\nrest of mentions it is lower cased; is there a reason for this?\n\n - I am not sure if it is necessary to say that no monolingual corpus is used\nin Section 4.1.\n\n - It seems that there is something wrong with Figure 4a since the colours for\nthe energy values are not shown for every character.\n\n - In Table 1, the results for model (3) (Chung et al. 2016) for Cs-En were not\ntaken from the papers, since they are not reported. If the authors computed\nthese results by themselves (as it seems) they should mention it.\n\n - I would not say that French is morphologically poor, but rather that it is\nnot that rich as Slavic languages such as Czech.\n\n - Why a link is provided for WMT'15 training corpora but not for WMT'14?\n\n - Several references are incomplete\n\nTypos:\n\n  - \"..is the bilingual, parallel corpora provided...\" -> \"..are the bilingual,\nparallel corpora provided...\"\n\n  - \"Luong and Manning (2016) uses\" -> \"Luong and Manning (2016) use\"\n\n  - \"HGRU (It is\" -> \"HGRU (it is\"\n\n  - \"coveres\" -> \"covers\"\n\n  - \"both consists of two-layer RNN, each has 1024\" -> \"both consist of\ntwo-layer RNN, each have 1024\"\n\n  - \"the only difference between CNMT and DCNMT is CNMT\" -> \"the only\ndifference between CNMT and DCNMT is that CNMT\""
  },
  {
    "people": [
      "Lee",
      "Nguyen",
      "Nguyen"
    ],
    "review": "- Strengths:\nClear description of methods and evaluation\nSuccessfully employs and interprets a variety of evaluations\nSolid demonstration of practicality of technique in real-world interactive\ntopic modeling\n\n- Weaknesses:\nMissing related work on anchor words\nEvaluation on 20 Newsgroups is not ideal\nTheoretical contribution itself is small \n\n- General Discussion:\nThe authors propose a new method of interactive user specification of topics\ncalled Tandem Anchors. The approach leverages the anchor words algorithm, a\nmatrix-factorization approach to learning topic models, by replacing the\nindividual anchors inferred from the Gram-Schmidt algorithm with constructed\nanchor pseudowords created by combining the sparse vector representations of\nmultiple words that for a topic facet. The authors determine that the use of a\nharmonic mean function to construct pseudowords is optimal by demonstrating\nthat classification accuracy of document-topic distribution vectors using these\nanchors produces the most improvement over Gram-Schmidt. They also demonstrate\nthat their work is faster than existing interactive methods, allowing\ninteractive iteration, and show in a user study that the multiword anchors are\neasier and more effective for users.\n\nGenerally, I like this contribution a lot: it is a straightforward modification\nof an existing algorithm that actually produces a sizable benefit in an\ninteractive setting. I appreciated the authors\u2019 efforts to evaluate their\nmethod on a variety of scales. While I think the technical contribution in\nitself is relatively small (a strategy to assemble pseudowords based on topic\nfacets) the thoroughness of the evaluation merited having it be a full paper\ninstead of a short paper. It would have been nice to see more ideas as to how\nto build these facets in the absence of convenient sources like category titles\nin 20 Newsgroups or when initializing a topic model for interactive learning.\n\nOne frustration I had with this paper is that I find evaluation on 20\nNewsgroups to not be great for topic modeling: the documents are widely\ndifferent lengths, preprocessing matters a lot, users have trouble making sense\nof many of the messages, and naive bag-of-words models beat topic models by a\nsubstantial margin. Classification tasks are useful shorthand for how well a\ntopic model corresponds to meaningful distinctions in the text by topic; a task\nlike classifying news articles by section or reviews by the class of the\nsubject of the review might be more appropriate. It would also have been nice\nto see a use case that better appealed to a common expressed application of\ntopic models, which is the exploration of a corpus.\n\nThere were a number of comparisons I think were missing, as the paper contains\nlittle reference to work since the original proposal of the anchor word model.\nIn addition to comparing against standard Gram-Schmidt, it would have been good\nto see the method from Lee et. al. (2014), \u201cLow-dimensional Embeddings for\nInterpretable Anchor-based Topic Inference\u201d. I also would have liked to have\nseen references to Nguyen et. al. (2013), \u201cEvaluating Regularized Anchor\nWords\u201d and Nguyen et. al. (2015) \u201cIs Your Anchor Going Up or Down? Fast and\nAccurate Supervised Topic Models\u201d, both of which provide useful insights into\nthe anchor selection process.\n\nI had some smaller notes:\n- 164: \u2026entire dataset\n- 164-166: I\u2019m not quite sure what you mean here. I think you are claiming\nthat it takes too long to do one pass? My assumption would have been you would\nuse only a subset of the data to retrain the model instead of a full sweep, so\nit would be good to clarify what you mean.\n- 261&272: any reason you did not consider the and operator or element-wise\nmax? They seem to correspond to the ideas of union and intersection from the or\noperator and element-wise min, and it wasn\u2019t clear to me why the ones you\nchose were better options.\n- 337: Usenet should be capitalized\n- 338-340: Why fewer than 100 (as that is a pretty aggressive boundary)? Also,\ndid you remove headers, footers, and/or quotes from the messages?\n- 436-440: I would have liked to see a bit more explanation of what this tells\nus about confusion.\n- 692: using tandem anchors\n\nOverall, I think this paper is a meaningful contribution to interactive topic\nmodeling that I would like to see available for people outside the machine\nlearning community to investigate, classify, and test hypotheses about their\ncorpora.\n\nPOST-RESPONSE: I appreciate the thoughtful responses of the authors to my\nquestions. I would maintain that for some of the complimentary related work\nthat it's useful to compare to non-interactive work, even if it does something\ndifferent."
  },
  {
    "people": [
      "Li",
      "Dethlefs",
      "Pennebaker"
    ],
    "review": "This paper presents evaluation metrics for lyrics generation exploring the need\nfor the lyrics to be original,but in a similar style to an artist whilst being\nfluent and co-herent. The paper is well written and the motivation for the\nmetrics are well explained.  \n\nThe authors describe both hand annotated metrics (fluency, co-herence and\nmatch) and an automatic metric for \u2018Similarity'. Whilst the metric for\nSimilarity is unique and interesting the paper does not give any evidence of\nthis as an effective automatic metric as correlations between this metric and\nthe others are low, (which they say that they should be used separately). The\nauthors claim it can be used to meaningfully analyse system performance but we\nhave to take their word for it as again there is no correlation with any\nhand-annotated performance metric.  Getting worse scores than a baseline system\nisn\u2019t evidence that the metric captures quality (e.g. you could have a very\nstrong baseline).\n\nSome missing references, e.g. recent work looking at automating co-herence,\ne.g. using mutual information density (e.g. Li et al. 2015). In addition, some\nreference to style matching from the NLG community are missing (e.g. Dethlefs\net al. 2014 and the style matching work by Pennebaker)."
  },
  {
    "people": [
      "Wu"
    ],
    "review": "This paper studies how to properly evaluate systems that produce ghostwriting\nof rap lyrics.\nThe authors present manual evaluation along three key aspects: fluency,\ncoherence, and style matching.\nThey also introduce automatic metrics that consider uniqueness via maximum\ntraining similarity, and stylistic similarity via rhyme density.\n\nI can find some interesting analysis and discussion in the paper.\nThe way for manually evaluating style matching especially makes sense to me.\n\nThere also exist a few important concerns for me.\n\nI am not convinced about the appropriateness of only doing fluency/coherence\nratings at line level.\nThe authors mention that they are following Wu (2014), but I find that work\nactually studying a different setting of hip hop lyrical challenges and\nresponses, which should be treated at line level in nature.\nWhile in this work, a full verse consists of multiple lines that normally\nshould be topically and structurally coherent.\nCurrently I cannot see any reason why not to evaluate fluency/coherence for a\nverse as a whole.\n\nAlso, I do not reckon that one should count so much on automatic metrics, if\nthe main goal is to ``generate similar yet unique lyrics''.\nFor uniqueness evaluation, the calculations are performed on verse level.\nHowever, many rappers may only produce lyrics within only a few specific topics\nor themes.\nIf a system can only extract lines from different verses, presumably we might\nalso get a fluent, coherent verse with low verse level similarity score, but we\ncan hardly claim that the system ``generalizes'' well.\nFor stylistic similarity with the specified artist, I do not think rhyme\ndensity can say it all, as it is position independent and therefore may not be\nenough to reflect the full information of style of an artist.\n\nIt does not seem that the automatic metrics have been verified to be well\ncorrelated with corresponding real manual ratings on uniqueness or stylistic\nmatching.\nI also wonder if one needs to evaluate semantic information commonly expressed\nby a specified rapper as well, other than only caring about rhythm.\n\nMeanwhile, I understand the motivation for this study is the lack of *sound*\nevaluation methodology.\nHowever, I still find one statement particularly weird:\n``our methodology produces a continuous numeric score for the whole verse,\nenabling better comparison.''\nIs enabling comparisons really more important than making slightly vague but\nmore reliable, more convincing judgements?\n\nMinor issue:\nIncorrect quotation marks in Line 389"
  },
  {
    "people": [
      "kanye west"
    ],
    "review": "This paper proposes to present a more comprehensive evaluation methodology for\nthe assessment of automatically generated rap lyrics (as being similar to a\ntarget artist).  While the assessment of the generation of creative work is\nvery challenging and of great interest to the community, this effort falls\nshort of its claims of a comprehensive solution to this problem.\n\nAll assessment of this nature ultimately falls to a subjective measure -- can\nthe generated sample convince an expert that the generated sample was produced\nby the true artist rather than an automated preocess?  This is essentially a\nmore specific version of a Turing Test.   The effort to automate some parts of\nthe evaluation to aid in optimization and to understand how humans assess\nartistic similarity is valuable.  However, the specific findings reported in\nthis work do not encourage a belief that these have been reliably identified.\n\nSpecifically -- Consider the central question: Was a sample generated by a\ntarget artist?        The human annotators who were asked this were not able to\nconsistently respond to this question.        This means either 1) the annotators did\nnot have sufficient expertise to perform the task, or 2) the task was too\nchallenging, or some combination of the two.  \n\nThe proposed automatic measures also failed to show a reliable agreement to\nhuman raters performing the same task.        This dramatically limits their efficacy\nin providing a proxy for human assessment.   The low interannotator agreement\nmay be \"expected\" because the task is subjective, but the idea of decomposing\nthe evaluation into fluency and coherence components is meant to make it more\ntractable, and thereby improve the consistency of rater scores.  A low IAA for\nan evaluation metric is a cause for concern and limits its viability as a\ngeneral purpose tool.  \n\nSpecific questions/comments:\n\n* Why is a line-by-line level evaluation prefered to a verse level analysis. \nSpecifically for \"coherence\", a line by line analysis limits the scope of\ncoherence to consequtive lines.\n\n* Style matching -- This term assumes that these 13 artists each have a\ndistinct style, and always operate in that style. I would argue that some of\nthese artists (kanye west, eminem, jay z, drake, tupac and notorious big) have\nproduced work in multiple styles.  A more accurate term for this might be\n\"artist matching\".\n\n* In Section 4.2 The central automated component of the evaluation is low\ntf*idf with existing verses, and similar rhyme density.  Given the limitations\nof rhyme density -- how well does this work.  Even with the manual intervention\ndescribed?\n\n* In Section 6.2 -- This description should include how many judges were used\nin this study? In how many cases did the judges already know the verse they\nwere judging?  In this case the test will not assess how easy it is to match\nstyle, but rather, the judges recall and rap knowledge."
  },
  {
    "people": [
      "Bahdanau",
      "Chen"
    ],
    "review": "- Strengths:\n\nAuthors generate a dataset of \u201crephrased\u201d captions and are planning to make\nthis dataset publicly available.\n\nThe way authors approached DMC task has an advantage over VQA or caption\ngeneration in terms of metrics. It is easier and more straightforward to\nevaluate problem of choosing the best caption. Authors use accuracy metric.\nWhile for instance caption generation requires metrics like BLUE or Meteor\nwhich are limited in handling semantic similarity.\n\nAuthors propose an interesting approach to \u201crephrasing\u201d, e.g. selecting\ndecoys. They draw decoys form image-caption dataset. E.g. decoys for a single\nimage come from captions for other images. These decoys however are similar to\neach other both in terms of surface (bleu score) and semantics (PV similarity).\nAuthors use lambda factor to decide on the balance between these two components\nof the similarity score. I think it would be interesting to employ these for\nparaphrasing.\n\nAuthors support their motivation for the task with evaluation results. They\nshow that a system trained with the focus on differentiating between similar\ncaptions performs better than a system that is trained to generate captions\nonly. These are, however, showing that system that is tuned for a particular\ntask performs better on this task.\n\n- Weaknesses:\n\n It is not clear why image caption task is not suitable for comprehension task\nand why author\u2019s system is better for this. In order to argue that system can\ncomprehend image and sentence semantics better one should apply learned\nrepresentation, e.g. embeddings. E.g. apply representations learned by\ndifferent systems on the same task for comparison.\n\nMy main worry about the paper is that essentially authors converge to using\nexisting caption generation techniques, e.g. Bahdanau et al., Chen et al.\n\nThey way formula (4) is presented is a bit confusing. From formula it seems\nthat both decoy and true captions are employed for both loss terms. However, as\nit makes sense, authors mention that they do not use decoy for the second term.\nThat would hurt mode performance as model would learn to generate decoys as\nwell. The way it is written in the text is ambiguous, so I would make it more\nclear either in the formula itself or in the text. Otherwise it makes sense for\nthe model to learn to generate only true captions while learning to distinguish\nbetween true caption and a decoy.\n\n- General Discussion:\n\nAuthors formulate a task of Dual Machine Comprehension. They aim to accomplish\nthe task by challenging computer system to solve a problem of choosing between\ntwo very similar captions for a given image. Authors argue that a system that\nis able to solve this problem has to \u201cunderstand\u201d the image and captions\nbeyond just keywords but also capture semantics of captions and their alignment\nwith image semantics.\n\nI think paper need to make more focus on why chosen approach is better than\njust caption generation and why in their opinion caption generation is less\nchallenging for learning image and text representation and their alignment.\n\nFor formula (4). I wonder if in the future it is possible to make model to\nlearn \u201cnot to generate\u201d decoys by adjusting second loss term to include\ndecoys but with a negative sign. Did authors try something similar?"
  },
  {
    "people": [
      "Gurevych",
      "Kiselev",
      "J. Eckle-Kohler",
      "M. Matuschek"
    ],
    "review": "This paper presents a graph-based approach for producing sense-disambiguated\nsynonym sets from a collection of undisambiguated synonym sets.  The authors\nevaluate their approach by inducing these synonym sets from Wiktionary and from\na collection of Russian dictionaries, and then comparing pairwise synonymy\nrelations (using precision, recall, and F1) against WordNet and BabelNet (for\nthe English synonym sets) or RuThes and Yet Another RussNet (for the Russian\nsynonym sets).\n\nThe paper is very well written and structured.              The experiments and\nevaluations\n(or at least the prose parts) are very easy to follow.              The methodology\nis\nsensible and the analysis of the results cogent.  I was happy to observe that\nthe objections I had when reading the paper (such as the mismatch in vocabulary\nbetween the synonym dictionaries and gold standards) ended up being resolved,\nor at least addressed, in the final pages.\n\nThe one thing about the paper that concerns me is that the authors do not seem\nto have properly understood the previous work, which undercuts the stated\nmotivation for this paper.\n\nThe first instance of this misunderstanding is in the paragraph beginning on\nline 064, where OmegaWiki is lumped in with Wiktionary and Wikipedia in a\ndiscussion of resources that are \"not formally structured\" and that contain\n\"undisambiguated synonyms\".  In reality, OmegaWiki is distinguished from the\nother two resources by using a formal structure (a relational database) based\non word senses rather than orthographic forms.              Translations, synonyms,\nand\nother semantic annotations in OmegaWiki are therefore unambiguous.\n\nThe second, and more serious, misunderstanding comes in the three paragraphs\nbeginning on lines 092, 108, and 120.  Here the paper claims that both BabelNet\nand UBY \"rely on English WordNet as a pivot for mapping of existing resources\"\nand criticizes this mapping as being \"error-prone\".  Though it is true that\nBabelNet uses WordNet as a pivot, UBY does not.  UBY is basically a\ngeneral-purpose specification for the representation of lexical-semantic\nresources and of links between them.  It exists independently of any given\nlexical-semantic resource (including WordNet) and of any given alignment\nbetween resources (including ones based on \"similarity of dictionary\ndefinitions\" or \"cross-lingual links\").  Its maintainers have made available\nvarious databases adhering to the UBY spec; these contain a variety of\nlexical-semantic resources which have been aligned with a variety of different\nmethods.  A given UBY database can be *queried* for synsets, but UBY itself\ndoes not *generate* those synsets.  Users are free to produce their own\ndatabases by importing whatever lexical-semantic resources and alignments\nthereof are best suited to their purposes.  The three criticisms of UBY on\nlines 120 to 125 are therefore entirely misplaced.\n\nIn fact, I think at least one of the criticisms is not appropriate even with\nrespect to BabelNet.  The authors claim that Watset may be superior to BabelNet\nbecause BabelNet's mapping and use of machine translation are error-prone.  The\nimplication here is that Watset's method is error-free, or at least\nsignificantly less error-prone.  This is a very grandiose claim that I do not\nbelieve is supported by what the authors ought to have known in advance about\ntheir similarity-based sense linking algorithms and graph clustering\nalgorithms, let alone by the results of their study.  I think this criticism\nought to be moderated.              Also, I think the third criticism (BabelNet's\nreliance\non WordNet as a pivot) somewhat misses the point -- surely the most important\nissue to highlight isn't the fact that the pivot is English, but rather that\nits synsets are already manually sense-annotated.\n\nI think the last paragraph of \u00a71 and the first two paragraphs of \u00a72 should be\nextensively revised. They should focus on the *general* problem of generating\nsynsets by sense-level alignment/translation of LSRs (see Gurevych et al., 2016\nfor a survey), rather than particularly on BabelNet (which uses certain\nparticular methods) and UBY (which doesn't use any particular methods, but can\naggregate the results of existing ones).  It may be helpful to point out\nsomewhere that although alignment/translation methods *can* be used to produce\nsynsets or to enrich existing ones, that's not always an explicit goal of the\nprocess.  Sometimes it's just a serendipitous (if noisy) side-effect of\naligning/translating resources with differing granularities.\n\nFinally, at several points in the paper (lines 153, 433), the \"synsets\" of TWSI\nof JoBimText are criticized for including too many words that are hypernyms,\nco-hypnomyms, etc. instead of synonyms.  But is this problem really unique to\nTWSI and JoBimText?  That is, how often do hypernyms, co-hypernyms, etc. appear\nin the output of Watset?  (We can get only a very vague idea of this from\ncomparing Tables 3 and 5, which analyze only synonym relations.)  If Watset\nreally is better at filtering out words with other semantic relations, then it\nwould be nice to see some quantitative evidence of this.\n\nSome further relatively minor points that should nonetheless be fixed:\n\n* Lines 047 to 049: The sentence about Kiselev et al. (2015) seems rather\nuseless.  Why bother mentioning their analysis if you're not going to tell us\nwhat they found?\n\n* Line 091: It took me a long time to figure out how \"wat\" has any relation to\n\"discover the correct word sense\".  I suppose this is supposed to be a pun on\n\"what\".  Maybe it would have been better to call the approach \"Whatset\"?  Or at\nleast consider rewording the sentence to better explain the pun.\n\n* Figure 2 is practically illegible owing to the microscopic font.  Please\nincrease the text size!\n\n* Similarly, Tables 3, 4, and 5 are too small to read comfortably.  Please use\na larger font.              To save space, consider abbreviating the headers (\"P,\n\"R\",\n\"F1\") and maybe reporting scores in the range 0\u2013100 instead of 0\u20131, which\nwill eliminate a leading 0 from each column.\n\n* Lines 517\u2013522: Wiktionary is a moving target.  To help others replicate or\ncompare against your work, please indicate the date of the Wiktionary database\ndump you used.\n\n* Throughout: The constant switching between Times and Computer Modern is\ndistracting.  The root of this problem is a longstanding design flaw in the ACL\n2017 LaTeX style file, but it's exacerbated by the authors' decision to\noccasionally set numbers in math mode, even in running text.  Please fix this\nby removing\n\n\\usepackage{times}\n\nfrom the preamble and replacing it with either\n\n\\usepackage{newtxtext}\n\\usepackage{newtxmath}\n\nor\n\n\\usepackage{mathptmx}\n\nReferences:\n\nI Gurevych, J. Eckle-Kohler, and M. Matuschek, 2016. Linked Lexical Knowledge\nBases: Foundations and Applications, volume 34 of Synthesis Lectures on Human\nLanguage Technologies, chapter 3: Linking Algorithms, pages 29-44. Morgan &\nClaypool.\n\n----\n\nI have read the author response."
  },
  {
    "people": [
      "Teng",
      "Teng"
    ],
    "review": "Strengths:\n\n- Innovative idea: sentiment through regularization\n- Experiments appear to be done well from a technical point of view\n- Useful in-depth analysis of the model\n\nWeaknesses:\n\n- Very close to distant supervision\n- Mostly poorly informed baselines\n\nGeneral Discussion:\n\nThis paper presents an extension of the vanilla LSTM model that\nincorporates sentiment information through regularization.  The\nintroduction presents the key claims of the paper: Previous CNN\napproaches are bad when no phrase-level supervision is present.\nPhrase-level annotation is expensive. The contribution of this paper is\ninstead a \"simple model\" using other linguistic resources.\n\nThe related work section provides a good review of sentiment\nliterature. However, there is no mention of previous attempts at\nlinguistic regularization (e.g., [YOG14]).\n\nThe explanation of the regularizers in section 4 is rather lengthy and\nrepetitive. The listing on p. 3 could very well be merged with the\nrespective subsection 4.1-4.4. Notation in this section is inconsistent\nand generally hard to follow. Most notably, p is sometimes used with a\nsubscript and sometimes with a superscript.  The parameter \\beta is\nnever explicitly mentioned in the text. It is not entirely clear to me\nwhat constitutes a \"position\" t in the terminology of the paper. t is a\nparameter to the LSTM output, so it seems to be the index of a\nsentence. Thus, t-1 is the preceding sentence, and p_t is the prediction\nfor this sentence. However, the description of the regularizers talks\nabout preceding words, not sentences, but still uses. My assumption here\nis that p_t is actually overloaded and may either mean the sentiment of\na sentence or a word. However, this should be made clearer in the text.\n\nOne dangerous issue in this paper is that the authors tread a fine line\nbetween regularization and distant supervision in their work. The\nproblem here is that there are many other ways to integrate lexical\ninformation from about polarity, negation information, etc. into a model\n(e.g., by putting the information into the features). The authors\ncompare against a re-run or re-implementation of Teng et al.'s NSCL\nmodel. Here, it would be important to know whether the authors used the\nsame lexicons as in their own work. If this is not the case, the\ncomparison is not fair. Also, I do not understand why the authors cannot\nrun NSCL on the MR dataset when they have access to an implementation of\nthe model. Would this not just be a matter of swapping the datasets? The\nremaining baselines do not appear to be using lexical information, which\nmakes them rather poor. I would very much like to see a vanilla LSTM run\nwhere lexical information is simply appended to the word vectors.\n\nThe authors end the paper with some helpful analysis of the\nmodels. These experiments show that the model indeed learns\nintensification and negation to some extent. In these experiments, it\nwould be interesting to know how the model behaves with\nout-of-vocabulary words (with respect to the lexicons). Does the model\nlearn beyond memorization, and does generalization happen for words that\nthe model has not seen in training? Minor remark here: the figures and\ntables are too small to be read in print.\n\nThe paper is mostly well-written apart from the points noted above.  It\ncould benefit from some proofreading as there are some grammatical\nerrors and typos left. In particular, the beginning of the abstract is\nhard to read.\n\nOverall, the paper pursues a reasonable line of research. The largest\npotential issue I see is a somewhat shaky comparison to related\nwork. This could be fixed by including some stronger baselines in the\nfinal model. For me, it would be crucial to establish whether\ncomparability is given in the experiments, and I hope that the authors\ncan shed some light on this in their response.\n\n[YOG14] http://www.aclweb.org/anthology/P14-1074\n\n--------------\n\nUpdate after author response\n\nThank you for clarifying the concerns about the experimental setup. \n\nNSCL: I do now believe that the comparison is with Teng et al. is fair.\n\nLSTM: Good to know that you did this. However, this is a crucial part of the\npaper. As it stands, the baselines are weak. Marginal improvement is still too\nvague, better would be an open comparison including a significance test.\n\nOOV: I understand how the model is defined, but what is the effect on OOV\nwords? This would make for a much more interesting additional experiment than\nthe current regularization experiments."
  },
  {
    "people": [
      "Teng"
    ],
    "review": "- Strengths:\nThis paper proposes a nice way to combine the neural model (LSTM) with\nlinguistic knowledge (sentiment lexicon, negation and intensity). The method is\nsimple yet effective. It achieves the state-of-the-art performance on Movie\nReview dataset and is competitive against the best models on SST dataset.    \n\n- Weaknesses:\nSimilar idea has also been used in (Teng et al., 2016). Though this work is \nmore elegant in the framework design and mathematical representation, the\nexperimental comparison with (Teng et al., 2016) is not as convincing as the\ncomparisons with the rest methods. The authors only reported the\nre-implementation results on the sentence level experiment of SST and did not\nreport their own phrase-level results.\n\nSome details are not well explained, see discussions below.\n\n- General Discussion:\n\nThe reviewer has the following questions/suggestions about this work,\n\n1. Since the SST dataset has phrase-level annotations, it is better to show the\nstatistics of the times that negation or intensity words actually take effect.\nFor example, how many times the word \"nothing\" appears and how many times it\nchanges the polarity of the context.\n\n2. In section 4.5, the bi-LSTM is used for the regularizers. Is bi-LSTM used to\npredict the sentiment label?\n\n3. The authors claimed that \"we only use the sentence-level annotation since\none of\nour goals is to avoid expensive phrase-level annotation\". However, the reviewer\nstill suggest to add the results. Please report them in the rebuttal phase if\npossible.\n\n4. \"s_c is a parameter to be optimized but could also be set fixed with prior\nknowledge.\"  The reviewer didn't find the specific definition of s_c in the\nexperiment section, is it learned or set fixed?  What is the learned or fixed\nvalue?\n\n5. In section 5.4 and 5.5, it is suggested to conduct an additional experiment\nwith part of the SST dataset where only phrases with negation/intensity words\nare included. Report the results on this sub-dataset with and without the\ncorresponding regularizer can be more convincing."
  },
  {
    "people": [
      "Dozat",
      "Chen",
      "Chen",
      "Martinez",
      "Dozat",
      "Manning"
    ],
    "review": "The paper describes a deep-learning-based model for parsing the creole\nSingaporean English to Universal Dependencies. They implement a parser based on\nthe model by Dozat and Manning (2016) and add neural stacking (Chen et al.,\n2016) to it. They train an English model and then use some of the hidden\nrepresentations of the English model as input to their Singlish parser. This\nallows them to make use of the much larger English training set along with a\nsmall Singlish treebank, which they annotate. They show that their approach\n(LAS 76.57) works better than just using an English parser (LAS 65.6) or\ntraining a parser on their small Singlish data set (LAS 64.01). They also\nanalyze for which\ncommon constructions, their approach improves parsing quality. \n\nThey also describe and evaluate a stacked POS model based on Chen et al.\n(2016), they discuss how common constructions should be analyzed in the UD\nframework, and they provide an annotated treebank of 1,200 sentences. 100 of\nthem were annotated by two people and their inter-annotator agreement was 85.3\nUAS and 75.7 LAS.\n\n- Strengths:\n\n - They obtain good results and their experimental setup appears to be solid.\n\n - They perform many careful analyses and explore the influence on many\nparameters of their model.\n\n - They provide a small Singlish treebank annotated according to the Universal\nDependencies v1.4 guidelines.\n\n - They propose very sound guidelines on how to analyze common Singlish\nconstructions in UD.\n\n - Their method is linguistically informed and they nicely exploit similarity\nbetween standard English and the creole Singaporean English.\n\n - The paper presents methods for a low-resource language.\n\n - They are not just applying an existing English method to another language\nbut instead present a method that can be potentially used for other closely\nrelated language pairs.\n\n - They use a well-motivated method for selecting the sentences to include in\ntheir treebank.\n\n - The paper is very well written and easy to read.\n\n- Weaknesses:\n\n - The annotation quality seems to be rather poor. They performed double\nannotation of 100 sentences and their inter-annotator agreement is just 75.72%\nin terms of LAS. This makes it hard to assess how reliable the estimate of the\nLAS of their model is, and the LAS of their model is in fact slightly higher\nthan the inter-annotator agreement. \n\nUPDATE: Their rebuttal convincingly argued that the second annotator who just\nannotated the 100 examples to compute the IAA didn't follow the annotation\nguidelines for several common constructions. Once the second annotator fixed\nthese issues, the IAA was reasonable, so I no longer consider this a real\nissue.\n\n- General Discussion:\n\nI am a bit concerned about the apparently rather poor annotation quality of the\ndata and how this might influence the results, but overall, I liked the paper\na lot and I think this would be a good contribution to the conference.\n\n- Questions for the authors:\n\n - Who annotated the sentences? You just mention that 100 sentences were\nannotated by one of the authors to compute inter=annotator agreement but you\ndon't mention who annotated all the sentences.\n\n - Why was the inter-annotator agreement so low? In which cases was there\ndisagreement? Did you subsequently discuss and fix the sentences for which\nthere was disagreement?\n\n - Table A2: There seem to be a lot of discourse relations (almost as many as\ndobj relations) in your treebank. Is this just an artifact of the colloquial\nlanguage or did you use \"discourse\" for things that are not considered\n\"discourse\" in other languages in UD?\n\n - Table A3: Are all of these discourse particles or discourse + imported\nvocab? If the latter, perhaps put them in separate tables, and glosses would be\nhelpful.\n\n- Low-level comments:\n\n - It would have been interesting if you had compared your approach to the one\nby Martinez et al. (2017, https://arxiv.org/pdf/1701.03163.pdf). Perhaps you\nshould mention this paper in the reference section.\n\n - You use the word \"grammar\" in a slightly strange way. I think replacing\n\"grammar\" with syntactic constructions would make it clearer what you try to\nconvey. (e.g., line 90)\n\n - Line 291: I don't think this can be regarded as a variant of\nit-extraposition. But I agree with the analysis in Figure 2, so perhaps just\nget rid of this sentence.\n\n - Line 152: I think the model by Dozat and Manning (2016) is no longer\nstate-of-the art, so perhaps just replace it with \"very high performing model\"\nor something like that.\n\n - It would be helpful if you provided glosses in Figure 2."
  },
  {
    "people": [
      "Silveira"
    ],
    "review": "The authors construct a new dataset of 1200 Singaporean English (Singlish)\nsentences annotated with Universal Dependencies. They show that they can\nimprove the performance of a POS tagger and a dependency parser on the Singlish\ncorpus by integrating English syntactic knowledge via a neural stacking model.\n\n- Strengths:\nSinglish is a low-resource language. The NLP community needs more data for low\nresource languages, and the dataset accompanying this paper is a useful\ncontribution. There is also relatively little NLP research on creoles, and the\npotential of using transfer-learning to analyze creoles, and this paper makes a\nnice contribution in that area.\n\nThe experimental setup used by the authors is clear. They provide convincing\nevidence that incorporating knowledge from an English-trained parser into a\nSinglish parser outperforms both an English-only parser and a Singlish-only\nparser on the Singlish data. They also provide a good overview of the relevant\ndifferences between English and Singlish for the purposes of syntactic parser\nand a useful analysis of how different parsing models handle these\nSinglish-specific constructions.\n\n- Weaknesses:\n\nThere are three main issues I see with this paper:\n*  There is insufficient comparison to the UD annotation of non-English\nlanguages. Many of the constructions they bring up as specific to Singlish are\nalso present in other UD languages, and the annotations should ideally be\nconsistent between Singlish and these languages.\n*  I'd like to see an analysis on the impact of training data size. A central\nclaim of this paper is that using English data can improve performance on a\nlow-resource language like Singlish. How much more Singlish data would be\nneeded before the English data became unnecessary?\n*  What happens if you train a single POS/dep parsing model on the concatenated\nUD Web and Singlish datasets? This is much simpler than incorporating neural\nstacking. The case for neural stacking is stronger if it can outperform this\nbaseline.\n\n- General Discussion:\nLine 073: \u201cPOS taggers and dependency parsers perform poorly on such Singlish\ntexts based on our observations\u201d - be more clear that you will quantify this\nlater. As such, it seems a bit hand-wavy.\n\nLine 169: Comparison to neural network models for multi-lingual parsing. As far\nas I can tell, you don't directly try the approach of mapping Singlish and\nEnglish word embeddings into the same embedding space.\n\nLine 212: Introduction of UD Eng. At this point, it is appropriate to point out\nthat the Singlish data is also web data, so the domain matches UD Eng.\n\nLine 245: \u201cAll borrowed words are annotated according to their original\nmeanings\u201d. Does this mean they have the same POS as in  the language from\nwhich they were borrowed? Or the POS of their usage in Singlish?\n\nFigure 2: Standard English glosses would be very useful in understanding the\nconstructions and checking the correctness of the UD relations used.\n\nLine 280: Topic prominence: You should compare with the \u201cdislocated\u201d label\nin UD. From the UD paper: \u201cThe dislocated relation captures preposed (topics)\nand postposed elements\u201d. The syntax you are describing sounds similar to a\ntopic-comment-style syntax; if it is different, then you should make it clear\nhow.\n\nLine 294: \u201cSecond, noun phrases used to modify the predicate with the\npresence of a preposition is regarded as a \u201cnsubj\u201d (nominal subject).\u201d\nHere, I need a gloss to determine if this analysis makes sense. If the phrase\nis really being used to modify the predicate, then this should not be nsubj. UD\nmakes a distinction between core arguments (nsubj, dobj, etc) and modifiers. If\nthis is a case of modification, then you should use one of the modification\nrelations, not a core argument relation. Should clarify the language here.\n\nLine 308: \u201cIn UD-Eng standards, predicative \u201cbe\u201d is the only verb used as\na copula, which often depends on its complement to avoid copular head.\u201d This\nis an explicit decision made in UD, to increase parallelism with non-copular\nlanguages (e.g., Singlish). You should call this out. I think the rest of the\ndiscussion of copula handling is not necessary.\n\nLine 322: \u201cNP deletion: Noun-phrase (NP) deletion often results in null\nsubjects or objects.\u201d This is common in other languages (zero-anaphora in\ne.g. Spanish, Italian, Russian, Japanese\u2026 )Would be good to point this out,\nand also point to how this is dealt with in UD in those languages (I believe\nthe same way you handle it).\n\nLing 330: Subj/verb inversion is common in interrogatives in other languages\n(\u201cFue Marta al supermercado/Did Marta go to the supermarket?\u201d). Tag\nquestions are present in English (though perhaps are not as frequent). You\nshould make sure that your analysis is consistent with these languages.\n\nSec 3.3 Data Selection and Annotation:\nThe way you chose the Singlish sentences, of course an English parser will do\npoorly (they are chosen to be disimilar to sentences an English parser has seen\nbefore). But do you have a sense of how a standard English parser does overall\non Singlish, if it is not filtered this way? How common are sentences with\nout-of-vocabulary terms or the constructions you discussed in 3.2?\n\nA language will not necessarily capture unusual sentence structure,\nparticularly around long-distance dependencies. Did you investigate whether\nthis method did a good job of capturing sentences with the grammatical\ndifferences to English you discussed in Section 3.2?\n\nLine 415: \u201cthe inter-annotator agreement has an unlabeled attachment score\n(UAS) of 85.30% and a labeled attachment score (LAS) of 75.72%.\u201d\n*  What\u2019s the agreement on POS tags? Is this integrated with LAS?\n*  Note that in Silveira et al 2014, which produced UD-Eng, they measured 94%\ninter-annotator agreement on a per-token basis. Why the discrepancy?\n\nPOS tagging and dep parsing sections:\nFor both POS-tagging and dep parsing, I\u2019d like to see some analysis on the\neffect of training set size. E.g., how much more Singlish data would be needed\nto train a POS tagger/dep parser entirely on Singlish and get the same accuracy\nas the stacked model?\n\nWhat happens if you just concatenate the datasets? E.g., train a model on a\nhybrid dataset of EN and Singlish, and see what the result is?\n\nLine 681: typo: \u201cpre-rained\u201d should be \u201cpre-trained\u201d\n\n742 \u201cThe neural stacking model leads to the biggest improvement over nearly\nall categories except for a slightly lower yet competitive performance on \u201cNP\nDeletion\u201d cases\u201d --- seems that the English data strongly biases the parser\nto expect an explicit subj/obj. you could try deleting subj/obj from some\nEnglish sentences to improve performance on this construction."
  },
  {
    "people": [
      "Rabin"
    ],
    "review": "The authors present a new formula for assessing readability of Vietnamese\ntexts. The formula is developed based on a multiple regression analysis with\nthree features. Furthermore, the authors have developed and annotated a new\ntext corpus with three readability classes (easy, middle, hard).\n\nResearch on languages other than English is interesting and important,\nespecially when it comes to low-resource languages. Therefore, the corpus might\nbe a nice additional resource for research (but it seems that the authors will\nnot publish it - is that right?). However, I don't think the paper is\nconvincing in its current shape or will influence future research. Here are my\nreasons:\n\n- The authors provide no reasons why there is a need for delevoping a new\nformula for readability assessments, given that there already exist two\nformulas for Vietnamese with almost the same features. What are the\ndisadvantages of those formulas and why is the new formula presented in this\npaper better?\n\n- In general, the experimental section lacks comparisons with previous work and\nanalysis of results. The authors claim that the accuracy of their formula (81%\non their corpus) is \"good and can be applied in practice\". What would be the\naccuracy of other formulas that already exist and what are the pros and cons of\nthose existing formulas compared to the new one?\n\n- As mentioned before, an analysis of results is missing, e.g. which word /\nsentence lengths / number of difficult words are considered as easy/middle/hard\nby their model?\n\n- A few examples how their formula could be applied in a practical application\nwould be nice as well.\n\n- The related work section is rather a \"background\" section since it only\npresents previously published formulas. What I'm missing is a more general\ndiscussion of related work. There are some papers that might be interesting for\nthat, e.g., DuBay 2004: \"The principles of readability\", or Rabin 1988:\n\"Determining difficulty levels of text written in languages other than English\"\n\n- Since Vietnamese is syllable-based and not word-based, I'm wondering how the\nauthors get \"words\" in their study. Do they use a particular approach for\nmerging syllables? And if yes, which approach do they use and what's the\naccuracy of the approach?\n\n- All in all, the content of the paper (experiments, comparisons, analysis,\ndiscussion, related work) is not enough for a long paper.\n\nAdditional remarks:\n\n- The language needs improvements\n\n- Equations: The usage of parentheses and multiplying operators is inconsistent\n\n- Related works section: The usage of capitalized first letters is inconsistent"
  },
  {
    "people": [
      "Salton"
    ],
    "review": "- Strengths:\n\nThis paper proposes to apply NLP to speech transcripts (narratives and\ndescriptions) in order to identify patients with MCI (mild cognitive\nimpairment, ICD-10 code F06.7). The authors claim that they were able to\ndistinguish between healthy control participants and patients with MCI (lines\n141-144). However in the conclusion, lines 781-785, they say that \u201c\u2026\naccuracy ranging from 60% to 85% \u2026. means that it is not easy to distinguish\nbetween healthy subjects and those with cognitive impairments\u201d. So the paper\nbeginning is more optimistic than the conclusion but anyway the message is\nencouraging and the reader becomes curious to see more details about what has\nbeen actually done.\n\nThe corpus submitted in the dataset is constructed for 20 healthy patients and\n20 control participants only (20+20), and it is non-understandable for people\nwho do not speak Portuguese. It would be good to incorporate more technological\ndetails in the article and probably to include at least one example of a short\ntranscript that is translated to English, and eventually a (part of a) sample\nnetwork with embeddings for this transcript.\n\n- Weaknesses:\n\nThe paper starts with a detailed introduction and review of relevant work. Some\nof the cited references are more or less NLP background so they can be omitted\ne.g. (Salton 1989) in section 4.2.3. Other references are not directly related\nto the topic e.g. \u201csentiment classification\u201d and \u201cpedestrian detection in\nimages\u201d, lines 652-654, and they can be omitted too. In general lines\n608-621, section 4.2.3 can be shortened as well etc. etc. The suggestion is to\ncompress the first 5 pages, focusing the review strictly on the paper topic,\nand consider the technological innovation in more detail, incl. samples of\nEnglish translations of the ABCD and/or Cindarela narratives.\n\nThe relatively short narratives in Portuguese esp. in ABCD dataset open the\nquestion how the similarities between words have been found, in order to\nconstruct word embeddings. In lines 272-289 the authors explain that they\ngenerate word-level networks from continuous word representations. What is the\nsource for learning the continuous word representations; are these the datasets\nABCD+Cinderella only, or external corpora were used? In lines 513-525 it is\nwritten that sub-word level (n-grams) networks were used to generate word\nembeddings. Again, what is the source for the training? Are we sure that the\ntwo kinds of networks together provide better accuracy? And what are the\n\u201cout-of-vocabulary words\u201d (line 516), from where they come?\n\n- General Discussion:\n\nIt is important to study how NLP can help to discover cognitive impairments;\nfrom this perspective the paper is interesting. Another interesting aspect is\nthat it deals with NLP for Portuguese, and it is important to explain how one\ncomputes embeddings for a language with relatively fewer resources (compared to\nEnglish). \n\nThe text needs revision: shortening sections 1-3, compressing 4.1 and adding\nmore explanations about the experiments. Some clarification about the NURC/SP\nN. 338 EF and 331 D2 transcription norms can be given.\n\nTechnical comments:\n\nLine 029: \u2018\u2026 as it a lightweight \u2026\u2019 -> shouldn\u2019t this be \u2018\u2026 as in\na lightweight \u2026\u2019\n\nLine 188: PLN -> NLP\n\nLine 264: \u2018out of cookie out of the cookie\u2019 \u2013 some words are repeated\ntwice \n\nTable 3, row 2, column 3: 72,0 -> 72.0\n\nLines 995-996: the DOI number is the same as the one at lines 1001-1002; the\nlink behind the title at lines 992-993 points to the next paper in the list"
  },
  {
    "people": [
      "Juan",
      "Lourdes Araujo",
      "Andres Duque Fernandez",
      "Le",
      "Tho Thi Ngoc",
      "Minh Le Nguyen",
      "Akira Shimazu"
    ],
    "review": "- Strengths:\nNicely written and understandable.\nClearly organized. Targeted answering of research questions, based on \ndifferent experiments.\n\n- Weaknesses:\nMinimal novelty. The \"first sentence\" heuristic has been in the summarization\nliterature for many years. This work essentially applies this heuristic\n(evolved) in the keyword extraction setting. This is NOT to say that the work\nis trivial: it is just not really novel.\n\nLack of state-of-the-art/very recent methods. The experiment on the system\nevaluation vs state-of-the-art systems simply uses strong baselines. Even\nthough the experiment answers the question \"does it perform better than\nbaselines?\", I am not confident it illustrates that the system performs better\nthan the current state-of-the-art. This somewhat reduces the value of the\npaper.\n\n- General Discussion:\nOverall the paper is good and I propose that it be published and presented. \n\nOn the other hand, I would propose that the authors position themselves (and\nthe system performance) with respect to:\nMartinez\u2010Romo, Juan, Lourdes Araujo, and Andres Duque Fernandez. \"SemGraph:\nExtracting keyphrases following a novel semantic graph\u2010based approach.\"\nJournal of the Association for Information Science and Technology 67.1 (2016):\n71-82.\n(with which the work holds remarkable resemblance in some points)\n\nLe, Tho Thi Ngoc, Minh Le Nguyen, and Akira Shimazu. \"Unsupervised Keyphrase\nExtraction: Introducing New Kinds of Words to Keyphrases.\" Australasian Joint\nConference on Artificial Intelligence. Springer International Publishing, 2016."
  },
  {
    "people": [
      "Zhou",
      "Xu"
    ],
    "review": "- General Discussion:\n\nThis paper extends Zhou and Xu's ACL 2015 approach to semantic role labeling\nbased on deep BiLSTMs. In addition to applying recent best practice techniques,\nleading to further quantitative improvements, the authors provide an insightful\nqualitative analysis of their results. The paper is well written and has a\nclear structure. The authors provide a comprehensive overview of related work\nand compare results to a representative set of other SRL models that hace been\napplied on the same data sets.\n\nI found the paper to be interesting and convincing. It is a welcome research\ncontribution that not only shows that NNs work well, but also analyzes merits\nand shortcomings of an end-to-end learning approach.\n\n- Strengths:\n\nStrong model, insightful discussion/error analysis.\n\n- Weaknesses:\n\nLittle to no insights regarding the SRL task itself."
  },
  {
    "people": [
      "Zhou",
      "Xu"
    ],
    "review": "This paper presents a new state-of-the-art deep learning model for semantic\nrole labeling (SRL) that is a natural extension of the previous\nstate-of-the-art system (Zhou and Xu, 2015) with recent best practices for\ninitialization and regularization in the deep learning literature.\nThe model gives a 10% relative error reduction which is a big gain on this\ntask. The paper also gives in-depth empirical analyses to reveal the strengths\nand the remaining issues, that give a quite valuable information to the\nresearchers in this field. \n\nEven though I understand that the improvement of 3 point in F1 measure is a\nquite meaningful result from the engineering point of view, I think the main\ncontribution of the paper is on the extensive analysis in the experiment\nsection and a further in-depth investigation on analysis section. The detailed\nanalyses shown in Section 4 are performed in a quite reasonable way and give\nboth comparable results in SRL literature and novel information such as\nrelation between accuracies in syntactic parsing and SRL. This type of analysis\nhad often been omitted in recent papers. However, it is definitely important\nfor further improvement.\n\nThe paper is well-written and well-structured. \nI really enjoyed the paper and would like to see it accepted."
  },
  {
    "people": [
      "Wen",
      "Wen",
      "Perez-Beltrachini\net al",
      "Walker",
      "Marilyn Walker",
      "Amanda Stent",
      "Fran\u00e7ois Mairesse",
      "Rashmi Prasad",
      "Amy Isard",
      "Perez-Beltrachini",
      "Perez-Beltrachini",
      "Wen",
      "Wen",
      "Wen",
      "Wen"
    ],
    "review": "- Strengths:\n\nThis paper presents a step in the direction of developing more challenging\ncorpora for training sentence planners in data-to-text NLG, which is an\nimportant and timely direction. \n\n- Weaknesses:\n\nIt is unclear whether the work reported in this paper represents a substantial\nadvance over Perez-Beltrachini et al.'s (2016) method for selecting content. \nThe authors do not directly compare the present paper to that one. It appears\nthat the main novelty of this paper is the additional analysis, which is\nhowever rather superficial.\n\nIt is good that the authors report a comparison of how an NNLG baseline fares\non this corpus in comparison to that of Wen et al. (2016).  However, the\nBLEU scores in Wen et al.'s paper appear to be much much higher, suggesting\nthat this NNLG baseline is not sufficient for an informative comparison.\n\n- General Discussion:\n\nThe authors need to more clearly articulate why this paper should count as a\nsubstantial advance over what has been published already by Perez-Beltrachini\net al, and why the NNLG baseline should be taken seriously.  In contrast to\nLREC, it is not so common for ACL to publish a main session paper on a corpus\ndevelopment methodology in the absence of some new results of a system making\nuse of the corpus.\n\nThe paper would also be stronger if it included an analysis of the syntactic\nconstructions in the two corpora, thereby more directly bolstering the case\nthat the new corpus is more complex.  The exact details of how the number of\ndifferent path shapes are determined should also be included, and ideally\nassociated with the syntactic constructions.\n\nFinally, the authors should note the limitation that their method does nothing\nto include richer discourse relations such as Contrast, Consequence,\nBackground, etc., which have long been central to NLG. In this respect, the\ncorpora described by Walker et al. JAIR-2007 and Isard LREC-2016 are more\ninteresting and should be discussed in comparison to the method here.\n\nReferences\n\nMarilyn Walker, Amanda Stent, Fran\u00e7ois Mairesse, and\nRashmi Prasad. 2007. Individual and domain adaptation\nin sentence planning for dialogue. Journal of\nArtificial Intelligence Research (JAIR), 30:413\u2013456.\n\nAmy Isard, 2016. \u201cThe Methodius Corpus of Rhetorical Discourse\nStructures and Generated Texts\u201d , Proceedings of the Tenth Conference\non Language Resources and Evaluation (LREC 2016), Portoro\u017e, Slovenia,\nMay 2016.\n\n---\nAddendum following author response:\n\nThank you for the informative response.  As the response offers crucial\nclarifications, I have raised my overall rating.  Re the comparison to\nPerez-Beltrachini et al.: While this is perhaps more important to the PC than\nto the eventual readers of the paper, it still seems to this reviewer that the\nadvance over this paper could've been made much clearer.  While it is true that\nPerez-Beltrachini et al. \"just\" cover content selection, this is the key to how\nthis dataset differs from that of Wen et al.  There doesn't really seem to be\nmuch to the \"complete methodology\" of constructing the data-to-text dataset\nbeyond obvious crowd-sourcing steps; to the extent these steps are innovative\nor especially crucial, this should be highlighted.  Here it is interesting that\n8.7% of the crowd-sourced texts were rejected during the verification step;\nrelated to Reviewer 1's concerns, it would be interesting to see some examples\nof what was rejected, and to what extent this indicates higher-quality texts\nthan those in Wen et al.'s dataset.  Beyond that, the main point is really that\ncollecting the crowd-sourced texts makes it possible to make the comparisons\nwith the Wen et al. corpus at both the data and text levels (which this\nreviewer can see is crucial to the whole picture).\n\nRe the NNLG baseline, the issue is that the relative difference between the\nperformance of this baseline on the two corpora could disappear if Wen et al.'s\nsubstantially higher-scoring method were employed.  The assumption that this\nrelative difference would remain even with fancier methods should be made\nexplicit, e.g. by acknowledging the issue in a footnote.  Even with this\nlimitation, the comparison does still strike this reviewer as a useful\ncomponent of the overall comparison between the datasets.\n\nRe whether a paper about dataset creation should be able to get into ACL\nwithout system results:  though this indeed not unprecedented, the key issue is\nperhaps how novel and important the dataset is likely to be, and here this\nreviewer acknowledges the importance of the dataset in comparison to existing\nones (even if the key advance is in the already published content selection\nwork).\n\nFinally, this reviewer concurs with Reviewer 1 about the need to clarify the\nrole of domain dependence and what it means to be \"wide coverage\" in the final\nversion of the paper, if accepted."
  },
  {
    "people": [
      "Duboue",
      "Kutlak",
      "Wen"
    ],
    "review": "- Strengths:\n* Potentially valuable resource\n* Paper makes some good points\n\n- Weaknesses:\n* Awareness of related work (see below)\n* Is what the authors are trying to do (domain-independent microplanning) even\npossible (see below)\n* Are the crowdsourced texts appropriate (see below)\n\n- General Discussion:\nThis is an interesting paper which presents a potentially valuable resource,\nand I in many ways I am sympathetic to it.  However, I have some high-level\nconcerns, which are not addressed in the paper.  Perhaps the authors can\naddress these in their response.\n\n(1) I was a bit surprised by the constant reference and comparison to Wen 2016,\nwhich is a fairly obscure paper I have not previously heard of.  It would be\nbetter if the authors justified their work by comparison to well-known corpora,\nsuch as the ones they list in Section 2. Also, there are many other NLG\nprojects that looked at microplanning issue when verbalising DBPedia, indeed\nthere was a workshop in 2016 with many papers on NLG and DBPedia\n(https://webnlg2016.sciencesconf.org/  and\nhttp://aclweb.org/anthology/W/W16/#3500); see also previous work by Duboue and\nKutlak.  I would like to see less of a fixation on Wen (2016), and more\nawareness of other work on NLG and DBPedia.\n\n(2) Microplanning tends to be very domain/genre dependent.  For example,\npronouns are used much more often in novels than in aircraft maintenance\nmanuals.   This is why so much work has focused on domain-dependent resources. \n  So there are some real questions about whether it is possible even in theory\nto train a \"wide-coverage microplanner\".  The authors do not discuss this at\nall; they need to show they are aware of this concern.\n\n(3) I would be concerned about the quality of the texts obtained from\ncrowdsourcing.              A lot of people dont write very well, so it is not at all\nclear\nto me that gathering example texts from random crowdsourcers is going to\nproduce a good corpus for training microplanners.  Remember that the ultimate\ngoal of microplanning is to produce texts that are easy to *read*.  Imitating\nhuman writers (which is what this paper does, along with most learning\napproaches to microplanning) makes sense if we are confident that the human\nwriters have produced well-written easy-to-read texts.              Which is a\nreasonable\nassumption if the writers are professional journalists (for example), but a\nvery dubious one if the writers are random crowdsourcers.\n\nFrom a presentational perspective, the authors should ensure that all text in\ntheir paper meets the ACL font size criteria.  Some of the text in Fig 1 and\n(especially) Fig 2 is tiny and very difficult to read; this text should be the\nsame font size as the text in the body of the paper.\n\nI will initially rate this paper as borderline.  I look forward to seeing the\nauthor's response, and will adjust my rating accordingly."
  },
  {
    "people": [
      "Peter Ackoyd"
    ],
    "review": "- Strengths:\nThe paper empirically verifies that using external knowledge is a benefit.\n\n- Weaknesses:\nReal world NLP applications should utilize external knowledge for making better\npredictions. The authors propose Rare Entity prediction task to demonstrate\nthis is the case. However, the motivation of the task is not fully justified.\nWhy is this task important? How would real world NLP applications benefit from\nthis task? The paper lacks a convincing argument for proposing a new task. For\ncurrent reading comprehension task, the evidence for a correct answer can be\nfound in a given text, thus we are interested in learning a model of the world\n(i.e causality for example), or a basic reasoning model. Comparing to reading\ncomprehension, rare entity prediction is rather unrealistic as humans are\nterrible with remembering name. The authors mentioned that the task is\ndifficult due to the large number of rare entities, however challenging tasks\nwith the same or even more difficult level exist, such as predicting correct\nmorphological form of a word in morphologically rich languages. Such tasks have\nobvious applications in machine translation for example.\n\n- General Discussion:\nIt would be helpful if the authors characterize the dataset in more details.\nFrom figure 1 and table 4, it seems to me that overlapping entities is an\nimportant feature. There is noway i can predict the **blank** in figure 1 if I\ndon't see the word London in Peter Ackoyd description. That's being said,\nbefore brutalizing neural networks, it is essential to understand the\ncharacteristic of the data and the cognitive process that searches for the\nright answer.\n\nGiven the lack of characteristic of the dataset, I find that the baselines are\ninappropriate. First of all, the CONTENC is a natural choice at the first sigh.\nHowever as the authors mentioned that candidate entities are rare, the\nembeddings of those entities are unrealizable. As a consequence, it is expected\nthat CONTENC doesn't work well. Would it is fairer if the embeddings are\ninitialized from pre-trained vectors on massive dataset? One would expect some\nsort of similarity between Larnaca and Cyprus in the embedding space and\nCONTENC would make a correct prediction in Table 4. What would be the\nperformance of TF-IDF+COS and AVGEMB+COS if only entities are used to compute\nthose vectors?\n\nFrom modeling perspective, I appreciate that the authors chose a sigmoid\npredictor that output a numerical score between (0,1). This would help avoiding\nnormalization over the list of candidates, which are rare and is difficult to\nlearn reliable weights for those. However, a sidestep technique does exist,\nsuch as Pointer Network. A representation h_i for C_i (*blank* included) can be\ncomputed by an LSTM or BiLSTM, then Pointer Network would give a probabilistic\ninterpretation p(e_k|C_i) \\propto exp(dot(d_{e_k}, h_i)). In my opinion,\nPointer Network would be an appropriate baseline. Another related note: Does\nthe unbalanced set of negative/positive labels affect the training? During\ntraining, the models make 1 positive prediction while number of negative\npredictions is at least 4 times higher?\n\nWhile I find the task of Rare Entity prediction is unrealistic, having the\ndataset, it would be more interesting to learn about the reasoning process that\nleads to the right answer such as which set of words the model attends to when\nmaking prediction."
  },
  {
    "people": [
      "Bollegala"
    ],
    "review": "This paper considers the problem of KB completion and proposes ITransF for this\npurpose. Unlike STransE that assigns each relation an independent matrix, this\npaper proposes to share the parameters between different relations. A model is\nproposed where a tensor D is constructed that contains various relational\nmatrices as its slices and a selectional vector \\alpha is used to select a\nsubset of relevant relational matrix for composing a particular semantic\nrelation. The paper then discuss a technique to make \\alpha sparse.\nExperimental results on two standard benchmark datasets shows the superiority\nof ITransF over prior proposals.\n\nThe paper is overall well written and the experimental results are good.\nHowever, I have several concerns regarding this work that I hope the authors\nwill answer in their response.\n\n1. Just by arranging relational matrices in a tensor and selecting (or more\nappropriately considering a linearly weighted sum of the relational matrices)\ndoes not ensure any information sharing between different relational matrices.\nThis would have been the case if you had performed some of a tensor\ndecomposition and projected the different slices (relational matrices) into\nsome common lower-dimensional core tensor. It is not clear why this approach\nwas not taken despite the motivation to share information between different\nrelational matrices.\n2. The two requirements (a) to share information across different relational\nmatrices and (b) make the attention vectors sparse are some what contradictory.\nIf the attention vector is truly sparse and has many zeros then information\nwill not flow into those slices during optimisation. \n3. The authors spend a lot of space discussing techniques for computing sparse\nattention vectors. The authors mention in page 3 that \\ell_1 regularisation did\nnot work in their preliminary experiments. However, no experimental results are\nshown for \\ell_1 regularisation nor they explain why \\ell_1 is not suitable for\nthis task. To this reviewer, it appears as an obvious baseline to try,\nespecially given the ease of optimisation. You use \\ell_0 instead and get into\nNP hard optimisations because of it. Then you propose a technique and a rather\ncrude approximation in the end to solve it. All that trouble could be spared if\n\\ell_1 was used.\n4. The vector \\alpha is performing a selection or a weighing over the slices of\nD. It is slightly misleading to call this as \u201cattention\u201d as it is a term\nused in NLP for a more different type of models (see attention model used in\nmachine translation).\n5. It is not clear why you need to initialise the optimisation by pre-trained\nembeddings from TransE. Why cannot you simply randomly initialise the\nembeddings as done in TransE and then update them? It is not fair to compare\nagainst TransE if you use TransE as your initial point.\n\nLearning the association between semantic relations is an idea that has been\nused in related problems in NLP such as relational similarity measurement\n[Turney JAIR 2012] and relation adaptation [Bollegala et al. IJCAI 2011]. It\nwould be good to put the current work with respect to such prior proposals in\nNLP for modelling inter-relational correlation/similarity.\n\nThanks for providing feedback. I have read it."
  },
  {
    "people": [
      "Moses",
      "Moses",
      "Moses",
      "Moses",
      "Moses",
      "Moses",
      "Moses"
    ],
    "review": "- Summary: \n\nThe paper introduces a new dataset for a sarcasm interpretation task\nand a system (called Sarcasm SIGN) based on machine translation framework\nMoses. The new dataset was collected from 3000 sarcastic tweets (with hashtag\n`#sarcasm) and 5 interpretations for each from humans. The Sarcasm SIGN is\nbuilt\nbased on Moses by replacing sentimental words by their corresponding clusters\non the source side (sarcasm) and then de-cluster their translations on the\ntarget side (non-sarcasm). Sarcasm SIGN performs on par with Moses on the MT\nevaluation metrics, but outperforms Moses in terms of fluency and adequacy. \n\n- Strengths:\n\nthe paper is well written\n\nthe dataset is collected in a proper manner\n\nthe experiments are carefully done and the analysis is sound.\n\n- Weaknesses:\n\nlack statistics of the datsets (e.g. average length, vocabulary size)\n\nthe baseline (Moses) is not proper because of the small size of the dataset\n\nthe assumption \"sarcastic tweets often differ from their non sarcastic\ninterpretations in as little as one sentiment word\" is not supported by the\ndata. \n\n- General Discussion: This discussion gives more details about the weaknesses\nof the paper. \n\nHalf of the paper is about the new dataset for sarcasm interpretation.\nHowever, the paper doesn't show important information about the dataset such as\naverage length, vocabulary size. More importantly, the paper doesn't show any\nstatistical evidence to support their method of focusing on sentimental words. \n\nBecause the dataset is small (only 3000 tweets), I guess that many words are\nrare. Therefore, Moses alone is not a proper baseline. A proper baseline should\nbe a MT system that can handle rare words very well. In fact, using\nclustering and declustering (as in Sarcasm SIGN) is a way to handle rare words.\n\nSarcasm SIGN is built based on the assumption that \"sarcastic tweets often\ndiffer from their non sarcastic interpretations in as little as one sentiment\nword\". Table 1 however strongly disagrees with this assumption: the human\ninterpretations are often different from the tweets at not only sentimental\nwords. I thus strongly suggest the authors to give statistical evidence from\nthe dataset that supports their assumption. Otherwise, the whole idea of\nSarcasm SIGN is just a hack.\n\n--------------------------------------------------------------\n\nI have read the authors' response. I don't change my decision because of the\nfollowing reasons: \n\n- the authors wrote that \"the Fiverr workers might not take this strategy\": to\nme it is not the spirit of corpus-based NLP. A model must be built to fit given\ndata, not that the data must follow some assumption that the model is built on.\n\n- the authors wrote that \"the BLEU scores of Moses and SIGN are above 60, which\nis generally considered decent in the MT literature\": to me the number 60\ndoesn't \nshow anything at all because the sentences in the dataset are very short. And\nthat,\nif we look at table 6, %changed of Moses is only 42%, meaning that even more\nthan half of the time translation is simply copying, the BLUE score is more\nthan 60.\n\n- \"While higher scores might be achieved with MT systems that explicitly\naddress rare words, these systems don't focus on sentiment words\": it's true,\nbut I was wondering whether sentiment words are rare in the corpus. If they\nare, those MT systems should obviously handle them (in addition to other rare\nwords)."
  },
  {
    "people": [
      "Aslam",
      "Aslam",
      "Javed A.",
      "Pelekhov",
      "Ekaterina",
      "Rus",
      "Daniela"
    ],
    "review": "This paper focuses on interpreting sarcasm written in Twitter identifying\nsentiment words and then using a machine translation engine to find an\nequivalent not sarcastic tweet. \n\nEDIT: Thank you for your answers, I appreaciate it. I added one line commenting\nabout it.\n\n- Strengths:\n\nAmong the positive aspects of your work, I would like to mention the parallel\ncorpus you presented. I think it will be very useful for other researchers in\nthe area for identifying and interpreting sarcasm in social media. An important\ncontribution is also the attempt to evaluate the parallel corpora using\nexisting measures such as the ones used in MT tasks. But also because you used\nhuman judgements to evaluate the corpora in 3 aspects: fluency, adequacy and\nequivalent sentiment.\n\n- Room for improvement:\n\nTackling the problem of interpretation as a monolingual machine translations\ntask is interesting, while I do appreciate the intent to compare the MT with\ntwo architectures, I think that due the relatively small dataset (needed for\nRNN) used it was predictable that the \u201cNeural interpretation\u201d is performing\nworse than \u201cmoses interpretation\u201d. You came to the same conclusion after\nseeing the results in Table3. In addition to comparing with this architecture,\nI would've liked to see other configuration of the MT used with moses. Or at\nleast, you should provide some explanation of why you use the configuration\ndescribed in lines 433 through 442; to me this choice is not justified. \n  - thank you for your response, I understand it is difficult to write down all\nthe details but I hope you include a line with some of your answer in the\npaper, I believe this could add valuable information.\n\nWhen you presented SING, it is clear that you evaluate some of its components\nbeforehand, i.e. the MT. But other important components are not evaluated,\nparticularly, the clustering you used of positive and negative words. While you\ndid said you used k-means as a clustering algorithm it is not clear to me why\nyou wanted to create clusters with 10 words. Why not test with other number of\nk, instead of 7 and 16, for positive and negative words respectively. Also you\ncould try another algorithm beside kmeans, for instance, the star clustering\nalgorithm (Aslam et al. 2004), that do not require a k parameter. \n   - thanks for clarifying.\n\nYou say that SIGN searches the tweet for sentiment words if it found one it\nchanges it for the cluster ID that contain that word. I am assuming that there\nis not a limit for the number of sentiment words found, and the MT decides by\nitself how many sentiment words to change. For example, for the tweet provided\nin Section 9: \u201cConstantly being irritated, anxious and depressed is a great\nfeeling\u201d the clustering stage of SIGN should do something like \u201cConstantly\nbeing cluster-i, cluster-j and cluster-k is a cluster-h feeling\u201d, Is that\ncorrect? If not, please explain what SIGN do.\n    - Thanks for clarifying\n\n- Minor comments:\n\nIn line 704, section 7, you said: \u201cSIGN-context\u2019s interpretations differ\nfrom the original sarcastic tweet in 68.5% of the cases, which come closer to\nthe 73.8% in the gold standard human interpretations.\u201d This means that 25% of\nthe human interpretations are the same as the original tweet? Do you have any\nidea why is that?\n\nIn section 6, line 539 you could eliminate the footnote 7 by adding \u201cits\ncluster ID\u201d or \u201cits cluster number\u201d.\n\nReferences:\nAslam, Javed A., Pelekhov, Ekaterina, and Rus, Daniela. \"The star clustering\nalgorithm for static and dynamic information organization..\" Journal of Graph\nAlgorithms and Applications 8.1 (2004): 95-129. <http://eudml.org/doc/51529>."
  },
  {
    "people": [
      "Cohen"
    ],
    "review": "- Strengths: 1) an interesting task, 2) the paper is very clearly written, easy\nto follow, 3) the created data set may be\nuseful for other researchers, 4) a detailed analysis of the performance of the\nmodel.\n\n- Weaknesses: 1) no method adapted from related work for a result comparison 2)\nsome explanations about the uniqueness of the task and discussion on\nlimitations of previous research for solving this problem can be added to\nemphasize the research contributions further. \n\n- General Discussion: The paper presents supervised and weakly supervised\nmodels for frame classification in tweets. Predicate rules are generated\nexploiting language-based and Twitter behavior-based signals, which are then\nsupplied to the probabilistic soft logic framework to build classification\nmodels. 17 political frames are classified in tweets in a multi-label\nclassification task. The experimental results demonstrate the benefit of the\npredicates created using the behavior-based signals. Please find my more\nspecific comments below:\n\nThe paper should have a discussion on how frame classification differs from\nstance classification. Are they both under the same umbrella but with different\nlevels of granularity?\n\nThe paper will benefit from adding a brief discussion on how exactly the\ntransition from long congressional speech to short tweets adds to the\nchallenges of the task. For example, does past research rely on any specific\ncross-sentential features that do not apply to tweets? Consider adapting the\nmethod of a frame classification work on\ncongressional speech (or a stance classification work on any text) to the\nextent possible due to its limitations on Twitter data, to compare with the\nresults of this work.\n\nIt seems \u201cweakly supervised\u201d and \u201cunsupervised\u201d \u2013 these two terms\nhave been interchangeably used in the paper (if this is not the case, please\nclarify in author response). I believe \"weakly supervised\" is\nthe\nmore technically correct terminology under the setup of this work that should\nbe used consistently throughout. The initial unlabeled data may not have been\nlabeled by human annotators, but the classification does use weak or noisy\nlabels of some sort, and the keywords do come from experts. The presented\nmethod does not use completely unsupervised data as traditional unsupervised\nmethods such as clustering, topic models or word embeddings would.  \n\nThe calculated Kappa may not be a straightforward reflection of the difficulty\nof\nframe classification for tweets (lines: 252-253), viewing it as a proof is a\nrather strong claim. The Kappa here merely represents the\nannotation difficulty/disagreement. Many factors can contribute to a low value \nsuch as poorly written annotation\nguidelines, selection of a biased annotator, lack of annotator training etc.\n(on\ntop of any difficulty of frame classification for tweets by human annotators,\nwhich the authors actually intend to relate to).\n73.4% Cohen\u2019s Kappa is strong enough for this task, in my opinion, to rely on\nthe annotated labels. \n\nEq (1) (lines: 375-377) will ignore any contextual information (such as\nnegation\nor conditional/hypothetical statements impacting the contributing word) when\ncalculating similarity of a frame and a tweet. Will this have any effect on the\nframe prediction model? Did the authors consider using models that can\ndetermine similarity with larger text units such as perhaps using skip thought\nvectors or vector compositionality methods?  \n\nAn ideal set up would exclude the annotated data from calculating statistics\nused to select the top N bi/tri-grams (line: 397 mentions entire tweets data\nset has been used), otherwise statistics from any test fold (or labeled data in\nthe weakly supervised setup) still leaks into\nthe selection process. I do not think this would have made any difference in\nthe current selection of the bi/tri-grams or results as the size of the\nunlabeled data is much larger, but would have constituted a cleaner\nexperimental setup.  \n\nPlease add precision and recall results in Table 4. \n\nMinor:\nplease double check any rules for footnote placements concerning placement\nbefore or after the punctuation."
  },
  {
    "people": [
      "Boydstun"
    ],
    "review": "- Strengths: The authors address a very challenging, nuanced problem in\npolitical discourse reporting a relatively high degree of success.\n\nThe task of political framing detection may be of interest to the ACL\ncommunity.\n\nThe paper is very well written.\n\n- Weaknesses: Quantitative results are given only for the author's PSL model\nand not compared against any traditional baseline classification algorithms,\nmaking it unclear to what degree their model is necessary. Poor comparison with\nalternative approaches makes it difficult to know what to take away from the\npaper.\n\nThe qualitative investigation is interesting, but the chosen visualizations are\ndifficult to make sense of and add little to the discussion. Perhaps it would\nmake sense to collapse across individual politicians to create a clearer\nvisual.\n\n- General Discussion: The submission is well written and covers a topic which\nmay be of interest to the ACL community. At the same time, it lacks proper\nquantitative baselines for comparison. \n\nMinor comments:\n\n- line 82: A year should be provided for the Boydstun et al. citation\n\n- It\u2019s unclear to me why similar behavior (time of tweeting) should\nnecessarily be indicative of similar framing and no citation was given to\nsupport this assumption in the model.\n\n- The related work goes over quite a number of areas, but glosses over the work\nmost clearly related (e.g. PSL models and political discourse work) while\nspending too much time mentioning work that is only tangential (e.g.\nunsupervised models using Twitter data).\n\n- Section 4.2 it is unclear whether Word2Vec was trained on their dataset or if\nthey used pre-trained embeddings.\n\n- The authors give no intuition behind why unigrams are used to predict frames,\nwhile bigrams/trigrams are used to predict party.\n\n- The authors note that temporal similarity worked best with one hour chunks,\nbut make no mention of how important this assumption is to their results. If\nthe authors are unable to provide full results for this work, it would still be\nworthwhile to give the reader a sense of what performance would look like if\nthe time window were widened.\n\n- Table 4: Caption should make it clear these are F1 scores as well as\nclarifying how the F1 score is weighted (e.g. micro/macro). This should also be\nmade clear in the \u201cevaluation metrics\u201d section on page 6."
  },
  {
    "people": [
      "Dowty",
      "Dowty",
      "David",
      "Bruni",
      "Elia",
      "Silberer",
      "Carina",
      "Vittorio Ferrari",
      "Mirella Lapata",
      "Levin",
      "Grice",
      "Sorower"
    ],
    "review": "Thank you for the author response. It addresses some my concerns, though much\nof it are promises (\"we will...\") -- necessarily so, given space constraints,\nbut then, this is precisely the problem: I would like to see the revision to\nthe paper to be able to check that the drawbacks have been fixed. The changes\nneeded are quite substantial, and the new experimental results that they\npromise to include will not have undergone review if the paper is accepted at\nthis stage. I'm still not sure that we can simply leave it to the authors to\nmake the necessary changes without a further reviewing round. I upgrade my\nscore to a 3 to express this ambivalence (I do like the research in the paper,\nbut it's extremely messy in its presentation).\n\n--------------\n\n- Strengths:\n\nThe topic of the paper is very creative and the purpose of the research really\nworthwhile: the paper aims at extracting common knowledge from text, overcoming\nthe well-known problem of reporting bias (the fact that people will not state\nthe obvious, such as the fact that a person is usually bigger than a ball), by\ndoing joint inference on information that is possible to extract from text.\n\n- Weaknesses:\n\n1) Many aspects of the approach need to be clarified (see detailed comments\nbelow). What worries me the most is that I did not understand how the approach\nmakes knowledge about objects interact with knowledge about verbs such that it\nallows us to overcome reporting bias. The paper gets very quickly into highly\ntechnical details, without clearly explaining the overall approach and why it\nis a good idea.\n\n2) The experiments and the discussion need to be finished. In particular, there\nis no discussion of the results of one of the two tasks tackled (lower half of\nTable 2), and there is one obvious experiment missing: Variant B of the\nauthors' model gives much better results on the first task than Variant A, but\nfor the second task only Variant A is tested -- and indeed it doesn't improve\nover the baseline. \n\n- General Discussion:\n\nThe paper needs quite a bit of work before it is ready for publication. \n\n- Detailed comments:\n\n026 five dimensions, not six\n\nFigure 1, caption: \"implies physical relations\": how do you know which physical\nrelations it implies?\n\nFigure 1 and 113-114: what you are trying to do, it looks to me, is essentially\nto extract lexical entailments (as defined in formal semantics; see e.g. Dowty\n1991) for verbs. Could you please explicit link to that literature?\n\nDowty, David. \"Thematic proto-roles and argument selection.\" Language (1991):\n547-619.\n\n135 around here you should explain the key insight of your approach: why and\nhow does doing joint inference over these two pieces of information help\novercome reporting bias?\n\n141 \"values\" ==> \"value\"?\n\n143 please also consider work on multimodal distributional semantics, here\nand/or in the related work section. The\nfollowing two papers are particularly related to your goals:\n\nBruni, Elia, et al. \"Distributional semantics in technicolor.\" Proceedings of\nthe 50th Annual Meeting of the Association for Computational Linguistics: Long\nPapers-Volume 1. Association for Computational Linguistics, 2012.\n\nSilberer, Carina, Vittorio Ferrari, and Mirella Lapata. \"Models of Semantic\nRepresentation with Visual Attributes.\" ACL (1). 2013.\n\n146 please clarify that your contribution is the specific task and approach --\ncommonsense knowledge extraction from language is long-standing task.\n\n152 it is not clear what \"grounded\" means at this point\n\nSection 2.1: why these dimensions, and how did you choose them?\n\n177 explain terms \"pre-condition\" and \"post-condition\", and how they are\nrelevant here\n\n197-198 an example of the full distribution for an item (obtained by the model,\nor crowd-sourced, or \"ideal\") would help.\n\nFigure 2. I don't really see the \"x is slower than y\" part: it seems to me like\nthis is related to the distinction, in formal semantics, between stage-level\nvs. individual-level\npredicates: when a person throws a ball, the ball is faster than the person\n(stage-level) but\nit's not true in general that balls are faster than people (individual-level).\nI guess this is related to the\npre-condition vs. post-condition issue. Please spell out the type of\ninformation that you want to extract.\n\n248 \"Above definition\": determiner missing\n\nSection 3\n\n\"Action verbs\": Which 50 classes do you pick, and you do you choose them? Are\nthe verbs that you pick all explicitly tagged as action verbs by Levin? \n\n306ff What are \"action frames\"? How do you pick them?\n\n326 How do you know whether the frame is under- or over-generating?\n\nTable 1: are the partitions made by frame, by verb, or how? That is, do you\nreuse verbs or frames across partitions? Also, proportions are given for 2\ncases (2/3 and 3/3 agreement), whereas counts are only given for one case;\nwhich?\n\n336 \"with... PMI\": something missing (threshold?)\n\n371 did you do this partitions randomly?\n\n376 \"rate *the* general relationship\"\n\n378 \"knowledge dimension we choose\": ? (how do you choose which dimensions you\nwill annotate for each frame?)\n\nSection 4\n\nWhat is a factor graph? Please give enough background on factor graphs for a CL\naudience to be able to follow your approach. What are substrates, and what is\nthe role of factors? How is the factor graph different from a standard graph?\nMore generally, at the beginning of section 4 you should give a higher level\ndescription of how your model works and why it is a good idea.\n\n420 \"both classes of knowledge\": antecedent missing.\n\n421 \"object first type\"\n\n445 so far you have been only talking about object pairs and verbs, and\nsuddenly selectional preference factors pop in. They seem to be a crucial part\nof your model -- introduce earlier? In any case, I didn't understand their\nrole.\n\n461 \"also\"?\n\n471 where do you get verb-level similarities from?\n\nFigure 3: I find the figure totally unintelligible. Maybe if the text was\nclearer it would be interpretable, but maybe you can think whether you can find\na way to convey your model a bit more intuitively. Also, make sure that it is\nreadable in black-and-white, as per ACL submission instructions.\n\n598 define term \"message\" and its role in the factor graph.\n\n621 why do you need a \"soft 1\" instead of a hard 1?\n\n647ff you need to provide more details about the EMB-MAXENT classifier (how did\nyou train it, what was the input data, how was it encoded), and also explain\nwhy it is an appropriate baseline.\n\n654 \"more skimp seed knowledge\": ?\n\n659 here and in 681, problem with table reference (should be Table 2). \n\n664ff I like the thought but I'm not sure the example is the right one: in what\nsense is the entity larger than the revolution? Also, \"larger\" is not the same\nas \"stronger\".\n\n681 as mentioned above, you should discuss the results for the task of\ninferring knowledge on objects, and also include results for model (B)\n(incidentally, it would be better if you used the same terminology for the\nmodel in Tables 1 and 2)\n\n778 \"latent in verbs\": why don't you mention objects here?\n\n781 \"both tasks\": antecedent missing\n\nThe references should be checked for format, e.g. Grice, Sorower et al\nfor capitalization, the verbnet reference for bibliographic details."
  },
  {
    "people": [
      "Dmitry Davidov",
      "Ari Rappoport"
    ],
    "review": "Summary: This paper aims to learn common sense relationships between object\ncategories (e.g comparative size, weight, strength, rigidness, and speed) from\nunstructured text.  The key insight of the paper is to leverage the correlation\nof action verbs to these comparative relations (e.g x throw y => x larger y).\n\nStrengths:\n\n- The paper proposes a novel method to address an important problem of mining\ncommon sense attribute relations from text.\n\nWeaknesses:\n\n- I would have liked to see more examples of objects pairs, action verbs, and\npredicted attribute relations.                          What are some interesting\naction\nverbs\nand\ncorresponding attributes relations?  The paper also lacks analysis/discussion \non what kind of mistakes their model makes.\n\n- The number of object pairs (3656) in the dataset is very small.  How many\ndistinct object categories are there?  How scalable is this approach to larger\nnumber of object pairs?\n\n- It's a bit unclear how the frame similarity factors and attributes similarity\nfactors are selected.\n\nGeneral Discussion/Suggestions:\n\n- The authors should discuss the following work and compare against mining\nattributes/attribute distributions directly and then getting a comparative\nmeasure.  What are the advantages offered by the proposed method compared to a\nmore direct approach?\n\nExtraction and approximation of numerical attributes from the Web\nDmitry Davidov, Ari Rappoport\nACL 2010\n\nMinor typos:\n\n1. In the abstract (line 026), the authors mention 'six' dimensions, but in the\npaper, there is only five.\n\n2. line 248: Above --> The above\n\n3. line 421: object first --> first\n\n4. line 654: more skimp --> a smaller\n\n5. line 729: selctional --> selectional"
  },
  {
    "people": [
      "Goldstein",
      "O'Connor",
      "Peter Hoff",
      "Michael Ward",
      "Gerrish",
      "O'Connor",
      "Gerrish",
      "O'Connor"
    ],
    "review": "- Strengths: Useful modeling contribution, and potentially useful annotated\ndata, for an important problem -- event extraction for the relationships\nbetween countries as expressed in news text.\n\n- Weaknesses: Many points are not explained well in the paper. \n\n- General Discussion:\n\nThis work tackles an important and interesting event extraction problem --\nidentifying positive and negative interactions between pairs of countries in\nthe world (or rather, between actors affiliated with countries).  The primary\ncontribution is an application of supervised, structured neural network models\nfor sentence-level event/relation extraction.  While previous work has examined\ntasks in the overall area, to my knowledge there has not been any publicly\navailble sentence-level annotated data for the problem -- the authors here make\na contribution as well by annotating some data included with the submission; if\nit is released, it could be useful for future researchers in this area.\n\nThe proposed models -- which seem to be an application of various\ntree-structured recursive neural network models -- demonstrate a nice\nperformance increase compared to a fairly convincing, broad set of baselines\n(if we are able to trust them; see below).  The paper also presents a manual\nevaluation of the inferred time series from a news corpus which is nice to see.\n\nI'm torn about this paper.  The problem is a terrific one and the application\nof the recursive models seems like a contribution to this problem. \nUnfortunately, many aspects of the models, experimentation, and evaluation are\nnot explained very well.  The same work, with a more carefully written paper,\ncould be really great.\n\nSome notes:\n\n- Baselines need more explanation.  For example, the sentiment lexicon is not\nexplained for the SVM.                    The LSTM classifier is left highly\nunspecified\n(L407-409) -- there are multiple different architectures to use an LSTM for\nclassification.  How was it trained?  Is there a reference for the approach? \nAre the authors using off-the-shelf code (in which case, please refer and cite,\nwhich would also make it easier for the reader to understand and replicate if\nnecessary)?  It would be impossible to replicate based on the two-line\nexplanation here.  \n\n- (The supplied code does not seem to include the baselines, just the recursive\nNN models.  It's great the authors supplied code for part of the system so I\ndon't want to penalize them for missing it -- but this is relevant since the\npaper itself has so few details on the baselines that they could not really be\nreplicated based on the explanation in the paper.)\n\n- How were the recursive NN models trained?\n\n- The visualization section is only a minor contribution; there isn't really\nany innovation or findings about what works or doesn't work here.\n\nLine by line:\n\nL97-99: Unclear. Why is this problem difficult?  Compared to what? (also the\nsentence is somewhat ungrammatical...)\n\nL231 - the trees are binarized, but how?\n\nFootnote 2 -- \"the tensor version\" - needs citation to explain what's being\nreferred to.\n\nL314: How are non-state verbs defined?                    Does the definition of\n\"event\nword\"s\nhere come from any particular previous work that motivates it?                   \nPlease\nrefer to\nsomething appropriate or related.\n\nFootnote 4: of course the collapsed form doesn't work, because the authors\naren't using dependency labels -- the point of stanford collapsed form is to\nremove prepositions from the dependeny path and instead incorporate them into\nthe labels.\n\nL414: How are the CAMEO/TABARI categories mapped to positive and negative\nentries?  Is performance sensitive to this mapping?  It seems like a hard task\n(there are hundreds of those CAMEO categories....) Did the authors consider\nusing the Goldstein scaling, which has been used in political science, as well\nas the cited work by O'Connor et al.?  Or is it bad to use for some reason?\n\nL400-401: what is the sentiment lexicon and why is it appropriate for the task?\n\nL439-440: Not clear.  \"We failed at finding an alpha meeting the requirements\nfor the FT model.\"  What does that mean? What are the requirements? What did\nthe authors do in their attempt to find it?\n\nL447,L470: \"precision and recall values are based on NEG and POS classes\". \nWhat does this mean?  So there's a 3x3 contingency table of gold and predicted\n(POS, NEU, NEG) classes, but this sentence leaves ambiguous how precision and\nrecall are calculated from this information.\n\n5.1 aggregations: this seems fine though fairly ad-hoc.  Is this temporal\nsmoothing function a standard one?  There's not much justification for it,\nespecially given something simpler like a fixed window average could have been\nused.\n\n5.2 visualizations: this seems pretty ad-hoc without much justification for the\nchoices.  The graph visualization shown does not seem to illustrate much. \nShould also discuss related work in 2d spatial visualization of country-country\nrelationships by Peter Hoff and Michael Ward.\n\n5.3\nL638-639: \"unions of countries\" isn't a well defined concept.  mMybe the\nauthors mean \"international organizations\"?\n\nL646-648: how were these 5 strong and 5 weak peaks selected?  In particular,\nhow were they chosen if there were more than 5 such peaks?\n\nL680-683: This needs more examples or explanation of what it means to judge the\npolarity of a peak.  What does it look like if the algorithm is wrong?               \n   \nHow\nhard was this to assess?  What was agreement rate if that can be judged?\n\nL738-740: The authors claim Gerrish and O'Connor et al. have a different\n\"purpose and outputs\" than the authors' work.  That's not right.  Both those\nworks try to do both (1) extract time series or other statistical information\nabout the polarity of the relationships between countries, and *also* (2)\nextract topical keywords to explain aspects of the relationships.  The paper\nhere is only concerned with #1 and less concerned with #2, but certainly the\nprevious work addresses #1.  It's fine to not address #2 but this last sentence\nseems like a pretty odd statement.\n\nThat raises the question -- Gerrish and O'Connor both conduct evaluations with\nan external database of country relations developed in political science\n(\"MID\", military interstate disputes).              Why don't the authors of this\nwork do\nthis evaluation as well?  There are various weaknesses of the MID data, but the\nevaluation approach needs to be discussed or justified."
  },
  {
    "people": [
      "Dong",
      "Lapata",
      "Lapata",
      "Jia",
      "Liang"
    ],
    "review": "The paper presents a neural model for predicting SQL queries directly from\nnatural language utterances, without going through an intermediate formalism.\nIn addition, an interactive online feedback loop is proposed and tested on a\nsmall scale.\n\n- Strengths:\n\n1\\ The paper is very clearly written, properly positioned, and I enjoyed\nreading it.\n\n2\\ The proposed model is tested and shown to perform well on 3 different\ndomains (academic, geographic queries, and flight booking)\n\n3\\ The online feedback loop is interesting and seems promising, despite of the\nsmall scale of the experiment.\n\n4\\ A new semantic corpus is published as part of this work, and additionally\ntwo\nexisting corpora are converted to SQL format, which I believe would be\nbeneficial for future work in this area.\n\n- Weaknesses / clarifications:\n\n1\\ Section 4.2 (Entity anonymization) - I am not sure I understand the choice\nof the length of span for querying the search engine. Why and how is it\nprogressively reduced? (line 333).\n\n2\\ Section 5 (Benchmark experiments) - If I understand correctly, the feedback\nloop (algorithm 1) is *not* used for these experiments. If this is indeed the\ncase, I'm not sure when does data augmentation occur. Is all the annotated\ntraining data augmented with paraphrases? When is the \"initial data\" from\ntemplates added? Is it also added to the gold training set? If so, I think it's\nnot surprising that it doesn't help much, as the gold queries may be more\ndiverse.  In any case, I think this should be stated more clearly. In addition,\nI think it's interesting to see what's the performance of the \"vanilla\" model,\nwithout any augmentation, I think that this is not reported in the paper.\n\n3\\ Tables 2 and 3: I find the evaluation metric used here somewhat unclear. \nDoes the accuracy measure the correctness of the execution of the query (i.e.,\nthe retrieved answer) as the text seem to indicate? (Line 471 mentions\n*executing* the query). Alternatively, are the queries themselves compared? (as\nseems to be the case for Dong and Lapata in Table 2). If this is done\ndifferently for different systems (I.e., Dong and Lapata), how are these\nnumbers comparable? In addition, the text mentions the SQL model has \"slightly\nlower accuracy than the best non-SQL results\" (Line 515), yet in table 2 the\ndifference is almost 9 points in accuracy.  What is the observation based upon?\nWas some significance test performed? If not, I think the results are still\nimpressive for direct to SQL parsing, but that the wording should be changed,\nas the difference in performance does seem significant.\n\n4\\ Line 519 - Regarding the data recombination technique used in Jia and Liang\n(2016): Since this technique is applicable in this scenario, why not try it as\nwell?  Currently it's an open question whether this will actually improve\nperformance. Is this left as future work, or is there something prohibiting the\nuse of this technique?\n\n5\\ Section 6.2 (Three-stage online experiment) - several details are missing /\nunclear:\n\n* What was the technical background of the recruited users?\n\n* Who were the crowd workers, how were they recruited and trained?\n\n* The text says \"we recruited 10 new users and asked them to issue at least 10\nutterances\". Does this mean 10 queries *each* (e.g., 100 overall), or 10 in\ntotal (1 for each).\n\n* What was the size of the initial (synthesized) training  set? \n\n* Report statistics of the queries - some measure of their lexical variability\n/ length / complexity of the generated SQL? This seems especially important for\nthe first phase, which is doing surprisingly well. Furthermore, since SCHOLAR\nuses SQL and NL, it would have been nice if it were attached to this\nsubmission, to allow its review during this period.\n\n6\\ Section 6.3 (SCHOLAR dataset)\n\n* The dataset seems pretty small in modern standards (816 utterances in total),\nwhile one of the main advantages of this process is its scalability. What\nhindered the creation of a much larger dataset?\n\n* Comparing performance - is it possible to run another baseline on this newly\ncreated dataset to compare against the reported 67% accuracy obtained in this\npaper (line 730).\n\n7\\ Evaluation of interactive learning experiments (Section 6): I find the\nexperiments to be somewhat hard to replicate as they involve manual queries of\nspecific annotators. For example, who's to say if the annotators in the last\nphase just asked simpler questions? I realise that this is always problematic\nfor online learning scenarios, but I think that an effort should be made\ntowards an objective comparison. For starters, the statistics of the queries\n(as I mentioned earlier) is a readily available means to assess whether this\nhappens. Second, maybe there can be some objective held out test set? This is\nproblematic as the model relies on the seen queries, but scaling up the\nexperiment (as I suggested above) might mitigate this risk. Third, is it\npossible to assess a different baseline using this online technique? I'm not\nsure whether this is applicable given that previous methods were not devised as\nonline learning methods.\n\n- Minor comments:\n\n1\\ Line 48 - \"requires\" -> \"require\"\n\n2\\ Footnote 1 seems too long to me. Consider moving some of its content to the\nbody of the text.\n\n3\\ Algorithm 1: I'm not sure what \"new utterances\" refers to (I guess it's new\nqueries from users?). I think that an accompanying caption to the algorithm\nwould make the reading easier.\n\n4\\ Line 218 - \"Is is\" -> \"It is\"\n\n5\\ Line 278 mentions an \"anonymized\" utterance. This confused me at the first\nreading, and if I understand correctly it refers to the anonymization described\nlater in 4.2. I think it would be better to forward reference this. \n\n- General Discussion:\n\nOverall, I like the paper, and given answers to the questions I raised above,\nwould like to see it appear in the conference.\n\n- Author Response:\n\nI appreciate the detailed response made by the authors, please include these\ndetails in a final version of the paper."
  },
  {
    "people": [
      "Wang",
      "Wong",
      "Mooney"
    ],
    "review": "This paper proposes an approach to learning a semantic parser using an\nencoder-decoder neural architecture, with the distinguishing feature that the\nsemantic output is full SQL queries. The method is evaluated over two standard\ndatasets (Geo880 and ATIS), as well as a novel dataset relating to document\nsearch.\n\nThis is a solid, well executed paper, which takes a relatively well\nestablished technique in the form of an encoder-decoder with some trimmings\n(e.g. data augmentation through paraphrasing), and uses it to generate SQL\nqueries, with the purported advantage that SQL queries are more expressive\nthan other semantic formalisms commonly used in the literature, and can be\nedited by untrained crowd workers (familiar with SQL but not semantic\nparsing). I buy that SQL is more expressive than the standard semantic\nformalisms, but then again, were there really any queries in any of your three\ndatasets where the standard formalisms are unable to capture the full\nsemantics of the query? I.e. are they really the best datasets to showcase the\nexpressivity of SQL? Also, in terms of what your model learns, what fraction\nof SQL does it actually use? I.e. how much of the extra expressivity in SQL is\nyour model able to capture? Also, does it have biases in terms of the style of\nqueries that it tends to generate? That is, I wanted to get a better sense of\nnot just the *potential* of SQL, but the actuality of what your model is able\nto capture, and the need for extra expressivity relative to the datasets you\nexperiment over. Somewhat related to this, at the start of Section 5, you\nassert that it's harder to directly produce SQL. You never actually show this,\nand this seems to be more a statement of the expressivity of SQL than anything\nelse (which returns me to the question of how much of SQL is the model\nactually generating).\n\nNext, I would really have liked to have seen more discussion of the types of\nSQL queries your model generates, esp. for the second part of the evaluation,\nover the SCHOLAR dataset. Specifically, when the query is ill-formed, in what\nways is it ill-formed? When a crowd worker is required to post-edit the query,\nhow much effort does that take them? Equally, how correct are the crowd\nworkers at constructing SQL queries? Are they always able to construct perfect\nqueries (experience would suggest that this is a big ask)? In a similar vein\nto having more error analysis in the paper, I would have liked to have seen\nagreement numbers between annotators, esp. for Incomplete Result queries,\nwhich seems to rely heavily on pre-existing knowledge on the part of the\nannotator and therefore be highly subjective.\n\nOverall, what the paper achieves is impressive, and the paper is well\nexecuted; I just wanted to get more insights into the true ability of the\nmodel to generate SQL, and a better sense of what subset of the language it\ngenerates.\n\nA couple of other minor things:\n\nl107: \"non-linguists can write SQL\" -- why refer to \"non-linguists\" here? Most\nlinguists wouldn't be able to write SQL queries either way; I think the point\nyou are trying to make is simply that \"annotators without specific training in\nthe semantic translation of queries\" are able to perform the task\n\nl218: \"Is is\" -> \"It is\"\n\nl278: it's not clear what an \"anonymized utterance\" is at this point of the\npaper\n\nl403: am I right in saying that you paraphrase only single words at a time?\nPresumably you exclude \"entities\" from paraphrasing?\n\nl700: introduce a visual variable in terms of line type to differentiate the\nthree lines, for those viewing in grayscale\n\nThere are various inconsistencies in the references, casing issues\n(e.g. \"freebase\", \"ccg\"), Wang et al. (2016) is missing critical publication\ndetails, and there is an \"In In\" for Wong and Mooney (2007)"
  },
  {
    "people": [
      "Ammar",
      "Tsvetkov",
      "Johnson",
      "Waleed",
      "Ammar",
      "George Mulcaire",
      "Miguel Ballesteros",
      "Chris Dyer",
      "Noah\nA.\nSmith",
      "Yulia Tsvetkov",
      "Sunayana Sitaram",
      "Manaal Faruqui",
      "Guillaume Lample",
      "Patrick\nLittell",
      "David Mortensen",
      "Alan W. Black",
      "Lori Levin",
      "Chris Dyer",
      "Melvin Johnson",
      "Mike Schuster",
      "Quoc V. Le",
      "Maxim Krikun",
      "Yonghui Wu",
      "Zhifeng\nChen",
      "Nikhil Thorat"
    ],
    "review": "The paper introduces a simple and effective method for morphological paradigm\ncompletion in low-resource settings. The method uses a character-based seq2seq\nmodel trained on a mix of examples in two languages: a resource-poor language\nand a closely-related resource-rich language; each training example is\nannotated with a paradigm properties and a language ID. Thus, the model enables\ntransfer learning across languages when the two languages share common\ncharacters and common paradigms. While the proposed multi-lingual solution is\nnot novel (similar architectures have been explored in syntax, language\nmodeling, and MT), the novelty of this paper is to apply the approach to\nmorphology. Experimental results show substantial improvements over monolingual\nbaselines, and include a very thorough analysis of the impact of language\nsimilarities on the quality of results. The paper is interesting, very clearly\nwritten, I think it\u2019ll be a nice contribution to the conference program. \n\nDetailed comments: \n\n\u2014 My main question is why the proposed general multilingual methodology was\nlimited to pairs of languages, rather than to sets of similar languages? For\nexample, all Romance languages could be included in the training to improve\nSpanish paradigm completion, and all Slavic languages with Cyrillic script\ncould be mixed to improve Ukrainian. It would be interesting to see the\nextension of the models from bi-lingual to multilingual settings. \n\n\u2014 I think Arabic is not a fair (and fairly meaningless) baseline, given how\ndifferent is its script and morphology from the target languages. A more\ninteresting baseline would be, e.g., a language with a partially shared\nalphabet but a different typology. For example, a Slavic language with Latin\nscript could be used as a baseline language for Romance languages. If Arabic is\nexcluded, and if we consider a most distant language in the same the same\nfamily as a baseline, experimental results are still strong. \n\n\u2014 A half-page discussion of contribution of Arabic as a regularizer also adds\nlittle to the paper; I\u2019d just remove Arabic from all the experiments and\nwould add a regularizer (which, according to footnote 5, works even better than\nadding Arabic as a transfer language).              \n\n\u2014 Related work is missing a line of work on \u201clanguage-universal\u201d RNN\nmodels that use basically the same approach: they learn shared parameters for\ninputs in multiple languages, and add a language tag to the input to mediate\nbetween languages. Related studies include a multilingual parser (Ammar et al.,\n2016), language models (Tsvetkov et al., 2016), and machine translation\n(Johnson et al., 2016 )\n\nMinor: \n\u2014 I don\u2019t think that the claim is correct in line 144 that POS tags are\neasy to transfer across languages. Transfer of POS annotations is also a\nchallenging task.  \n\nReferences: \n\nWaleed              Ammar, George Mulcaire, Miguel Ballesteros, Chris Dyer, and Noah\nA.\nSmith. \"Many languages, one parser.\u201d TACL 2016. \n\nYulia Tsvetkov, Sunayana Sitaram, Manaal Faruqui, Guillaume Lample, Patrick\nLittell, David Mortensen, Alan W. Black, Lori Levin, and Chris Dyer. \"Polyglot\nneural language models: A case study in cross-lingual phonetic representation\nlearning.\u201d NAACL 2016.\n\nMelvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng\nChen, Nikhil Thorat et al. \"Google's Multilingual Neural Machine Translation\nSystem: Enabling Zero-Shot Translation.\" arXiv preprint arXiv:1611.04558 2016.\n\n-- Response to author response: \n\nThanks for your response & I'm looking forward to reading the final version!"
  },
  {
    "people": [
      "Ranzato",
      "Mnih",
      "Tobias Schnabel",
      "Igor\nLabutov",
      "David Mimno",
      "Thorsten Joachims",
      "Manaal Faruqui",
      "Yulia Tsvetkov",
      "Pushpendre Rastogi",
      "Chris Dyer"
    ],
    "review": "TMP\nStrength: The paper propose DRL-Sense model that shows a marginal improvement\non SCWS dataset and a significant improvement on ESL-50 and RD-300 datasets.\n\nWeakness:\nThe technical aspects of the paper raise several concerns:\nCould the authors clarify two drawbacks in 3.2? The first drawback states that\noptimizing equation (2) leads to the underestimation of the probability of\nsense. As I understand, eq(2) is the expected reward of sense selection, z_{ik}\nand z_{jl} are independent actions and there are only two actions to optimize.\nThis should be relatively easy. In NLP setting, optimizing the expected rewards\nover a sequence of actions for episodic-task has been proven doable (Sequence\nLevel Training with Recurrent Neural Networks, Ranzato 2015) even in a more\nchallenging setting of machine translation where the number of actions ~30,000\nand the average sequence length ~30 words. The DRL-Sense model has maximum 3\nactions and it does not have sequential nature of RL. This makes it hard to\naccept the claim about the first drawback.\n\nThe second drawback, accompanied with the detail math in Appendix A, states\nthat the update formula is to minimize the likelihood due to the log-likelihood\nis negative. Note that most out-of-box optimizers (Adam, SGD, Adadelta, \u2026)\nminimize a function f, however, a common practice when we want to maximize f we\njust minimize -f. Since the reward defined in the paper is negative, any\nstandard optimizer can be use on the expected of the negative reward, which is\nalways greater than 0. This is often done in many modeling tasks such as\nlanguage model, we minimize negative log-likelihood instead of maximizing the\nlikelihood. The authors also claim that when \u201cthe log-likelihood reaches 0,\nit also indicates that the likelihood reaches infinity and computational flow\non U and V\u201d (line 1046-1049). Why likelihood\u2192infinity? Should it be\nlikelihood\u21921?\n\nCould the authors also explain how DRL -Sense is based on Q-learning? The\nhorizon in the model is length of 1. There is no transition between\nstate-actions and there is not Markov-property as I see it (k, and l are draw\nindependently). I am having trouble to see the relation between Q-learning and\nDRL-Sense.  In (Mnih et al., 2013), the reward is given from the environment\nwhereas in the paper, the rewards is computed by the model. What\u2019s the reward\nin DRL-Sense? Is it 0, for all the (state, action) pairs or the cross-entropy\nin eq(4)?  \n\nCross entropy is defined as H(p, q) = -\\sum_{x} q(x)\\log q(x), which variable\ndo the authors sum over in (4)? I see that q(C_t, z_{i, k}) is a scalar\n(computed in eq(3)), while Co(z_{ik}, z_{jl}) is a distribution over total\nnumber of senses eq(1). These two categorial variables do not have the same\ndimension, how is cross-entropy H in eq(4) is computed then?\n\nCould the authors justify the dropout exploration? Why not epsilon-greedy\nexploration? Dropout is often used for model regularization, preventing\noverfitting. How do the authors know the gain in using dropout is because of\nexploration but regularization?\n\nThe authors states that Q-value is a probabilistic estimation (line 419), can\nyou elaborate what is the set of variables the distribution is defined? When\nyou sum over that set of variable, do you get 1? I interpret that Q is a\ndistribution over senses per word, however  definition of q in eq(3) does not\ncontain a normalizing constant, so I do not see q is a valid distribution. This\nalso related to the value 0.5 in section 3.4 as a threshold for exploration.\nWhy 0.5 is chosen here where q is just an arbitrary number between (0, 1) and\nthe constrain \\sum_z q(z) = 1 does not held? Does the authors allow the\ncreation of a new sense in the very beginning or after a few training epochs? I\nwould image that at the beginning of training, the model is unstable and\ncreating new senses might introduce noises to the model.  Could the authors\ncomment on that?\n\nGeneral discussion\nWhat\u2019s the justification for omitting negative samples in line 517? Negative\nsampling has been use successfully in word2vec due to the nature of the task:\nlearning representation. Negative sampling, however does not work well when the\nmain interest is modeling a distribution p() over senses/words. Noise\ncontrastive estimation is often preferred when it comes to modeling a\ndistribution. The DRL-Sense, uses collocation likelihood to compute the reward,\nI wonder how the approximation presented in the paper affects the learning of\nthe embeddings.\n\nWould the authors consider task-specific evaluation for sense embeddings as\nsuggested in recent research [1,2]\n\n[1] Evaluation methods for unsupervised word embeddings. Tobias Schnabel, Igor\nLabutov, David Mimno and Thorsten Joachims.\n\n[2] Problems With Evaluation of Word Embeddings Using Word Similarity Tasks .\nManaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi, Chris Dyer\n\n---\nI have read the response."
  },
  {
    "people": [
      "Ding",
      "Ding",
      "Ding",
      "Ding"
    ],
    "review": "In this work, the authors extend MS-COCO by adding an incorrect\ncaption to each existing caption, with only one word of difference.\nThe authors demonstrate that two state-of-the-art methods (one for VQA\nand one for captioning) perform extremely poorly at a) determining if\na caption is fake, b) determining which word in a fake caption is\nwrong, and c) selecting a replacement word for a given fake word.\n\nThis work builds upon a wealth of literature regarding the\nunderperformance of vision/language models relative to their apparent\ncapacities. I think this work makes concrete some of the big,\nfundamental questions in this area: are vision/language models doing\n\"interesting\" things, or not? The authors consider a nice mix of tasks\nand models to shed light on the \"broken-ness\" of these settings, and\nperform some insightful analyses of factors associated with model\nfailure (e.g., Figure 3).\n\nMy biggest concerns with the paper are similarity to Ding et al. That\nbeing said, I do think the authors make some really good points; Ding\net al. generate similar captions, but the ones here differ by only one\nword and *still* break the models -- I think that's a justifiably\nfundamental difference. That observation demonstrates that Ding et\nal.'s engineering is not a requirement, as this simple approach still\nbreaks things catastrophically.\n\nAnother concern is the use of NeuralTalk to select the \"hardest\"\nfoils.              While a clever idea, I am worried that the use of this model\ncreates a risk of self-reinforcement bias, i.e., NeuralTalk's biases\nare now fundamentally \"baked-in\" to FOIL-COCO. \n\nI think the results section could be a bit longer, relative to the\nrest of the paper (e.g. I would've liked more than one paragraph -- I\nliked this part!)\n\nOverall, I do like this paper, as it nicely builds upon some results\nthat highlight defficiencies in vision/language integration. In the\nend, the Ding et al. similarity is not a \"game-breaker,\" I think -- if\nanything, this work shows that vision/language models are so easy to\nfool, Ding et al.'s method is not even required.\n\nSmall things:\n\nI would've liked to have seen another baseline that simply\nconcatenates BoW + extracted CNN features and trains a softmax\nclassifier over them. The \"blind\" model is a nice touch, but what\nabout a \"dumb\" vision+langauge baseline? I bet that would do close to\nas well as the LSTM/Co-attention. That could've made the point of the\npaper even stronger.\n\n330: What is a supercategory? Is this from WordNet? Is this from COCO?\nI understand the idea, but not the specifics.\n\n397: has been -> were\n\n494: that -> than\n\n693: artefact -> undesirable artifacts (?)\n\n701: I would have included a chance model in T1's table -- is 19.53%\n[Line 592] a constant-prediction baseline? Is it 50% (if so, can't we\nflip all of the \"blind\" predictions to get a better baseline?) I am\nnot entirely clear, and I think a \"chance\" line here would fix a lot\nof this confusion.\n\n719: ariplane\n\n~~\nAfter reading the author response...\n\nI think this author response is spot-on. Both my concerns of NeuralTalk biases\nand additional baselines were addressed, and I am confident that these can be\naddressed in the final version, so I will keep my score as-is."
  },
  {
    "people": [
      "S. I. Wang",
      "P. Liang",
      "C. Manning"
    ],
    "review": "Thanks for the response. I look forward to reading about the effect of\nincentives and the ambiguity of the language in the domain.\n\nReview before author response:\nThe paper proposes a way to build natural language interfaces by allowing a set\nof users to define new concepts and syntax. It's an (non-trivial) extension of\nS. I. Wang, P. Liang, and C. Manning. 2016. Learning language games through\ninteraction\n\nQuestions:\n- What is the size of the vocabulary used \n- Is it possible to position this paper with respect to previous work on\ninverse reinforcement learning and imitation learning ?\n\nStrengths:\n- The paper is well written\n- It provides a compelling direction/solution to the problem of dealing with a\nlarge set of possible programs while learning natural language interfaces. \n\nWeaknesses:\n- The authors should discuss the effect of the incentives on the final\nperformance ? Were other alternatives considered ? \n- While the paper claims that the method can be extended to more practical\ndomains, it is not clear to me how straightforward it is going to be. How\nsensitive is the method to the size of the vocabulary required in a domain ?\nWould increased ambiguity in natural language create new problems ? These\nquestions are not discussed in the current experiments.\n- A real-world application would definitely strengthen the paper even more."
  },
  {
    "people": [
      "Tasha"
    ],
    "review": "Strengths:\nThe paper presents a new method that exploits word senses to improve the task\nof lexical substitutability.  Results show improvements over prior methods.\n\nWeaknesses:\nAs a reader of a ACL paper, I usually ask myself what important insight can I\ntake away from the paper, and from a big picture point of view, what does the\npaper add to the fields of natural language processing and computational\nlinguistics.  How does the task of lexical substitutability in general and this\npaper in particular help either in improving an NLP system or provide insight\nabout language?  I can't find a good answer answer to either question after\nreading this paper.\n\nAs a practitioner who wants to improve natural language understanding system, I\nam more focused on the first question -- does the lexical substitutability task\nand the improved results compared to prior work presented here help any end\napplication?  Given the current state of high performing systems, any discrete\nclustering of words (or longer utterances) often break down when compared to\ncontinuous representations words (see all the papers that utilitize discrete\nlexical semantics to achieve a task versus words' distributed representations\nused as an input to the same task; e.g. machine translation, question\nanswering, sentiment analysis, text classification and so forth).  How do the\nauthors motivate work on lexical substitutability given that discrete lexical\nsemantic representations often don't work well?  The introduction cites a few\npapers from several years back that are mostly set up in small data scenarios,\nand given that this word is based on English, I don't see why one would use\nthis method for any task.  I would be eager to see the authors' responses to\nthis general question of mine.\n\nAs a minor point, to further motivate this, consider the substitutes presented\nin Table 1.\n1. Tasha snatched it from him to rip away the paper.\n2. Tasha snatched it from him to rip away the sheet.\n\nTo me, these two sentences have varying meanings -- what if he was holding on\nto a paper bag?  In that scenario, can the word \"paper\" be substituted by\n\"sheet\"?  At least, in my understanding, it cannot.  Hence, there is so much\nsubjectivity in this task that lexical substitutes can completely alter the\nsemantics of the original sentence.\n\nMinor point(s):\n - Citations in Section 3.1.4 are missing.\n\nAddition: I have read the author response and I am sticking to my earlier\nevaluation of the paper."
  },
  {
    "people": [
      "Levy"
    ],
    "review": "This paper proposes a method for recognizing lexical entailment (specifically,\nhypernymy) in context. The proposed method represents each context by\naveraging, min-pooling, and max-pooling its word embeddings. These\nrepresentations are combined with the target word's embedding via element-wise\nmultiplication. The in-context representation of the left-hand-side argument is\nconcatenated to that of the right-hand-side argument's, creating a single\nvectorial representation of the input. This input is then fed into a logistic\nregression classifier.\n\nIn my view, the paper has two major weaknesses. First, the classification model\nused in this paper (concat + linear classifier) was shown to be inherently\nunable to learn relations in \"Do Supervised Distributional Methods Really Learn\nLexical Inference Relations?\" (Levy et al., 2015). Second, the paper makes\nsuperiority claims in the text that are simply not substantiated in the\nquantitative results. In addition, there are several clarity and experiment\nsetup issues that give an overall feeling that the paper is still half-baked.\n\n= Classification Model =\n\nConcatenating two word vectors as input for a linear classifier was\nmathematically proven to be incapable of learning a relation between words\n(Levy et al., 2015). What is the motivation behind using this model in the\ncontextual setting?\n\nWhile this handicap might be somewhat mitigated by adding similarity features,\nall these features are symmetric (including the Euclidean distance, since |L-R|\n= |R-L|). Why do we expect these features to detect entailment?\n\nI am not convinced that this is a reasonable classification model for the task.\n\n= Superiority Claims =\n\nThe authors claim that their contextual representation is superior to\ncontext2vec. This is not evident from the paper, because:\n\n1) The best result (F1) in both table 3 and table 4 (excluding PPDB features)\nis the 7th row. To my understanding, this variant does not use the proposed\ncontextual representation; in fact, it uses the context2vec representation for\nthe word type.\n\n2) This experiment uses ready-made embeddings (GloVe) and parameters\n(context2vec) that were tuned on completely different datasets with very\ndifferent sizes. Comparing the two is empirically flawed, and probably biased\ntowards the method using GloVe (which was a trained on a much larger corpus).\n\nIn addition, it seems that the biggest boost in performance comes from adding\nsimilarity features and not from the proposed context representation. This is\nnot discussed.\n\n= Miscellaneous Comments =\n\n- I liked the WordNet dataset - using the example sentences is a nice trick.\n\n- I don\u2019t quite understand why the task of cross-lingual lexical entailment\nis interesting or even reasonable.\n\n- Some basic baselines are really missing. Instead of the \"random\" baseline,\nhow well does the \"all true\" baseline perform? What about the context-agnostic\nsymmetric cosine similarity of the two target words?\n\n- In general, the tables are very difficult to read. The caption should make\nthe tables self-explanatory. Also, it is unclear what each variant means;\nperhaps a more precise description (in text) of each variant could help the\nreader understand?\n\n- What are the PPDB-specific features? This is really unclear.\n\n- I could not understand 8.1.\n\n- Table 4 is overfull.\n\n- In table 4, the F1 of \"random\" should be 0.25.\n\n- Typo in line 462: should be \"Table 3\"\n\n= Author Response =\n\nThank you for addressing my comments. Unfortunately, there are still some\nstanding issues that prevent me from accepting this paper:\n\n- The problem I see with the base model is not that it is learning prototypical\nhypernyms, but that it's mathematically not able to learn a relation.\n\n- It appears that we have a different reading of tables 3 and 4. Maybe this is\na clarity issue, but it prevents me from understanding how the claim that\ncontextual representations substantially improve performance is supported.\nFurthermore, it seems like other factors (e.g. similarity features) have a\ngreater effect."
  },
  {
    "people": [
      "Julie Weeds",
      "Daoud Clarke",
      "Jeremy Reffin",
      "David Weir",
      "Bill Keller",
      "COLING"
    ],
    "review": "This paper addresses the task of lexical entailment detection in context, e.g.\nis \"chess\" a kind of \"game\" given a sentence containing each of the words --\nrelevant for QA. The major contributions are:\n\n(1) a new dataset derived from WordNet using synset exemplar sentences, and \n\n(2) a \"context relevance mask\" for a word vector, accomplished by elementwise\nmultiplication with feature vectors derived from the context sentence. Fed to a\nlogistic regression classifier, the masked word vectors just beat state of the\nart on entailment prediction on a PPDB-derived dataset from previous\nliterature. Combined with other existing features, they beat state of the art\nby a few points. They also beats the baseline on the new WN-derived dataset,\nalthough the best-scoring method on that dataset doesn't use the masked\nrepresentations.\n\nThe paper also introduces some simple word similarity features (cosine,\neuclidean distance) which accompany other cross-context similarity features\nfrom previous literature. All of the similarity features, together, improve the\nclassification results by a large amount, but the features in the present paper\nare a relatively small contribution.\n\nThe task is interesting, and the work seems to be correct as far as it goes,\nbut incremental. The method of producing the mask vectors is taken from\nexisting literature on encoding variable-length sequences into min/max/mean\nvectors, but I don't think they've been used as masks before, so this is novel.\nHowever, excluding the PPDB features it looks like the best result does not use\nthe representation introduced in the paper.\n\nA few more specific points:\n\nIn the creation of the new Context-WN dataset, are there a lot of false\nnegatives resulting from similar synsets in the \"permuted\" examples? If you\ntake word w, with synsets i and j, is it guaranteed that the exemplar context\nfor a hypernym synset of j is a bad entailment context for i? What if i and j\nare semantically close?\n\nWhy does the masked representation hurt classification with the\ncontext-agnostic word vectors (rows 3, 5 in Table 3) when row 1 does so well?\nWouldn't the classifier learn to ignore the context-agnostic features?\n\nThe paper should make clearer which similarity measures are new and which are\nfrom previous literature. It currently says that previous lit used the \"most\nsalient\" similarity features, but that's not informative to the reader.\n\nThe paper should be clearer about the contribution of the masked vectors vs the\nsimilarity features. It seems like similarity is doing most of the work.\n\nI don't understand the intuition behind the Macro-F1 measure, or how it relates\nto \"how sensitive are our models to changes in context\" -- what changes? How do\nwe expect Macro-F1 to compare with F1?\n\nThe cross-language task is not well motivated.\n\nMissing a relevant citation: Learning to Distinguish Hypernyms and Co-Hyponyms.\nJulie Weeds, Daoud Clarke, Jeremy Reffin, David Weir and Bill Keller. COLING\n2014.\n\n==\n\nI have read the author response. As noted in the original reviews, a quick\nexamination of the tables shows that the similarity features make the largest\ncontribution to the improvement in F-score on the two datasets (aside from PPDB\nfeatures). The author response makes the point that similarities include\ncontextualized representations. However, the similarity features are a mixed\nbag, including both contextualized and non-contextualized representations. This\nwould need to be teased out more (as acknowledged in the response).\n\nNeither Table 3 nor 4 gives results using only the masked representations\nwithout the similarity features. This makes the contribution of the masked\nrepresentations difficult to isolate."
  },
  {
    "people": [
      "Zhou",
      "Xu",
      "Ouchi",
      "Shibata",
      "Ouchi"
    ],
    "review": "This paper proposes new prediction models for Japanese SRL task by adopting the\nEnglish state-of-the-art model of (Zhou and Xu, 2015).\nThe authors also extend the model by applying the framework of Grid-RNNs in\norder to handle the interactions between the arguments of multiple predicates.\n\nThe evaluation is performed on the well-known benchmark dataset in Japanese\nSRL, and obtained a significantly better performance than the current state of\nthe art system.\n\nStrengths:\nThe paper is well-structured and well-motivated.\nThe proposed model obtains an improvement in accuracy compared with the current\nstate of the art system.\nAlso, the model using Grid-RNNs achieves a slightly better performance than\nthat of proposed single-sequential model, mainly due to the improvement on the\ndetection of zero arguments, that is the focus of this paper.\n\nWeakness:\nTo the best of my understanding, the main contribution of this paper is an\nextension of the single-sequential model to the multi-sequential model. The\nimpact of predicate interactions is a bit smaller than that of (Ouchi et al.,\n2015). There is a previous work (Shibata et al., 2016) that extends the (Ouchi\net al., 2015)'s model\nwith neural network modeling. I am curious about the comparison between them."
  },
  {
    "people": [
      "Li",
      "Hovy",
      "Li",
      "Hovy"
    ],
    "review": "The paper introduces an extension of the entity grid model. A convolutional\nneural network is used to learn sequences of entity transitions indicating\ncoherence, permitting better generalisation over longer sequences of entities\nthan the direct estimates of transition probabilities in the original model.\n\nThis is a nice and well-written paper. Instead of proposing a fully neural\napproach, the authors build on existing work and just use a neural network to\novercome specific issues in one step. This is a valid approach, but it would be\nuseful to expand the comparison to the existing neural coherence model of Li\nand Hovy. The authors admit being surprised by the very low score the Li and\nHovy model achieves on their task. This makes the reader wonder if there was an\nerror in the experimental setup, if the other model's low performance is\ncorpus-dependent and, if so, what results the model proposed in this paper\nwould achieve on a corpus or task where the other model is more successful. A\ndeeper investigation of these factors would strengthen the argument\nconsiderably.\n\nIn general the paper is very fluent and readable, but in many places definite\narticles are missing (e.g. on lines 92, 132, 174, 488, 490, 547, 674, 764 and\nprobably more). I would suggest proofreading the paper specifically with\narticle usage in mind. The expression \"...limits the model to do X...\", which\nis used repeatedly, sounds a bit unusual. Maybe \"limits the model's capacity to\ndo X\" or \"stops the model from doing X\" would be clearer.\n\n--------------\n\nFinal recommendation adjusted to 4 after considering the author response. I\nagree that objective difficulties running other people's software shouldn't be\nheld against the present authors. The efforts made to test the Li and Hovy\nsystem, and the problems encountered in doing so, should be documented in the\npaper. I would also suggest that the authors try to reproduce the results of Li\nand Hovy on their original data sets as a sanity check (unless they have\nalready done so), just to see if that works for them."
  },
  {
    "people": [
      "Eriguchi"
    ],
    "review": "- Strengths:\nThe paper presents an interesting extension to attention-based neural MT\napproaches, which leverages source-sentence chunking as additional piece of\ninformation from the source sentence. The model is modified such that this\nchunking information is used differently by two recurrent layers: while one\nfocuses in generating a chunk at a time, the other focuses on generating the\nwords within the chunk. This is interesting. I believe readers will enjoy\ngetting to know this approach and how it performs.\nThe paper is very clearly written, and alternative approaches are clearly\ncontrasted. The evaluation is well conducted, has a direct contrast with other\npapers (and evaluation tables), and even though it could be strengthened (see\nmy comments below), it is convincing.\n\n- Weaknesses:\nAs always, more could be done in the experiments section to strengthen the case\nfor chunk-based models. For example, Table 3 indicates good results for Model 2\nand Model 3 compared to previous papers, but a careful reader will wonder\nwhether these improvements come from switching from LSTMs to GRUs. In other\nwords, it would be good to see the GRU tree-to-sequence result to verify that\nthe chunk-based approach is still best.\n\nAnother important aspect is the lack of ensembling results. The authors put a\nlot of emphasis is claiming that this is the best single NMT model ever\npublished. While this is probably true, in the end the best WAT system for\nEng-Jap is at 38.20 (if I'm reading the table correctly) - it's an ensemble of\n3. If the authors were able to report that their 3-way chunk-based ensemble\ncomes top of the table, then this paper could have a much stronger impact.\n\nFinally, Table 3 would be more interesting if it included decoding times. The\nauthors mention briefly that the character-based model is less time-consuming\n(presumably based on Eriguchi et al.'16), but no cite is provided, and no\nnumbers from chunk-based decoding are reported either. Is the chunk-based model\nfaster or slower than word-based? Similar? Who know... Adding a column to Table\n3 with decoding times would give more value to the paper.\n\n- General Discussion:\nOverall I think the paper is interesting and worth publishing. I have minor\ncomments and suggestions to the authors about how to improve their presentation\n(in my opinion, of course). \n\n* I think they should clearly state early on that the chunks are supplied\nexternally - in other words, that the model does not learn how to chunk. This\nonly became apparent to me when reading about CaboCha on page 6 - I don't think\nit's mentioned earlier, and it is important.\n\n* I don't see why the authors contrast against the char-based baseline so often\nin the text (at least a couple of times they boast a +4.68 BLEU gain). I don't\nthink readers are bothered... Readers are interested in gains over the best\nbaseline.\n\n* It would be good to add a bit more detail about the way UNKs are being\nhandled by the neural decoder, or at least add a citation to the\ndictionary-based replacement strategy being used here.\n\n* The sentence in line 212 (\"We train a GRU that encodes a source sentence into\na single vector\") is not strictly correct. The correct way would be to say that\nyou do a bidirectional encoder that encodes the source sentence into a set of\nvectors... at least, that's what I see in Figure 2.\n\n* The motivating example of lines 69-87 is a bit weird. Does \"you\" depend on\n\"bite\"? Or does it depend on the source side? Because if it doesn't depend on\n\"bite\", then the argument that this is a long-dependency problem doesn't really\napply."
  },
  {
    "people": [
      "Li",
      "Li",
      "Li"
    ],
    "review": "- Summary\n\nThis paper introduces chunk-level architecture for existing NMT models. Three\nmodels are proposed to model the correlation between word and chunk modelling\non the target side in the existing NMT models. \n\n- Strengths:\n\nThe paper is well-written and clear about the proposed models and its\ncontributions. \n\nThe proposed models to incorporating chunk information into NMT models are\nnovel and well-motivated. I think such models can be generally applicable for\nmany other language pairs. \n\n- Weaknesses:\n\nThere are some minor points, listed as follows:\n\n1) Figure 1: I am a bit surprised that the function words dominate the content\nones in a Japanese sentence. Sorry I may not understand Japanese. \n\n2) In all equations, sequences/vectors (like matrices) should be represented\nas bold texts to distinguish from scalars, e.g., hi, xi, c, s, ...\n\n3) Equation 12: s_j-1 instead of s_j.\n\n4) Line 244: all encoder states should be referred to bidirectional RNN states.\n\n5) Line 285: a bit confused about the phrase \"non-sequential information such\nas chunks\". Is chunk still sequential information???\n\n6) Equation 21: a bit confused, e.g, perhaps insert k into s1(w) like s1(w)(k)\nto indicate the word in a chunk.  \n\n7) Some questions for the experiments:\n\nTable 1: source language statistics? \n\nFor the baselines, why not running a baseline (without using any chunk\ninformation) instead of using (Li et al., 2016) baseline (|V_src| is\ndifferent)? It would be easy to see the effect of chunk-based models. Did (Li\net al., 2016) and other baselines use the same pre-processing and\npost-processing steps? Other baselines are not very comparable. After authors's\nresponse, I still think that (Li et al., 2016) baseline can be a reference but\nthe baseline from the existing model should be shown. \n\nFigure 5: baseline result will be useful for comparison? chunks in the\ntranslated examples are generated *automatically* by the model or manually by\nthe authors? Is it possible to compare the no. of chunks generated by the model\nand by the bunsetsu-chunking toolkit? In that case, the chunk information for\nDev and Test in Table 1 will be required. BTW, the authors's response did not\naddress my point here. \n\n8) I am bit surprised about the beam size 20 used in the decoding process. I\nsuppose large beam size is likely to make the model prefer shorter generated\nsentences. \n\n9) Past tenses should be used in the experiments, e.g.,\n\nLine 558: We *use* (used) ...\n\nLine 579-584: we *perform* (performed) ... *use* (used) ...\n\n...\n\n- General Discussion:\n\nOverall, this is a solid work - the first one tackling the chunk-based NMT;\nand it well deserves a slot at ACL."
  },
  {
    "people": [
      "Hidley",
      "McKeown",
      "David Lewis",
      "Lewis",
      "Hidey\nand"
    ],
    "review": "This paper develops an LSTM-based model for classifying connective uses for\nwhether they indicate that a causal relation was intended. The guiding idea is\nthat the expression of causal relations is extremely diverse and thus not\namenable to syntactic treatment, and that the more abstract representations\ndelivered by neural models are therefore more suitable as the basis for making\nthese decisions.\n\nThe experiments are on the AltLex corpus developed by Hidley and McKeown. The\nresults offer modest but consistent support for the general idea, and they\nprovide some initial insights into how best to translate this idea into a\nmodel. The paper distribution includes the TensorFlow-based models used for the\nexperiments.\n\nSome critical comments and questions:\n\n* The introduction is unusual in that it is more like a literature review than\na full overview of what the paper contains. This leads to some redundancy with\nthe related work section that follows it. I guess I am open to a non-standard\nsort of intro, but this one really doesn't work: despite reviewing a lot of\nideas, it doesn't take a stand on what causation is or how it is expressed, but\nrather only makes a negative point (it's not reducible to syntax). We aren't\nreally told what the positive contribution will be except for the very general\nfinal paragraph of the section.\n\n* Extending the above, I found it disappointing that the paper isn't really\nclear about the theory of causation being assumed. The authors seem to default\nto a counterfactual view that is broadly like that of David Lewis, where\ncausation is a modal sufficiency claim with some other counterfactual\nconditions added to it. See line 238 and following; that arrow needs to be a\nvery special kind of implication for this to work at all, and there are\nwell-known problems with Lewis's theory (see\nhttp://bcopley.com/wp-content/uploads/CopleyWolff2014.pdf). There are comments\nelsewhere in the paper that the authors don't endorse the counterfactual view,\nbut then what is the theory being assumed? It can't just be the temporal\nconstraint mentioned on page 3!\n\n* I don't understand the comments regarding the example on line 256. The\nauthors seem to be saying that they regard the sentence as false. If it's true,\nthen there should be some causal link between the argument and the breakage.\nThere are remaining issues about how to divide events into sub-events, and\nthese impact causal theories, but those are not being discussed here, leaving\nme confused.\n\n* The caption for Figure 1 is misleading, since the diagram is supposed to\ndepict only the \"Pair_LSTM\" variant of the model. My bigger complaint is that\nthis diagram is needlessly imprecise. I suppose it's okay to leave parts of the\nstandard model definition out of the prose, but then these diagrams should have\na clear and consistent semantics. What are all the empty circles between input\nand the \"LSTM\" boxes? The prose seems to say that the model has a look-up\nlayer, a Glove layer, and then ... what? How many layers of representation are\nthere? The diagram is precise about the pooling tanh layers pre-softmax, but\nnot about this. I'm also not clear on what the \"LSTM\" boxes represent. It seems\nlike it's just the leftmost/final representation that is directly connected to\nthe layers above. I suggest depicting that connection clearly.\n\n* I don't understand the sentence beginning on line 480. The models under\ndiscussion do not intrinsically require any padding. I'm guessing this is a\nrequirement of TensorFlow and/or efficient training. That's fine. If that's\ncorrect, please say that. I don't understand the final clause, though. How is\nthis issue even related to the question of what is \"the most convenient way to\nencode the causal meaning\"? I don't see how convenience is an issue or how this\nrelates directly to causal meaning.\n\n* The authors find that having two independent LSTMs (\"Stated_LSTM\") is\nsomewhat better than one where the first feeds into the second. This issue is\nreminiscent of discussions in the literature on natural language entailment,\nwhere the question is whether to represent premise and hypothesis independently\nor have the first feed into the second. I regard this as an open question for\nentailment, and I bet it needs further investigation for causal relations too.\nSo I can't really endorse the sentence beginning on line 587: \"This behaviour\nmeans that our assumption about the relation between the meanings of the two\ninput events does not hold, so it is better to encode each argument\nindependently and then to measure the relation between the arguments by using\ndense layers.\" This is very surprising since we are talking about subparts of a\nsentence that might share a lot of information.\n\n* It's hard to make sense of the hyperparameters that led to the best\nperformance across tasks. Compare line 578 with line 636, for example. Should\nwe interpret this or just attribute it to the unpredictability of how these\nmodels interact with data?\n\n* Section 4.3 concludes by saying, of the connective 'which then', that the\nsystem can \"correctly disambiguate its causal meaning\", whereas that of Hidey\nand McKeown does not. That might be correct, but one example doesn't suffice to\nshow it. To substantiate this point, I suggest making up a wide range of\nexamples that manifest the ambiguity and seeing how often the system delivers\nthe right verdict. This will help address the question of whether it got lucky\nwith the example from table 8."
  },
  {
    "people": [
      "McKeown",
      "McKeown"
    ],
    "review": "This paper proposes a method for detecting causal relations between clauses,\nusing neural networks (\"deep learning\", although, as in many studies, the\nnetworks are not particularly deep).  Indeed, while certain discourse\nconnectives are unambiguous regarding the relation they signal (e.g. 'because'\nis causal) the paper takes advantage of a recent dataset (called AltLex, by\nHidey and McKeown, 2016) to solve the task of identifying causal vs. non-causal\nrelations when the relation is not explicitly marked.  Arguing that\nconvolutional networks are not as adept as representing the relevant features\nof clauses as LSTMs, the authors propose a classification architecture which\nuses a Glove-based representation of clauses, input in an LSTM layer, followed\nby three densely connected layers (tanh) and a final decision layer with a\nsoftmax.\n\nThe best configuration of the system improves by 0.5-1.5% F1 over Hidey and\nMCkeown's 2016 one (SVM classifier).  Several examples of generalizations where\nthe system performs well are shown (indicator words that are always causal in\nthe training data, but are found correctly to be non causal in the test data).\nTherefore, I appreciate that the system is analyzed qualitatively and \nquantitatively.\n\nThe paper is well written, and the description of the problem is particularly\nclear. However a clarification of the differences between this task and the \ntask of implicit connective recognition would be welcome.  This could possibly \ninclude a discussion of why previous methods for implicit connective \nrecognition cannot be used in this case.\n\nIt is very appreciable that the authors uploaded their code to the submission\nsite (I inspected it briefly but did not execute it).  Uploading the (older)\ndata (with the code) is also useful as it provides many examples.  It was not\nclear to me what is the meaning of the 0-1-2 coding in the TSV files, given\nthat the paper mentions binary classification. I wonder also, given that this\nis the data from Hidey and McKeown, if the authors have the right to repost it\nas they do.  -- One point to clarify in the paper would be the meaning of\n\"bootstrapping\", which apparently extends the corpus by about 15%: while the\nconstruction of the corpus is briefly but clearly explained in the paper, the\nadditional bootstrapping is not. \n\nWhile it is certainly interesting to experiment with neural networks on this\ntask, the merits of the proposed system are not entirely convincing.  It seems\nindeed that the best configuration (among 4-7 options) is found on the test\ndata, and it is this best configuration that is announced as improving over\nHidey by \"2.13% F1\".  However, a fair comparison would involve selecting the\nbest configuration on the devset.\n\nMoreover, it is not entirely clear how significant the improvement is. On the\none hand, it should be possible, given the size of the dataset, to compute some\nstatistical significance indicators.  On the other hand, one should consider\nalso the reliability of the gold-standard annotation itself (possibly from the\ncreators of the dataset).  Upon inspection, the annotation obtained from the\nEnglish/SimpleEnglish Wikipedia is not perfect, and therefore the scores might\nneed to be considered with a grain of salt.\n\nFinally, neural methods have been previously shown to outperform human\nengineered features for binary classification tasks, so in a sense the results \nare rather a confirmation of a known property. It would be interesting to see\nexperiments with simpler networks used as baselines, e.g. a 1-layer LSTM.  The\nanalysis of results could try to explain why the neural method seems to favor \nprecision over recall."
  },
  {
    "people": [
      "Filannino"
    ],
    "review": "This paper presents an approach to tag word senses with temporal information\n(past, present, future or atemporal). They model the problem using a\ngraph-based semi-supervised classification algorithm that allows to combine\nitem specific information - such as the presence of some temporal indicators in\nthe glosses - and the structure of Wordnet - that is semantic relations between\nsynsets \u00e2\u0080\u0093, and to take into account unlabeled data. They perform a full\nannotation of Wordnet, based on a set of training data labeled in a previous\nwork and using the rest of Wordnet as unlabeled data. Specifically, they take\nadvantage of the structure of the label set by breaking the task into a binary\nformulation (temporal vs atemporal), then using the data labeled as temporal to\nperform a finer grained tagging (past, present or future). In order to\nintrinsically evaluate their approach, they annotate a subset of synsets in\nWordnet using crowd-sourcing. They compare their system to the results obtained\nby a state-of-the-art time tagger (Stanford's SUTime) using an heuristic as a\nbackup strategy, and to previous works. They obtain improvements around 11% in\naccuracy, and show that their approach allows performance higher than previous\nsystems using only 400 labeled data. Finally, they perform an evaluation of\ntheir resource on an existing task (TempEval-3) and show improvements of about\n10% in F1 on 4 labels.\n\nThis paper is well-constructed and generally clear, the approach seems sound\nand well justified. This work led to the development of a resource with fine\ngrained temporal information at the word sense level that would be made\navailable and could be used to improve various NLP tasks. I have a few remarks,\nespecially concerning the settings of the experiments.\n\nI think that more information should be given on the task performed in the\nextrinsic evaluation section. An example could be useful to understand what the\nsystem is trying to predict (the features describe \u00e2\u0080\u009centity pairs\u00e2\u0080\u009d but it\nhas not been made clear before what are these pairs) and what are the features\n(especially, what are the entity attributes? What is the POS for a pair, is it\none dimension or two? Are the lemmas obtained automatically?). The sentence\ndescribing the labels used is confusing, I'm not sure to understand what\n\u00e2\u0080\u009cevent to document creation time\u00e2\u0080\u009d and \u00e2\u0080\u009cevent to same sentence event\u00e2\u0080\u009d\nmeans, are they the kind of pairs considered? Are they relations (as they are\ndescribed as relation at the beginning of p.8)? I find unclear the footnote\nabout the 14 relations: why the other relations have to be ignored, what makes\na mapping too \u00e2\u0080\u009ccomplex\u00e2\u0080\u009d? Also, are the scores macro or micro averaged?\nFinally, the ablation study seems to indicate a possible redundancy between\nLexica and Entity with quite close scores, any clue about this behavior?\n\nI have also some questions about the use of the SVM.  For the extrinsic\nevaluation, the authors say that they optimized the parameters of the\nalgorithm: what are these parameters?  And since a SVM is also used within the\nMinCut framework, is it optimized and how? Finally, if it's the LibSVM library\nthat is used (Weka wrapper), I think a reference to LibSVM should be included. \n\nOther remarks:\n- It would be interesting to have the number of examples per label in the gold\ndata, the figures are given for coarse grained labels (127 temporal vs 271\natemporal), but not for the finer grained.\n- It would also be nice to have an idea of the number of words that are\nambiguous at the temporal level, words like \u00e2\u0080\u009cpresent\u00e2\u0080\u009d.\n- It is said in the caption of the table 3 that the results presented are\n\u00e2\u0080\u009csignificantly better\u00e2\u0080\u009d but no significancy test is indicated, neither any\np-value.\n\nMinor remarks:\n- Related work: what kind of task was performed in (Filannino and Nenadic,\n2014)?\n- Related work: \u00e2\u0080\u009crequires a post-calibration procedure\u00e2\u0080\u009d, needs a reference\n(and p.4 in 3.3 footnote it would be clearer to explain calibration)\n- Related work: \u00e2\u0080\u009ctheir model differ from ours\u00e2\u0080\u009d, in what?\n- Table 3 is really too small: maybe, remove the parenthesis, put the\n\u00e2\u0080\u009c(p,r,f1)\u00e2\u0080\u009d in the caption and give only two scores, e.g. prec and f1. The\ncaption should also be reduced.\n- Information in table 4 would be better represented using a graph.\n- Beginning of p.7: 1064 \u00e2\u0086\u0092 1264\n- TempEval-3: reference ?\n- table 6: would be made clearer by ordering the scores for one column\n- p.5, paragraph 3: atemporal) \u00e2\u0086\u0092 atemporal"
  },
  {
    "people": [
      "R\u00c3\u00b6der"
    ],
    "review": "This paper proposes a new method for the evaluation of topic models that\npartitions the top n words of each topic into clusters or \"buckets\" based on\ncosine similarity of their associated word embeddings. In the simplest setup,\nthe words are considered one by one, and each is either put into an existing\n\"bucket\" \u00e2\u0080\u0093 if its cosine similarity to the other words in the bucket is below\na certain threshold \u00e2\u0080\u0093 or a new bucket is created for the word. Two more\ncomplicated methods based on eigenvectors and reorganisation are also\nsuggested. The method is evaluated on three standard data sets and in a  weakly\nsupervised text classification setting. It outperforms or is en par with the\nstate of the art (R\u00c3\u00b6der et al., 2015).\n\nThe basic idea behind the paper is rather simple and has a certain ad\nhoc-flavour. The authors do not offer any new explanations for why topic\nquality should be measurable in terms of word\u00e2\u0080\u0093word similarity. It is not\nobvious to me why this should be so, given that topics and word embeddings are\ndefined with respect to two rather different notions of context (document vs.\nsequential context). At the same time, the proposed method seems to work quite\nwell. (I would like to see some significance tests for Table 1 though.)\n\nOverall the paper is clearly written, even though there are some language\nissues. Also, I found the description of the techniques in Section 3 a bit hard\nto follow; I believe that this is mostly due to the authors using passive voice\n(\"the threshold is computed as\") in places were they were actually making a\ndesign choice. I find that the authors should try to explain the different\nmethods more clearly, with one subsection per method. There seems to be some\nspace for that: The authors did not completely fill the 8 pages of content, and\nthey could easily downsize the rather uninformative \"trace\" of the method on\npage 3.\n\nOne question that I had was how sensitive the proposed technique was to\ndifferent word embeddings. For example, how would the scores be if the authors\nhad used word2vec instead of GloVe?"
  },
  {
    "people": [
      "McDonald",
      "Mcdonald",
      "J Tiedemann",
      "C G\u00c3\u00b3mez-Rodr\u00c3\u00adguez",
      "Jiang Guo",
      "Wanxiang Che",
      "David\nYarowsky",
      "Haifeng Wang",
      "Ting Liu",
      "G Mulcaire",
      "M Ballesteros",
      "C Dyer"
    ],
    "review": "This paper presents results on the UD treebanks to test delexicalized transfer\nparsers and an unsupervised parser which is enriched with external\nprobabilities.\n\nThe paper is interesting, but I think it could be improved further.\n\n(5.2) \"McDonald et al. (2011) presented 61.7% of averaged accuracy over 8\nlanguages. On the same languages, our transfer parser on UD reached 70.1%.\"\nMcdonald et al could not use the UD treebanks since they were not available,\nyou should definitely state that this is the case here.\n\nIn footnote 9 you say: \"We used the Malt parser with its default feature set.\nTuning in this specific delexicalized task would probably bring a\nbit better results.\" You are using MaltParser with default settings, why don't\nyou use MaltOptimizer? Optimizing one model would be very easy. \nIn the same way MSTParser could be optimized further.\nIn the same line, why don't you use more recent parsers that produce better\nresults? These parsers have been already applied to universal dependencies with\nthe leave one out setup (see references below). For instance, the authors say\nthat  the unsupervised parser \"performs better for languages from less\nresourced language families (non-Indo-European)\", it would be interesting to\nsee whether this holds with more recent (and cross lingual) parsers.\n\nProbabilities: Why do you use this probabilities? it seems like a random\ndecision (Tables 3-4) (esp 3), at least we need more details or a set of\nexperiments to see whether they make sense or not.\n\nThere are some papers that the authors should take into account.\n\n1. Cross-Lingual Dependency Parsing with Universal Dependencies and Predicted\nPoS Labels\nJ Tiedemann\n2. One model, two languages: training bilingual parsers with harmonized\ntreebanks\nD Vilares, MA Alonso, C G\u00c3\u00b3mez-Rodr\u00c3\u00adguez  (it presents results with\nMaltParser)\n\nAnd for results with more recent parsers (and also delexicalized parsers):\n1. Crosslingual dependency parsing based on distributed representations. \nJiang Guo, Wanxiang Che, David\nYarowsky, Haifeng Wang, and Ting Liu. 2015.  In Proc. of ACL\n\n2. Many languages, one parser\nW Ammar, G Mulcaire, M Ballesteros, C Dyer, NA Smith\n\n-Minor points:\n I don't think we need Table 1 and Table 2, this could be solved with a\nfootnote to the UD website. Perhaps Table 2 should be included due to the\nprobabilities, but Table 1 definitely not."
  },
  {
    "people": [
      "Lin",
      "Ammar",
      "Duer",
      "Levin",
      "Plank",
      "Goldberg"
    ],
    "review": "The aim of this paper is to show that distributional information stored in word\nvector models contain information about POS labels. They use a version of the\nBNC annotated with UD POS and in which words have been replaced by lemmas. They\ntrain word embeddings on this corpus, then use the resulting vectors to train a\nlogistic classifier to predict the word POS. Evaluations are performed on the\nsame corpus (using cross-validation) as well as on other corpora. Results are\nclearly presented and discussed and analyzed at length.\n\nThe paper is clear and well-written. The main issue with this paper is that it\ndoes not contain anything new in terms of NLP or ML. It describe a set of\nstraightforward experiments without any new NLP or ML ideas or methods. Results\nare interesting indeed, in so far that they provide an empirical grounding to\nthe notion of POS. In that regard, it is certainly worth being published in a\n(quantitative/emprirical) linguistic venue.\n\nOn another note, the literature on POS tagging and POS induction using word\nembeddings should be cited more extensively (cf. for instance Lin, Ammar, Duer\nand Levin 2015; Ling et al. 2015 [EMNLP]; Plank, S\u00c3\u00b8gaard and Goldberg\n2016...)."
  },
  {
    "people": [
      "Croft",
      "Christodoulopoulos",
      "Yatbaz",
      "Mikolov"
    ],
    "review": "## General comments:\nThis paper presents an exploration of the connection between part-of-speech\ntags and word embeddings. Specifically the authors use word embeddings to draw\nsome interesting (if not somewhat straightforward) conclusions about the\nconsistency of PoS tags and the clear connection of word vector representations\nto PoS. The detailed error analysis (outliers of classification) is definitely\na strong point of this paper.\n\nHowever, the paper seems to have missing one critical main point: the reason\nthat corpora such as the BNC were PoS tagged in the first place. Unlike a\npurely linguistic exploration of morphosyntactic categories (which are\nunderlined by a semantic prototype theory - e.g. see Croft, 1991), these\ncorpora were created and tagged to facilitate further NLP tasks, mostly\nparsing. The whole discussion could then be reframed as whether the\ndistinctions made by the distributional vectors are more beneficial to parsing\nas compared to the original tags (or UPOS for that matter). \n\nAlso, this paper is missing a lot of related work in the context of\ndistributional PoS induction. I recommend starting with the review\nChristodoulopoulos et al. 2010 and adding some more recent non-DNN work\nincluding Blunsom and Cohn (2011), Yatbaz et al. (2012), etc. In light of this\nbody of work, the results of section 5 are barely novel (there are systems with\nmore restrictions in terms of their external knowledge that achieve comparable\nresults).\n\n## Specific issues\nIn the abstract one of the contributed results is that \"distributional vectors\ndo contain information about PoS affiliation\". Unless I'm misunderstanding the\nsentence, this is hardly a new result, especially for English: every\ndistributionally-based PoS induction system in the past 15 years that presents\n\"many-to-one\" or \"cluster purity\" numbers shows the same result.\n\nThe assertion in lines 79-80 (\"relations between... vectors... are mostly\nsemantic\") is not correct: the <MIKOLOV or COLOBERT> paper (and subsequent\nwork) shows that there is a lot of syntactic information in these vectors. Also\nsee previous comment about cluster purity scores. In fact you revert that\nstatement in the beginning of section 2 (lines 107-108).\n\nWhy move to UPOS? Surely the fine-grained distinctions of the original tagset\nare more interesting.\n\nI do not understand footnote 3. Were these failed attempts performed by you or\nother works? Under what criteria did they fail? What about Brown cluster\nvectors? They almost perfectly align with UPOS tags.\n\nIs the observation that \"proper nouns are not much similar to common nouns\"\n(lines 331-332) that interesting? Doesn't the existence of \"the\" (the most\nfrequent function word) almost singlehandedly explain this difference?\n\nWhile I understand the practical reasons for analysing the most frequent\nword/tag pairs, it would be interesting to see what happens in the tail, both\nin terms of the vectors and also for the types of errors the classifier makes.\nYou could then try to imagine alternatives to pure distributional (and\nmorphological - since you're lemmatizing) features that would allow better\ngeneralizations of the PoS tags to these low-frequency words.\n\n## Minor issues\nChange the sentential references to \\newcite{}: e.g. \"Mikolov et al. (2013b)\nshowed\""
  },
  {
    "people": [
      "Kendall"
    ],
    "review": "I am buying some of the motivation: the proposed method is much faster to train\nthan it is to train a neural network. Also, it keeps some properties of the\ndistribution when going to lower dimensionality. \n\nHowever, I am not convinced why it is so important for vectors to be\ntransformable with PPMI.\n\nMost importantly, there is no direct comparison to related work.\n\nDetailed comments:\n\n- p.3: The definition of Kendall's tau that the authors use is strange. This is\nNOT the original formula; I am not sure what it is and where it comes from.\n\n- p.3: Why not use Spearman correlation as is standard in semantic tasks (and\nas teh authors do at evaluation time)?\n\n- The datasets chosen for evaluation are not the standard ones for measuring\nsemantic relatedness that the NLP community prefers. It is nice to try other\nsets, but I would recommend to also include results on the standard ones.\n\n- I can only see two lines on Figure 1. Where is the third line?\n\n- There is no direct comparison to related work, just a statement that \n\nSome typos:\n\n- large extend -- extent"
  },
  {
    "people": [
      "Faruqui",
      "Dyer",
      "Luong",
      "Gardner"
    ],
    "review": "This paper describes four methods of obtaining multilingual word embeddings and\na modified QVEC metric for evaluating the efficacy of these embeddings. The\nembedding methods are: \n\n(1) multiCluster : Uses a dictionary to map words to multilingual clusters.\nCluster embeddings are then obtained which serve as embeddings for the words\nthat reside in each cluster. \n\n(2) multiCCA : Extends the approach presented by Faruqui and Dyer (2014) for\nembedding bilingual words, to multilingual words by using English embeddings as\nthe anchor space. Bilingual dictionaries (other_language -> English) are then\nused to obtain projections from other monolingual embeddings for words in other\nlanguages to the anchor space. \n\n(3) multiSkip : Extends the approach presented by Luong et al. (2015b) for\nembedding using source and target context (via alignment), to the multilingual\ncase by extending the objective function to include components for all\navailable parallel corpora. \n\n(4) Translation invariance : Uses a low rank decomposition of the word PMI\nmatrix with an objective with includes bilingual alignment frequency\ncomponents. May only work for  bilingual embeddings. \n\nThe evaluation method uses CCA to maximize the correlation between the word\nembeddings and possibly hand crafted linguistic data. Basis vectors are\nobtained for the aligned dimensions which produce a score which is invariant to\nrotation and linear transformations. The proposed method also extends this to\nmultilingual evaluations. \n\nIn general, the paper is well written and describes the work clearly. A few\nmajor issues:\n\n(1) What is the new contribution with respect to the translation invariance\nembedding approach of Gardner et al.? If it is the extension to multilingual\nembeddings, a few lines explaining the novelty would help. \n\n(2) The use of super-sense annotations across multiple languages is a problem.\nThe number of features in the intersection of multiple languages may become\nreally small. How do the authors propose to address this problem (beyond\nfootnote 9)?\n\n(3) How much does coverage affect the score in table 2? For example, for\ndependency parsing, multi cluster and multiCCA have significantly different\ncoverage numbers with scores that are close. \n\n(4) In general, the results in table 3 do not tell a consistent story. Mainly,\nfor most of the intrinsic metrics, the multilingual embedding techniques do not\nseem to perform the best.  Given that one of the primary goals of this paper\nwas to create embeddings that perform well under the word translation metric\n(intra-language), it is disappointing that the method that performs best (by\nfar) is the invariance approach. It is also strange that the multi-cluster\napproach, which discards inter-cluster (word and language) semantic information\nperforms the best with respect to the extrinsic metrics.\n\nOther questions for the authors:\n\n(1) What is the loss in performance by fixing the word embeddings in the\ndependency parsing task? What was the gain by simply using these embeddings as\nalternatives to the random embeddings in the LSTM stack parser? \n\n(2) Is table 1 an average over the 17 embeddings described in section 5.1? \n\n(3) Are there any advantages of using the multi-Skip approach instead of\nlearning bilingual embeddings and performing multi-CCA to learning projections\nacross the distinct spaces?\n\n(4) The dictionary extraction approach (from parallel corpora via alignments or\nfrom google translate) may not reflect the challenges of using real lexicons.\nDid you explore the use of any real multi-lingual dictionaries?"
  },
  {
    "people": [
      "Coulmance",
      "Guo",
      "Ravindran",
      "Raykar",
      "Saha",
      "Hill",
      "Cho",
      "Jean",
      "Devin",
      "Bengio",
      "Y.",
      "Lu",
      "Wang",
      "Bansal",
      "Gimpel",
      "Livescu",
      "K.",
      "Faruqui",
      "Dyer",
      "C.",
      "Suster",
      "Titov",
      "van Noord",
      "G.",
      "Guo",
      "Che",
      "Wang",
      "Liu",
      "T.",
      "Resnik"
    ],
    "review": "This paper proposes two dictionary-based methods for estimating multilingual\nword embeddings, one motivated in clustering (MultiCluster) and another in\ncanonical correlation analysis (MultiCCA).\nIn addition, a supersense similarity measure is proposed that improves on QVEC\nby substituting its correlation component with CCA, and by taking into account\nmultilingual evaluation.\n The evaluation is performed on a wide range of tasks using the web portal\ndeveloped by the authors; it is shown that in some cases the proposed\nrepresentation methods outperform two other baselines.\n\nI think the paper is very well written, and represents a substantial amount of\nwork done. The presented representation-learning and evaluation methods are\ncertainly timely. I also applaud the authors for the meticulous documentation.\n\nMy general feel about this paper, however, is that it goes (perhaps) in too\nmuch breadth at the expense of some depth. I'd prefer to see a thorougher\ndiscussion of results (e.g. regarding the conflicting outcome for MultiCluster\nbetween 59- and 12-language set-up; regarding the effect of estimation\nparameters and decisions in MultiCluster/CCA). So, while I think the paper is\nof high practical value to me and the research community (improved QVEC\nmeasure, web portal), I frankly haven't learned that much from reading it, i.e.\nin terms of research questions addressed and answered.\n\nBelow are some more concrete remarks.\n\nIt would make sense to include the correlation results (Table 1) for\nmonolingual QVEC and QVEC-CCA as well. After all, it is stated in l.326--328\nthat the proposed QVEC-CCA is an improvement over QVEC.\n\nMinor:\nl. 304: \"a combination of several cross-lingual word similarity datasets\" ->\nthis sounds as though they are of different nature, whereas they are really of\nthe same kind, just different languages, right?\n\np. 3: two equations exceed the column margin\n\nLines 121 and 147 only mention Coulmance et al and Guo et al when referring to\nthe MultiSkip baseline, but section 2.3 then only mentions Luong et al. So,\nwhat's the correspondence between these works?\n\nWhile I think the paper does reasonable justice in citing the related works,\nthere are more that are relevant and could be included:\n\nMultilingual embeddings and clustering:\nChandar A P, S., Lauly, S., Larochelle, H., Khapra, M. M., Ravindran, B.,\nRaykar, V. C., and Saha, A. (2014). An autoencoder approach to learning\nbilingual word representations. In NIPS.\nHill, F., Cho, K., Jean, S., Devin, C., and Bengio, Y. (2014). Embedding word\nsimilarity with neural machine translation. arXiv preprint arXiv:1412.6448.\nLu, A., Wang, W., Bansal, M., Gimpel, K., & Livescu, K. (2015). Deep\nmultilingual correlation for improved word embeddings. In NAACL.\nFaruqui, M., & Dyer, C. (2013). An Information Theoretic Approach to Bilingual\nWord Clustering. In ACL.\n\nMultilingual training of embeddings for the sake of better source-language\nembeddings:\nSuster, S., Titov, I., and van Noord, G. (2016). Bilingual learning of\nmulti-sense embeddings with discrete autoencoders. In NAACL-HLT.\nGuo, J., Che, W., Wang, H., and Liu, T. (2014). Learning sense-specific word\nembeddings by exploiting bilingual resources. In COLING.\n\nMore broadly, translational context has been explored e.g. in\nDiab, M., & Resnik, P. (2002). An unsupervised method for word sense tagging\nusing parallel corpora. In ACL."
  },
  {
    "people": [
      "Axelrod"
    ],
    "review": "The paper describes an MT training data selection approach that scores and\nranks general-domain sentences using a CNN classifier. Comparison to prior work\nusing continuous or n-gram based language models is well done, even though  it\nis not clear of the paper also compared against bilingual data selection (e.g.\nsum of difference of cross-entropies).\nThe motivation to use a CNN instead of an RNN/LSTM was first unclear to me, but\nit is a strength of the paper to argue that certain sections of a text/sentence\nare more important than others and this is achieved by a CNN. However, the\npaper does not experimentally show whether a BOW or SEQ (or the combination of\nboth( representation is more important and why.\nThe textual description of the CNN (one-hot or semi-supervised using\npre-trained embeddings) \nis clear, detailed, and points out the important aspects. However, a picture of\nthe layers showing how inputs are combined would be worth a thousand words.\n\nThe paper is overall well written, but some parentheses for citations are not\nnecessary (\\citet vs. \\citep) (e.g line 385).\n\nExperiments and evaluation support the claims of the paper, but I am a little\nbit concerned about the method of determining the number of selected in-domain\nsentences (line 443) based on a separate validation set:\n- What validation data is used here? It is also not clear on what data\nhyperparameters of the CNN models are chosen. How sensitive are the models to\nthis?\n- Table 2 should really compare scores of different approaches with the same\nnumber of sentences selected. As Figure 1 shows, the approach of the paper\nstill seems to outperform the baselines in this case. \n\nOther comments:\n- I would be interested in an experiment that compares the technique of the\npaper against baselines when more in-domain data is available, not just the\ndevelopment set.\n- The results or discussion section could feature some example sentences\nselected by the different methods to support the claims made in section 5.4.\n- In regards to the argument of abstracting away from surface forms in 5.4:\nAnother baseline to compare against could have been the work of Axelrod, 2015,\nwho replace some words with POS tags to reduce LM data sparsity to see whether\nthe word2vec embeddings provide an additional advantage over this.\n- Using the sum of source and target classification scores is very similar to\nsource & target Lewis-Moore LM data selection: sum of difference of\ncross-entropies. A reference to this work around line 435 would be reasonable.\n\nFinally, I wonder if you could learn weights for the sum of both source &\ntarget classification scores by extending the CNN model to the\nbilingual/parallel setting."
  },
  {
    "people": [
      "Johnson",
      "Zhang"
    ],
    "review": "The paper describes a method for in-domain data selection for SMT with a\nconvolutional neural network classifier, applying the same framework as Johnson\nand Zhang, 2015. The method performs about 0.5 BLEU points better than language\nmodel based data selection, and, unlike the other methods, is robust even if\nonly a very small in-domain data set is provided. \n\nThe paper claims improvements of 3.1 BLEU points. However, from the results we\nsee that improvements of this magnitude are only achieved if there are\nin-domain data in the training set - training only on the in-domain data\nalready produces +2.8 BLEU. It might be interesting to also compare this to a\nsystem which interpolates separate in- and out-domain models. \n\nThe more impressive result, in my opinion, comes from the second experiment,\nwhich demonstrates that the CNN classifier is still effective if there is very\nlittle in-domain data. However, the second experiment is only run on the zh2en\ntask which includes actual in-domain data in the training set, possibly making\nselection easier. Would the result also hold for the other tasks, where there\nis no in-domain data in the training set? The results for the en2es and en2zh\ntask already point in this direction, since the development sets only contain a\nfew hundred sentence pairs. I think the claim would be better supported if\nresults were reported for all tasks when only 100 sentence pairs are used for\ntraining.  \n\nWhen translating social media text one often has to face very different\nproblems from other domains, the most striking being a high OOV rate due to\nnon-conventional spelling (for Latin scripts, at least). The texts can also\ncontain special character sequences such as usernames, hashtags or emoticons.\nWas there any special preprocessing or filtering step applied to the data?  \nSince data selection cannot address the OOV problem, it would be interesting to\nknow in more detail what kinds of improvements are made through adaptation via\ndata selection, maybe by providing examples.   \n\nThe following remarks concern specific sections:\n\nSection 3.2:\n- It could be made clearer how the different vectors (word embeddings, segment\nvectors and one-hot vectors) are combined in the model. An illustration of the\narchitecture would be very helpful. \n- What was the \"designated loss function\"?\n\nSection 5.2:\nFor completeness' sake, it could be mentioned how the system weights were\ntuned."
  },
  {
    "people": [
      "Henderson",
      "Dyer",
      "Henderson",
      "Dyer"
    ],
    "review": "This paper presents a Stack LSTM parser based on the work of Henderson et al.\n(2008, 2013) on joint syntactic/semantic transition-based parsing and Dyer et\nal. (2015) on stack LSTM syntactic parsing. The use of the transition system\nfrom the former and the stack LSTM from the latter shows interesting results\ncompared to the joint systems on the CoNLL 2008 and 2009 shared tasks.\n\nI like this paper a lot because it is well-written, well-explained, the related\nwork is good and the results are very interesting. The methodology is sound\n(with a minor concern regarding the Chinese embeddings, leading me to believe\nthan very good embeddings can be more informative than a very clever model...).\n\nMoreover, the description of the system is clear, the hyperparameters are\njustified and the discussion is interesting.\n\nThe only thing I would say is that the proposed system lacks originality in the\nsense that the work of Henderson et al. puts the basis of semi-synchronised\njoint syntax-semantic transition-based parsing several years ago and Dyer et\nal. came up with the stack LSTM last year, so it is not a new method, per say.\nBut in my opinion, we were waiting for such a parser to be designed and so I'm\nglad it was done here."
  },
  {
    "people": [
      "Henderson",
      "Henderson",
      "Henderson",
      "Henderson"
    ],
    "review": "General comments\n================\n\nThe paper presents a joint syntactic and semantic transition-based dependency\nparser,\ninspired from the joint parser of Henderson et al. (2008).\nThe authors claim two main differences:\n- vectorial representations are used for the whole parser's state, instead of\nthe top elements of the stack / the last parser's configurations\n- the algorithm is a plain greedy search\n\nThe key idea is to take advantage of stack LSTMs so that the vector\nrepresenting the state of the parser\nkeeps memory of potentially large scoped syntactic features, which\nare known to be decisive features for semantic role labeling\n(such as the path between the predicate and the candidate role filler head).\n\nThe system is tested on the CoNLL 2008 data set (English) and on the\nmultilingual CoNLL 2009 data set.\nThe authors compare their system's performance to previously reported\nperformances,\nshowing their system does well compared to the 2008 / 2009 systems, \nbut less compared to more recent proposals (cf. bottom of table 3).\nThey emphasized though that the proposed system does not require any hand-craft\nfeatures,\nand is fast due to the simple greedy algorithm.\n\nThe paper is well written and describes a substantial amount of work,\nbuilding on the recently popular LSTMs, applied to the Henderson et al.\nalgorithm\nwhich appears now to have been somewhat visionary.\n\nI have reservations concerning the choice of the simple greedy algorithm:\nit renders results not comparable to some of the cited works.\nIt would not have been too much additional work nor space to provide for\ninstance beam-searched performance.\n\nMore detailed comments / questions\n==================================\n\nSection 2:\n\nA comment on the presence of both A1 and C-A1 links would help understanding\nbetter the target task of the paper.\n\nA summary of the differences between the set of transitions used in this work\nand that of Henderson et al. should be provided. In its current form, it is\ndifficult to \ntell what is directly reused from Henderson et al. and what is new / slightly\nmodified.\n\nSection 3.3\n\nWhy do you need representations concatenating the word predicate and its\ndisambiguated sense,\nthis seems redundant since the disambiguated sense are specific to a predicate\n?\n\nSection 4\n\nThe organization if the 4.1 / 4.2 sections is confusing concerning\nmultilinguality.\nConll 2008 focused on English, and CoNLL 2009 shared task extended it to a few\nother languages."
  },
  {
    "people": [
      "Hendersen"
    ],
    "review": "This paper performs an overdue circling-back to the problem of joint semantic\nand syntactic dependency parsing, applying the recent insights from neural\nnetwork models. Joint models are one of the most promising things about the\nsuccess of transition-based neural network parsers.\n\nThere are two contributions here. First, the authors present a new transition\nsystem, that seems better than the Hendersen (2008) system it is based on. The\nother contribution is to show that the neural network succeeds on this problem,\nwhere linear models had previously struggled. The authors attribute this\nsuccess to the ability of the neural network to automatically learn which\nfeatures to extract. However, I think there's another advantage to the neural\nnetwork here, that might be worth mentioning. In a linear model, you need to\nlearn a weight for each feature/class pair. This means that if you jointly\nlearn two problems, you have to learn many more parameters. The neural network\nis much more economical in this respect.\n\nI suspect the transition-system would work just as well with a variety of other\nneural network models, e.g. the global beam-search model of Andor (2016). There\nare many other orthogonal improvements that could be made. I expect extensions\nto the authors' method to produce state-of-the-art results.\n\nIt would be nice to see an attempt to derive a dynamic\noracle for this transition system, even if it's only in an appendix or in\nfollow-up work. At first glance, it seems similar to the\narc-eager oracle. The M-S action excludes all semantic arcs between the word at\nthe start of the buffer and the words on the semantic stack, and the M-D action\nexcludes all semantic arcs between the word at the top of the stack and the\nwords in the buffer. The L and R actions seem to each exclude the reverse arc,\nand no other."
  },
  {
    "people": [
      "Alexandre Bouchard-Cote",
      "Tandy Warnow",
      "Luay Nakhleh",
      "Andrew Kitchen",
      "Moran",
      "Johann-Mattis List",
      "Steven Moran",
      "Andrew Kitchen",
      "Christopher Ehret",
      "Shiferaw Assefa",
      "Connie J. Mulligan"
    ],
    "review": "This paper proposes a method for discovering correspondences between languages\nbased on MDL. The author model correspondences between words sharing the same\nmeaning in a number of Slavic languages. They develop codes for rules that\nmatch substrings in two or more languages and formulate an MDL objective that\nbalances the description of the model and the data given the model. \nThe model is trained with EM and tested on a set of 13 Slavic languages. The\nresults are shown by several distance measures, a phylogenetic tree, and\nexample of found correspondences. \n\nThe motivation and formulation of the approach makes sense. MDL seems like a\nreasonable tool to attack the problem and the motivation for employing EM is\npresented nicely. I must admit, though, that some of the derivations were not\nentirely clear to me.\nThe authors point out the resemblance of the MDL objective to Bayesian\ninference, and one thinks of the application of Bayesian inference in\n(biological) phylogenetic inference, e.g. using the MrBayes tool. An empirical\ncomparison here could be insightful.  \n\nRelated work: \n- Lacking comparison to methods for borrowing and cognate detection or other\ncomputational methods for historical linguistics. For example, the studies by\nAlexandre Bouchard-Cote, Tandy Warnow, Luay Nakhleh and Andrew Kitchen. Some\nmay not have available tools to apply in the given dataset, but one can mention\nList and Moran (2013). There are also relevant tools for biological phylogeny\ninference that can be applied (paup, MrBayes, etc.). \n\nApproach and methodology\n- Alignment procedure: the memory/runtime bottleneck appears to be a major\ndrawback, allowing the comparison of only 5 languages at most. As long as\nmultiple languages are involved, and phylogenetic trees, it would be\ninteresting to see more languages. I'm curious what ideas the authors have for\ndealing with this issue. \n- Phylogenetic tree: using neighbor joining for creating phylogenetic trees is\nknown to have disadvantages (like having to specify the root manually). How\nabout more sophisticated methods?  \n- Do you run EM until convergence or have some other stopping criterion? \n\nData\n- Two datasets are mixed, one of cognates and one not necessarily (the Swadesh\nlists). Have you considered how this might impact the results? \n- The data is in orthographic form, which might hide many correspondences. This\nis especially apparent in languages with different scripts. Therefore the\nlearned rules might indicate change of script more than real linguistic\ncorrespondences. This seems like a shortcoming that could be avoided by working\non the level of phonetic transcriptions.\n\nUnclear points\n- What is the \"optimal unigram for symbol usages in all rules\"? (line 286)\n- The merging done in the maximization step was not entirely clear to me. \n\nMinor issue\n- \"focus in on\" -> \"focus on\" (line 440)\n\nRefs\nJohann-Mattis List, Steven Moran. 2013. An Open Source Toolkit for Quantitative\nHistorical Linguistics. Proceedings of the 51st Annual Meeting of the\nAssociation for Computational Linguistics: System Demonstrations, pages\n13\u00e2\u0080\u009318, Sofia, Bulgaria. Association for Computational Linguistics.\nhttp://www.aclweb.org/anthology/P13-4003.  \nAndrew Kitchen, Christopher Ehret, Shiferaw Assefa and Connie J. Mulligan.\n2009. Bayesian phylogenetic analysis of Semitic languages identifies an Early\nBronze Age origin of Semitic in the Near East"
  },
  {
    "people": [
      "Cao",
      "Ziqiang",
      "Sujian Li",
      "Yang Liu",
      "Wenjie Li",
      "Nguyen",
      "Dat Quoc",
      "Richard Billingsley",
      "Lan Du",
      "Mark Johnson",
      "Shamanta",
      "Debakar",
      "Motahar Naim",
      "Parang Saraf",
      "Naren Ramakrishnan",
      "Shahriar Hossain",
      "Roder"
    ],
    "review": "This paper proposes a neural-styled topic model, extending the objective of\nword2vec to also learn document embeddings, which it then constrains through\nsparsification, hence mimicking the output of a topic model.\n\nI really liked the model that the authors proposed, and found the examples\npresented by the authors to be highly promising. What was really missing from\nthe paper, however, was any empirical evaluation of the model -- evaluation\nentirely falls back on tables of examples, without any indication of how\nrepresentative the examples are, or any attempt to directly compare with\nstandard or neural topic models. Without empirical evaluation, it is\nimpossible to get a sense of the true worth of the model, making it very hard\nto accept the paper. Some ideas of how the authors could have achieved this:\n(1) use the topic representation of each document in a supervised document\ncategorisation setup to compare against a topic model with the same topic\ncardinality (i.e. as an indirect evaluation of the quality of the\nrepresentation); or (2) through direct evaluation over a dataset with document\nsimilarity annotations (based on pairwise comparison over topic vectors).\n\nIt's fantastic that you are releasing code, but you have compromised anonymity\nin publishing the github link in the submitted version of the paper (strictly\nspeaking, this is sufficient for the paper to be rejected outright, but I\nleave that up to the PCs)\n\nOther issues:\n\n- how did you select the examples in Figures 3-6? presenting a subset of the\n  actual topics etc. potentially reeks of cherry picking.\n\n- in Section 2.2.1 you discuss the possibility of calculating word\n  representations for topics based on pairwise comparison with each word in\n  the vocabulary, but this is going to be an extremely expensive process for a\n  reasonable vocab size and number of topics; is this really feasible?\n\n- you say that you identify \"tokens\" using SpaCy in Section 3.1 -- how? You\n  extract noun chunks (but not any other chunk type), similarly to the Section\n  3.2, or something else? Given that you go on to say that you use word2vec\n  pre-trained embeddings (which include only small numbers of multiword\n  terms), it wasn't clear what you were doing here.\n\n- how does your model deal with OOV terms? Yes, in the experiments you report\n  in the paper you appear to train the model over the entire document\n  collection so it perhaps isn't an immediate problem, but there will be\n  contexts where you want to apply the trained model to novel documents, in\n  which case the updating of the word2vec token embeddings is going to mean\n  that any non-updated (OOV, relative to the training collection) word2vec\n  embeddings are not going to be directly comparable to the tuned embeddings.\n\n- the finding that 20 topics worked best over the 20 Newsgroups corpus wasn't\n  surprising given its composition. Possibly another (very simple) form of\n  evaluation here could have been based on some information-theoretic\n  comparison relative to the true document labels, where again you would have\n  been able to perform a direct comparison with LDA etc.\n\n- a couple of other neural topic models that you meed to compare yourself with\n  are:\n\nCao, Ziqiang, Sujian Li, Yang Liu, Wenjie Li, and Heng Ji. \"A Novel Neural\nTopic Model and Its Supervised Extension.\" In AAAI, pp. 2210-2216. 2015.\n\nNguyen, Dat Quoc, Richard Billingsley, Lan Du, and Mark Johnson. \"Improving\nTopic Models with Latent Feature Word Representations.\" Transactions of the\nAssociation for Computational Linguistics 3 (2015): 299-313.\n\nShamanta, Debakar, Sheikh Motahar Naim, Parang Saraf, Naren Ramakrishnan, and\nM. Shahriar Hossain. \"Concurrent Inference of Topic Models and Distributed\nVector Representations.\" In Machine Learning and Knowledge Discovery in\nDatabases, pp. 441-457. Springer International Publishing, 2015.\n\nLow-level things:\n\nline 315: \"it's similarity\" -> \"its similarity\"\n\nline 361: what does it mean for the \"topic basis\" to be affected (and the\n\"are\" is awkward here)\n\n- in the caption of Figure 5, the examples should perhaps be \"terms\" rather\n  than \"words\"\n\n- the reference formatting is all over the place, e.g. \"Advances in ...\",\n  \"Advances in Neural ...\", Roder et al. is missing the conference name, etc."
  },
  {
    "people": [
      "Pang",
      "Lee"
    ],
    "review": "This paper is about introducing eye-tracking features for sentiment analysis as\na type of cognitive feature.  I think that the idea of introducing eye-tracking\nfeatures as a proxy for cognitive load for sentiment analysis is an interesting\none.  \n\nI think the discussion on the features and comparison of feature sets is clear\nand very helpful.  I also like that the feasibility of the approach is\naddressed in section 7.\n\nI wonder if it would help the evaluation if the datasets didn't conflate\ndifferent domains, e.g., the movie review corpus and the tweet corpus.             \nFor one\nit might improve the prediction of movie review (resp. tweets) if the tweets\n(resp. movie reviews) weren't in the training.              It would also make the\nresults\neasier to interpret.  The results in Table 2 would seem rather low compared to\nstate-of-the art results for the Pang and Lee data, but look much better if\ncompared to results for Twitter data.\n\nIn Section 3.3, there are no overlapping snippets in the training data and\ntesting data of datasets 1 and 2, right?  Even if they come from the same\nsources (e.g., Pang & Lee and Sentiment 140).\n\nMinor: some of the extra use of bold is distracting (or maybe it's just me);"
  },
  {
    "people": [
      "Winograd",
      "Raghunathan"
    ],
    "review": "The authors present a new version of the coreference task tailored to\nWikipedia. The task is to identify the coreference chain specifically\ncorresponding to the entity that the Wikipedia article is about.  The authors\nannotate 30 documents with all coreference chains, of which roughly 25% of the\nmentions refer to the \"main concept\" of the article. They then describe some\nsimple baselines and a basic classifier which outperforms these. Moreover, they\nintegrate their classifier into the Stanford (rule-based) coreference system\nand see substantial benefit over all state-of-the-art systems on Wikipedia.\n\nI think this paper proposes an interesting twist on coreference that makes good\nsense from an information extraction perspective, has the potential to somewhat\nrevitalize and shake up coreference research, and might bridge the gap in an\ninteresting way between coreference literature and entity linking literature. \nI am sometimes unimpressed by papers that dredge up a new task that standard\nsystems perform poorly on and then propose a tweak so that their system does\nbetter. However, in this case, the actual task itself is quite motivating to me\nand rather than the authors fishing for a new domain to run things in, it\nreally does feel like \"hey, wait, these standard systems perform poorly in a\nsetting that's actually pretty important.\"\n\nTHE TASK: Main concept resolution is an intriguing task from an IE perspective.\n I can imagine many times where documents revolve primarily around a particular\nentity (biographical documents, dossiers or briefings about a person or event,\nclinical records, etc.) and where the information we care about extracting is\nspecific to that entity. The standard coreference task has always had the issue\nof large numbers of mentions that would seemingly be pretty irrelevant for most\nIE problems (like generic mentions), and this task is unquestionably composed\nof mentions that actually do matter.\n\nFrom a methodology standpoint, the notion of a \"main concept\" provides a bit of\na discourse anchor that is useful for coreference, but there appears to still\nbe substantial overhead to improve beyond the baselines, particularly on\nnon-pronominal mentions. Doing coreference directly on Wikipedia also opens the\ndoors for more interesting use of knowledge, which the authors illustrate here.\nSo I think this domain is likely to be an interesting testbed for ideas which\nwould improve coreference overall, but which in the general setting would be\nmore difficult to get robust improvements with and which would be dwarfed by\nthe amount of work dealing with other aspects of the problem.\n\nMoreover, unlike past work which has carved off a slice of coreference (e.g.\nthe Winograd schema work), this paper makes a big impact on the metrics of the\n*overall* coreference problem on a domain (Wikipedia) that many in the ACL\ncommunity are pretty interested in.\n\nTHE TECHNIQUES: Overall, the techniques are not the strong point of this paper,\nthough they do seem to be effective. The features seem pretty sensible, but it\nseems like additional conjunctions of these may help (and it's unclear whether\nthe authors did any experimentation in this vein).  The authors should also\nstate earlier in the work that their primary MC resolution system is a binary\nclassifier; this is not explicitly stated early enough and the model is left\nundefined throughout the description of featurization.\n\nMINOR DETAILS:\n\nOrganization: I would perhaps introduce the dataset immediately after \"Related\nWorks\" (i.e. have it be the new Section 3) so that concrete results can be\ngiven in \"Baselines\", further motivating \"Approach\".\n\nWhen Section 4 refers to Dcoref and Scoref, you should cite the Stanford papers\nor make it clear that it's the Stanford coreference system (many will be\nunfamiliar with the Dcoref/Scoref names).\n\nThe use of the term \"candidate list\" was unclear, especially in the following:\n\n\"We leverage the hyperlink structure of the article in order to enrich the list\nof mentions with shallow semantic attributes. For each link found within the\narticle under consideration, we look through the candidate list for all\nmentions that match the surface string of the link.\"\n\nPlease make it clear that the \"candidate list\" is the set of mentions in the\narticle that are possible candidates for being coreferent with the MC.        I think\nmost readers will understand that this module is supposed to import semantic\ninformation from the link structure of Wikipedia (e.g. if a mention is\nhyperlinked to an article that is female in Freebase, that mention is female),\nso try to keep the terminology clear.\n\nSection 6.1 says \"we consider the union of WCR mentions and all mentions\npredicted by the method described in (Raghunathan et al., 2010).\" However,\nSection 4.1 implies that these are the same? I'm missing where additional WCR\nmentions would be extracted."
  },
  {
    "people": [
      "Kazama",
      "Torisawa",
      "Tackstrom",
      "Tsai",
      "Roth"
    ],
    "review": "This paper is concerned with cross-lingual direct transfer of NER models using\na very recent cross-lingual wikification model. In general, the key idea is not\nhighly innovative and creative, as it does not really propose any core new\ntechnology. The contribution is mostly incremental, and marries the two\nresearch paths: (1) direct transfer for downstream NLP tasks (such as NER,\nparsing, or POS tagging), and (2) very recent developments in the cross-lingual\nwikification technology. However, I pretty much liked the paper, as it is built\non a coherent and clear story with enough experiments and empirical evidence to\nsupport its claims, with convincing results. I still have several comments\nconcerning the presentation of the work.\n\nRelated work: a more detailed description in related work on how this paper\nrelates to work of Kazama and Torisawa (2007) is needed. It is also required to\nstate a clear difference with other related NER system that in one way or\nanother relied on the encyclopaedic Wikipedia knowledge. The differences are\nindeed given in the text, but they have to be further stressed to facilitate\nreading and placing the work in context. \n\nAlthough the authors argue why they decided to leave out POS tags as features,\nit would still be interesting to report experiments with POS tags features\nsimilar to Tackstrom et al.: the reader might get an overview supported by\nempirical evidence regarding the usefulness (or its lack) of such features for\ndifferent languages (i.e., for the languages for which universal POS are\navailable at least). \n\nSection 3.3 could contribute from a running example, as I am still not exactly\nsure how the edited model from Tsai and Roth works now (i.e., the given\ndescription is not entirely clear).\n\nSince the authors mention several times that the approaches from Tackstrom et\nal. (2012) and Nothman et al. (2012) are orthogonal to theirs and that they can\nbe combined with the proposed approach, it would be beneficial if they simply\nreported some preliminary results on a selection of languages using the\ncombination of the models. It will add more flavour to the discussion. Along\nthe same line, although I do acknowledge that this is also orthogonal approach,\nwhy not comparing with a strong projection baseline, again to put the results\ninto more even more context, and show the usefulness (or limitations) of\nwikification-based approaches.\n\nWhy is Dutch the best training language for Spanish, and Spanish the best\nlanguage for Yoruba? Only a statistical coincidence or something more\ninteresting is going on there? A paragraph or two discussing these results in\nmore depth would be quite interesting.\n\nAlthough the idea is sound, the results from Table 4 are not that convincing\nwith only small improvements detected (and not in all scenarios). A statistical\nsignificance test reported for the results from Table 4 could help support the\nclaims.\n\nMinor comments:\n\n- Sect. 2.1: Projection can also be performed via methods that do not require\nparallel data, which makes such models more widely applicable (even for\nlanguages that do not have any parallel resources): e.g., see the work of\nPeirsman and Pado (NAACL 2009) or Vulic and Moens (EMNLP 2013) which exploit\nbilingual semantic spaces instead of direct alignment links to perform the\ntransfer.\n\n- Several typos detected in the text, so the paper should gain quite a bit from\na more careful proofreading (e.g., first sentence of Section 3: \"as a the base\nmodel\"; This sentence is not 'parsable', Page 3: \"They avoid the traditional\npipeline of NER then EL by...\", \"to disambiguate every n-grams\" on Page 8)"
  },
  {
    "people": [
      "Johannsen",
      "Agic"
    ],
    "review": "I reviewed this paper earlier, when it was an ACL 2016 short paper draft. At\nthat point, it had a flaw in the experiment setup, which is now corrected.\n\nSince back then I suggested I'd be willing to accept the draft for another *ACL\nevent provided that the flaw is corrected, I now see no obstacles in doing so.\n\nAnother reviewer did point out that the setup of the paper is somewhat\nartificial if we focus on real low-resource languages, relating to the costs of\n*finding* vs. *paying* the annotators. I believe this should be exposed in the\nwriteup not to oversell the method.\n\nThere are relevant lines of work in annotation projection for extremely\nlow-resource languages, e.g., Johannsen et al. (2016, ACL) and Agic et al.\n(2015, ACL). It would be nice to reflect on those in the related work\ndiscussion for completeness.\n\nIn summary, I think this is a nice contribution, and I vote accept.\n\nIt should be indicated whether the data is made available. I evaluate those\nparts in good faith now, presuming public availability of research."
  },
  {
    "people": [
      "Bahdanau",
      "Le"
    ],
    "review": "This paper investigates three simple weight-pruning techniques for NMT, and\nshows that pruning weights based on magnitude works best, and that retraining\nafter pruning can recover original performance, even with fairly severe\npruning.\n\nThe main strength of paper is that the technique is very straightforward and\nthe results are good. It\u00e2\u0080\u0099s also clearly written and does a nice job covering\nprevious work.\n\nA weakness is that the work isn\u00e2\u0080\u0099t very novel, being just an application of a\nknown technique to a new kind of neural net and application (namely NMT), with\nresults that aren\u00e2\u0080\u0099t very surprising. \n\nIt\u00e2\u0080\u0099s not clear to me what practical significance these results have, since to\ntake advantage of them you would need sparse matrix representations, which are\ntrickier to get working fast on a GPU - and after all, speed is the main\nproblem with NMT, not space. (There may be new work that changes this picture,\nsince the field is evolving fast, but if so you need to describe it, and\ngenerally do a better job explaining why we should care about pruning.)\n\nA suggestion for dealing with the above weakness would be to use the pruning\nresults to inform architecture changes. For instance, figure 3 suggests that\nyou might be able to reduce the number of hidden layers to two, and also\npotentially reduce the dimension of source and target embeddings.\n\nAnother suggestion is that you try to make a link between pruning+retraining\nand dropout (eg \u00e2\u0080\u009cA Theoretically Grounded Application of Dropout in Recurrent\nNeural Networks\u00e2\u0080\u009d, Gal, arXiv 2016).\n\nDetailed comments:\n\nLine 111: \u00e2\u0080\u009csoftmax weights\u00e2\u0080\u009d - \u00e2\u0080\u009coutput embeddings\u00e2\u0080\u009d may be a preferable\nterm\n\nS3.2: It\u00e2\u0080\u0099s misleading to call n the \u00e2\u0080\u009cdimension\u00e2\u0080\u009d of the network, and\nspecify all parameter sizes as integer multiples of this number as if this were\na logical constraint.\n\nLine 319: You should cite Bahdanau et al here for the attention idea, rather\nthan Luong for their use of it.\n\nS3.3: Class-uniform and class-distribution seem very similar (and naturally get\nvery similar results); consider dropping one or the other.\n\nFigure 3 suggestion that you could hybridize pruning: use class-blind for most\nclasses, but class-uniform for the embeddings.\n\nFigure 4 should show perplexity too.\n\nWhat pruning is used in section 4.2 & figure 6?\n\nFigure 7: does loss pertain to training or test corpora?\n\nFigure 8: This seems to be missing softmax weights. I found this diagram\nsomewhat hard to interpret; it might be better to give relevant statistics,\nsuch as the proportion of each class that is removed by class-blind pruning at\nvarious levels.\n\nLine 762: You might want to cite Le et al, \u00e2\u0080\u009cA Simple Way to Initialize\nRecurrent Networks of Rectified Linear Units\u00e2\u0080\u009d, arXiv 2015."
  },
  {
    "people": [
      "Martin",
      "Foster",
      "Kuhn",
      "Zens",
      "Stanton"
    ],
    "review": "This paper applies the idea of translation model pruning to neural MT. The\nauthors explore three simple threshold and histogram pruning schemes, two of\nwhich are applied separately to each weight class, while the third is applied\nto the entire model. The authors also show that retraining the models produces\nperformance equal to the full model, even when 90% of the weights are pruned.\nAn extensive analysis explains the superiority of the class-blind pruning\nscheme, as well as the performance boost through retraining. \n\nWhile the main idea of the paper is simple, it seems quite useful for\nmemory-restricted applications of NMT. I particularly liked the analysis\nsection which gives further insight into the model components that are usually\ntreated like black boxes. While these insights are interesting by themselves,\nthe paper's main motivation is model compression. This argument would be\nstronger if the paper included some numbers on actual memory consumption of the\ncompressed model in comparison to the uncompressed model.     \n\nSome minor remarks:\n- There is a substantial amount of work on pruning translation models in\nphrase-based SMT, which could be referenced in related work, e.g. \nJohnson, J., Martin, J., Foster, G. and Kuhn, R.: Improving Translation Quality\nby Discarding Most of the Phrasetable. EMNLP 07 or\nZens, R., Stanton, D. and Peng X.: A Systematic Comparison of Phrase Table\nPruning Techniques. EMNLP 12\n\n- It took me a while to understand Figure 5. I would find it more informative\nto add an additional barplot under figure 4 showing highest discarded weight\nmagnitude by class. This would also allow a comparison across all pruning\nmethods."
  },
  {
    "people": [
      "Almeda",
      "Martins",
      "Ribeyre",
      "Martins"
    ],
    "review": "This paper presents a transition-based graph parser able to cope with the rich\nrepresentations of a semantico-cognitive annotation scheme, instantiated in the\nUCCA corpora. The authors start first by exposing what, according to them,\nshould cover a semantic-based annotation scheme: (i) being graph-based\n(possibility for a token/node of having multiple governors) (2) having\nnon-terminal nodes (representing complex structures \u00e2\u0080\u0093 syntactic -: coordinate\nphrases, lexical: multiword expression) and (3) allowing discontinuous elements\n(eg. Verbs+particules). Interestingly, none of these principles is tied to a\nsemantic framework, they could also work for syntax or other representation\nlayers. The authors quickly position their work by first introducing the larger\ncontext of broad-coverage semantic parsing then their annotation scheme of\nchoice (UCCA).              They then present 3 sets of parsing experiments: (i) one\ndevoted to phrase-based parsing using the Stanford parser and an UCCA to\nconstituency conversion, (ii) one devoted to dependency parsing using an UCCA\nto dependency conversion and finally (iii) the core of their proposal, a  set\nof experiments showing that their transition-based graph parser is suitable for\ndirect parsing of UCCA graphs.\n\nI found this work interesting but before considering a publication, I have\nseveral concerns with regards to the methodology and the empirical\njustifications:\n\nThe authors claimed that there are the first to propose a parser for a\nsemantically-oriented scheme such as theirs. Of course, they are. But with all\ndue respect to the work behind this scheme, it is made of graphs with a various\nlevel of under-specified structural arguments and semantically oriented label\n(Process, state) and nothing in their transition sets treats the specificities\nof such a graph. Even the transitions related to the remote edges could have\nbeen handled by the other ones assuming a difference in the label set itself\n(like adding an affix for example). If we restrict the problem to graph\nparsing, many works post the 2014-2015 semeval shared tasks (Almeda and\nMartins, 2014,2015 ; Ribeyre et al, 2014-2015) proposed an extension to\ntransition-based graph parser or an adaptation of a higher-model one, and\nnothing precludes their use on this data set.  It\u00e2\u0080\u0099s mostly the use of a\nspecific feature template that anchors this model to this scheme (even though\nit\u00e2\u0080\u0099s less influencial than the count features and the unigram one). Anyway,\nbecause the above-mentioned graph-parsers are available [1,2] I don\u00e2\u0080\u0099t\nunderstand why they couldn\u00e2\u0080\u0099t be used as a baseline or source of comparisons.\nRegarding the phrase-based  experiments using uparse, it could have been also\nvalidated by another parser from Fernandez-Gonzales and Martins (2015) which\ncan produce LCFRS-like parsing as good as Uparse (ref missing when you first\nintroduced uparse).  \n\nBecause this scheme supports a more abstract view of syntaxico-semantic\nstructures than most of the SDP treebanks, it would have been important to use\nthe same metrics as in the related shared task. At this point in the field,\nmany systems, models and data set are competing and I think that the lack of\ncomparison points with other models and parsers is detrimental to this work as\nwhole. Yet I found it interesting and because we\u00e2\u0080\u0099re at crossing time in term\nof where to go next, I think that this paper should be discussed at a\nconference such as ConLL.\n\nNote in random order\n-         please introduce the \u00e2\u0080\u009cgrounded semantic\u00e2\u0080\u009d before page 2, you use\nthat phrase before\n-         why haven\u00e2\u0080\u0099t you try to stick to constituent-tree with rich node\nlabels and propagater traces and then train/parse with the Berkeley parser? It\ncould have been a good baseline. \n-         The conversion to surface dependency trees is in my mind useless: you\nloose too many information, here a  richer conversion such as the one from\n\u00e2\u0080\u0098Schluter et al, 2014, Semeval SDP) should have been used.\n-         Can you expand on \u00e2\u0080\u009cUCCA graphs may contains implicit unit that have\nno correspondent in the text\u00e2\u0080\u009d  or provide a ref or an example.\n-         You mentioned other representations such as MRS and DRT, this raises\nthe fact that your scheme doesn\u00e2\u0080\u0099t seem to allow for a modelling of quantifier\nscope information. It\u00e2\u0080\u0099s thus fully comparable to other more syntax-oriented\nscheme. It\u00e2\u0080\u0099s indeed more abstract than DM for example and probably more\nunderspecified than the semantic level of the PCEDT but how much? How really\ninformative is this scheme and how really \u00e2\u0080\u009cparsable\u00e2\u0080\u009d is it? According to\nyour scores, it seems \u00e2\u0080\u009charder\u00e2\u0080\u009d but an  error analysis would have been\nuseful.\n- As I said before, the 3 principles you devised could apply to a lot of\nthings,  they look a bit ad-hoc to me and would probably need to take place in\na much wider (and a bit clearer) introduction. What are you trying to argue\nfor: a parser that can parse UCCA? a model suitable for semantic analysis ? or\na semantic oriented scheme that can actually be parsable?  you're trying to say\nall of those in a very dense way and it's borderline to be be confusing.\n\n[1] http://www.corentinribeyre.fr/projects/view/DAGParser\n[2] https://github.com/andre-martins/TurboParser and\nhttps://github.com/andre-martins/TurboParser/tree/master/semeval2014_data"
  },
  {
    "people": [
      "Agic"
    ],
    "review": "The paper presents the first broad-coverage semantic parsers for UCCA, one\nspecific approach to graph-based semantic representations. Unlike CoNLL\nsemantic dependency graphs, UCCA graphs can contain \"nonterminal\" nodes which\ndo not represent words in the string. Unlike AMRs, UCCA graphs are \"grounded\",\nwhich the authors take to mean that the text tokens appear as nodes in the\nsemantic representation. The authors present a number of parsing methods,\nincluding a transition-based parser that directly constructs UCCA parses, and\nevaluate them.\n\nGiven that UCCA and UCCA-annotated data exist, it seems reasonable to develop a\nsemantic parser for UCCA. However, the introduction and background section hit\na wrong note to my ear, in that they seem to argue that UCCA is the _only_\ngraph-based semantic representation (SR) formalism that makes sense to be\nstudied. This doesn't work for me, and also seems unnecessary -- a good UCCA\nparser could be a nice contribution by itself.\n\nI do not entirely agree with the three criteria for semantic representation\nformalisms the authors lay out in the introduction. For instance, it is not\nclear to me that \"nonterminal nodes\" contribute any expressive capacity. Sure,\nit can be inconvenient to have to decide which word is the head of a\ncoordinated structure, but exactly what information is it that could only be\nrepresented with a nonterminal and not e.g. with more informative edge labels?\nAlso, the question of discontinuity does not even arise in SRs that are not\n\"grounded\". The advantages of \"grounded\" representations over AMR-style ones\ndid not become clear to me. I also think that the word \"grounded\" has been used\nfor enough different concepts in semantics in the past ten years, and would\nencourage the authors to find a different one (\"anchored\"? \"lexicalized\"?).\nThus I feel that the entire introductory part of the paper should be phrased\nand argued much more carefully.\n\nThe parser itself seems fine, although I did not check the details. However, I\ndid not find the evaluation results very impressive. On the \"primary\" edges,\neven a straightforward MaltParser outperforms the BSP parser presented here,\nand the f-scores on the \"remote\" edges (which a dependency-tree parser like\nMalt cannot compute directly) are not very high either. Furthermore, the\nconversion of dependency graphs to dependency trees has been studied quite a\nbit under the name \"tree approximations\" in the context of the CoNLL 2014 and\n2015 shared tasks on semantic dependency parsing (albeit without \"nonterminal\"\nnodes). Several authors have proposed methods for reconstructing the edges that\nwere deleted in the graph-to-tree conversion; for instance, Agic et al. (2015),\n\"Semantic dependency graph parsing using tree approximations\" discuss the\nissues involved in this reconstruction in detail. By incorporating such\nmethods, it is likely that the f-score of the MaltParser (and the LSTM-based\nMaltParser!) could be improved further, and the strength of the BSP parser\nbecomes even less clear to me."
  },
  {
    "people": [
      "Reddy",
      "Reddy"
    ],
    "review": "General comments\n=============================\nThe paper reports experiments on predicting the level of compositionality of\ncompounds in English. \nThe dataset used is a previously existing set of 90 compounds, whose\ncompositionality was ranked from 1 to 5\n(by a non specified number of judges).\nThe general form of each experiment is to compute a cosine similarity between\nthe vector of the compound (treated as one token) and a composition of the\nvectors of the components.\nEvaluation is performed using a Spearman correlation between the cosine\nsimilarity and the human judgments.\n\nThe experiments vary\n- for the vectors used: neural embeddings versus syntactic-context count\nvectors\n- and for the latter case, whether plain or \"aligned\" vectors should be used,\nfor the dependent component of the compound. The alignment tries to capture a\nshift from the dependent to the head. Alignment were proposed in a previous\nsuppressed reference.\n\nThe results indicate that syntactic-context count vectors outperform\nembeddings, and the use of aligned alone performs less well than non-modified\nvectors, and a highly-tuned combination of aligned and unaligned vectors\nprovides a slight improvement.\n\nRegarding the form of the paper, I found the introduction quite well written,\nbut other parts (like section 5.1) are difficult to read, although the\nunderlying notions are not very complicated. Rephrasing with running examples\ncould help.\n\nRegarding the substance, I have several concerns:\n\n- the innovation with respect to Reddy et al. seems to be the use of the\naligned vectors\nbut they have been published in a previous \"suppressed reference\" by the\nauthors.\n\n- the dataset is small, and not enough described. In particular, ranges of\nfrequences are quite likely to impact the results. \nSince the improvements using aligned vectors are marginal, over a small\ndataset, in which it is unclear how the choice of the compounds was performed,\nI find that the findings in the paper are quite fragile.\n\nMore detailed comments/questions\n================================\n\nSection 3\n\nI don't understand the need for the new name \"packed anchored tree\".\nIt seems to me a plain extraction of the paths between two lexical items in a\ndependency tree,\nnamely a plain extension of what is traditionally done in syntactic\ndistributional representations of words\n(which typically (as far as Lin 98) use paths of length one, or length 2, with\ncollapsed prepositions).\n\nFurther, why is it called a tree? what are \"elementary APTs\" (section 5.1) ?\n\nTable 2 : didn't you forget to mention that you discard features of order more\nthan 3 \n(and that's why for instance NMOD.overline(NSUBJ).DOBJ does not appear in\nleftmost bottom cell of table 2\nOr does it have to do with the elimination of some incompatible types you\nmention\n(for which an example should be provided, I did not find it very clear).\n\nSection 4:\n\nSince the Reddy et al. dataset is central to your work, it seems necessary to\nexplain how the 90 compounds were selected. What are the frequency ranges of\nthe compounds / the components etc... ? There is a lot of chance that results\nvary depending on the frequency ranges.\n\nHow many judgments were provided for a given compound? Are there many compounds\nwith same final compositionality score? Isn't it a problem when ranking them to\ncompute the Spearman correlation ?\n\nApparently you use \"constituent\" for a component of the N N sequence. I would\nsuggest \"component\", as \"constituent\" also has the sense of \"phrase\" (syntagm).\n\n\"... the intuition that if a constituent is used literally within a phrase then\nit is highly likely that the compound and the constituent share co-occurrences\"\n: note the intuition is certainly true if the constituent is the head of the\nphrase, otherwise much less true (e.g. \"spelling bee\" does not have the\ndistribution of \"spelling\").\n\nSection 5\n\n\"Note that the elementary representation for the constituent of a compound\nphrase will not contain any of the contextual features associated with the\ncompound phrase token unless they occurred with the constituent in some other\ncontext. \"\nPlease provide a running example in order to help the reader follow which\nobject you're talking about.\nDoes \"compound phrase token\" refer to the merged components of the compound?\n\nSection 5.1\n\nI guess that \"elementary APTs\" are a triplet target word w + dependency path r\n+ other word w'?\nI find the name confusing.\n\nClarify whether \"shifted PMI\" refer to PMI as defined in equation (3).\n\n\"Removing features which tend to go with lots of\n things (low positive PMI) means that these phrases\n appear to have been observed in a very small num-\n ber of (highly informative) contexts.\"\nDo \"these phrases\" co-refer with \"things\" here?\nThe whole sentence seems contradictory, please clarify.\n\n\"In general, we would expect there to be little 558\noverlap between APTs which have not been prop-\nerly aligned.\"\nWhat does \"not properly aligned\" means? You mean not aligned at all?\n\nI don't understand paragraph 558 to 563.\nWhy should the potential overlap be considerable\nin the particular case of the NMOD relation between the two components?\n\nParagraph 575 to 580 is quite puzzling.\nWhy does the whole paper make use of higher order dependency features\nand then suddenly, at the critical point of actually measuring the crucial\nmetric\nof similarity between composed and observed phrasal vectors, you use\nfirst order features only?\n\nNote 3 is supposed to provide an answer, but I don't understand the explanation\nof why the 2nd order paths in the composed representations are not reliable,\nplease clarify.\n\nSection 6\n\n\"Smoothing the PPMI calculation with a value of \u00ce\u00b1 = 0.75 generally has a 663\nsmall positive effect.\"\ndoes not seem so obvious from table 3.\n\nWhat are the optimal values for h and q in equation 8 and 9? They are important\nin order to estimate\nhow much of \"hybridity\" provides the slight gains with respect to the unaligned\nresults.\n\nIt seems that in table 4 results correspond to using the add combination, it\ncould help to have this in the legend.\nAlso, couldn't you provide the results from the word2vec vectors for the\ncompound phrases?\n\nI don't understand the intuition behind the FREQ baseline. Why would a frequent\ncompound tend to be compositional? This suggests maybe a bias in the dataset."
  },
  {
    "people": [
      "Sogaard"
    ],
    "review": "This paper describes a new deterministic dependency parsing algorithm and\nanalyses its behaviour across a range of languages.\nThe core of the algorithm is a set of rules defining permitted dependencies\nbased on POS tags.\nThe algorithm starts by ranking words using a slightly biased PageRank over a\ngraph with edges defined by the permitted dependencies.\nStepping through the ranking, each word is linked to the closest word that will\nmaintain a tree and is permitted by the head rules and a directionality\nconstraint.\n\nOverall, the paper is interesting and clearly presented, though seems to differ\nonly slightly from Sogaard (2012), \"Unsupervised Dependency Parsing without\nTraining\".\nI have a few questions and suggestions:\n\nHead Rules (Table 1) - It would be good to have some analysis of these rules in\nrelation to the corpus.\nFor example, in section 3.1 the fact that they do not always lead to a\nconnected graph is mentioned, but not how frequently it occurs, or how large\nthe components typically are.\n\nI was surprised that head direction was chosen using the test data rather than\ntraining or development data.\nGiven how fast the decision converges (10-15 sentences), this is not a major\nissue, but a surprising choice.\n\nHow does tie-breaking for words with the same PageRank score work?\nDoes it impact performance significantly, or are ties rare enough that it\ndoesn't have an impact?\n\nThe various types of constraints (head rules, directionality, distance) will\nlead to upper bounds on possible performance of the system.\nIt would be informative to include oracle results for each constraint, to show\nhow much they hurt the maximum possible score.\nThat would be particularly helpful for guiding future work in terms of where to\ntry to modify this system.\n\nMinor:\n\n- 4.1, \"we obtain [the] rank\"\n\n- Table 5 and Table 7 have columns in different orders. I found the Table 7\narrangement clearer.\n\n- 6.1, \"isolate the [contribution] of both\""
  },
  {
    "people": [
      "S\u00c3\u00b8gaard"
    ],
    "review": "This paper presents a way to parse trees (namely the universal dependency\ntreebanks) by relying only on POS and by using a modified version of the\nPageRank to give more way to some meaningful words (as opposed to stop words).\n\nThis idea is interesting though very closed to what was done in S\u00c3\u00b8gaard\n(2012)'s paper. The personalization factor giving more weight to the main\npredicate is nice but it would have been better to take it to the next level.\nAs far as I can tell, the personalization is solely used for the main predicate\nand its weight of 5 seems arbitrary.\n\nRegarding the evaluation and the detailed analyses, some charts would have been\nbeneficial, because it is sometimes hard to get the gist out of the tables.\nFinally, it would have been interesting to get the scores of the POS tagging in\nthe prediction mode to be able to see if the degradation in parsing performance\nis heavily correlated to the degradation in tagging performance (which is what\nwe expect).\n\nAll in all, the paper is interesting but the increment over the work of\nS\u00c3\u00b8gaard (2012) is small.\n\nSmaller issues:\n-------------------\n\nl. 207 : The the main idea -> The main idea"
  },
  {
    "people": [
      "Wiseman",
      "Sam Wiseman",
      "Alexander M. Rush",
      "Jason Weston",
      "Stuart M.\nShieber"
    ],
    "review": "This paper models event linking using CNNs. Given event mentions, the authors\ngenerate vector representations based on word embeddings passed through a CNN\nand followed by max-pooling. They also concatenate the resulting\nrepresentations with several word embeddings around the mention. Together with\ncertain pairwise features, they produce a vector of similarities using a\nsingle-layer neural network, and compute a coreference score. \nThe model is tested on an ACE dataset and an expanded version with performance\ncomparable to previous feature-rich systems.\nThe main contribution of the paper, in my opinion, is in developing a neural\napproach for entity linking that combines word embeddings with several\nlinguistic features. It is interesting to find out that just using the word\nembeddings is not sufficient for good performance. Fortunately, the linguistic\nfeatures used are limited and do not require manually-crafted external\nresources.  \n\nExperimental setting\n- It appears that gold trigger words are used rather than predicted ones. The\nauthors make an argument why this is reasonable, although I still would have\nliked to see performance with predicted triggers. This is especially\nproblematic as one of the competitor systems used predicted triggers, so the\ncomparison isn't fair. \n- The fact that different papers use different train/test splits is worrisome.\nI would encourage the authors to stick to previous splits as much as possible. \n\nUnclear points\n- The numbers indicating that cross-sentential information is needed are\nconvincing. However, the last statement in the second paragraph (lines 65-70)\nwas not clear to me.\n- Embeddings for positions are said to be generaties \"in a way similar to word\nembeddings\". How exactly? Are they randomly initialized? Are they lexicalized?\nIt is not clear to me why a relative position next to one word should have the\nsame embedding as a relative position next to a different word.\n- How exactly are left vs right neighbors used to create the representation\n(lines 307-311)? Does this only affect the max-pooling operation?\n- The word embeddings of one word before and one word after the trigger words\nare appended to it. This seems a bit arbitrary. Why one word before and after\nand not some other choice?  \n- It is not clear how the event-mention representation v_e (line 330) is used?\nIn the following sections only v_{sent+lex} appear to be used, not v_e.\n- How are pairwise features used in section 3.2? Most features are binary, so I\nassume they are encoded as a binary vector, but what about the distance feature\nfor example? And, are these kept fixed during training?\n\nOther issues and suggestions\n- Can the approach be applied to entity coreference resolution as well? This\nwould allow comparing with more previous work and popular datasets like\nOntoNotes. \n- The use of a square function as nonlinearity is interesting. Is it novel? Do\nyou think it has applicability in other tasks?\n- Datasets: one dataset is publicly available, but results are also presented\nwith ACE++, which is not. Do you have plans to release it? It would help other\nresearchers compare new methods. At least, it would have been good to see a\ncomparison to the feature-rich systems also on this dataset.\n- Results: some of the numbers reported in the results are quite close.\nSignificance testing would help substantiating the comparisons.\n- Related work: among the work on (entity) coreference resolution, one might\nmention the neural network approach by Wiseman et al. (2015)  \n\nMinor issues\n- line 143, \"that\" is redundant. \n- One of the baselines is referred to as \"same type\" in table 6, but \"same\nevent\" in the text (line 670).        \n\nRefs\n- Learning Anaphoricity and Antecedent Ranking Features for Coreference\nResolution. Sam Wiseman, Alexander M. Rush, Jason Weston, and Stuart M.\nShieber. ACL 2015."
  }
]